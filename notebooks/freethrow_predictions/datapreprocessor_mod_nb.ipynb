{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    For SMOTENC to work properly:\n",
    "        check if there are ordinal or nominal categoricals and that there is numeric features\n",
    "        if there is categoricals (ordinal or nominal) then we go with SMOTENC, only categoricals go with SMOTEN, and numericals only goes into the numeric criteria for SMOTE or user defined option\n",
    "\n",
    "        If both categorical and numerical features in dataset then treat both ordinal and nominal features as ordinal beforehand ordinal encoding on all categoricals (nominal and ordinal) prior to SMOTENC (if ordinal or nominal categorical features available)\n",
    "        Before SMOTENC: Only apply Ordinal Encoding to categorical features.\n",
    "        Apply SMOTENC on the ordinal-encoded training data only\n",
    "        After SMOTENC: Apply One-Hot Encoding to the resampled dataset.\n",
    "\n",
    "    For easy inverse transform:\n",
    "        After SMOTENC, apply one-hot encoding on your training set (already resampled) to provide the training datasets\n",
    "        Then save the encoders/transformers/scalars/feature order needed\n",
    "        Reload encoders/transformers/scalars/feature order\n",
    "        Use the reloaded encoders/transformers/scalars/feature order your untouched test dataset (ordinal-encoded but not resampled).\n",
    "        Inverse transform by undoing the one hot encoding, then ordinal encoding\n",
    "        Reload encoders/transformers/scalars/feature order for predict mode\n",
    "        Use the reloaded encoders/transformers/scalars/feature order your untouched predict dataset (ordinal-encoded but not resampled).\n",
    "        Inverse transform by undoing the one hot encoding, then ordinal encoding\n",
    "\n",
    "Hence, the code snippet you shared does exactly that:\n",
    "\n",
    "    Ordinal → SMOTENC → One-Hot → Model.\n",
    "    To invert, do the steps in reverse: One-Hot → Ordinal.\n",
    "\n",
    "Reverse Transformation Order for Outputs:\n",
    "\n",
    "    Step 1: Undo one-hot encoding → get ordinal integers.\n",
    "    Step 2: Undo ordinal encoding → get original categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 13:44:05,380 [INFO] example_usage_class: 📥 Loading original dataset from ../../data/processed/final_ml_dataset.csv...\n",
      "2024-12-31 13:44:05,412 [INFO] example_usage_class: ✅ Original dataset loaded successfully.\n",
      "2024-12-31 13:44:05,413 [INFO] example_usage_class: 🔍 Selecting and filtering dataset based on defined features...\n",
      "2024-12-31 13:44:05,417 [INFO] example_usage_class: ✅ Selected features filtered successfully.\n",
      "2024-12-31 13:44:05,432 [INFO] example_usage_class: ✅ Features and metadata saved to ../../ml-preprocessing-utils/data/dataset/test/features_info/features_metadata.pkl\n",
      "2024-12-31 13:44:05,440 [INFO] example_usage_class: ✅ Features and metadata loaded from ../../ml-preprocessing-utils/data/dataset/test/features_info/features_metadata.pkl\n",
      "2024-12-31 13:44:05,442 [INFO] example_usage_class: 📥 Loading dataset from ../../data/processed/final_ml_dataset.csv...\n",
      "2024-12-31 13:44:05,455 [INFO] example_usage_class: ✅ Dataset loaded from ../../data/processed/final_ml_dataset.csv\n",
      "2024-12-31 13:44:05,455 [INFO] example_usage_class: ✅ Original dataset loaded successfully.\n",
      "2024-12-31 13:44:05,456 [INFO] example_usage_class: 🔍 Filtering dataset for selected features...\n",
      "2024-12-31 13:44:05,458 [INFO] example_usage_class: ✅ Dataset filtered based on selected features.\n",
      "2024-12-31 13:44:05,459 [INFO] example_usage_class: ✅ Dataset filtered successfully.\n",
      "2024-12-31 13:44:05,459 [INFO] example_usage_class: 📁 Separating columns into defined categories...\n",
      "2024-12-31 13:44:05,460 [INFO] example_usage_class: ✅ Columns separated into defined categories.\n",
      "2024-12-31 13:44:05,460 [INFO] example_usage_class: ✅ Features loaded and dataset filtered successfully.\n",
      "2024-12-31 13:44:05,461 [INFO] example_usage_class: ✅ Features loaded and dataset filtered successfully.\n",
      "2024-12-31 13:44:05,462 [INFO] example_usage_class: \n",
      "📊 Processed DataFrame (first 5 rows):\n",
      "2024-12-31 13:44:05,469 [INFO] example_usage_class:   player_estimated_hand_length_cm_category  release_ball_direction_x  \\\n",
      "0                                   Medium                  0.377012   \n",
      "1                                   Medium                  0.417644   \n",
      "2                                   Medium                  0.383086   \n",
      "3                                   Medium                  0.292054   \n",
      "4                                   Medium                  0.389917   \n",
      "\n",
      "   release_ball_direction_z  release_ball_direction_y  elbow_release_angle  \\\n",
      "0                  0.926203                  0.002969            72.325830   \n",
      "1                  0.901906                 -0.110176            58.430068   \n",
      "2                  0.922353                 -0.050096            73.883728   \n",
      "3                  0.956396                  0.003209            71.424835   \n",
      "4                  0.918894                 -0.059987            73.619348   \n",
      "\n",
      "   elbow_max_angle  wrist_release_angle  wrist_max_angle  knee_release_angle  \\\n",
      "0       106.272118            28.102765        35.918406           32.646276   \n",
      "1       101.798798            32.766626        38.693790           33.834026   \n",
      "2       106.989633            26.220943        38.998089           33.550293   \n",
      "3       104.160865            22.649881        36.353038           29.568453   \n",
      "4       103.832024            21.754498        34.532455           32.856306   \n",
      "\n",
      "   knee_max_angle  release_ball_speed  calculated_release_angle  \\\n",
      "0       63.541007            9.907618                 62.959206   \n",
      "1       65.565635           11.826803                 64.964999   \n",
      "2       65.257346           10.283314                 60.314694   \n",
      "3       64.652573            9.164302                 63.418903   \n",
      "4       64.165568           11.113489                 60.900817   \n",
      "\n",
      "   release_ball_velocity_x  release_ball_velocity_y  release_ball_velocity_z  \\\n",
      "0                 3.735294                 0.029412                 9.176471   \n",
      "1                 4.939394                -1.303030                10.666667   \n",
      "2                 3.939394                -0.515152                 9.484848   \n",
      "3                 2.676471                 0.029412                 8.764706   \n",
      "4                 4.333333                -0.666667                10.212121   \n",
      "\n",
      "   result  \n",
      "0       0  \n",
      "1       1  \n",
      "2       0  \n",
      "3       0  \n",
      "4       1  \n",
      "2024-12-31 13:44:05,470 [INFO] example_usage_class: \n",
      "📁 Separated Column Assets:\n",
      "2024-12-31 13:44:05,470 [INFO] example_usage_class: ordinal_categoricals: []\n",
      "2024-12-31 13:44:05,471 [INFO] example_usage_class: nominal_categoricals: ['player_estimated_hand_length_cm_category']\n",
      "2024-12-31 13:44:05,471 [INFO] example_usage_class: numericals: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z']\n",
      "2024-12-31 13:44:05,471 [INFO] example_usage_class: y_variable: ['result']\n"
     ]
    }
   ],
   "source": [
    "# feature_manager.py\n",
    "\n",
    "import logging\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from typing import Optional, List, Dict, Any, Tuple\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,  # Set to DEBUG for more verbose output\n",
    "    format='%(asctime)s [%(levelname)s] %(name)s: %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger('feature_manager')\n",
    "\n",
    "\n",
    "class FeatureManager:\n",
    "    def __init__(self, save_path: str = 'features_metadata.pkl'):\n",
    "        self.save_path = save_path\n",
    "    \n",
    "    def save_features(\n",
    "        self,\n",
    "        features_df: pd.DataFrame,\n",
    "        ordinal_categoricals: Optional[List[str]],\n",
    "        nominal_categoricals: Optional[List[str]],\n",
    "        numericals: Optional[List[str]],\n",
    "        y_variable: List[str],\n",
    "        dataset_csv_path: str\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Save selected features and their metadata to a pickle file.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if features_df.empty:\n",
    "                raise ValueError(\"features_df is empty. Cannot save empty features.\")\n",
    "            if not os.path.exists(dataset_csv_path):\n",
    "                raise FileNotFoundError(f\"Dataset CSV file does not exist at path: {dataset_csv_path}\")\n",
    "            \n",
    "            # Prepare metadata\n",
    "            data_to_save = {\n",
    "                'features': features_df.columns.tolist(),\n",
    "                'ordinal_categoricals': ordinal_categoricals or [],\n",
    "                'nominal_categoricals': nominal_categoricals or [],\n",
    "                'numericals': numericals or [],\n",
    "                'y_variable': y_variable,\n",
    "                'dataset_csv_path': dataset_csv_path\n",
    "            }\n",
    "            \n",
    "            # Ensure the directory exists\n",
    "            os.makedirs(os.path.dirname(self.save_path), exist_ok=True)\n",
    "            \n",
    "            # Save metadata to pickle\n",
    "            with open(self.save_path, 'wb') as f:\n",
    "                pickle.dump(data_to_save, f)\n",
    "            logger.info(f\"✅ Features and metadata saved to {self.save_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Failed to save features and metadata: {e}\")\n",
    "            raise  # Re-raise exception after logging\n",
    "    \n",
    "    def load_features_and_dataset(\n",
    "        self,\n",
    "        debug: bool = False\n",
    "    ) -> Tuple[pd.DataFrame, Dict[str, List[str]]]:\n",
    "        \"\"\"\n",
    "        Load features and metadata from a pickle file, then load and filter the original dataset based on the loaded features.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if not os.path.exists(self.save_path):\n",
    "                raise FileNotFoundError(f\"The file {self.save_path} does not exist.\")\n",
    "            \n",
    "            # Load metadata from pickle\n",
    "            with open(self.save_path, 'rb') as f:\n",
    "                loaded_data = pickle.load(f)\n",
    "            logger.info(f\"✅ Features and metadata loaded from {self.save_path}\")\n",
    "            \n",
    "            # Extract metadata\n",
    "            selected_features = loaded_data.get('features', [])\n",
    "            ordinal_categoricals = loaded_data.get('ordinal_categoricals', [])\n",
    "            nominal_categoricals = loaded_data.get('nominal_categoricals', [])\n",
    "            numericals = loaded_data.get('numericals', [])\n",
    "            y_variable = loaded_data.get('y_variable', [])\n",
    "            dataset_path = loaded_data.get('dataset_csv_path', '')\n",
    "            \n",
    "            logger.debug(f\"Dataset Path Retrieved: {dataset_path}\")\n",
    "            logger.debug(f\"Number of Features Selected: {len(selected_features)}\")\n",
    "            logger.debug(f\"Features Selected: {selected_features}\")\n",
    "            \n",
    "            # Validate dataset_path\n",
    "            if not dataset_path:\n",
    "                logger.error(\"Dataset path is empty in the loaded metadata.\")\n",
    "                raise ValueError(\"Dataset path is not provided in the loaded metadata.\")\n",
    "            \n",
    "            if not os.path.exists(dataset_path):\n",
    "                logger.error(f\"Dataset CSV file does not exist at path: {dataset_path}\")\n",
    "                raise FileNotFoundError(f\"Dataset CSV file not found at {dataset_path}\")\n",
    "            \n",
    "            # Load the original dataset\n",
    "            logger.info(f\"📥 Loading dataset from {dataset_path}...\")\n",
    "            original_df = load_base_data_for_dataset(dataset_path)\n",
    "            logger.info(\"✅ Original dataset loaded successfully.\")\n",
    "            \n",
    "            # Filter the dataset based on selected features\n",
    "            logger.info(\"🔍 Filtering dataset for selected features...\")\n",
    "            filtered_df = filter_base_data_for_select_features(\n",
    "                dataset=original_df,\n",
    "                feature_names=selected_features,\n",
    "                debug=debug\n",
    "            )\n",
    "            logger.info(\"✅ Dataset filtered successfully.\")\n",
    "            \n",
    "            # Separate column assets\n",
    "            logger.info(\"📁 Separating columns into defined categories...\")\n",
    "            column_assets = separate_column_assets(\n",
    "                feature_names=selected_features,\n",
    "                ordinal_categoricals=ordinal_categoricals,\n",
    "                nominal_categoricals=nominal_categoricals,\n",
    "                numericals=numericals,\n",
    "                y_variable=y_variable\n",
    "            )\n",
    "            logger.debug(f\"Column Assets Separated: {column_assets}\")\n",
    "            \n",
    "            logger.info(\"✅ Features loaded and dataset filtered successfully.\")\n",
    "            return filtered_df, column_assets\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Failed to load features and dataset: {e}\")\n",
    "            raise  # Re-raise exception after logging\n",
    "\n",
    "\n",
    "def load_base_data_for_dataset(dataset_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load the dataset from a CSV file.\n",
    "\n",
    "    Args:\n",
    "        dataset_path (str): Path to the dataset CSV file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Loaded dataset.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(dataset_path)\n",
    "        logger.info(f\"✅ Dataset loaded from {dataset_path}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Failed to load dataset from {dataset_path}: {e}\")\n",
    "        raise\n",
    "\n",
    "def filter_base_data_for_select_features(dataset: pd.DataFrame, feature_names: List[str], debug: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filter the dataset to include only the selected features.\n",
    "\n",
    "    Args:\n",
    "        dataset (pd.DataFrame): The original dataset.\n",
    "        feature_names (List[str]): List of feature names to retain.\n",
    "        debug (bool): Flag to enable debug logging.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Filtered dataset.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        filtered_df = dataset[feature_names].copy()\n",
    "        if debug:\n",
    "            logger.debug(f\"Filtered dataset shape: {filtered_df.shape}\")\n",
    "        logger.info(\"✅ Dataset filtered based on selected features.\")\n",
    "        return filtered_df\n",
    "    except KeyError as e:\n",
    "        logger.error(f\"❌ One or more features not found in the dataset: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Failed to filter dataset: {e}\")\n",
    "        raise\n",
    "\n",
    "def separate_column_assets(\n",
    "    feature_names: List[str],\n",
    "    ordinal_categoricals: List[str],\n",
    "    nominal_categoricals: List[str],\n",
    "    numericals: List[str],\n",
    "    y_variable: List[str]\n",
    ") -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Separate feature names into their respective categories.\n",
    "\n",
    "    Args:\n",
    "        feature_names (List[str]): List of feature names.\n",
    "        ordinal_categoricals (List[str]): List of ordinal categorical feature names.\n",
    "        nominal_categoricals (List[str]): List of nominal categorical feature names.\n",
    "        numericals (List[str]): List of numerical feature names.\n",
    "        y_variable (List[str]): List containing the target variable name.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, List[str]]: Dictionary containing separated feature categories.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        remaining_features = set(feature_names) - set(ordinal_categoricals) - set(nominal_categoricals) - set(numericals) - set(y_variable)\n",
    "        if remaining_features:\n",
    "            logger.warning(f\"⚠️ The following features were not categorized: {remaining_features}\")\n",
    "        \n",
    "        column_assets = {\n",
    "            'ordinal_categoricals': ordinal_categoricals,\n",
    "            'nominal_categoricals': nominal_categoricals,\n",
    "            'numericals': numericals,\n",
    "            'y_variable': y_variable\n",
    "        }\n",
    "        logger.info(\"✅ Columns separated into defined categories.\")\n",
    "        return column_assets\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Failed to separate column assets: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "\n",
    "# example of feature manager usages\n",
    "\n",
    "import logging\n",
    "import pandas as pd\n",
    "import os\n",
    "# from feature_manager import FeatureManager\n",
    "\n",
    "# Configure logging for this script\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,  # Set to DEBUG for more verbose output\n",
    "    format='%(asctime)s [%(levelname)s] %(name)s: %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger('example_usage_class')\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Initialize FeatureManager with the desired save_path\n",
    "        save_path = '../../ml-preprocessing-utils/data/dataset/test/features_info/features_metadata.pkl'  # Pickle file path\n",
    "        feature_manager = FeatureManager(save_path=save_path)\n",
    "        \n",
    "        start_dataset_path = '../../data/processed/final_ml_dataset.csv'  # Original dataset CSV path\n",
    "        \n",
    "        # Check if the original dataset exists\n",
    "        if not os.path.exists(start_dataset_path):\n",
    "            logger.error(f\"Original dataset does not exist at path: {start_dataset_path}\")\n",
    "            raise FileNotFoundError(f\"Original dataset not found at {start_dataset_path}\")\n",
    "        \n",
    "        # Load the original dataset\n",
    "        logger.info(f\"📥 Loading original dataset from {start_dataset_path}...\")\n",
    "        original_df = pd.read_csv(start_dataset_path)\n",
    "        logger.info(\"✅ Original dataset loaded successfully.\")\n",
    "        \n",
    "        # Define feature categories and column names\n",
    "        ordinal_categoricals = []\n",
    "        nominal_categoricals = ['player_estimated_hand_length_cm_category']\n",
    "        numericals = [\n",
    "            'release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y',\n",
    "            'elbow_release_angle', 'elbow_max_angle',\n",
    "            'wrist_release_angle', 'wrist_max_angle',\n",
    "            'knee_release_angle', 'knee_max_angle',\n",
    "             'release_ball_speed', 'calculated_release_angle',\n",
    "            'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z'\n",
    "        ]\n",
    "        y_variable = ['result']\n",
    "        final_keep_list = ordinal_categoricals + nominal_categoricals + numericals + y_variable\n",
    "        \n",
    "        # Verify that all columns in final_keep_list exist in the dataset\n",
    "        missing_columns = set(final_keep_list) - set(original_df.columns)\n",
    "        if missing_columns:\n",
    "            logger.error(f\"The following columns are missing in the dataset: {missing_columns}\")\n",
    "            raise ValueError(f\"Missing columns in the dataset: {missing_columns}\")\n",
    "        \n",
    "        # Apply the filter to keep only the selected columns\n",
    "        logger.info(\"🔍 Selecting and filtering dataset based on defined features...\")\n",
    "        selected_features_df = original_df[final_keep_list]\n",
    "        logger.info(\"✅ Selected features filtered successfully.\")\n",
    "        \n",
    "        # Save features and metadata\n",
    "        try:\n",
    "            feature_manager.save_features(\n",
    "                features_df=selected_features_df,\n",
    "                ordinal_categoricals=ordinal_categoricals,\n",
    "                nominal_categoricals=nominal_categoricals,\n",
    "                numericals=numericals,\n",
    "                y_variable=y_variable,\n",
    "                dataset_csv_path=start_dataset_path\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Failed to save features and metadata: {e}\")\n",
    "            raise\n",
    "        \n",
    "        # Load features and dataset\n",
    "        try:\n",
    "            filtered_df, column_assets = feature_manager.load_features_and_dataset(\n",
    "                debug=True  # Set to False to reduce verbosity\n",
    "            )\n",
    "            logger.info(\"✅ Features loaded and dataset filtered successfully.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Failed to load features and dataset: {e}\")\n",
    "            raise\n",
    "        \n",
    "        # Display processed data\n",
    "        logger.info(\"\\n📊 Processed DataFrame (first 5 rows):\")\n",
    "        logger.info(f\"{filtered_df.head()}\")\n",
    "        \n",
    "        # Display separated column assets\n",
    "        logger.info(\"\\n📁 Separated Column Assets:\")\n",
    "        for key, value in column_assets.items():\n",
    "            logger.info(f\"{key}: {value}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ An unexpected error occurred: {e}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clustering_module.py\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from typing import Optional, Dict\n",
    "import numpy as np\n",
    "\n",
    "class ClusteringModule:\n",
    "    def __init__(self, model_type: str = 'K-Means', model_params: Optional[Dict] = None, debug: bool = False):\n",
    "        \"\"\"\n",
    "        Initialize the ClusteringModule with the specified clustering algorithm.\n",
    "\n",
    "        Args:\n",
    "            model_type (str): Type of the clustering algorithm ('K-Means', 'DBSCAN', 'AgglomerativeClustering').\n",
    "            model_params (dict, optional): Parameters for the clustering algorithm.\n",
    "            debug (bool): Flag to enable detailed debugging.\n",
    "        \"\"\"\n",
    "        self.debug = debug\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        self.logger.setLevel(logging.DEBUG if self.debug else logging.INFO)\n",
    "        handler = logging.StreamHandler()\n",
    "        formatter = logging.Formatter('%(asctime)s [%(levelname)s] %(message)s')\n",
    "        handler.setFormatter(formatter)\n",
    "        if not self.logger.handlers:\n",
    "            self.logger.addHandler(handler)\n",
    "        \n",
    "        self.model_type = model_type\n",
    "        self.model_params = model_params or {}\n",
    "        self.model = self._initialize_model()\n",
    "        self.cluster_labels = None\n",
    "        self.evaluation_metrics = {}\n",
    "    \n",
    "    def _initialize_model(self):\n",
    "        \"\"\"\n",
    "        Initialize the clustering algorithm based on the specified type and parameters.\n",
    "\n",
    "        Returns:\n",
    "            Clustering algorithm instance.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if self.model_type == 'K-Means':\n",
    "                model = KMeans(**self.model_params)\n",
    "            elif self.model_type == 'DBSCAN':\n",
    "                model = DBSCAN(**self.model_params)\n",
    "            elif self.model_type == 'AgglomerativeClustering':\n",
    "                model = AgglomerativeClustering(**self.model_params)\n",
    "            else:\n",
    "                self.logger.error(f\"Unsupported clustering model type: {self.model_type}\")\n",
    "                raise ValueError(f\"Unsupported clustering model type: {self.model_type}\")\n",
    "            self.logger.debug(f\"Initialized {self.model_type} with parameters: {self.model_params}\")\n",
    "            return model\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error initializing clustering model: {e}\")\n",
    "            raise e\n",
    "    \n",
    "    def fit(self, X: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Fit the clustering model to the data.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): Preprocessed feature data.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(f\"Fitting {self.model_type} model...\")\n",
    "            self.model.fit(X)\n",
    "            self.cluster_labels = self.model.labels_\n",
    "            self.logger.info(f\"✅ {self.model_type} model fitted successfully.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Failed to fit {self.model_type} model: {e}\")\n",
    "            raise e\n",
    "    \n",
    "    def predict(self, X: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Predict cluster labels for new data.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): New preprocessed feature data.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Cluster labels.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(f\"Predicting clusters using {self.model_type} model...\")\n",
    "            labels = self.model.predict(X)\n",
    "            self.logger.info(\"✅ Clustering predictions made successfully.\")\n",
    "            return labels\n",
    "        except AttributeError:\n",
    "            self.logger.error(\"❌ The clustering model does not support prediction.\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Failed to predict clusters: {e}\")\n",
    "            raise e\n",
    "    \n",
    "    def evaluate(self, X: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Evaluate the clustering model using various metrics.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): Preprocessed feature data.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(f\"Evaluating {self.model_type} model...\")\n",
    "            self.evaluation_metrics['silhouette_score'] = silhouette_score(X, self.cluster_labels)\n",
    "            self.evaluation_metrics['calinski_harabasz_score'] = calinski_harabasz_score(X, self.cluster_labels)\n",
    "            self.evaluation_metrics['davies_bouldin_score'] = davies_bouldin_score(X, self.cluster_labels)\n",
    "            self.logger.info(\"✅ Clustering evaluation completed successfully.\")\n",
    "            self.logger.debug(f\"Silhouette Score: {self.evaluation_metrics['silhouette_score']:.4f}\")\n",
    "            self.logger.debug(f\"Calinski-Harabasz Score: {self.evaluation_metrics['calinski_harabasz_score']:.4f}\")\n",
    "            self.logger.debug(f\"Davies-Bouldin Score: {self.evaluation_metrics['davies_bouldin_score']:.4f}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Failed to evaluate clustering model: {e}\")\n",
    "            raise e\n",
    "    \n",
    "    def plot_clusters(self, X: pd.DataFrame, output_dir: str):\n",
    "        \"\"\"\n",
    "        Plot the clustering results (only for 2D data).\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): Preprocessed feature data.\n",
    "            output_dir (str): Directory to save the plot.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if X.shape[1] != 2:\n",
    "                self.logger.warning(\"Plotting is only supported for 2D data.\")\n",
    "                return\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            sns.scatterplot(x=X.iloc[:, 0], y=X.iloc[:, 1], hue=self.cluster_labels, palette='viridis')\n",
    "            plt.title(f\"{self.model_type} Clustering Results\")\n",
    "            plt.savefig(os.path.join(output_dir, f\"{self.model_type}_clusters.png\"))\n",
    "            plt.close()\n",
    "            self.logger.info(f\"✅ Clustering plot saved to '{output_dir}'.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Failed to plot clusters: {e}\")\n",
    "            raise e\n",
    "    \n",
    "    def save_model(self, filepath: str):\n",
    "        \"\"\"\n",
    "        Save the trained clustering model to disk.\n",
    "\n",
    "        Args:\n",
    "            filepath (str): Path to save the model.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            joblib.dump(self.model, filepath)\n",
    "            self.logger.info(f\"✅ Clustering model saved to '{filepath}'.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Failed to save clustering model: {e}\")\n",
    "            raise e\n",
    "    \n",
    "    def load_model(self, filepath: str):\n",
    "        \"\"\"\n",
    "        Load a trained clustering model from disk.\n",
    "\n",
    "        Args:\n",
    "            filepath (str): Path to load the model from.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.model = joblib.load(filepath)\n",
    "            self.logger.info(f\"✅ Clustering model loaded from '{filepath}'.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Failed to load clustering model: {e}\")\n",
    "            raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:47,980 [INFO] main_preprocessing: ---\n",
      "Processing Model: Tree Based Classifier\n",
      "---\n",
      "2024-12-31 14:26:47,987 [INFO] __main__: ✅ Features and metadata loaded from ../../ml-preprocessing-utils/data/dataset/test/features_info/features_metadata.pkl\n",
      "2024-12-31 14:26:47,989 [INFO] __main__: 📥 Loading dataset from ../../data/processed/final_ml_dataset.csv...\n",
      "2024-12-31 14:26:47,996 [INFO] __main__: ✅ Dataset loaded from ../../data/processed/final_ml_dataset.csv\n",
      "2024-12-31 14:26:47,996 [INFO] __main__: ✅ Original dataset loaded successfully.\n",
      "2024-12-31 14:26:47,997 [INFO] __main__: 🔍 Filtering dataset for selected features...\n",
      "2024-12-31 14:26:47,998 [INFO] __main__: ✅ Dataset filtered based on selected features.\n",
      "2024-12-31 14:26:47,998 [INFO] __main__: ✅ Dataset filtered successfully.\n",
      "2024-12-31 14:26:47,998 [INFO] __main__: 📁 Separating columns into defined categories...\n",
      "2024-12-31 14:26:47,999 [INFO] __main__: ✅ Columns separated into defined categories.\n",
      "2024-12-31 14:26:47,999 [INFO] __main__: ✅ Features loaded and dataset filtered successfully.\n",
      "2024-12-31 14:26:48,000 [INFO] main_preprocessing: ✅ Features loaded and dataset filtered successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:48,000 [INFO] Starting: Final Preprocessing Pipeline in 'train' mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:48,000 [INFO] DataPreprocessor: Starting: Final Preprocessing Pipeline in 'train' mode.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:48,001 [INFO] Step: filter_columns\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:48,001 [INFO] DataPreprocessor: Step: filter_columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:48,002 [INFO] ✅ Filtered DataFrame to include only specified features. Shape: (125, 16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:48,002 [INFO] DataPreprocessor: ✅ Filtered DataFrame to include only specified features. Shape: (125, 16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:48,004 [INFO] ✅ Column filtering completed successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:48,004 [INFO] DataPreprocessor: ✅ Column filtering completed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:48,006 [INFO] Step: Split Dataset into Train and Test\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:48,006 [INFO] DataPreprocessor: Step: Split Dataset into Train and Test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:48,013 [INFO] Step: Handle Missing Values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:48,013 [INFO] DataPreprocessor: Step: Handle Missing Values\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:48,021 [INFO] Step: Test for Normality\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:48,021 [INFO] DataPreprocessor: Step: Test for Normality\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:48,027 [INFO] Step: Handle Outliers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:48,027 [INFO] DataPreprocessor: Step: Handle Outliers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:48,041 [INFO] Step: Generate Preprocessor Recommendations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:48,041 [INFO] DataPreprocessor: Step: Generate Preprocessor Recommendations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:48,048 [INFO] ✅ Preprocessor fitted on training data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:48,048 [INFO] DataPreprocessor: ✅ Preprocessor fitted on training data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:48,057 [INFO] Step: Implement SMOTE (Train Only)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:48,057 [INFO] DataPreprocessor: Step: Implement SMOTE (Train Only)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:48,059 [INFO] Class Distribution before SMOTE: {1: 52, 0: 27}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:48,059 [INFO] DataPreprocessor: Class Distribution before SMOTE: {1: 52, 0: 27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:48,060 [INFO] Imbalance Ratio (Minority/Majority): 0.5192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:48,060 [INFO] DataPreprocessor: Imbalance Ratio (Minority/Majority): 0.5192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:48,061 [INFO] Dataset contains both numerical and categorical features. Using SMOTENC.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:48,061 [INFO] DataPreprocessor: Dataset contains both numerical and categorical features. Using SMOTENC.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:48,068 [INFO] Applied SMOTENC. Resampled dataset shape: (104, 15)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:48,068 [INFO] DataPreprocessor: Applied SMOTENC. Resampled dataset shape: (104, 15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:48,070 [INFO] Step: Save Transformers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:48,070 [INFO] DataPreprocessor: Step: Save Transformers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:48,085 [INFO] Transformers saved at '../../ml-preprocessing-utils/data/dataset/test/transformers/transformers.pkl'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:48,085 [INFO] DataPreprocessor: Transformers saved at '../../ml-preprocessing-utils/data/dataset/test/transformers/transformers.pkl'.\n",
      "2024-12-31 14:26:48,087 [INFO] InverseTransform: ✅ Inverse transformation completed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:48,087 [INFO] ✅ Inverse transformations applied successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:48,087 [INFO] DataPreprocessor: ✅ Inverse transformations applied successfully.\n",
      "2024-12-31 14:26:48,089 [INFO] main_preprocessing: ✅ Preprocessing completed successfully in train mode.\n",
      "2024-12-31 14:26:48,140 [INFO] main_preprocessing: ✅ Preprocessed data saved to '../../ml-preprocessing-utils/data/dataset/test/processed_data/Tree_Based_Classifier'.\n",
      "Inverse Transformed Test Data for Tree Based Classifier:\n",
      "   release_ball_direction_x  release_ball_direction_z  \\\n",
      "0                  0.389917                  0.918894   \n",
      "1                  0.254867                  0.949685   \n",
      "2                  0.285478                  0.953757   \n",
      "3                  0.379350                  0.924839   \n",
      "4                  0.151629                  0.983647   \n",
      "\n",
      "   release_ball_direction_y  elbow_release_angle  elbow_max_angle  \\\n",
      "0                 -0.059987            73.619348       103.832024   \n",
      "1                 -0.182048            67.358777       105.506006   \n",
      "2                 -0.094078            66.195867       104.714966   \n",
      "3                 -0.027690            57.723558       104.086466   \n",
      "4                 -0.097198            62.628834       104.002652   \n",
      "\n",
      "   wrist_release_angle  wrist_max_angle  knee_release_angle  knee_max_angle  \\\n",
      "0            21.754498        34.532455           32.856306       64.165568   \n",
      "1            26.738474        35.990742           33.981724       63.757296   \n",
      "2            24.793301        37.829797           29.563399       61.355174   \n",
      "3            27.624330        33.893509           27.569106       64.080024   \n",
      "4            29.715538        39.048699           32.831332       64.880979   \n",
      "\n",
      "   release_ball_speed  calculated_release_angle  release_ball_velocity_x  \\\n",
      "0           11.113489                 60.900817                 4.333333   \n",
      "1            9.987366                 60.402805                 2.545455   \n",
      "2            9.341053                 59.661842                 2.666667   \n",
      "3           10.943758                 63.128630                 4.151515   \n",
      "4            7.564887                 68.267426                 1.147059   \n",
      "\n",
      "   release_ball_velocity_y  release_ball_velocity_z  \\\n",
      "0                -0.666667                10.212121   \n",
      "1                -1.818182                 9.484848   \n",
      "2                -0.878788                 8.909091   \n",
      "3                -0.303030                10.121212   \n",
      "4                -0.735294                 7.441176   \n",
      "\n",
      "  player_estimated_hand_length_cm_category  \n",
      "0                                   Medium  \n",
      "1                                   Medium  \n",
      "2                                   Medium  \n",
      "3                                   Medium  \n",
      "4                                   Medium  \n",
      "2024-12-31 14:26:48,143 [INFO] main_preprocessing: ✅ All preprocessing tasks completed successfully for model 'Tree Based Classifier'.\n",
      "2024-12-31 14:26:48,144 [INFO] main_preprocessing: ---\n",
      "Processing Model: Logistic Regression\n",
      "---\n",
      "2024-12-31 14:26:48,154 [INFO] __main__: ✅ Features and metadata loaded from ../../ml-preprocessing-utils/data/dataset/test/features_info/features_metadata.pkl\n",
      "2024-12-31 14:26:48,157 [INFO] __main__: 📥 Loading dataset from ../../data/processed/final_ml_dataset.csv...\n",
      "2024-12-31 14:26:48,167 [INFO] __main__: ✅ Dataset loaded from ../../data/processed/final_ml_dataset.csv\n",
      "2024-12-31 14:26:48,167 [INFO] __main__: ✅ Original dataset loaded successfully.\n",
      "2024-12-31 14:26:48,168 [INFO] __main__: 🔍 Filtering dataset for selected features...\n",
      "2024-12-31 14:26:48,170 [INFO] __main__: ✅ Dataset filtered based on selected features.\n",
      "2024-12-31 14:26:48,170 [INFO] __main__: ✅ Dataset filtered successfully.\n",
      "2024-12-31 14:26:48,170 [INFO] __main__: 📁 Separating columns into defined categories...\n",
      "2024-12-31 14:26:48,171 [INFO] __main__: ✅ Columns separated into defined categories.\n",
      "2024-12-31 14:26:48,172 [INFO] __main__: ✅ Features loaded and dataset filtered successfully.\n",
      "2024-12-31 14:26:48,172 [INFO] main_preprocessing: ✅ Features loaded and dataset filtered successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:48,173 [INFO] Starting: Final Preprocessing Pipeline in 'train' mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:48,173 [INFO] DataPreprocessor: Starting: Final Preprocessing Pipeline in 'train' mode.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:48,174 [INFO] Step: filter_columns\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:48,174 [INFO] DataPreprocessor: Step: filter_columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:48,176 [INFO] ✅ Filtered DataFrame to include only specified features. Shape: (125, 16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:48,176 [INFO] DataPreprocessor: ✅ Filtered DataFrame to include only specified features. Shape: (125, 16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:48,177 [INFO] ✅ Column filtering completed successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:48,177 [INFO] DataPreprocessor: ✅ Column filtering completed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:48,179 [INFO] Step: Split Dataset into Train and Test\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:48,179 [INFO] DataPreprocessor: Step: Split Dataset into Train and Test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:48,184 [INFO] Step: Handle Missing Values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:48,184 [INFO] DataPreprocessor: Step: Handle Missing Values\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:48,193 [INFO] Step: Test for Normality\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:48,193 [INFO] DataPreprocessor: Step: Test for Normality\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:48,200 [INFO] Step: Handle Outliers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:48,200 [INFO] DataPreprocessor: Step: Handle Outliers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:48,217 [INFO] Step: Generate Preprocessor Recommendations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:48,217 [INFO] DataPreprocessor: Step: Generate Preprocessor Recommendations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:48,225 [INFO] ✅ Preprocessor fitted on training data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:48,225 [INFO] DataPreprocessor: ✅ Preprocessor fitted on training data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:48,234 [INFO] Step: Implement SMOTE (Train Only)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:48,234 [INFO] DataPreprocessor: Step: Implement SMOTE (Train Only)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:48,235 [INFO] Class Distribution before SMOTE: {1: 52, 0: 27}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:48,235 [INFO] DataPreprocessor: Class Distribution before SMOTE: {1: 52, 0: 27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:48,236 [INFO] Imbalance Ratio (Minority/Majority): 0.5192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:48,236 [INFO] DataPreprocessor: Imbalance Ratio (Minority/Majority): 0.5192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:48,237 [INFO] Dataset contains both numerical and categorical features. Using SMOTENC.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:48,237 [INFO] DataPreprocessor: Dataset contains both numerical and categorical features. Using SMOTENC.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:48,246 [INFO] Applied SMOTENC. Resampled dataset shape: (104, 15)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:48,246 [INFO] DataPreprocessor: Applied SMOTENC. Resampled dataset shape: (104, 15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:48,247 [INFO] Step: Save Transformers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:48,247 [INFO] DataPreprocessor: Step: Save Transformers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:48,260 [INFO] Transformers saved at '../../ml-preprocessing-utils/data/dataset/test/transformers/transformers.pkl'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:48,260 [INFO] DataPreprocessor: Transformers saved at '../../ml-preprocessing-utils/data/dataset/test/transformers/transformers.pkl'.\n",
      "2024-12-31 14:26:48,262 [INFO] InverseTransform: ✅ Inverse transformation completed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:48,262 [INFO] ✅ Inverse transformations applied successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:48,262 [INFO] DataPreprocessor: ✅ Inverse transformations applied successfully.\n",
      "2024-12-31 14:26:48,263 [INFO] main_preprocessing: ✅ Preprocessing completed successfully in train mode.\n",
      "2024-12-31 14:26:48,303 [INFO] main_preprocessing: ✅ Preprocessed data saved to '../../ml-preprocessing-utils/data/dataset/test/processed_data/Logistic_Regression'.\n",
      "Inverse Transformed Test Data for Logistic Regression:\n",
      "   release_ball_direction_x  release_ball_direction_z  \\\n",
      "0                  0.389917                  0.918894   \n",
      "1                  0.254867                  0.949685   \n",
      "2                  0.285478                  0.953757   \n",
      "3                  0.379350                  0.924839   \n",
      "4                  0.151629                  0.983647   \n",
      "\n",
      "   release_ball_direction_y  elbow_release_angle  elbow_max_angle  \\\n",
      "0                 -0.059987            73.619348       103.832024   \n",
      "1                 -0.182048            67.358777       105.506006   \n",
      "2                 -0.094078            66.195867       104.714966   \n",
      "3                 -0.027690            57.723558       104.086466   \n",
      "4                 -0.097198            62.628834       104.002652   \n",
      "\n",
      "   wrist_release_angle  wrist_max_angle  knee_release_angle  knee_max_angle  \\\n",
      "0            21.754498        34.532455           32.856306       64.165568   \n",
      "1            26.738474        35.990742           33.981724       63.757296   \n",
      "2            24.793301        37.829797           29.563399       61.355174   \n",
      "3            27.624330        33.893509           27.569106       64.080024   \n",
      "4            29.715538        39.048699           32.831332       64.880979   \n",
      "\n",
      "   release_ball_speed  calculated_release_angle  release_ball_velocity_x  \\\n",
      "0           11.113489                 60.900817                 4.333333   \n",
      "1            9.987366                 60.402805                 2.545455   \n",
      "2            9.341053                 59.661842                 2.666667   \n",
      "3           10.943758                 63.128630                 4.151515   \n",
      "4            7.564887                 68.267426                 1.147059   \n",
      "\n",
      "   release_ball_velocity_y  release_ball_velocity_z  \\\n",
      "0                -0.666667                10.212121   \n",
      "1                -1.818182                 9.484848   \n",
      "2                -0.878788                 8.909091   \n",
      "3                -0.303030                10.121212   \n",
      "4                -0.735294                 7.441176   \n",
      "\n",
      "  player_estimated_hand_length_cm_category  \n",
      "0                                   Medium  \n",
      "1                                   Medium  \n",
      "2                                   Medium  \n",
      "3                                   Medium  \n",
      "4                                   Medium  \n",
      "2024-12-31 14:26:48,306 [INFO] main_preprocessing: ✅ All preprocessing tasks completed successfully for model 'Logistic Regression'.\n",
      "2024-12-31 14:26:48,307 [INFO] main_preprocessing: ---\n",
      "Processing Model: K-Means\n",
      "---\n",
      "2024-12-31 14:26:48,319 [INFO] main_preprocessing: ✅ Clustering input data loaded from '../../ml-preprocessing-utils/data/dataset/test/data/final_ml_dataset.csv'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:48,320 [INFO] Starting: Final Preprocessing Pipeline in 'clustering' mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:48,320 [INFO] DataPreprocessor: Starting: Final Preprocessing Pipeline in 'clustering' mode.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:48,320 [INFO] Step: filter_columns\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:48,320 [INFO] DataPreprocessor: Step: filter_columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:48,322 [INFO] ✅ Filtered DataFrame to include only specified features. Shape: (125, 15)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:48,322 [INFO] DataPreprocessor: ✅ Filtered DataFrame to include only specified features. Shape: (125, 15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:48,322 [INFO] ✅ Column filtering completed successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:48,322 [INFO] DataPreprocessor: ✅ Column filtering completed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:48,323 [INFO] Step: Preprocess Clustering\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:48,323 [INFO] DataPreprocessor: Step: Preprocess Clustering\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:48,325 [INFO] Step: Handle Missing Values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:48,325 [INFO] DataPreprocessor: Step: Handle Missing Values\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:48,333 [INFO] Step: Handle Outliers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:48,333 [INFO] DataPreprocessor: Step: Handle Outliers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,276 [INFO] Skipping normality tests for clustering.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,276 [INFO] DataPreprocessor: Skipping normality tests for clustering.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,277 [INFO] Step: Generate Preprocessor Recommendations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,277 [INFO] DataPreprocessor: Step: Generate Preprocessor Recommendations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,286 [INFO] ✅ Preprocessor fitted on training data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,286 [INFO] DataPreprocessor: ✅ Preprocessor fitted on training data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,290 [INFO] Step: Save Transformers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,290 [INFO] DataPreprocessor: Step: Save Transformers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,311 [INFO] Transformers saved at '../../ml-preprocessing-utils/data/dataset/test/transformers/transformers.pkl'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,311 [INFO] DataPreprocessor: Transformers saved at '../../ml-preprocessing-utils/data/dataset/test/transformers/transformers.pkl'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,313 [INFO] ✅ Clustering data preprocessed successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,313 [INFO] DataPreprocessor: ✅ Clustering data preprocessed successfully.\n",
      "2024-12-31 14:26:49,313 [INFO] main_preprocessing: ✅ Preprocessing completed successfully in clustering mode.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,314 [INFO] Fitting K-Means model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,314 [INFO] ClusteringModule: Fitting K-Means model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,325 [INFO] ✅ K-Means model fitted successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,325 [INFO] ClusteringModule: ✅ K-Means model fitted successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,327 [INFO] Evaluating K-Means model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,327 [INFO] ClusteringModule: Evaluating K-Means model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,332 [INFO] ✅ Clustering evaluation completed successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,332 [INFO] ClusteringModule: ✅ Clustering evaluation completed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,332 [WARNING] Plotting is only supported for 2D data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,332 [WARNING] ClusteringModule: Plotting is only supported for 2D data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,344 [INFO] ✅ Clustering model saved to '../../ml-preprocessing-utils/data/dataset/test/clustering_output/K-Means_model.pkl'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,344 [INFO] ClusteringModule: ✅ Clustering model saved to '../../ml-preprocessing-utils/data/dataset/test/clustering_output/K-Means_model.pkl'.\n",
      "2024-12-31 14:26:49,345 [INFO] main_preprocessing: ✅ Clustering model saved to '../../ml-preprocessing-utils/data/dataset/test/clustering_output/K-Means_model.pkl'.\n",
      "2024-12-31 14:26:49,345 [INFO] main_preprocessing: ✅ All clustering tasks completed successfully for model 'K-Means'.\n",
      "2024-12-31 14:26:49,346 [INFO] main_preprocessing: ---\n",
      "Processing Model: Linear Regression\n",
      "---\n",
      "2024-12-31 14:26:49,354 [INFO] __main__: ✅ Features and metadata loaded from ../../ml-preprocessing-utils/data/dataset/test/features_info/features_metadata.pkl\n",
      "2024-12-31 14:26:49,356 [INFO] __main__: 📥 Loading dataset from ../../data/processed/final_ml_dataset.csv...\n",
      "2024-12-31 14:26:49,364 [INFO] __main__: ✅ Dataset loaded from ../../data/processed/final_ml_dataset.csv\n",
      "2024-12-31 14:26:49,364 [INFO] __main__: ✅ Original dataset loaded successfully.\n",
      "2024-12-31 14:26:49,365 [INFO] __main__: 🔍 Filtering dataset for selected features...\n",
      "2024-12-31 14:26:49,366 [INFO] __main__: ✅ Dataset filtered based on selected features.\n",
      "2024-12-31 14:26:49,366 [INFO] __main__: ✅ Dataset filtered successfully.\n",
      "2024-12-31 14:26:49,366 [INFO] __main__: 📁 Separating columns into defined categories...\n",
      "2024-12-31 14:26:49,367 [INFO] __main__: ✅ Columns separated into defined categories.\n",
      "2024-12-31 14:26:49,367 [INFO] __main__: ✅ Features loaded and dataset filtered successfully.\n",
      "2024-12-31 14:26:49,368 [INFO] main_preprocessing: ✅ Features loaded and dataset filtered successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,368 [INFO] Starting: Final Preprocessing Pipeline in 'train' mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,368 [INFO] DataPreprocessor: Starting: Final Preprocessing Pipeline in 'train' mode.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,369 [INFO] Step: filter_columns\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,369 [INFO] DataPreprocessor: Step: filter_columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,372 [INFO] ✅ Filtered DataFrame to include only specified features. Shape: (125, 16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,372 [INFO] DataPreprocessor: ✅ Filtered DataFrame to include only specified features. Shape: (125, 16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,374 [INFO] ✅ Column filtering completed successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,374 [INFO] DataPreprocessor: ✅ Column filtering completed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,376 [INFO] Step: Split Dataset into Train and Test\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,376 [INFO] DataPreprocessor: Step: Split Dataset into Train and Test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,382 [INFO] Step: Handle Missing Values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,382 [INFO] DataPreprocessor: Step: Handle Missing Values\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,390 [INFO] Step: Test for Normality\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,390 [INFO] DataPreprocessor: Step: Test for Normality\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,396 [INFO] Step: Handle Outliers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,396 [INFO] DataPreprocessor: Step: Handle Outliers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,424 [INFO] Step: Generate Preprocessor Recommendations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,424 [INFO] DataPreprocessor: Step: Generate Preprocessor Recommendations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,432 [INFO] ✅ Preprocessor fitted on training data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,432 [INFO] DataPreprocessor: ✅ Preprocessor fitted on training data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,441 [INFO] ⚠️ SMOTE not applied: Not a classification model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,441 [INFO] DataPreprocessor: ⚠️ SMOTE not applied: Not a classification model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,443 [INFO] Step: Save Transformers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,443 [INFO] DataPreprocessor: Step: Save Transformers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,456 [INFO] Transformers saved at '../../ml-preprocessing-utils/data/dataset/test/transformers/transformers.pkl'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,456 [INFO] DataPreprocessor: Transformers saved at '../../ml-preprocessing-utils/data/dataset/test/transformers/transformers.pkl'.\n",
      "2024-12-31 14:26:49,458 [INFO] InverseTransform: ✅ Inverse transformation completed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,459 [INFO] ✅ Inverse transformations applied successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,459 [INFO] DataPreprocessor: ✅ Inverse transformations applied successfully.\n",
      "2024-12-31 14:26:49,460 [INFO] main_preprocessing: ✅ Preprocessing completed successfully in train mode.\n",
      "2024-12-31 14:26:49,504 [INFO] main_preprocessing: ✅ Preprocessed data saved to '../../ml-preprocessing-utils/data/dataset/test/processed_data/Linear_Regression'.\n",
      "Inverse Transformed Test Data for Linear Regression:\n",
      "   release_ball_direction_x  release_ball_direction_z  \\\n",
      "0                  0.377012                  0.926203   \n",
      "1                  0.389917                  0.918894   \n",
      "2                  0.449165                  0.889523   \n",
      "3                  0.319733                  0.947018   \n",
      "4                  0.151629                  0.983647   \n",
      "\n",
      "   release_ball_direction_y  elbow_release_angle  elbow_max_angle  \\\n",
      "0                  0.002969            72.325830       106.272118   \n",
      "1                 -0.059987            73.619348       103.832024   \n",
      "2                 -0.083668            62.317104       107.201000   \n",
      "3                  0.030451            70.001020       105.040498   \n",
      "4                 -0.097198            62.628834       104.002652   \n",
      "\n",
      "   wrist_release_angle  wrist_max_angle  knee_release_angle  knee_max_angle  \\\n",
      "0            28.102765        35.918406           32.646276       63.541007   \n",
      "1            21.754498        34.532455           32.856306       64.165568   \n",
      "2            24.246267        34.948768           29.608084       66.532715   \n",
      "3            22.114982        39.926291           30.437252       62.350896   \n",
      "4            29.715538        39.048699           32.831332       64.880979   \n",
      "\n",
      "   release_ball_speed  calculated_release_angle  release_ball_velocity_x  \\\n",
      "0            9.907618                 62.959206                 3.735294   \n",
      "1           11.113489                 60.900817                 4.333333   \n",
      "2           13.762914                 61.613806                 6.181818   \n",
      "3            9.951489                 62.090573                 3.181818   \n",
      "4            7.564887                 68.267426                 1.147059   \n",
      "\n",
      "   release_ball_velocity_y  release_ball_velocity_z  \\\n",
      "0                 0.029412                 9.176471   \n",
      "1                -0.666667                10.212121   \n",
      "2                -1.151515                12.242424   \n",
      "3                 0.303030                 9.424242   \n",
      "4                -0.735294                 7.441176   \n",
      "\n",
      "  player_estimated_hand_length_cm_category  \n",
      "0                                   Medium  \n",
      "1                                   Medium  \n",
      "2                                   Medium  \n",
      "3                                   Medium  \n",
      "4                                   Medium  \n",
      "2024-12-31 14:26:49,508 [INFO] main_preprocessing: ✅ All preprocessing tasks completed successfully for model 'Linear Regression'.\n",
      "2024-12-31 14:26:49,508 [INFO] main_preprocessing: ---\n",
      "Processing Model: Tree Based Regressor\n",
      "---\n",
      "2024-12-31 14:26:49,516 [INFO] __main__: ✅ Features and metadata loaded from ../../ml-preprocessing-utils/data/dataset/test/features_info/features_metadata.pkl\n",
      "2024-12-31 14:26:49,518 [INFO] __main__: 📥 Loading dataset from ../../data/processed/final_ml_dataset.csv...\n",
      "2024-12-31 14:26:49,525 [INFO] __main__: ✅ Dataset loaded from ../../data/processed/final_ml_dataset.csv\n",
      "2024-12-31 14:26:49,526 [INFO] __main__: ✅ Original dataset loaded successfully.\n",
      "2024-12-31 14:26:49,526 [INFO] __main__: 🔍 Filtering dataset for selected features...\n",
      "2024-12-31 14:26:49,528 [INFO] __main__: ✅ Dataset filtered based on selected features.\n",
      "2024-12-31 14:26:49,529 [INFO] __main__: ✅ Dataset filtered successfully.\n",
      "2024-12-31 14:26:49,529 [INFO] __main__: 📁 Separating columns into defined categories...\n",
      "2024-12-31 14:26:49,529 [INFO] __main__: ✅ Columns separated into defined categories.\n",
      "2024-12-31 14:26:49,530 [INFO] __main__: ✅ Features loaded and dataset filtered successfully.\n",
      "2024-12-31 14:26:49,530 [INFO] main_preprocessing: ✅ Features loaded and dataset filtered successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,531 [INFO] Starting: Final Preprocessing Pipeline in 'train' mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,531 [INFO] DataPreprocessor: Starting: Final Preprocessing Pipeline in 'train' mode.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,531 [INFO] Step: filter_columns\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,531 [INFO] DataPreprocessor: Step: filter_columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,533 [INFO] ✅ Filtered DataFrame to include only specified features. Shape: (125, 16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,533 [INFO] DataPreprocessor: ✅ Filtered DataFrame to include only specified features. Shape: (125, 16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,535 [INFO] ✅ Column filtering completed successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,535 [INFO] DataPreprocessor: ✅ Column filtering completed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,537 [INFO] Step: Split Dataset into Train and Test\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,537 [INFO] DataPreprocessor: Step: Split Dataset into Train and Test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,543 [INFO] Step: Handle Missing Values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,543 [INFO] DataPreprocessor: Step: Handle Missing Values\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,553 [INFO] Step: Test for Normality\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,553 [INFO] DataPreprocessor: Step: Test for Normality\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,561 [INFO] Step: Handle Outliers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,561 [INFO] DataPreprocessor: Step: Handle Outliers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,580 [INFO] Step: Generate Preprocessor Recommendations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,580 [INFO] DataPreprocessor: Step: Generate Preprocessor Recommendations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,587 [INFO] ✅ Preprocessor fitted on training data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,587 [INFO] DataPreprocessor: ✅ Preprocessor fitted on training data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,597 [INFO] ⚠️ SMOTE not applied: Not a classification model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,597 [INFO] DataPreprocessor: ⚠️ SMOTE not applied: Not a classification model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,599 [INFO] Step: Save Transformers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,599 [INFO] DataPreprocessor: Step: Save Transformers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,611 [INFO] Transformers saved at '../../ml-preprocessing-utils/data/dataset/test/transformers/transformers.pkl'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,611 [INFO] DataPreprocessor: Transformers saved at '../../ml-preprocessing-utils/data/dataset/test/transformers/transformers.pkl'.\n",
      "2024-12-31 14:26:49,612 [WARNING] InverseTransform: No scaler found for numerical features ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z']. Skipping inverse transformation.\n",
      "2024-12-31 14:26:49,612 [INFO] InverseTransform: ✅ Inverse transformation completed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,613 [INFO] ✅ Inverse transformations applied successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,613 [INFO] DataPreprocessor: ✅ Inverse transformations applied successfully.\n",
      "2024-12-31 14:26:49,614 [INFO] main_preprocessing: ✅ Preprocessing completed successfully in train mode.\n",
      "2024-12-31 14:26:49,655 [INFO] main_preprocessing: ✅ Preprocessed data saved to '../../ml-preprocessing-utils/data/dataset/test/processed_data/Tree_Based_Regressor'.\n",
      "Inverse Transformed Test Data for Tree Based Regressor:\n",
      "  player_estimated_hand_length_cm_category\n",
      "0                                   Medium\n",
      "1                                   Medium\n",
      "2                                   Medium\n",
      "3                                   Medium\n",
      "4                                   Medium\n",
      "2024-12-31 14:26:49,656 [INFO] main_preprocessing: ✅ All preprocessing tasks completed successfully for model 'Tree Based Regressor'.\n",
      "2024-12-31 14:26:49,657 [INFO] main_preprocessing: ---\n",
      "Processing Model: Support Vector Machine\n",
      "---\n",
      "2024-12-31 14:26:49,666 [INFO] __main__: ✅ Features and metadata loaded from ../../ml-preprocessing-utils/data/dataset/test/features_info/features_metadata.pkl\n",
      "2024-12-31 14:26:49,668 [INFO] __main__: 📥 Loading dataset from ../../data/processed/final_ml_dataset.csv...\n",
      "2024-12-31 14:26:49,675 [INFO] __main__: ✅ Dataset loaded from ../../data/processed/final_ml_dataset.csv\n",
      "2024-12-31 14:26:49,676 [INFO] __main__: ✅ Original dataset loaded successfully.\n",
      "2024-12-31 14:26:49,676 [INFO] __main__: 🔍 Filtering dataset for selected features...\n",
      "2024-12-31 14:26:49,678 [INFO] __main__: ✅ Dataset filtered based on selected features.\n",
      "2024-12-31 14:26:49,678 [INFO] __main__: ✅ Dataset filtered successfully.\n",
      "2024-12-31 14:26:49,679 [INFO] __main__: 📁 Separating columns into defined categories...\n",
      "2024-12-31 14:26:49,679 [INFO] __main__: ✅ Columns separated into defined categories.\n",
      "2024-12-31 14:26:49,679 [INFO] __main__: ✅ Features loaded and dataset filtered successfully.\n",
      "2024-12-31 14:26:49,680 [INFO] main_preprocessing: ✅ Features loaded and dataset filtered successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,681 [INFO] Starting: Final Preprocessing Pipeline in 'train' mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,681 [INFO] DataPreprocessor: Starting: Final Preprocessing Pipeline in 'train' mode.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,682 [INFO] Step: filter_columns\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,682 [INFO] DataPreprocessor: Step: filter_columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,684 [INFO] ✅ Filtered DataFrame to include only specified features. Shape: (125, 16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,684 [INFO] DataPreprocessor: ✅ Filtered DataFrame to include only specified features. Shape: (125, 16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,685 [INFO] ✅ Column filtering completed successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,685 [INFO] DataPreprocessor: ✅ Column filtering completed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,686 [INFO] Step: Split Dataset into Train and Test\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,686 [INFO] DataPreprocessor: Step: Split Dataset into Train and Test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,691 [INFO] Step: Handle Missing Values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,691 [INFO] DataPreprocessor: Step: Handle Missing Values\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,701 [INFO] Step: Test for Normality\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,701 [INFO] DataPreprocessor: Step: Test for Normality\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,709 [INFO] Step: Handle Outliers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,709 [INFO] DataPreprocessor: Step: Handle Outliers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,729 [INFO] Step: Generate Preprocessor Recommendations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,729 [INFO] DataPreprocessor: Step: Generate Preprocessor Recommendations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,737 [INFO] ✅ Preprocessor fitted on training data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,737 [INFO] DataPreprocessor: ✅ Preprocessor fitted on training data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,746 [INFO] Step: Implement SMOTE (Train Only)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,746 [INFO] DataPreprocessor: Step: Implement SMOTE (Train Only)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,748 [INFO] Class Distribution before SMOTE: {1: 52, 0: 27}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,748 [INFO] DataPreprocessor: Class Distribution before SMOTE: {1: 52, 0: 27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,749 [INFO] Imbalance Ratio (Minority/Majority): 0.5192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,749 [INFO] DataPreprocessor: Imbalance Ratio (Minority/Majority): 0.5192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,750 [INFO] Dataset contains both numerical and categorical features. Using SMOTENC.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,750 [INFO] DataPreprocessor: Dataset contains both numerical and categorical features. Using SMOTENC.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,757 [INFO] Applied SMOTENC. Resampled dataset shape: (104, 15)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,757 [INFO] DataPreprocessor: Applied SMOTENC. Resampled dataset shape: (104, 15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,759 [INFO] Step: Save Transformers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,759 [INFO] DataPreprocessor: Step: Save Transformers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,774 [INFO] Transformers saved at '../../ml-preprocessing-utils/data/dataset/test/transformers/transformers.pkl'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,774 [INFO] DataPreprocessor: Transformers saved at '../../ml-preprocessing-utils/data/dataset/test/transformers/transformers.pkl'.\n",
      "2024-12-31 14:26:49,776 [INFO] InverseTransform: ✅ Inverse transformation completed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,777 [INFO] ✅ Inverse transformations applied successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:26:49,777 [INFO] DataPreprocessor: ✅ Inverse transformations applied successfully.\n",
      "2024-12-31 14:26:49,778 [INFO] main_preprocessing: ✅ Preprocessing completed successfully in train mode.\n",
      "2024-12-31 14:26:49,823 [INFO] main_preprocessing: ✅ Preprocessed data saved to '../../ml-preprocessing-utils/data/dataset/test/processed_data/Support_Vector_Machine'.\n",
      "Inverse Transformed Test Data for Support Vector Machine:\n",
      "   release_ball_direction_x  release_ball_direction_z  \\\n",
      "0                  0.389917                  0.918894   \n",
      "1                  0.254867                  0.949685   \n",
      "2                  0.285478                  0.953757   \n",
      "3                  0.379350                  0.924839   \n",
      "4                  0.151629                  0.983647   \n",
      "\n",
      "   release_ball_direction_y  elbow_release_angle  elbow_max_angle  \\\n",
      "0                 -0.059987            73.619348       103.832024   \n",
      "1                 -0.182048            67.358777       105.506006   \n",
      "2                 -0.094078            66.195867       104.714966   \n",
      "3                 -0.027690            57.723558       104.086466   \n",
      "4                 -0.097198            62.628834       104.002652   \n",
      "\n",
      "   wrist_release_angle  wrist_max_angle  knee_release_angle  knee_max_angle  \\\n",
      "0            21.754498        34.532455           32.856306       64.165568   \n",
      "1            26.738474        35.990742           33.981724       63.757296   \n",
      "2            24.793301        37.829797           29.563399       61.355174   \n",
      "3            27.624330        33.893509           27.569106       64.080024   \n",
      "4            29.715538        39.048699           32.831332       64.880979   \n",
      "\n",
      "   release_ball_speed  calculated_release_angle  release_ball_velocity_x  \\\n",
      "0           11.113489                 60.900817                 4.333333   \n",
      "1            9.987366                 60.402805                 2.545455   \n",
      "2            9.341053                 59.661842                 2.666667   \n",
      "3           10.943758                 63.128630                 4.151515   \n",
      "4            7.564887                 68.267426                 1.147059   \n",
      "\n",
      "   release_ball_velocity_y  release_ball_velocity_z  \\\n",
      "0                -0.666667                10.212121   \n",
      "1                -1.818182                 9.484848   \n",
      "2                -0.878788                 8.909091   \n",
      "3                -0.303030                10.121212   \n",
      "4                -0.735294                 7.441176   \n",
      "\n",
      "  player_estimated_hand_length_cm_category  \n",
      "0                                   Medium  \n",
      "1                                   Medium  \n",
      "2                                   Medium  \n",
      "3                                   Medium  \n",
      "4                                   Medium  \n",
      "2024-12-31 14:26:49,826 [INFO] main_preprocessing: ✅ All preprocessing tasks completed successfully for model 'Support Vector Machine'.\n",
      "2024-12-31 14:26:49,827 [INFO] main_preprocessing: ✅ All model processing completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# data_preprocessor.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Optional, Tuple, Any\n",
    "import logging\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.preprocessing import PowerTransformer, OrdinalEncoder, OneHotEncoder, StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, SMOTENC, SMOTEN, BorderlineSMOTE\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import probplot\n",
    "import joblib  # For saving/loading transformers\n",
    "from inspect import signature  # For parameter validation in SMOTE\n",
    "\n",
    "class DataPreprocessor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_type: str,\n",
    "        column_assets: Dict[str, List[str]],\n",
    "        mode: str,  # 'train', 'predict', 'clustering'\n",
    "        options: Optional[Dict] = None,\n",
    "        debug: bool = False,\n",
    "        normalize_debug: bool = False,\n",
    "        normalize_graphs_output: bool = False,\n",
    "        graphs_output_dir: str = './plots',\n",
    "        transformers_dir: str = './transformers'\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the DataPreprocessor with model type, column assets, and user-defined options.\n",
    "\n",
    "        Args:\n",
    "            model_type (str): Type of the machine learning model (e.g., 'Logistic Regression').\n",
    "            column_assets (Dict[str, List[str]]): Dictionary containing lists of columns for different categories.\n",
    "            mode (str): Operational mode ('train', 'predict', 'clustering').\n",
    "            options (Optional[Dict]): User-defined options for preprocessing steps.\n",
    "            debug (bool): General debug flag to control overall verbosity.\n",
    "            normalize_debug (bool): Flag to display normalization plots.\n",
    "            normalize_graphs_output (bool): Flag to save normalization plots.\n",
    "            graphs_output_dir (str): Directory to save plots.\n",
    "            transformers_dir (str): Directory to save/load transformers.\n",
    "        \"\"\"\n",
    "        self.model_type = model_type\n",
    "        self.column_assets = column_assets\n",
    "        self.mode = mode.lower()\n",
    "        if self.mode not in ['train', 'predict', 'clustering']:\n",
    "            raise ValueError(\"Mode must be one of 'train', 'predict', or 'clustering'.\")\n",
    "        self.options = options or {}\n",
    "        self.debug = debug\n",
    "        self.normalize_debug = normalize_debug\n",
    "        self.normalize_graphs_output = normalize_graphs_output\n",
    "        self.graphs_output_dir = graphs_output_dir\n",
    "        self.transformers_dir = transformers_dir\n",
    "\n",
    "        # Initialize categorical_indices to prevent AttributeError\n",
    "        self.categorical_indices = []\n",
    "\n",
    "        # Define model categories for accurate processing\n",
    "        self.model_category = self.map_model_type_to_category()\n",
    "\n",
    "        if self.model_category == 'unknown':\n",
    "            self.logger = logging.getLogger(self.__class__.__name__)\n",
    "            self.logger.error(f\"Model category for '{self.model_type}' is unknown. Check your configuration.\")\n",
    "            raise ValueError(f\"Model category for '{self.model_type}' is unknown. Check your configuration.\")\n",
    "\n",
    "        # Initialize y_variable based on mode and model category\n",
    "        if self.mode in ['train', 'predict'] and self.model_category in ['classification', 'regression']:\n",
    "            self.y_variable = column_assets.get('y_variable', [])\n",
    "            if not self.y_variable:\n",
    "                if self.mode == 'train':\n",
    "                    raise ValueError(\"Target variable 'y_variable' must be specified for supervised models in train mode.\")\n",
    "                # In predict mode, y_variable might not be present\n",
    "        else:\n",
    "            # For 'clustering' mode or unsupervised prediction\n",
    "            self.y_variable = []\n",
    "\n",
    "        # Fetch feature lists\n",
    "        self.ordinal_categoricals = column_assets.get('ordinal_categoricals', [])\n",
    "        self.nominal_categoricals = column_assets.get('nominal_categoricals', [])\n",
    "        self.numericals = column_assets.get('numericals', [])\n",
    "\n",
    "        # Initialize other variables\n",
    "        self.scaler = None\n",
    "        self.transformer = None\n",
    "        self.ordinal_encoder = None\n",
    "        self.nominal_encoder = None\n",
    "        self.preprocessor = None\n",
    "        self.smote = None\n",
    "        self.feature_reasons = {col: '' for col in self.ordinal_categoricals + self.nominal_categoricals + self.numericals}\n",
    "        self.preprocessing_steps = []\n",
    "        self.normality_results = {}\n",
    "        self.features_to_transform = []\n",
    "        self.nominal_encoded_feature_names = []\n",
    "        self.final_feature_order = []\n",
    "\n",
    "        # Initialize placeholders for clustering-specific transformers\n",
    "        self.cluster_transformers = {}\n",
    "        self.cluster_model = None\n",
    "        self.cluster_labels = None\n",
    "        self.silhouette_score = None\n",
    "\n",
    "        # Define default thresholds for SMOTE recommendations\n",
    "        self.imbalance_threshold = self.options.get('smote_recommendation', {}).get('imbalance_threshold', 0.1)\n",
    "        self.noise_threshold = self.options.get('smote_recommendation', {}).get('noise_threshold', 0.1)\n",
    "        self.overlap_threshold = self.options.get('smote_recommendation', {}).get('overlap_threshold', 0.1)\n",
    "        self.boundary_threshold = self.options.get('smote_recommendation', {}).get('boundary_threshold', 0.1)\n",
    "\n",
    "        self.pipeline = None  # Initialize pipeline\n",
    "\n",
    "        # Initialize logging\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        self.logger.setLevel(logging.DEBUG if self.debug else logging.INFO)\n",
    "        handler = logging.StreamHandler()\n",
    "        formatter = logging.Formatter('%(asctime)s [%(levelname)s] %(message)s')\n",
    "        handler.setFormatter(formatter)\n",
    "        if not self.logger.handlers:\n",
    "            self.logger.addHandler(handler)\n",
    "\n",
    "    def get_debug_flag(self, flag_name: str) -> bool:\n",
    "        \"\"\"\n",
    "        Retrieve the value of a specific debug flag from the options.\n",
    "        Args:\n",
    "            flag_name (str): The name of the debug flag.\n",
    "        Returns:\n",
    "            bool: The value of the debug flag.\n",
    "        \"\"\"\n",
    "        return self.options.get(flag_name, False)\n",
    "\n",
    "    def _log(self, message: str, step: str, level: str = 'info'):\n",
    "        \"\"\"\n",
    "        Internal method to log messages based on the step-specific debug flags.\n",
    "        \n",
    "        Args:\n",
    "            message (str): The message to log.\n",
    "            step (str): The preprocessing step name.\n",
    "            level (str): The logging level ('info', 'debug', etc.).\n",
    "        \"\"\"\n",
    "        debug_flag = self.get_debug_flag(f'debug_{step}')\n",
    "        if debug_flag:\n",
    "            if level == 'debug':\n",
    "                self.logger.debug(message)\n",
    "            elif level == 'info':\n",
    "                self.logger.info(message)\n",
    "            elif level == 'warning':\n",
    "                self.logger.warning(message)\n",
    "            elif level == 'error':\n",
    "                self.logger.error(message)\n",
    "\n",
    "    def map_model_type_to_category(self) -> str:\n",
    "        \"\"\"\n",
    "        Map the model_type string to a predefined category based on keywords.\n",
    "\n",
    "        Returns:\n",
    "            str: The model category ('classification', 'regression', 'clustering', etc.).\n",
    "        \"\"\"\n",
    "        classification_keywords = ['classifier', 'classification', 'logistic', 'svm', 'support vector machine', 'knn', 'neural network']\n",
    "        regression_keywords = ['regressor', 'regression', 'linear', 'knn', 'neural network']  # Removed 'svm'\n",
    "        clustering_keywords = ['k-means', 'clustering', 'dbscan', 'kmodes', 'kprototypes']\n",
    "\n",
    "        model_type_lower = self.model_type.lower()\n",
    "\n",
    "        for keyword in classification_keywords:\n",
    "            if keyword in model_type_lower:\n",
    "                return 'classification'\n",
    "\n",
    "        for keyword in regression_keywords:\n",
    "            if keyword in model_type_lower:\n",
    "                return 'regression'\n",
    "\n",
    "        for keyword in clustering_keywords:\n",
    "            if keyword in model_type_lower:\n",
    "                return 'clustering'\n",
    "\n",
    "        return 'unknown'\n",
    "\n",
    "    def filter_columns(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        step_name = \"filter_columns\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "\n",
    "        # Combine all feature lists\n",
    "        desired_features = self.numericals + self.ordinal_categoricals + self.nominal_categoricals\n",
    "\n",
    "        # For 'train' and 'clustering' modes, handle y_variable appropriately\n",
    "        if self.mode == 'train':\n",
    "            # Ensure y_variable is present in the data but not included in features\n",
    "            if not all(col in df.columns for col in self.y_variable):\n",
    "                missing_y = [col for col in self.y_variable if col not in df.columns]\n",
    "                self.logger.error(f\"Target variable(s) {missing_y} not found in the input data.\")\n",
    "                raise ValueError(f\"Target variable(s) {missing_y} not found in the input data.\")\n",
    "            # Exclude y_variable from features\n",
    "            desired_features = [col for col in desired_features if col not in self.y_variable]\n",
    "            # Retain y_variable in the filtered DataFrame\n",
    "            filtered_df = df[desired_features + self.y_variable].copy()\n",
    "        else:\n",
    "            # For 'predict' and 'clustering' modes, exclude y_variable\n",
    "            filtered_df = df[desired_features].copy()\n",
    "\n",
    "        # Identify missing features in the input DataFrame\n",
    "        missing_features = [col for col in desired_features if col not in df.columns]\n",
    "        if missing_features:\n",
    "            self.logger.error(f\"The following required features are missing in the input data: {missing_features}\")\n",
    "            raise ValueError(f\"The following required features are missing in the input data: {missing_features}\")\n",
    "\n",
    "        # Log the filtering action\n",
    "        self.logger.info(f\"✅ Filtered DataFrame to include only specified features. Shape: {filtered_df.shape}\")\n",
    "        self.logger.debug(f\"Selected Features: {desired_features}\")\n",
    "        if self.mode == 'train':\n",
    "            self.logger.debug(f\"Retained Target Variable(s): {self.y_variable}\")\n",
    "\n",
    "        return filtered_df\n",
    "\n",
    "    def split_dataset(\n",
    "        self,\n",
    "        X: pd.DataFrame,\n",
    "        y: Optional[pd.Series] = None\n",
    "    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame], Optional[pd.Series], Optional[pd.Series]]:\n",
    "        \"\"\"\n",
    "        Split the dataset into training and testing sets while retaining original indices.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): Features.\n",
    "            y (Optional[pd.Series]): Target variable.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, Optional[pd.DataFrame], Optional[pd.Series], Optional[pd.Series]]: X_train, X_test, y_train, y_test\n",
    "        \"\"\"\n",
    "        step_name = \"split_dataset\"\n",
    "        self.logger.info(\"Step: Split Dataset into Train and Test\")\n",
    "\n",
    "        # Debugging Statements\n",
    "        self._log(f\"Before Split - X shape: {X.shape}\", step_name, 'debug')\n",
    "        if y is not None:\n",
    "            self._log(f\"Before Split - y shape: {y.shape}\", step_name, 'debug')\n",
    "        else:\n",
    "            self._log(\"Before Split - y is None\", step_name, 'debug')\n",
    "\n",
    "        # Determine splitting based on mode\n",
    "        if self.mode == 'train' and self.model_category in ['classification', 'regression']:\n",
    "            if self.model_category == 'classification':\n",
    "                stratify = y if self.options.get('split_dataset', {}).get('stratify_for_classification', False) else None\n",
    "                test_size = self.options.get('split_dataset', {}).get('test_size', 0.2)\n",
    "                random_state = self.options.get('split_dataset', {}).get('random_state', 42)\n",
    "                X_train, X_test, y_train, y_test = train_test_split(\n",
    "                    X, y, \n",
    "                    test_size=test_size,\n",
    "                    stratify=stratify, \n",
    "                    random_state=random_state\n",
    "                )\n",
    "                self._log(\"Performed stratified split for classification.\", step_name, 'debug')\n",
    "            elif self.model_category == 'regression':\n",
    "                X_train, X_test, y_train, y_test = train_test_split(\n",
    "                    X, y, \n",
    "                    test_size=self.options.get('split_dataset', {}).get('test_size', 0.2),\n",
    "                    random_state=self.options.get('split_dataset', {}).get('random_state', 42)\n",
    "                )\n",
    "                self._log(\"Performed random split for regression.\", step_name, 'debug')\n",
    "        else:\n",
    "            # For 'predict' and 'clustering' modes or other categories\n",
    "            X_train = X.copy()\n",
    "            X_test = None\n",
    "            y_train = y.copy() if y is not None else None\n",
    "            y_test = None\n",
    "            self.logger.info(f\"No splitting performed for mode '{self.mode}' or model category '{self.model_category}'.\")\n",
    "\n",
    "        self.preprocessing_steps.append(\"Split Dataset into Train and Test\")\n",
    "\n",
    "        # Keep Indices Aligned Through Each Step\n",
    "        if X_test is not None and y_test is not None:\n",
    "            # Sort both X_test and y_test by index\n",
    "            X_test = X_test.sort_index()\n",
    "            y_test = y_test.sort_index()\n",
    "            self.logger.debug(\"Sorted X_test and y_test by index for alignment.\")\n",
    "\n",
    "        # Debugging: Log post-split shapes and index alignment\n",
    "        self._log(f\"After Split - X_train shape: {X_train.shape}, X_test shape: {X_test.shape if X_test is not None else 'N/A'}\", step_name, 'debug')\n",
    "        if self.model_category == 'classification' and y_train is not None and y_test is not None:\n",
    "            self.logger.debug(f\"Class distribution in y_train:\\n{y_train.value_counts(normalize=True)}\")\n",
    "            self.logger.debug(f\"Class distribution in y_test:\\n{y_test.value_counts(normalize=True)}\")\n",
    "        elif self.model_category == 'regression' and y_train is not None and y_test is not None:\n",
    "            self.logger.debug(f\"y_train statistics:\\n{y_train.describe()}\")\n",
    "            self.logger.debug(f\"y_test statistics:\\n{y_test.describe()}\")\n",
    "\n",
    "        # Check index alignment\n",
    "        if y_train is not None and X_train.index.equals(y_train.index):\n",
    "            self.logger.debug(\"X_train and y_train indices are aligned.\")\n",
    "        else:\n",
    "            self.logger.warning(\"X_train and y_train indices are misaligned.\")\n",
    "\n",
    "        if X_test is not None and y_test is not None and X_test.index.equals(y_test.index):\n",
    "            self.logger.debug(\"X_test and y_test indices are aligned.\")\n",
    "        elif X_test is not None and y_test is not None:\n",
    "            self.logger.warning(\"X_test and y_test indices are misaligned.\")\n",
    "\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "    def handle_missing_values(self, X_train: pd.DataFrame, X_test: Optional[pd.DataFrame] = None) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n",
    "        \"\"\"\n",
    "        Handle missing values for numerical and categorical features based on user options.\n",
    "        \"\"\"\n",
    "        step_name = \"handle_missing_values\"\n",
    "        self.logger.info(\"Step: Handle Missing Values\")\n",
    "\n",
    "        # Fetch user-defined imputation options or set defaults\n",
    "        impute_options = self.options.get('handle_missing_values', {})\n",
    "        numerical_strategy = impute_options.get('numerical_strategy', {})\n",
    "        categorical_strategy = impute_options.get('categorical_strategy', {})\n",
    "\n",
    "        # Numerical Imputation\n",
    "        numerical_imputer = None\n",
    "        new_columns = []\n",
    "        if self.numericals:\n",
    "            if self.model_category in ['regression', 'classification', 'clustering']:\n",
    "                default_num_strategy = 'median'  # Changed to median as per preprocessor_config.yaml\n",
    "            else:\n",
    "                default_num_strategy = 'median'\n",
    "            num_strategy = numerical_strategy.get('strategy', default_num_strategy)\n",
    "            num_imputer_type = numerical_strategy.get('imputer', 'SimpleImputer')  # Can be 'SimpleImputer', 'KNNImputer', etc.\n",
    "\n",
    "            self._log(f\"Numerical Imputation Strategy: {num_strategy.capitalize()}, Imputer Type: {num_imputer_type}\", step_name, 'debug')\n",
    "\n",
    "            # Initialize numerical imputer based on user option\n",
    "            if num_imputer_type == 'SimpleImputer':\n",
    "                numerical_imputer = SimpleImputer(strategy=num_strategy)\n",
    "            elif num_imputer_type == 'KNNImputer':\n",
    "                knn_neighbors = numerical_strategy.get('knn_neighbors', 5)\n",
    "                numerical_imputer = KNNImputer(n_neighbors=knn_neighbors)\n",
    "            else:\n",
    "                self.logger.error(f\"Numerical imputer type '{num_imputer_type}' is not supported.\")\n",
    "                raise ValueError(f\"Numerical imputer type '{num_imputer_type}' is not supported.\")\n",
    "\n",
    "            # Fit and transform ONLY on X_train\n",
    "            X_train[self.numericals] = numerical_imputer.fit_transform(X_train[self.numericals])\n",
    "            self.numerical_imputer = numerical_imputer  # Assign to self for saving\n",
    "            self.feature_reasons.update({col: self.feature_reasons.get(col, '') + f'Numerical: {num_strategy.capitalize()} Imputation | ' for col in self.numericals})\n",
    "            new_columns.extend(self.numericals)\n",
    "\n",
    "            if X_test is not None:\n",
    "                # Transform ONLY on X_test without fitting\n",
    "                X_test[self.numericals] = numerical_imputer.transform(X_test[self.numericals])\n",
    "\n",
    "        # Categorical Imputation\n",
    "        categorical_imputer = None\n",
    "        all_categoricals = self.ordinal_categoricals + self.nominal_categoricals\n",
    "        if all_categoricals:\n",
    "            default_cat_strategy = 'most_frequent'\n",
    "            cat_strategy = categorical_strategy.get('strategy', default_cat_strategy)\n",
    "            cat_imputer_type = categorical_strategy.get('imputer', 'SimpleImputer')\n",
    "\n",
    "            self._log(f\"Categorical Imputation Strategy: {cat_strategy.capitalize()}, Imputer Type: {cat_imputer_type}\", step_name, 'debug')\n",
    "\n",
    "            # Initialize categorical imputer based on user option\n",
    "            if cat_imputer_type == 'SimpleImputer':\n",
    "                categorical_imputer = SimpleImputer(strategy=cat_strategy)\n",
    "            elif cat_imputer_type == 'ConstantImputer':\n",
    "                fill_value = categorical_strategy.get('fill_value', 'Missing')\n",
    "                categorical_imputer = SimpleImputer(strategy='constant', fill_value=fill_value)\n",
    "            else:\n",
    "                self.logger.error(f\"Categorical imputer type '{cat_imputer_type}' is not supported.\")\n",
    "                raise ValueError(f\"Categorical imputer type '{cat_imputer_type}' is not supported.\")\n",
    "\n",
    "            # Fit and transform ONLY on X_train\n",
    "            X_train[all_categoricals] = categorical_imputer.fit_transform(X_train[all_categoricals])\n",
    "            self.categorical_imputer = categorical_imputer  # Assign to self for saving\n",
    "            self.feature_reasons.update({\n",
    "                col: self.feature_reasons.get(col, '') + (f'Categorical: Constant Imputation (Value={categorical_strategy.get(\"fill_value\", \"Missing\")}) | ' if cat_imputer_type == 'ConstantImputer' else f'Categorical: {cat_strategy.capitalize()} Imputation | ')\n",
    "                for col in all_categoricals\n",
    "            })\n",
    "            new_columns.extend(all_categoricals)\n",
    "\n",
    "            if X_test is not None:\n",
    "                # Transform ONLY on X_test without fitting\n",
    "                X_test[all_categoricals] = categorical_imputer.transform(X_test[all_categoricals])\n",
    "\n",
    "        self.preprocessing_steps.append(\"Handle Missing Values\")\n",
    "\n",
    "        # Debugging: Log post-imputation shapes and missing values\n",
    "        self._log(f\"Completed: Handle Missing Values. Dataset shape after imputation: {X_train.shape}\", step_name, 'debug')\n",
    "        self._log(f\"Missing values after imputation in X_train:\\n{X_train.isnull().sum()}\", step_name, 'debug')\n",
    "        self._log(f\"New columns handled: {new_columns}\", step_name, 'debug')\n",
    "\n",
    "        return X_train, X_test\n",
    "\n",
    "    def handle_outliers(self, X_train: pd.DataFrame, y_train: Optional[pd.Series] = None) -> Tuple[pd.DataFrame, Optional[pd.Series]]:\n",
    "        \"\"\"\n",
    "        Handle outliers based on the model's sensitivity and user options.\n",
    "\n",
    "        Args:\n",
    "            X_train (pd.DataFrame): Training features.\n",
    "            y_train (pd.Series, optional): Training target.\n",
    "\n",
    "        Returns:\n",
    "            tuple: X_train without outliers and corresponding y_train.\n",
    "        \"\"\"\n",
    "        step_name = \"handle_outliers\"\n",
    "        self.logger.info(\"Step: Handle Outliers\")\n",
    "        self._log(\"Starting outlier handling.\", step_name, 'debug')\n",
    "\n",
    "        debug_flag = self.get_debug_flag('debug_handle_outliers')\n",
    "        initial_shape = X_train.shape[0]\n",
    "        new_columns = []\n",
    "\n",
    "        # Fetch user-defined outlier handling options or set defaults\n",
    "        outlier_options = self.options.get('handle_outliers', {})\n",
    "        zscore_threshold = outlier_options.get('zscore_threshold', 3)\n",
    "        iqr_multiplier = outlier_options.get('iqr_multiplier', 1.5)\n",
    "        winsor_limits = outlier_options.get('winsor_limits', [0.05, 0.05])\n",
    "        isolation_contamination = outlier_options.get('isolation_contamination', 0.05)\n",
    "\n",
    "        # Check for target leakage: Ensure y_train is not used in transformations\n",
    "        if self.mode == 'train' and y_train is not None:\n",
    "            self._log(\"y_train is present. Confirming it's not used in outlier handling.\", step_name, 'debug')\n",
    "            # Add any specific checks if transformations accidentally use y_train\n",
    "            # For example, ensure that no columns derived from y_train are being modified\n",
    "\n",
    "        for col in self.numericals:\n",
    "            if self.model_category in ['regression', 'classification']:\n",
    "                # Z-Score Filtering\n",
    "                apply_zscore = outlier_options.get('apply_zscore', True)\n",
    "                if apply_zscore:\n",
    "                    z_scores = np.abs((X_train[col] - X_train[col].mean()) / X_train[col].std())\n",
    "                    mask_z = z_scores < zscore_threshold\n",
    "                    removed_z = (~mask_z).sum()\n",
    "                    X_train = X_train[mask_z]\n",
    "                    if y_train is not None:\n",
    "                        y_train = y_train.loc[X_train.index]\n",
    "                    self.feature_reasons[col] += f'Outliers handled with Z-Score Filtering (threshold={zscore_threshold}) | '\n",
    "                    self._log(f\"Removed {removed_z} outliers from '{col}' using Z-Score Filtering\", step_name, 'debug')\n",
    "\n",
    "                # IQR Filtering\n",
    "                apply_iqr = outlier_options.get('apply_iqr', True)\n",
    "                if apply_iqr:\n",
    "                    Q1 = X_train[col].quantile(0.25)\n",
    "                    Q3 = X_train[col].quantile(0.75)\n",
    "                    IQR = Q3 - Q1\n",
    "                    lower_bound = Q1 - iqr_multiplier * IQR\n",
    "                    upper_bound = Q3 + iqr_multiplier * IQR\n",
    "                    mask_iqr = (X_train[col] >= lower_bound) & (X_train[col] <= upper_bound)\n",
    "                    removed_iqr = (~mask_iqr).sum()\n",
    "                    X_train = X_train[mask_iqr]\n",
    "                    if y_train is not None:\n",
    "                        y_train = y_train.loc[X_train.index]\n",
    "                    self.feature_reasons[col] += f'Outliers handled with IQR Filtering (multiplier={iqr_multiplier}) | '\n",
    "                    self._log(f\"Removed {removed_iqr} outliers from '{col}' using IQR Filtering\", step_name, 'debug')\n",
    "\n",
    "            elif self.model_category == 'clustering':\n",
    "                # For clustering, apply IsolationForest for outlier detection\n",
    "                contamination = outlier_options.get('isolation_contamination', 0.05)\n",
    "                iso_forest = IsolationForest(contamination=contamination, random_state=42)\n",
    "                preds = iso_forest.fit_predict(X_train[[col]])\n",
    "                mask_iso = preds != -1\n",
    "                removed_iso = (preds == -1).sum()\n",
    "                X_train = X_train[mask_iso]\n",
    "                self.feature_reasons[col] += f'Outliers handled with IsolationForest (contamination={contamination}) | '\n",
    "                self._log(f\"Removed {removed_iso} outliers from '{col}' using IsolationForest\", step_name, 'debug')\n",
    "\n",
    "            else:\n",
    "                self.logger.warning(f\"Model category '{self.model_category}' not recognized for outlier handling.\")\n",
    "\n",
    "        self.preprocessing_steps.append(\"Handle Outliers\")\n",
    "\n",
    "        # Completion Logging\n",
    "        self._log(f\"Completed: Handle Outliers. Initial samples: {initial_shape}, Final samples: {X_train.shape[0]}\", step_name, 'debug')\n",
    "        self._log(f\"Missing values after outlier handling in X_train:\\n{X_train.isnull().sum()}\", step_name, 'debug')\n",
    "        self._log(f\"Outlier handling applied on columns: {new_columns}\", step_name, 'debug')\n",
    "\n",
    "        return X_train, y_train\n",
    "\n",
    "    def test_normality(self, X_train: pd.DataFrame) -> Dict[str, Dict]:\n",
    "        \"\"\"\n",
    "        Test normality for numerical features based on normality tests and user options.\n",
    "\n",
    "        Args:\n",
    "            X_train (pd.DataFrame): Training features.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, Dict]: Dictionary with normality test results for each numerical feature.\n",
    "        \"\"\"\n",
    "        step_name = \"Test for Normality\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "        debug_flag = self.get_debug_flag('debug_test_normality')\n",
    "        normality_results = {}\n",
    "\n",
    "        # Fetch user-defined normality test options or set defaults\n",
    "        normality_options = self.options.get('test_normality', {})\n",
    "        p_value_threshold = normality_options.get('p_value_threshold', 0.05)\n",
    "        skewness_threshold = normality_options.get('skewness_threshold', 1.0)\n",
    "        additional_tests = normality_options.get('additional_tests', [])  # e.g., ['anderson-darling']\n",
    "\n",
    "        for col in self.numericals:\n",
    "            data = X_train[col].dropna()\n",
    "            skewness = data.skew()\n",
    "            kurtosis = data.kurtosis()\n",
    "\n",
    "            # Determine which normality test to use based on sample size and user options\n",
    "            test_used = 'Shapiro-Wilk'\n",
    "            p_value = 0.0\n",
    "\n",
    "            if len(data) <= 5000:\n",
    "                from scipy.stats import shapiro\n",
    "                stat, p_val = shapiro(data)\n",
    "                test_used = 'Shapiro-Wilk'\n",
    "                p_value = p_val\n",
    "            else:\n",
    "                from scipy.stats import anderson\n",
    "                result = anderson(data)\n",
    "                test_used = 'Anderson-Darling'\n",
    "                # Determine p-value based on critical values\n",
    "                p_value = 0.0  # Default to 0\n",
    "                for cv, sig in zip(result.critical_values, result.significance_level):\n",
    "                    if result.statistic < cv:\n",
    "                        p_value = sig / 100\n",
    "                        break\n",
    "\n",
    "            # Apply user-defined or default criteria\n",
    "            if self.model_category in ['regression', 'classification', 'clustering']:\n",
    "                # Linear, Logistic Regression, and Clustering: Use p-value and skewness\n",
    "                needs_transform = (p_value < p_value_threshold) or (abs(skewness) > skewness_threshold)\n",
    "            else:\n",
    "                # Other models: Use skewness, and optionally p-values based on options\n",
    "                use_p_value = normality_options.get('use_p_value_other_models', False)\n",
    "                if use_p_value:\n",
    "                    needs_transform = (p_value < p_value_threshold) or (abs(skewness) > skewness_threshold)\n",
    "                else:\n",
    "                    needs_transform = abs(skewness) > skewness_threshold\n",
    "\n",
    "            normality_results[col] = {\n",
    "                'skewness': skewness,\n",
    "                'kurtosis': kurtosis,\n",
    "                'p_value': p_value,\n",
    "                'test_used': test_used,\n",
    "                'needs_transform': needs_transform\n",
    "            }\n",
    "\n",
    "            # Conditional Detailed Logging\n",
    "            if debug_flag:\n",
    "                self._log(f\"Feature '{col}': p-value={p_value:.4f}, skewness={skewness:.4f}, needs_transform={needs_transform}\", step_name, 'debug')\n",
    "\n",
    "        self.normality_results = normality_results\n",
    "        self.preprocessing_steps.append(step_name)\n",
    "\n",
    "        # Completion Logging\n",
    "        if debug_flag:\n",
    "            self._log(f\"Completed: {step_name}. Normality results computed.\", step_name, 'debug')\n",
    "        else:\n",
    "            self.logger.info(f\"Step '{step_name}' completed: Normality results computed.\")\n",
    "\n",
    "        return normality_results\n",
    "\n",
    "    def encode_categorical_variables(self, X_train: pd.DataFrame, X_test: Optional[pd.DataFrame] = None) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n",
    "        \"\"\"\n",
    "        Encode categorical variables using user-specified encoding strategies.\n",
    "        \"\"\"\n",
    "        step_name = \"encode_categorical_variables\"\n",
    "        self.logger.info(\"Step: Encode Categorical Variables\")\n",
    "        self._log(\"Starting categorical variable encoding.\", step_name, 'debug')\n",
    "\n",
    "        # Fetch user-defined encoding options or set defaults\n",
    "        encoding_options = self.options.get('encode_categoricals', {})\n",
    "        ordinal_encoding = encoding_options.get('ordinal_encoding', 'OrdinalEncoder')  # Options: 'OrdinalEncoder', 'None'\n",
    "        nominal_encoding = encoding_options.get('nominal_encoding', 'OneHotEncoder')  # Changed from 'OneHotEncoder' to 'OrdinalEncoder'\n",
    "        handle_unknown = encoding_options.get('handle_unknown', 'use_encoded_value')  # Adjusted for OrdinalEncoder\n",
    "\n",
    "        # Determine if SMOTENC is being used\n",
    "        smote_variant = self.options.get('implement_smote', {}).get('variant', None)\n",
    "        if smote_variant == 'SMOTENC':\n",
    "            nominal_encoding = 'OrdinalEncoder'  # Ensure compatibility\n",
    "\n",
    "        transformers = []\n",
    "        new_columns = []\n",
    "        if self.ordinal_categoricals and ordinal_encoding != 'None':\n",
    "            if ordinal_encoding == 'OrdinalEncoder':\n",
    "                transformers.append(\n",
    "                    ('ordinal', OrdinalEncoder(), self.ordinal_categoricals)\n",
    "                )\n",
    "                self._log(f\"Added OrdinalEncoder for features: {self.ordinal_categoricals}\", step_name, 'debug')\n",
    "            else:\n",
    "                self.logger.error(f\"Ordinal encoding method '{ordinal_encoding}' is not supported.\")\n",
    "                raise ValueError(f\"Ordinal encoding method '{ordinal_encoding}' is not supported.\")\n",
    "        if self.nominal_categoricals and nominal_encoding != 'None':\n",
    "            if nominal_encoding == 'OrdinalEncoder':\n",
    "                transformers.append(\n",
    "                    ('nominal', OrdinalEncoder(handle_unknown=handle_unknown), self.nominal_categoricals)\n",
    "                )\n",
    "                self._log(f\"Added OrdinalEncoder for features: {self.nominal_categoricals}\", step_name, 'debug')\n",
    "            elif nominal_encoding == 'FrequencyEncoder':\n",
    "                # Custom Frequency Encoding\n",
    "                for col in self.nominal_categoricals:\n",
    "                    freq = X_train[col].value_counts(normalize=True)\n",
    "                    X_train[col] = X_train[col].map(freq)\n",
    "                    if X_test is not None:\n",
    "                        X_test[col] = X_test[col].map(freq).fillna(0)\n",
    "                    self.feature_reasons[col] += 'Encoded with Frequency Encoding | '\n",
    "                    self._log(f\"Applied Frequency Encoding to '{col}'.\", step_name, 'debug')\n",
    "            else:\n",
    "                self.logger.error(f\"Nominal encoding method '{nominal_encoding}' is not supported.\")\n",
    "                raise ValueError(f\"Nominal encoding method '{nominal_encoding}' is not supported.\")\n",
    "\n",
    "        if not transformers and 'FrequencyEncoder' not in nominal_encoding:\n",
    "            self.logger.info(\"No categorical variables to encode.\")\n",
    "            self.preprocessing_steps.append(\"Encode Categorical Variables\")\n",
    "            self._log(f\"Completed: Encode Categorical Variables. No encoding was applied.\", step_name, 'debug')\n",
    "            return X_train, X_test\n",
    "\n",
    "        if transformers:\n",
    "            self.preprocessor = ColumnTransformer(\n",
    "                transformers=transformers,\n",
    "                remainder='passthrough'  # Keep other columns unchanged\n",
    "            )\n",
    "\n",
    "            # Fit and transform training data\n",
    "            X_train_encoded = self.preprocessor.fit_transform(X_train)\n",
    "            self._log(\"Fitted and transformed X_train with ColumnTransformer.\", step_name, 'debug')\n",
    "\n",
    "            # Transform testing data\n",
    "            if X_test is not None:\n",
    "                X_test_encoded = self.preprocessor.transform(X_test)\n",
    "                self._log(\"Transformed X_test with fitted ColumnTransformer.\", step_name, 'debug')\n",
    "            else:\n",
    "                X_test_encoded = None\n",
    "\n",
    "            # Retrieve feature names after encoding\n",
    "            encoded_feature_names = []\n",
    "            if self.ordinal_categoricals and ordinal_encoding == 'OrdinalEncoder':\n",
    "                encoded_feature_names += self.ordinal_categoricals\n",
    "            if self.nominal_categoricals and nominal_encoding == 'OrdinalEncoder':\n",
    "                encoded_feature_names += self.nominal_categoricals\n",
    "            elif self.nominal_categoricals and nominal_encoding == 'FrequencyEncoder':\n",
    "                encoded_feature_names += self.nominal_categoricals\n",
    "            passthrough_features = [col for col in X_train.columns if col not in self.ordinal_categoricals + self.nominal_categoricals]\n",
    "            encoded_feature_names += passthrough_features\n",
    "            new_columns.extend(encoded_feature_names)\n",
    "\n",
    "            # Convert numpy arrays back to DataFrames\n",
    "            X_train_encoded_df = pd.DataFrame(X_train_encoded, columns=encoded_feature_names, index=X_train.index)\n",
    "            if X_test_encoded is not None:\n",
    "                X_test_encoded_df = pd.DataFrame(X_test_encoded, columns=encoded_feature_names, index=X_test.index)\n",
    "            else:\n",
    "                X_test_encoded_df = None\n",
    "\n",
    "            # Store encoders for inverse transformation\n",
    "            self.ordinal_encoder = self.preprocessor.named_transformers_.get('ordinal', None)\n",
    "            self.nominal_encoder = self.preprocessor.named_transformers_.get('nominal', None)\n",
    "\n",
    "            self.preprocessing_steps.append(\"Encode Categorical Variables\")\n",
    "            self._log(f\"Completed: Encode Categorical Variables. X_train_encoded shape: {X_train_encoded_df.shape}\", step_name, 'debug')\n",
    "            self._log(f\"Columns after encoding: {encoded_feature_names}\", step_name, 'debug')\n",
    "            self._log(f\"Sample of encoded X_train:\\n{X_train_encoded_df.head()}\", step_name, 'debug')\n",
    "            self._log(f\"New columns added: {new_columns}\", step_name, 'debug')\n",
    "\n",
    "            return X_train_encoded_df, X_test_encoded_df\n",
    "\n",
    "    def generate_recommendations(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Generate a table of preprocessing recommendations based on the model type, data, and user options.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame containing recommendations for each feature.\n",
    "        \"\"\"\n",
    "        step_name = \"Generate Preprocessor Recommendations\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "        debug_flag = self.get_debug_flag('debug_generate_recommendations')\n",
    "\n",
    "        # Generate recommendations based on feature reasons\n",
    "        recommendations = {}\n",
    "        for col in self.ordinal_categoricals + self.nominal_categoricals + self.numericals:\n",
    "            reasons = self.feature_reasons.get(col, '').strip(' | ')\n",
    "            recommendations[col] = reasons\n",
    "\n",
    "        recommendations_table = pd.DataFrame.from_dict(\n",
    "            recommendations, \n",
    "            orient='index', \n",
    "            columns=['Preprocessing Reason']\n",
    "        )\n",
    "        if debug_flag:\n",
    "            self.logger.debug(f\"Preprocessing Recommendations:\\n{recommendations_table}\")\n",
    "        else:\n",
    "            self.logger.info(\"Preprocessing Recommendations generated.\")\n",
    "\n",
    "        self.preprocessing_steps.append(step_name)\n",
    "\n",
    "        # Completion Logging\n",
    "        if debug_flag:\n",
    "            self._log(f\"Completed: {step_name}. Recommendations generated.\", step_name, 'debug')\n",
    "        else:\n",
    "            self.logger.info(f\"Step '{step_name}' completed: Recommendations generated.\")\n",
    "\n",
    "        return recommendations_table\n",
    "\n",
    "    def save_transformers(self):\n",
    "        step_name = \"Save Transformers\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "        debug_flag = self.get_debug_flag('debug_save_transformers')\n",
    "        \n",
    "        # Ensure the transformers directory exists\n",
    "        os.makedirs(self.transformers_dir, exist_ok=True)\n",
    "        transformers_path = os.path.join(self.transformers_dir, 'transformers.pkl')  # Consistent file path\n",
    "        \n",
    "        transformers = {\n",
    "            'numerical_imputer': getattr(self, 'numerical_imputer', None),\n",
    "            'categorical_imputer': getattr(self, 'categorical_imputer', None),\n",
    "            'preprocessor': self.pipeline,   # Includes all preprocessing steps\n",
    "            'smote': self.smote,\n",
    "            'final_feature_order': self.final_feature_order,\n",
    "            'categorical_indices': getattr(self, 'categorical_indices', [])\n",
    "        }\n",
    "        try:\n",
    "            joblib.dump(transformers, transformers_path)\n",
    "            if debug_flag:\n",
    "                self._log(f\"Transformers saved at '{transformers_path}'.\", step_name, 'debug')\n",
    "            else:\n",
    "                self.logger.info(f\"Transformers saved at '{transformers_path}'.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Failed to save transformers: {e}\")\n",
    "            raise\n",
    "\n",
    "        self.preprocessing_steps.append(step_name)\n",
    "\n",
    "    def load_transformers(self) -> dict:\n",
    "        step_name = \"Load Transformers\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "        debug_flag = self.get_debug_flag('debug_load_transformers')  # Assuming a step-specific debug flag\n",
    "        transformers_path = os.path.join(self.transformers_dir, 'transformers.pkl')  # Correct path\n",
    "\n",
    "        # Debug log\n",
    "        self.logger.debug(f\"Loading transformers from: {transformers_path}\")\n",
    "\n",
    "        if not os.path.exists(transformers_path):\n",
    "            self.logger.error(f\"❌ Transformers file not found at '{transformers_path}'. Cannot proceed with prediction.\")\n",
    "            raise FileNotFoundError(f\"Transformers file not found at '{transformers_path}'.\")\n",
    "\n",
    "        try:\n",
    "            transformers = joblib.load(transformers_path)\n",
    "\n",
    "            # Extract transformers\n",
    "            numerical_imputer = transformers.get('numerical_imputer')\n",
    "            categorical_imputer = transformers.get('categorical_imputer')\n",
    "            preprocessor = transformers.get('preprocessor')\n",
    "            smote = transformers.get('smote', None)\n",
    "            final_feature_order = transformers.get('final_feature_order', [])\n",
    "            categorical_indices = transformers.get('categorical_indices', [])\n",
    "            self.categorical_indices = categorical_indices  # Set the attribute\n",
    "\n",
    "            # **Post-Loading Debugging:**\n",
    "            if preprocessor is not None:\n",
    "                try:\n",
    "                    # Do not attempt to transform dummy data here\n",
    "                    self.logger.debug(f\"Pipeline loaded. Ready to transform new data.\")\n",
    "                except AttributeError as e:\n",
    "                    self.logger.error(f\"Pipeline's `get_feature_names_out` is not available: {e}\")\n",
    "                    expected_features = []\n",
    "            else:\n",
    "                self.logger.error(\"❌ Preprocessor is not loaded.\")\n",
    "                raise AttributeError(\"Preprocessor is not loaded.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Failed to load transformers: {e}\")\n",
    "            raise\n",
    "\n",
    "        self.preprocessing_steps.append(step_name)\n",
    "\n",
    "        # Additional checks\n",
    "        if preprocessor is None:\n",
    "            self.logger.error(\"❌ Preprocessor is not loaded.\")\n",
    "\n",
    "        if debug_flag:\n",
    "            self._log(f\"Transformers loaded successfully from '{transformers_path}'.\", step_name, 'debug')\n",
    "        else:\n",
    "            self.logger.info(f\"Transformers loaded successfully from '{transformers_path}'.\")\n",
    "\n",
    "        # Set the pipeline\n",
    "        self.pipeline = preprocessor\n",
    "\n",
    "        # Return the transformers as a dictionary\n",
    "        return {\n",
    "            'numerical_imputer': numerical_imputer,\n",
    "            'categorical_imputer': categorical_imputer,\n",
    "            'preprocessor': preprocessor,\n",
    "            'smote': smote,\n",
    "            'final_feature_order': final_feature_order,\n",
    "            'categorical_indices': categorical_indices\n",
    "        }\n",
    "\n",
    "    def apply_scaling(self, X_train: pd.DataFrame, X_test: Optional[pd.DataFrame] = None) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n",
    "        \"\"\"\n",
    "        Apply scaling based on the model type and user options.\n",
    "\n",
    "        Args:\n",
    "            X_train (pd.DataFrame): Training features.\n",
    "            X_test (Optional[pd.DataFrame]): Testing features.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, Optional[pd.DataFrame]]: Scaled X_train and X_test.\n",
    "        \"\"\"\n",
    "        step_name = \"Apply Scaling (If Needed by Model)\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "        debug_flag = self.get_debug_flag('debug_apply_scaling')\n",
    "\n",
    "        # Fetch user-defined scaling options or set defaults\n",
    "        scaling_options = self.options.get('apply_scaling', {})\n",
    "        scaling_method = scaling_options.get('method', None)  # 'StandardScaler', 'MinMaxScaler', 'RobustScaler', 'None'\n",
    "        features_to_scale = scaling_options.get('features', self.numericals)\n",
    "\n",
    "        scaler = None\n",
    "        scaling_type = 'None'\n",
    "\n",
    "        if scaling_method is None:\n",
    "            # Default scaling based on model category\n",
    "            if self.model_category in ['regression', 'classification', 'clustering']:\n",
    "                # For clustering, MinMaxScaler is generally preferred\n",
    "                if self.model_category == 'clustering':\n",
    "                    scaler = MinMaxScaler()\n",
    "                    scaling_type = 'MinMaxScaler'\n",
    "                else:\n",
    "                    scaler = StandardScaler()\n",
    "                    scaling_type = 'StandardScaler'\n",
    "            else:\n",
    "                scaler = None\n",
    "                scaling_type = 'None'\n",
    "        else:\n",
    "            # Normalize the scaling_method string to handle case-insensitivity\n",
    "            scaling_method_normalized = scaling_method.lower()\n",
    "            if scaling_method_normalized == 'standardscaler':\n",
    "                scaler = StandardScaler()\n",
    "                scaling_type = 'StandardScaler'\n",
    "            elif scaling_method_normalized == 'minmaxscaler':\n",
    "                scaler = MinMaxScaler()\n",
    "                scaling_type = 'MinMaxScaler'\n",
    "            elif scaling_method_normalized == 'robustscaler':\n",
    "                scaler = RobustScaler()\n",
    "                scaling_type = 'RobustScaler'\n",
    "            elif scaling_method_normalized == 'none':\n",
    "                scaler = None\n",
    "                scaling_type = 'None'\n",
    "            else:\n",
    "                self.logger.error(f\"Scaling method '{scaling_method}' is not supported.\")\n",
    "                raise ValueError(f\"Scaling method '{scaling_method}' is not supported.\")\n",
    "\n",
    "        # Apply scaling if scaler is defined\n",
    "        if scaler is not None and features_to_scale:\n",
    "            self.scaler = scaler\n",
    "            if debug_flag:\n",
    "                self._log(f\"Features to scale: {features_to_scale}\", step_name, 'debug')\n",
    "\n",
    "            # Check if features exist in the dataset\n",
    "            missing_features = [feat for feat in features_to_scale if feat not in X_train.columns]\n",
    "            if missing_features:\n",
    "                self.logger.error(f\"The following features specified for scaling are missing in the dataset: {missing_features}\")\n",
    "                raise KeyError(f\"The following features specified for scaling are missing in the dataset: {missing_features}\")\n",
    "\n",
    "            X_train[features_to_scale] = scaler.fit_transform(X_train[features_to_scale])\n",
    "            if X_test is not None:\n",
    "                X_test[features_to_scale] = scaler.transform(X_test[features_to_scale])\n",
    "\n",
    "            for col in features_to_scale:\n",
    "                self.feature_reasons[col] += f'Scaling Applied: {scaling_type} | '\n",
    "\n",
    "            self.preprocessing_steps.append(step_name)\n",
    "            if debug_flag:\n",
    "                self._log(f\"Applied {scaling_type} to features: {features_to_scale}\", step_name, 'debug')\n",
    "                if hasattr(scaler, 'mean_'):\n",
    "                    self._log(f\"Scaler Parameters: mean={scaler.mean_}\", step_name, 'debug')\n",
    "                if hasattr(scaler, 'scale_'):\n",
    "                    self._log(f\"Scaler Parameters: scale={scaler.scale_}\", step_name, 'debug')\n",
    "                self._log(f\"Sample of scaled X_train:\\n{X_train[features_to_scale].head()}\", step_name, 'debug')\n",
    "                if X_test is not None:\n",
    "                    self._log(f\"Sample of scaled X_test:\\n{X_test[features_to_scale].head()}\", step_name, 'debug')\n",
    "            else:\n",
    "                self.logger.info(f\"Step '{step_name}' completed: Applied {scaling_type} to features: {features_to_scale}\")\n",
    "        else:\n",
    "            self.logger.info(\"No scaling applied based on user options or no features specified.\")\n",
    "            self.preprocessing_steps.append(step_name)\n",
    "            if debug_flag:\n",
    "                self._log(f\"Completed: {step_name}. No scaling was applied.\", step_name, 'debug')\n",
    "            else:\n",
    "                self.logger.info(f\"Step '{step_name}' completed: No scaling was applied.\")\n",
    "\n",
    "        return X_train, X_test\n",
    "\n",
    "    def implement_smote(self, X_train: pd.DataFrame, y_train: pd.Series) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "        \"\"\"\n",
    "        Implement SMOTE or its variants based on class imbalance.\n",
    "\n",
    "        Args:\n",
    "            X_train (pd.DataFrame): Training features (transformed).\n",
    "            y_train (pd.Series): Training target.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, pd.Series]: Resampled X_train and y_train.\n",
    "        \"\"\"\n",
    "        step_name = \"Implement SMOTE (Train Only)\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "\n",
    "        # Check if classification\n",
    "        if self.model_category != 'classification':\n",
    "            self.logger.info(\"SMOTE not applicable: Not a classification model.\")\n",
    "            self.preprocessing_steps.append(\"SMOTE Skipped\")\n",
    "            return X_train, y_train\n",
    "\n",
    "        # Calculate class distribution\n",
    "        class_counts = y_train.value_counts()\n",
    "        if len(class_counts) < 2:\n",
    "            self.logger.warning(\"SMOTE not applicable: Only one class present.\")\n",
    "            self.preprocessing_steps.append(\"SMOTE Skipped\")\n",
    "            return X_train, y_train\n",
    "\n",
    "        majority_class = class_counts.idxmax()\n",
    "        minority_class = class_counts.idxmin()\n",
    "        majority_count = class_counts.max()\n",
    "        minority_count = class_counts.min()\n",
    "        imbalance_ratio = minority_count / majority_count\n",
    "        self.logger.info(f\"Class Distribution before SMOTE: {class_counts.to_dict()}\")\n",
    "        self.logger.info(f\"Imbalance Ratio (Minority/Majority): {imbalance_ratio:.4f}\")\n",
    "\n",
    "        # Determine SMOTE variant based on dataset composition\n",
    "        has_numericals = len(self.numericals) > 0\n",
    "        has_categoricals = len(self.ordinal_categoricals) + len(self.nominal_categoricals) > 0\n",
    "\n",
    "        # Automatically select SMOTE variant\n",
    "        if has_numericals and has_categoricals:\n",
    "            smote_variant = 'SMOTENC'\n",
    "            self.logger.info(\"Dataset contains both numerical and categorical features. Using SMOTENC.\")\n",
    "        elif has_numericals and not has_categoricals:\n",
    "            smote_variant = 'SMOTE'\n",
    "            self.logger.info(\"Dataset contains only numerical features. Using SMOTE.\")\n",
    "        elif has_categoricals and not has_numericals:\n",
    "            smote_variant = 'SMOTEN'\n",
    "            self.logger.info(\"Dataset contains only categorical features. Using SMOTEN.\")\n",
    "        else:\n",
    "            smote_variant = 'SMOTE'  # Fallback\n",
    "            self.logger.info(\"Feature composition unclear. Using SMOTE as default.\")\n",
    "\n",
    "        # Initialize SMOTE based on the variant\n",
    "        if smote_variant == 'SMOTENC':\n",
    "            if not self.categorical_indices:\n",
    "                # Determine categorical indices if not already set\n",
    "                categorical_features = []\n",
    "                for name, transformer, features in self.pipeline.transformers_:\n",
    "                    if 'ord' in name or 'nominal' in name:\n",
    "                        if isinstance(transformer, Pipeline):\n",
    "                            encoder = transformer.named_steps.get('ordinal_encoder') or transformer.named_steps.get('onehot_encoder')\n",
    "                            if hasattr(encoder, 'categories_'):\n",
    "                                categorical_features.extend(range(len(encoder.categories_)))\n",
    "                self.categorical_indices = categorical_features\n",
    "                self.logger.debug(f\"Categorical feature indices for SMOTENC: {self.categorical_indices}\")\n",
    "            smote = SMOTENC(categorical_features=self.categorical_indices, random_state=42)\n",
    "            self.logger.debug(f\"Initialized SMOTENC with categorical features indices: {self.categorical_indices}\")\n",
    "        elif smote_variant == 'SMOTEN':\n",
    "            smote = SMOTEN(random_state=42)\n",
    "            self.logger.debug(\"Initialized SMOTEN.\")\n",
    "        else:\n",
    "            smote = SMOTE(random_state=42)\n",
    "            self.logger.debug(\"Initialized SMOTE.\")\n",
    "\n",
    "        # Apply SMOTE\n",
    "        try:\n",
    "            X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "            self.logger.info(f\"Applied {smote_variant}. Resampled dataset shape: {X_resampled.shape}\")\n",
    "            self.preprocessing_steps.append(\"Implement SMOTE\")\n",
    "            self.smote = smote  # Assign to self for saving\n",
    "            return X_resampled, y_resampled\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ SMOTE application failed: {e}\")\n",
    "            raise\n",
    "\n",
    "    def inverse_transform_data(self, X_transformed: np.ndarray) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Perform inverse transformation on the transformed data to reconstruct original feature values.\n",
    "\n",
    "        Args:\n",
    "            X_transformed (np.ndarray): The transformed feature data.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: The inverse-transformed DataFrame.\n",
    "        \"\"\"\n",
    "        if self.pipeline is None:\n",
    "            self.logger.error(\"Preprocessing pipeline has not been fitted. Cannot perform inverse transformation.\")\n",
    "            raise AttributeError(\"Preprocessing pipeline has not been fitted. Cannot perform inverse transformation.\")\n",
    "\n",
    "        preprocessor = self.pipeline\n",
    "        logger = logging.getLogger('InverseTransform')\n",
    "        if self.debug:\n",
    "            logger.setLevel(logging.DEBUG)\n",
    "        else:\n",
    "            logger.setLevel(logging.INFO)\n",
    "\n",
    "        logger.debug(\"Starting inverse transformation.\")\n",
    "\n",
    "        # Initialize dictionaries to hold inverse-transformed data\n",
    "        inverse_data = {}\n",
    "\n",
    "        # Initialize index tracker\n",
    "        start_idx = 0\n",
    "\n",
    "        # Iterate through each transformer in the ColumnTransformer\n",
    "        for name, transformer, features in preprocessor.transformers_:\n",
    "            if name == 'remainder':\n",
    "                continue  # Skip any remainder features\n",
    "            # Extract the transformed data for current transformer\n",
    "            if name == 'num':\n",
    "                end_idx = start_idx + len(features)\n",
    "                numerical_data = X_transformed[:, start_idx:end_idx]\n",
    "                scaler = transformer.named_steps.get('scaler', None)\n",
    "                if scaler and hasattr(scaler, 'inverse_transform'):\n",
    "                    numerical_inverse = scaler.inverse_transform(numerical_data)\n",
    "                    inverse_data.update({feature: numerical_inverse[:, idx] for idx, feature in enumerate(features)})\n",
    "                    logger.debug(f\"Numerical features {features} inverse transformed.\")\n",
    "                else:\n",
    "                    logger.warning(f\"No scaler found for numerical features {features}. Skipping inverse transformation.\")\n",
    "                start_idx = end_idx\n",
    "            elif name == 'ord':\n",
    "                ordinal_encoder = transformer.named_steps.get('ordinal_encoder', None)\n",
    "                if ordinal_encoder and hasattr(ordinal_encoder, 'inverse_transform'):\n",
    "                    ordinal_inverse = ordinal_encoder.inverse_transform(X_transformed[:, start_idx:start_idx + len(features)])\n",
    "                    inverse_data.update({feature: ordinal_inverse[:, idx] for idx, feature in enumerate(features)})\n",
    "                    logger.debug(f\"Ordinal features {features} inverse transformed.\")\n",
    "                    start_idx += len(features)\n",
    "                else:\n",
    "                    logger.warning(f\"No OrdinalEncoder found for features {features}. Skipping inverse transformation.\")\n",
    "            elif name.startswith('nom'):\n",
    "                onehot_encoder = transformer.named_steps.get('onehot_encoder', None)\n",
    "                if onehot_encoder and hasattr(onehot_encoder, 'inverse_transform'):\n",
    "                    # Determine the number of categories for this encoder\n",
    "                    n_categories = len(onehot_encoder.categories_[0])\n",
    "                    nominal_data = X_transformed[:, start_idx:start_idx + n_categories]\n",
    "                    nominal_inverse = onehot_encoder.inverse_transform(nominal_data)\n",
    "                    inverse_data.update({features[0]: nominal_inverse[:, 0]})\n",
    "                    logger.debug(f\"Nominal feature '{features[0]}' inverse transformed.\")\n",
    "                    start_idx += n_categories\n",
    "                else:\n",
    "                    logger.warning(f\"No OneHotEncoder found for nominal features {features}. Skipping inverse transformation.\")\n",
    "            else:\n",
    "                logger.warning(f\"Unknown transformer '{name}'. Skipping inversion.\")\n",
    "\n",
    "        # Create the inverse-transformed DataFrame\n",
    "        inverse_df = pd.DataFrame(inverse_data)\n",
    "\n",
    "        logger.debug(\"Inverse-transformed DataFrame constructed.\")\n",
    "        logger.debug(f\"Inverse-transformed DataFrame shape: {inverse_df.shape}\")\n",
    "\n",
    "        logger.info(\"✅ Inverse transformation completed successfully.\")\n",
    "\n",
    "        return inverse_df\n",
    "\n",
    "    def build_pipeline(self, X_train: pd.DataFrame) -> ColumnTransformer:\n",
    "        transformers = []\n",
    "\n",
    "        # Handle Numerical Features\n",
    "        if self.numericals:\n",
    "            numerical_strategy = self.options.get('handle_missing_values', {}).get('numerical_strategy', {}).get('strategy', 'median')\n",
    "            numerical_imputer = self.options.get('handle_missing_values', {}).get('numerical_strategy', {}).get('imputer', 'SimpleImputer')\n",
    "\n",
    "            if numerical_imputer == 'SimpleImputer':\n",
    "                num_imputer = SimpleImputer(strategy=numerical_strategy)\n",
    "            elif numerical_imputer == 'KNNImputer':\n",
    "                knn_neighbors = self.options.get('handle_missing_values', {}).get('numerical_strategy', {}).get('knn_neighbors', 5)\n",
    "                num_imputer = KNNImputer(n_neighbors=knn_neighbors)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported numerical imputer type: {numerical_imputer}\")\n",
    "\n",
    "            # Determine scaling method\n",
    "            scaling_method = self.options.get('apply_scaling', {}).get('method', None)\n",
    "            if scaling_method is None:\n",
    "                # Default scaling based on model category\n",
    "                if self.model_category in ['regression', 'classification', 'clustering']:\n",
    "                    # For clustering, MinMaxScaler is generally preferred\n",
    "                    if self.model_category == 'clustering':\n",
    "                        scaler = MinMaxScaler()\n",
    "                        scaling_type = 'MinMaxScaler'\n",
    "                    else:\n",
    "                        scaler = StandardScaler()\n",
    "                        scaling_type = 'StandardScaler'\n",
    "                else:\n",
    "                    scaler = 'passthrough'\n",
    "                    scaling_type = 'None'\n",
    "            else:\n",
    "                # Normalize the scaling_method string to handle case-insensitivity\n",
    "                scaling_method_normalized = scaling_method.lower()\n",
    "                if scaling_method_normalized == 'standardscaler':\n",
    "                    scaler = StandardScaler()\n",
    "                    scaling_type = 'StandardScaler'\n",
    "                elif scaling_method_normalized == 'minmaxscaler':\n",
    "                    scaler = MinMaxScaler()\n",
    "                    scaling_type = 'MinMaxScaler'\n",
    "                elif scaling_method_normalized == 'robustscaler':\n",
    "                    scaler = RobustScaler()\n",
    "                    scaling_type = 'RobustScaler'\n",
    "                elif scaling_method_normalized == 'none':\n",
    "                    scaler = 'passthrough'\n",
    "                    scaling_type = 'None'\n",
    "                else:\n",
    "                    raise ValueError(f\"Unsupported scaling method: {scaling_method}\")\n",
    "\n",
    "            numerical_transformer = Pipeline(steps=[\n",
    "                ('imputer', num_imputer),\n",
    "                ('scaler', scaler)\n",
    "            ])\n",
    "\n",
    "            transformers.append(('num', numerical_transformer, self.numericals))\n",
    "            self.logger.debug(f\"Numerical transformer added with imputer '{numerical_imputer}' and scaler '{scaling_type}'.\")\n",
    "\n",
    "        # Handle Ordinal Categorical Features\n",
    "        if self.ordinal_categoricals:\n",
    "            ordinal_strategy = self.options.get('encode_categoricals', {}).get('ordinal_encoding', 'OrdinalEncoder')\n",
    "            if ordinal_strategy == 'OrdinalEncoder':\n",
    "                ordinal_transformer = Pipeline(steps=[\n",
    "                    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                    ('ordinal_encoder', OrdinalEncoder())\n",
    "                ])\n",
    "                transformers.append(('ord', ordinal_transformer, self.ordinal_categoricals))\n",
    "                self.logger.debug(\"Ordinal transformer added with OrdinalEncoder.\")\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported ordinal encoding strategy: {ordinal_strategy}\")\n",
    "\n",
    "        # Handle Nominal Categorical Features\n",
    "        if self.nominal_categoricals:\n",
    "            nominal_strategy = self.options.get('encode_categoricals', {}).get('nominal_encoding', 'OneHotEncoder')\n",
    "            if nominal_strategy == 'OneHotEncoder':\n",
    "                nominal_transformer = Pipeline(steps=[\n",
    "                    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                    ('onehot_encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "                ])\n",
    "                transformers.append(('nominal', nominal_transformer, self.nominal_categoricals))\n",
    "                self.logger.debug(\"Nominal transformer added with OneHotEncoder.\")\n",
    "            elif nominal_strategy == 'OrdinalEncoder':\n",
    "                nominal_transformer = Pipeline(steps=[\n",
    "                    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                    ('ordinal_encoder', OrdinalEncoder())\n",
    "                ])\n",
    "                transformers.append(('nominal_ord', nominal_transformer, self.nominal_categoricals))\n",
    "                self.logger.debug(\"Nominal transformer added with OrdinalEncoder.\")\n",
    "            elif nominal_strategy == 'FrequencyEncoder':\n",
    "                # Implement custom Frequency Encoding\n",
    "                for feature in self.nominal_categoricals:\n",
    "                    freq = X_train[feature].value_counts(normalize=True)\n",
    "                    X_train[feature] = X_train[feature].map(freq)\n",
    "                    if X_test is not None:\n",
    "                        X_test[feature] = X_test[feature].map(freq).fillna(0)\n",
    "                    self.feature_reasons[feature] += 'Frequency Encoding applied | '\n",
    "                    self.logger.debug(f\"Frequency Encoding applied to '{feature}'.\")\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported nominal encoding strategy: {nominal_strategy}\")\n",
    "\n",
    "        if not transformers and 'FrequencyEncoder' not in nominal_strategy:\n",
    "            self.logger.error(\"No transformers added to the pipeline. Check feature categorization and configuration.\")\n",
    "            raise ValueError(\"No transformers added to the pipeline. Check feature categorization and configuration.\")\n",
    "\n",
    "        preprocessor = ColumnTransformer(transformers=transformers, remainder='passthrough')\n",
    "        self.logger.debug(\"ColumnTransformer constructed with the following transformers:\")\n",
    "        for t in transformers:\n",
    "            self.logger.debug(t)\n",
    "\n",
    "        preprocessor.fit(X_train)\n",
    "        self.logger.info(\"✅ Preprocessor fitted on training data.\")\n",
    "\n",
    "        # Determine categorical feature indices for SMOTENC if needed\n",
    "        if self.options.get('implement_smote', {}).get('variant', None) == 'SMOTENC':\n",
    "            if not self.categorical_indices:\n",
    "                categorical_features = []\n",
    "                for name, transformer, features in preprocessor.transformers_:\n",
    "                    if 'ord' in name or 'nominal' in name:\n",
    "                        if isinstance(transformer, Pipeline):\n",
    "                            encoder = transformer.named_steps.get('ordinal_encoder') or transformer.named_steps.get('onehot_encoder')\n",
    "                            if hasattr(encoder, 'categories_'):\n",
    "                                categorical_features.extend(range(len(encoder.categories_)))\n",
    "                self.categorical_indices = categorical_features\n",
    "                self.logger.debug(f\"Categorical feature indices for SMOTENC: {self.categorical_indices}\")\n",
    "\n",
    "        return preprocessor\n",
    "\n",
    "    def preprocess_train(self, X: pd.DataFrame, y: pd.Series) -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series, pd.DataFrame, Optional[pd.DataFrame]]:\n",
    "        # Step 1: Split Dataset\n",
    "        X_train_original, X_test_original, y_train_original, y_test = self.split_dataset(X, y)\n",
    "\n",
    "        # Step 2: Handle Missing Values\n",
    "        X_train_missing_values, X_test_missing_values = self.handle_missing_values(X_train_original, X_test_original)\n",
    "\n",
    "        # Step 3: Test for Normality\n",
    "        if self.model_category in ['regression', 'classification', 'clustering']:\n",
    "            self.test_normality(X_train_missing_values)\n",
    "\n",
    "        # Step 4: Handle Outliers\n",
    "        X_train_outliers_handled, y_train_outliers_handled = self.handle_outliers(X_train_missing_values, y_train_original)\n",
    "\n",
    "        # Retain a copy of X_test without outliers for reference\n",
    "        X_test_outliers_handled = X_test_missing_values.copy() if X_test_missing_values is not None else None\n",
    "\n",
    "        # Step 5: Generate Preprocessing Recommendations\n",
    "        recommendations = self.generate_recommendations()\n",
    "\n",
    "        # Step 6: Build and Fit the Pipeline\n",
    "        self.pipeline = self.build_pipeline(X_train_outliers_handled)\n",
    "\n",
    "        # Fit and transform training data using the pipeline\n",
    "        X_train_preprocessed = self.pipeline.fit_transform(X_train_outliers_handled)\n",
    "\n",
    "        # Transform test data\n",
    "        X_test_preprocessed = self.pipeline.transform(X_test_outliers_handled) if X_test_outliers_handled is not None else None\n",
    "\n",
    "        # Step 7: Implement SMOTE Variant (Train Only for Classification)\n",
    "        if self.model_category == 'classification':\n",
    "            try:\n",
    "                X_train_smoted, y_train_smoted = self.implement_smote(X_train_preprocessed, y_train_outliers_handled)\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"❌ SMOTE application failed: {e}\")\n",
    "                raise  # Reraise exception to prevent saving incomplete transformers\n",
    "        else:\n",
    "            X_train_smoted, y_train_smoted = X_train_preprocessed, y_train_outliers_handled\n",
    "            self.logger.info(\"⚠️ SMOTE not applied: Not a classification model.\")\n",
    "\n",
    "        # Step 8: Save Transformers (Including the Pipeline)\n",
    "        self.final_feature_order = list(self.pipeline.get_feature_names_out())\n",
    "        X_train_final = pd.DataFrame(X_train_smoted, columns=self.final_feature_order)\n",
    "        X_test_final = pd.DataFrame(X_test_preprocessed, columns=self.final_feature_order, index=X_test_original.index) if X_test_preprocessed is not None else None\n",
    "\n",
    "        try:\n",
    "            self.save_transformers()\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Saving transformers failed: {e}\")\n",
    "            raise  # Prevent further steps if saving fails\n",
    "\n",
    "        # Inverse transformations (optional, for interpretability)\n",
    "        try:\n",
    "            # Use the final test dataset (fully transformed) for inverse transformations\n",
    "            if X_test_final is not None:\n",
    "                X_test_inverse = self.inverse_transform_data(X_test_final.values)\n",
    "                self.logger.info(\"✅ Inverse transformations applied successfully.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Inverse transformations failed: {e}\")\n",
    "            X_test_inverse = None\n",
    "\n",
    "        # Return processed datasets\n",
    "        return X_train_final, X_test_final, y_train_smoted, y_test, recommendations, X_test_inverse\n",
    "\n",
    "    def transform(self, X: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Transform new data using the fitted preprocessing pipeline.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): New data to transform.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Preprocessed data.\n",
    "        \"\"\"\n",
    "        if self.pipeline is None:\n",
    "            self.logger.error(\"Preprocessing pipeline has not been fitted. Cannot perform inverse transformation.\")\n",
    "            raise AttributeError(\"Preprocessing pipeline has not been fitted. Cannot perform inverse transformation.\")\n",
    "        self.logger.debug(\"Transforming new data.\")\n",
    "        X_preprocessed = self.pipeline.transform(X)\n",
    "        if self.debug:\n",
    "            self.logger.debug(f\"Transformed data shape: {X_preprocessed.shape}\")\n",
    "        else:\n",
    "            self.logger.info(\"Data transformed.\")\n",
    "        return X_preprocessed\n",
    "\n",
    "    def preprocess_predict(self, X: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, Optional[pd.DataFrame]]:\n",
    "        \"\"\"\n",
    "        Preprocess new data for prediction.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): New data for prediction.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, pd.DataFrame, Optional[pd.DataFrame]]: X_preprocessed, recommendations, X_inversed\n",
    "        \"\"\"\n",
    "        step_name = \"Preprocess Predict\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "\n",
    "        # Log initial columns and feature count\n",
    "        self.logger.debug(f\"Initial columns in prediction data: {X.columns.tolist()}\")\n",
    "        self.logger.debug(f\"Initial number of features: {X.shape[1]}\")\n",
    "\n",
    "        # Load transformers\n",
    "        try:\n",
    "            transformers = self.load_transformers()\n",
    "            self.logger.debug(\"Transformers loaded successfully.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Failed to load transformers: {e}\")\n",
    "            raise\n",
    "\n",
    "        # Filter columns based on raw feature names\n",
    "        try:\n",
    "            X_filtered = self.filter_columns(X)\n",
    "            self.logger.debug(f\"Columns after filtering: {X_filtered.columns.tolist()}\")\n",
    "            self.logger.debug(f\"Number of features after filtering: {X_filtered.shape[1]}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Failed during column filtering: {e}\")\n",
    "            raise\n",
    "\n",
    "        # Handle missing values\n",
    "        try:\n",
    "            X_filtered, _ = self.handle_missing_values(X_filtered)\n",
    "            self.logger.debug(f\"Columns after handling missing values: {X_filtered.columns.tolist()}\")\n",
    "            self.logger.debug(f\"Number of features after handling missing values: {X_filtered.shape[1]}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Failed during missing value handling: {e}\")\n",
    "            raise\n",
    "\n",
    "        # Ensure all expected raw features are present\n",
    "        expected_raw_features = self.column_assets['numericals'] + self.column_assets['ordinal_categoricals'] + self.column_assets['nominal_categoricals']\n",
    "        provided_features = X_filtered.columns.tolist()\n",
    "\n",
    "        self.logger.debug(f\"Expected raw features: {expected_raw_features}\")\n",
    "        self.logger.debug(f\"Provided features: {provided_features}\")\n",
    "\n",
    "        missing_raw_features = set(expected_raw_features) - set(provided_features)\n",
    "        if missing_raw_features:\n",
    "            self.logger.error(f\"❌ Missing required raw feature columns in prediction data: {missing_raw_features}\")\n",
    "            raise ValueError(f\"Missing required raw feature columns in prediction data: {missing_raw_features}\")\n",
    "\n",
    "        # Handle unexpected columns (optional: ignore or log)\n",
    "        unexpected_features = set(provided_features) - set(expected_raw_features)\n",
    "        if unexpected_features:\n",
    "            self.logger.warning(f\"⚠️ Unexpected columns in prediction data that will be ignored: {unexpected_features}\")\n",
    "\n",
    "        # Ensure the order of columns matches the pipeline's expectation (optional)\n",
    "        X_filtered = X_filtered[expected_raw_features]\n",
    "        self.logger.debug(\"Reordered columns to match the pipeline's raw feature expectations.\")\n",
    "\n",
    "        # Transform data using the loaded pipeline\n",
    "        try:\n",
    "            X_preprocessed = self.pipeline.transform(X_filtered)\n",
    "            self.logger.debug(f\"Transformed data shape: {X_preprocessed.shape}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Transformation failed: {e}\")\n",
    "            raise\n",
    "\n",
    "        # Inverse transform for interpretability (optional)\n",
    "        try:\n",
    "            X_inversed = self.inverse_transform_data(X_preprocessed)\n",
    "            self.logger.debug(f\"Inverse-transformed data shape: {X_inversed.shape}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Inverse transformation failed: {e}\")\n",
    "            X_inversed = None\n",
    "\n",
    "        # Generate recommendations (if applicable)\n",
    "        try:\n",
    "            recommendations = self.generate_recommendations()\n",
    "            self.logger.debug(\"Generated preprocessing recommendations.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Failed to generate recommendations: {e}\")\n",
    "            recommendations = pd.DataFrame()\n",
    "\n",
    "        # Prepare outputs\n",
    "        return X_preprocessed, recommendations, X_inversed\n",
    "\n",
    "    def preprocess_clustering(self, X: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Preprocess data for clustering mode.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): Input features for clustering.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, pd.DataFrame]: X_processed, recommendations.\n",
    "        \"\"\"\n",
    "        step_name = \"Preprocess Clustering\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "        debug_flag = self.get_debug_flag('debug_handle_missing_values')  # Use relevant debug flags\n",
    "\n",
    "        # Handle Missing Values\n",
    "        X_missing, _ = self.handle_missing_values(X, None)\n",
    "        self.logger.debug(f\"After handling missing values: X_missing.shape={X_missing.shape}\")\n",
    "\n",
    "        # Handle Outliers\n",
    "        X_outliers_handled, _ = self.handle_outliers(X_missing, None)\n",
    "        self.logger.debug(f\"After handling outliers: X_outliers_handled.shape={X_outliers_handled.shape}\")\n",
    "\n",
    "        # Test Normality (optional for clustering)\n",
    "        if self.model_category in ['clustering']:\n",
    "            self.logger.info(\"Skipping normality tests for clustering.\")\n",
    "        else:\n",
    "            self.test_normality(X_outliers_handled)\n",
    "\n",
    "        # Generate Preprocessing Recommendations\n",
    "        recommendations = self.generate_recommendations()\n",
    "\n",
    "        # Build and Fit the Pipeline\n",
    "        self.pipeline = self.build_pipeline(X_outliers_handled)\n",
    "        self.logger.debug(\"Pipeline built and fitted.\")\n",
    "\n",
    "        # Transform the data\n",
    "        X_processed = self.pipeline.transform(X_outliers_handled)\n",
    "        self.logger.debug(f\"After pipeline transform: X_processed.shape={X_processed.shape}\")\n",
    "\n",
    "        # Optionally, inverse transformations can be handled if necessary\n",
    "\n",
    "        # Save Transformers (if needed)\n",
    "        # Not strictly necessary for clustering unless you plan to apply the same preprocessing on new data\n",
    "        self.save_transformers()\n",
    "\n",
    "        self.logger.info(\"✅ Clustering data preprocessed successfully.\")\n",
    "\n",
    "        return X_processed, recommendations\n",
    "\n",
    "\n",
    "    def final_preprocessing(\n",
    "        self, \n",
    "        data: pd.DataFrame\n",
    "    ) -> Tuple:\n",
    "        \"\"\"\n",
    "        Execute the full preprocessing pipeline based on the mode.\n",
    "\n",
    "        Args:\n",
    "            data (pd.DataFrame): Input dataset containing features and possibly the target variable.\n",
    "\n",
    "        Returns:\n",
    "            Tuple: Depending on mode:\n",
    "                - 'train': X_train, X_test, y_train, y_test, recommendations, X_test_inverse\n",
    "                - 'predict': X_preprocessed, recommendations, X_inverse\n",
    "                - 'clustering': X_processed, recommendations\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"Starting: Final Preprocessing Pipeline in '{self.mode}' mode.\")\n",
    "\n",
    "        # Step 0: Filter Columns\n",
    "        try:\n",
    "            data = self.filter_columns(data)\n",
    "            self.logger.info(\"✅ Column filtering completed successfully.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Column filtering failed: {e}\")\n",
    "            raise\n",
    "\n",
    "        if self.mode == 'train':\n",
    "            # Ensure y_variable is present in the data\n",
    "            if not all(col in data.columns for col in self.y_variable):\n",
    "                missing_y = [col for col in self.y_variable if col not in data.columns]\n",
    "                raise ValueError(f\"Target variable(s) {missing_y} not found in the dataset.\")\n",
    "\n",
    "            # Separate X and y\n",
    "            X = data.drop(self.y_variable, axis=1)\n",
    "            y = data[self.y_variable].iloc[:, 0] if len(self.y_variable) == 1 else data[self.y_variable]\n",
    "\n",
    "            if y is None:\n",
    "                raise ValueError(\"Target variable 'y' must be provided in train mode.\")\n",
    "            return self.preprocess_train(X, y)\n",
    "\n",
    "        elif self.mode == 'predict':\n",
    "            # For predict mode, y_variable is not used\n",
    "            X = data.copy()\n",
    "            # Ensure that transformers are loaded\n",
    "            if not os.path.exists(os.path.join(self.transformers_dir, 'transformers.pkl')):\n",
    "                self.logger.error(f\"❌ Transformers file not found at '{self.transformers_dir}'. Cannot proceed with prediction.\")\n",
    "                raise FileNotFoundError(f\"Transformers file not found at '{self.transformers_dir}'.\")\n",
    "\n",
    "            # Preprocess the data\n",
    "            X_preprocessed, recommendations, X_inversed = self.preprocess_predict(X)\n",
    "            self.logger.info(\"✅ Preprocessing completed successfully in predict mode.\")\n",
    "\n",
    "            return X_preprocessed, recommendations, X_inversed\n",
    "\n",
    "        elif self.mode == 'clustering':\n",
    "            # Clustering mode: Use all data as X; y is not used\n",
    "            X = data.copy()\n",
    "            return self.preprocess_clustering(X)\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Mode '{self.mode}' is not implemented.\")\n",
    "\n",
    "    # Optionally, implement a method to display column info for debugging\n",
    "    def _debug_column_info(self, df: pd.DataFrame, step: str = \"Debug Column Info\"):\n",
    "        \"\"\"\n",
    "        Display information about DataFrame columns for debugging purposes.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): The DataFrame to inspect.\n",
    "            step (str, optional): Description of the current step. Defaults to \"Debug Column Info\".\n",
    "        \"\"\"\n",
    "        self.logger.debug(f\"\\n📊 {step}: Column Information\")\n",
    "        for col in df.columns:\n",
    "            self.logger.debug(f\"Column '{col}': {df[col].dtype}, Unique Values: {df[col].nunique()}\")\n",
    "        self.logger.debug(\"\\n\")\n",
    "\n",
    "\n",
    "        \n",
    "  \n",
    "  # main.py\n",
    "\n",
    "import pandas as pd\n",
    "import logging\n",
    "import os\n",
    "import yaml\n",
    "import joblib\n",
    "# from data_preprocessor import DataPreprocessor\n",
    "# from clustering_module import ClusteringModule  # Ensure this is implemented\n",
    "# from feature_manager import FeatureManager  # Ensure this is implemented\n",
    "\n",
    "def load_dataset(path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load the dataset from a CSV file.\n",
    "\n",
    "    Args:\n",
    "        path (str): Path to the dataset CSV file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Loaded dataset.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Dataset not found at {path}\")\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "def load_config(config_path: str) -> dict:\n",
    "    \"\"\"\n",
    "    Load and parse the YAML configuration file.\n",
    "\n",
    "    Args:\n",
    "        config_path (str): Path to the preprocessor_config.yaml file.\n",
    "\n",
    "    Returns:\n",
    "        dict: Parsed configuration.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(config_path):\n",
    "        raise FileNotFoundError(f\"Configuration file not found at {config_path}\")\n",
    "    with open(config_path, 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    return config\n",
    "\n",
    "def construct_filepath(output_dir: str, identifier: str, dataset_key: str) -> str:\n",
    "    \"\"\"\n",
    "    Utility function to construct file paths for saving models and preprocessors.\n",
    "\n",
    "    Args:\n",
    "        identifier (str): Identifier for the file (e.g., 'trained_model', 'preprocessor').\n",
    "        dataset_key (str): Key representing the dataset type.\n",
    "\n",
    "    Returns:\n",
    "        str: Constructed file path.\n",
    "    \"\"\"\n",
    "    return os.path.join(output_dir, f\"{dataset_key}_{identifier}.pkl\")\n",
    "\n",
    "def main():\n",
    "    # ----------------------------\n",
    "    # Step 1: Load Configuration\n",
    "    # ----------------------------\n",
    "    config_path = '../../ml-preprocessing-utils/data/dataset/test/preprocessor_config/preprocessor_config.yaml'\n",
    "    try:\n",
    "        config = load_config(config_path)\n",
    "        logger_config = config.get('logging', {})\n",
    "        logger_level = logger_config.get('level', 'INFO').upper()\n",
    "        logger_format = logger_config.get('format', '%(asctime)s [%(levelname)s] %(message)s')\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to load configuration: {e}\")\n",
    "        return  # Exit if config loading fails\n",
    "\n",
    "    # ----------------------------\n",
    "    # Step 2: Configure Logging\n",
    "    # ----------------------------\n",
    "    debug_flag = config.get('logging', {}).get('debug', False)\n",
    "    logging.basicConfig(\n",
    "        level=logging.DEBUG if debug_flag else getattr(logging, logger_level, logging.INFO),\n",
    "        format=logger_format,\n",
    "        handlers=[\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "    logger = logging.getLogger('main_preprocessing')\n",
    "\n",
    "    # ----------------------------\n",
    "    # Step 3: Extract Feature Assets\n",
    "    # ----------------------------\n",
    "    features_config = config.get('features', {})\n",
    "    column_assets = {\n",
    "        'y_variable': features_config.get('y_variable', []),\n",
    "        'ordinal_categoricals': features_config.get('ordinal_categoricals', []),\n",
    "        'nominal_categoricals': features_config.get('nominal_categoricals', []),\n",
    "        'numericals': features_config.get('numericals', [])\n",
    "    }\n",
    "\n",
    "    # ----------------------------\n",
    "    # Step 4: Extract Execution Parameters\n",
    "    # ----------------------------\n",
    "    execution = config.get('execution', {})\n",
    "    shared_execution = execution.get('shared', {})\n",
    "    mode_execution = execution.get('train', {})  # Default to train mode\n",
    "    current_mode = mode_execution.get('mode', 'train').lower()\n",
    "\n",
    "    # ----------------------------\n",
    "    # Step 5: Get List of Model Types\n",
    "    # ----------------------------\n",
    "    model_types = config.get('model_types', ['Tree Based Classifier'])  # Default to one model if not specified\n",
    "\n",
    "    for current_model_type in model_types:\n",
    "        logger.info(f\"---\\nProcessing Model: {current_model_type}\\n---\")\n",
    "\n",
    "        # Step 6: Extract Mode for the Current Model\n",
    "        model_config = config.get('models', {}).get(current_model_type, {})\n",
    "        if not model_config:\n",
    "            logger.error(f\"No configuration found for model '{current_model_type}'. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Determine mode based on the model type\n",
    "        # For example, if the model is a clustering model, set mode to 'clustering'\n",
    "        if current_model_type in ['K-Means', 'Hierarchical Clustering', 'DBSCAN', 'KModes', 'KPrototypes']:\n",
    "            current_mode = 'clustering'\n",
    "        elif current_model_type in ['Logistic Regression', 'Tree Based Classifier', 'Support Vector Machine']:\n",
    "            current_mode = 'train'\n",
    "        elif current_model_type in ['Linear Regression', 'Tree Based Regressor']:\n",
    "            current_mode = 'train'\n",
    "        else:\n",
    "            current_mode = 'train'  # Default to train\n",
    "\n",
    "        # ----------------------------\n",
    "        # Step 7: Handle Modes for Each Model\n",
    "        # ----------------------------\n",
    "        if current_mode == 'train':\n",
    "            # Adjust output directories to prevent overwriting\n",
    "            execution_train = execution.get('train', {})\n",
    "            train_mode = 'train'\n",
    "\n",
    "            train_input_path = execution_train.get('input_path', '')\n",
    "            base_output_dir = execution_train.get('output_dir', './processed_data')\n",
    "            model_output_dir = os.path.join(base_output_dir, current_model_type.replace(\" \", \"_\"))\n",
    "            transformers_dir = execution_train.get('save_transformers_path', './transformers')  # Changed: Remove model name\n",
    "            normalize_debug = execution_train.get('normalize_debug', False)\n",
    "            normalize_graphs_output = execution_train.get('normalize_graphs_output', False)\n",
    "\n",
    "            # Validate essential paths\n",
    "            if not train_input_path:\n",
    "                logger.error(\"❌ 'input_path' for training mode is not specified in the configuration.\")\n",
    "                continue\n",
    "            if not os.path.exists(train_input_path):\n",
    "                logger.error(f\"❌ Training input dataset not found at {train_input_path}.\")\n",
    "                continue\n",
    "\n",
    "            # Initialize DataPreprocessor\n",
    "            preprocessor = DataPreprocessor(\n",
    "                model_type=current_model_type,\n",
    "                column_assets=column_assets,\n",
    "                mode=train_mode,\n",
    "                options=model_config,\n",
    "                debug=debug_flag,\n",
    "                normalize_debug=normalize_debug,\n",
    "                normalize_graphs_output=normalize_graphs_output,\n",
    "                graphs_output_dir=shared_execution.get('plot_output_dir', './plots'),\n",
    "                transformers_dir=transformers_dir  # Now a directory\n",
    "            )\n",
    "\n",
    "            # Initialize FeatureManager\n",
    "            save_path = config.get('execution', {}).get('shared', {}).get('features_metadata_path', '../../ml-preprocessing-utils/data/dataset/test/features_info/features_metadata.pkl')\n",
    "            feature_manager = FeatureManager(save_path=save_path)  # Ensure FeatureManager is correctly implemented\n",
    "\n",
    "            # Load Training Dataset via FeatureManager\n",
    "            try:\n",
    "                filtered_df, column_assets = feature_manager.load_features_and_dataset(\n",
    "                    debug=True  # Set to False to reduce verbosity\n",
    "                )\n",
    "                logger.info(\"✅ Features loaded and dataset filtered successfully.\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"❌ Failed to load features and dataset: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Execute Preprocessing\n",
    "            try:\n",
    "                X_train, X_test, y_train, y_test, recommendations, X_test_inverse = preprocessor.final_preprocessing(filtered_df)\n",
    "                logger.info(\"✅ Preprocessing completed successfully in train mode.\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"❌ Preprocessing failed in train mode: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Save Preprocessed Data\n",
    "            try:\n",
    "                os.makedirs(model_output_dir, exist_ok=True)\n",
    "                X_train.to_csv(os.path.join(model_output_dir, 'X_train.csv'), index=False)\n",
    "                y_train.to_csv(os.path.join(model_output_dir, 'y_train.csv'), index=False)\n",
    "                X_test.to_csv(os.path.join(model_output_dir, 'X_test.csv'), index=False)\n",
    "                y_test.to_csv(os.path.join(model_output_dir, 'y_test.csv'), index=False)\n",
    "                recommendations.to_csv(os.path.join(model_output_dir, 'preprocessing_recommendations.csv'), index=False)\n",
    "                logger.info(f\"✅ Preprocessed data saved to '{model_output_dir}'.\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"❌ Failed to save preprocessed data: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Optional: Visualize Inverse Transformations\n",
    "            try:\n",
    "                if X_test_inverse is not None:\n",
    "                    print(f\"Inverse Transformed Test Data for {current_model_type}:\")\n",
    "                    print(X_test_inverse.head())\n",
    "            except Exception as e:\n",
    "                logger.error(f\"❌ Error during visualization: {e}\")\n",
    "                continue\n",
    "\n",
    "            logger.info(f\"✅ All preprocessing tasks completed successfully for model '{current_model_type}'.\")\n",
    "\n",
    "        elif current_mode == 'predict':\n",
    "            # Adjust paths accordingly\n",
    "            execution_predict = execution.get('predict', {})\n",
    "            predict_mode = 'predict'\n",
    "\n",
    "            predict_input_path = execution_predict.get('prediction_input_path', '')\n",
    "            predictions_output_path = execution_predict.get('predictions_output_path', './predictions')\n",
    "            transformers_dir = execution_predict.get('load_transformers_path', './transformers')  # Correct directory\n",
    "            trained_model_path = execution_predict.get('trained_model_path', './models/trained_model.pkl')  # Path to load model\n",
    "            normalize_debug = execution_predict.get('normalize_debug', False)\n",
    "            normalize_graphs_output = execution_predict.get('normalize_graphs_output', False)\n",
    "\n",
    "            # Validate essential paths\n",
    "            if not predict_input_path:\n",
    "                logger.error(\"❌ 'prediction_input_path' for predict mode is not specified in the configuration.\")\n",
    "                continue\n",
    "            if not os.path.exists(predict_input_path):\n",
    "                logger.error(f\"❌ Prediction input dataset not found at {predict_input_path}.\")\n",
    "                continue\n",
    "            if not os.path.exists(trained_model_path):\n",
    "                logger.error(f\"❌ Trained model not found at {trained_model_path}.\")\n",
    "                continue\n",
    "            if not os.path.exists(transformers_dir):\n",
    "                logger.error(f\"❌ Transformers directory not found at {transformers_dir}.\")\n",
    "                continue\n",
    "\n",
    "            # Initialize DataPreprocessor\n",
    "            preprocessor = DataPreprocessor(\n",
    "                model_type=current_model_type,\n",
    "                column_assets=column_assets,\n",
    "                mode=predict_mode,\n",
    "                options=model_config,\n",
    "                debug=debug_flag,\n",
    "                normalize_debug=normalize_debug,\n",
    "                normalize_graphs_output=normalize_graphs_output,\n",
    "                graphs_output_dir=shared_execution.get('plot_output_dir', './plots'),\n",
    "                transformers_dir=transformers_dir  # Directory path\n",
    "            )\n",
    "\n",
    "            # Load Prediction Dataset\n",
    "            try:\n",
    "                df_predict = load_dataset(predict_input_path)\n",
    "                logger.info(f\"✅ Prediction input data loaded from '{predict_input_path}'.\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"❌ Failed to load prediction input data: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Execute Preprocessing for Prediction\n",
    "            try:\n",
    "                X_preprocessed, recommendations, X_inversed = preprocessor.preprocess_predict(X=df_predict)\n",
    "                logger.info(\"✅ Preprocessing completed successfully in predict mode.\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"❌ Preprocessing failed in predict mode: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Load Trained Model\n",
    "            # try:\n",
    "            #     trained_model = joblib.load(trained_model_path)\n",
    "            #     logger.info(f\"✅ Trained model loaded from '{trained_model_path}'.\")\n",
    "            # except Exception as e:\n",
    "            #     logger.error(f\"❌ Failed to load trained model: {e}\")\n",
    "            #     continue\n",
    "\n",
    "            # # Make Predictions\n",
    "            # try:\n",
    "            #     # Ensure X_preprocessed is a DataFrame with appropriate feature names\n",
    "            #     if isinstance(X_preprocessed, np.ndarray):\n",
    "            #         X_preprocessed_df = pd.DataFrame(X_preprocessed, columns=preprocessor.final_feature_order)\n",
    "            #     else:\n",
    "            #         X_preprocessed_df = X_preprocessed\n",
    "\n",
    "            #     # Make predictions\n",
    "            #     predictions = trained_model.predict(X_preprocessed_df)\n",
    "            #     logger.info(\"✅ Predictions made successfully.\")\n",
    "            # except Exception as e:\n",
    "            #     logger.error(f\"❌ Prediction failed: {e}\")\n",
    "            #     continue\n",
    "            y_new_pred = np.random.choice(['1', '0'], size=X_inversed.shape[0])  # Example for binary predictions\n",
    "            \n",
    "            # Attach Predictions to Inversed Data\n",
    "            if X_inversed is not None:\n",
    "                # Ensure predictions length matches the number of rows in X_inversed\n",
    "                if len(y_new_pred) == len(X_inversed):\n",
    "                    # Add predictions column\n",
    "                    X_inversed['predictions'] = y_new_pred\n",
    "                    logger.info(\"✅ Predictions attached to inversed data successfully.\")\n",
    "\n",
    "                    # Debugging Output AFTER attaching predictions\n",
    "                    print(f\"\\nUpdated INVERSED DATA with Predictions for {current_model_type}:\")\n",
    "                    print(X_inversed.head())  # Shows predictions column included\n",
    "                else:\n",
    "                    logger.error(\"❌ Predictions length does not match inversed data length.\")\n",
    "                    continue\n",
    "            else:\n",
    "                logger.error(\"❌ Inversed data is None. Cannot attach predictions.\")\n",
    "                continue\n",
    "\n",
    "            # Save Predictions\n",
    "            try:\n",
    "                os.makedirs(predictions_output_path, exist_ok=True)\n",
    "                predictions_filename = os.path.join(predictions_output_path, f'predictions_{current_model_type.replace(\" \", \"_\")}.csv')\n",
    "                if X_inversed is not None:\n",
    "                    X_inversed.to_csv(predictions_filename, index=False)\n",
    "                else:\n",
    "                    logger.error(\"❌ Inversed data is None. Predictions not saved.\")\n",
    "                    continue\n",
    "                logger.info(f\"✅ Predictions saved to '{predictions_filename}'.\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"❌ Failed to save predictions: {e}\")\n",
    "                continue\n",
    "\n",
    "            logger.info(f\"✅ All prediction tasks completed successfully for model '{current_model_type}'.\")\n",
    "\n",
    "        elif current_mode == 'clustering':\n",
    "            # Adjust paths accordingly\n",
    "            execution_clustering = execution.get('clustering', {})\n",
    "            clustering_mode = 'clustering'\n",
    "\n",
    "            clustering_input_path = execution_clustering.get('clustering_input_path', '')\n",
    "            clustering_output_dir = execution_clustering.get('clustering_output_dir', './clustering_output')\n",
    "            normalize_debug = execution_clustering.get('normalize_debug', False)\n",
    "            normalize_graphs_output = execution_clustering.get('normalize_graphs_output', False)\n",
    "\n",
    "            # Validate essential paths\n",
    "            if not clustering_input_path:\n",
    "                logger.error(\"❌ 'clustering_input_path' for clustering mode is not specified in the configuration.\")\n",
    "                continue\n",
    "            if not os.path.exists(clustering_input_path):\n",
    "                logger.error(f\"❌ Clustering input dataset not found at {clustering_input_path}.\")\n",
    "                continue\n",
    "\n",
    "            # Initialize DataPreprocessor\n",
    "            preprocessor = DataPreprocessor(\n",
    "                model_type=current_model_type,\n",
    "                column_assets=column_assets,\n",
    "                mode=clustering_mode,\n",
    "                options=model_config,\n",
    "                debug=debug_flag,\n",
    "                normalize_debug=normalize_debug,\n",
    "                normalize_graphs_output=normalize_graphs_output,\n",
    "                graphs_output_dir=shared_execution.get('plot_output_dir', './plots'),\n",
    "                transformers_dir=execution_clustering.get('save_transformers_path', './transformers')  # Changed: Remove model name\n",
    "            )\n",
    "\n",
    "            # Load Clustering Dataset\n",
    "            try:\n",
    "                df_clustering = load_dataset(clustering_input_path)\n",
    "                logger.info(f\"✅ Clustering input data loaded from '{clustering_input_path}'.\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"❌ Failed to load clustering input data: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Execute Preprocessing for Clustering\n",
    "            try:\n",
    "                X_processed, recommendations = preprocessor.final_preprocessing(df_clustering)\n",
    "                logger.info(\"✅ Preprocessing completed successfully in clustering mode.\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"❌ Preprocessing failed in clustering mode: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Initialize and Train Clustering Model\n",
    "            try:\n",
    "                # Load clustering model parameters from config\n",
    "                clustering_model_config = model_config.get('clustering_model_params', {})\n",
    "\n",
    "                clustering_module = ClusteringModule(\n",
    "                    model_type=current_model_type,\n",
    "                    model_params=clustering_model_config,\n",
    "                    debug=debug_flag\n",
    "                )\n",
    "\n",
    "                clustering_module.fit(X_processed)\n",
    "                clustering_module.evaluate(X_processed)\n",
    "                # Plot clusters if applicable\n",
    "                clustering_module.plot_clusters(X_processed, clustering_output_dir)\n",
    "                # Save the clustering model\n",
    "                os.makedirs(clustering_output_dir, exist_ok=True)\n",
    "                clustering_model_path = os.path.join(clustering_output_dir, f\"{current_model_type.replace(' ', '_')}_model.pkl\")\n",
    "                clustering_module.save_model(clustering_model_path)\n",
    "                logger.info(f\"✅ Clustering model saved to '{clustering_model_path}'.\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"❌ Clustering tasks failed: {e}\")\n",
    "                continue\n",
    "\n",
    "            logger.info(f\"✅ All clustering tasks completed successfully for model '{current_model_type}'.\")\n",
    "\n",
    "        else:\n",
    "            logger.error(f\"❌ Unsupported mode '{current_mode}'. Supported modes are 'train', 'predict', and 'clustering'.\")\n",
    "            continue\n",
    "\n",
    "    logger.info(\"✅ All model processing completed successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile ../../src/freethrow_predictions/ml/train_utils/train_utils.py\n",
    "import os  # Added missing import\n",
    "import joblib  # Added missing import\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    roc_auc_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    log_loss,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import logging\n",
    "import json\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Main function with MLflow integration\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "from inspect import signature\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, save_path=\"classification_report.txt\"):\n",
    "    \"\"\"\n",
    "    Evaluate the model and log performance metrics.\n",
    "\n",
    "    Parameters:\n",
    "    - model: Trained model to evaluate.\n",
    "    - X_test: Test features.\n",
    "    - y_test: True labels for the test data.\n",
    "    - save_path: Path to save the classification report.\n",
    "\n",
    "    Returns:\n",
    "    - metrics: Dictionary containing evaluation metrics.\n",
    "    \"\"\"\n",
    "    logger.info(\"Evaluating model...\")\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Check if the model supports probability predictions\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "        logger.info(f\"Predicted probabilities: {y_proba}\")\n",
    "    else:\n",
    "        y_proba = None\n",
    "        logger.info(\"Model does not support probability predictions.\")\n",
    "\n",
    "    # Calculate metrics with consistent key naming\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"precision\": precision_score(y_test, y_pred, average=\"weighted\", zero_division=0),\n",
    "        \"recall\": recall_score(y_test, y_pred, average=\"weighted\", zero_division=0),\n",
    "        \"f1_score\": f1_score(y_test, y_pred, average=\"weighted\", zero_division=0),\n",
    "        \"roc_auc\": roc_auc_score(y_test, y_proba) if y_proba is not None else None,\n",
    "        \"log_loss\": log_loss(y_test, y_proba) if y_proba is not None else None,\n",
    "    }\n",
    "\n",
    "    # Log metrics\n",
    "    logger.info(f\"Evaluation Metrics: {metrics}\")\n",
    "\n",
    "    # Generate and save classification report\n",
    "    report = classification_report(y_test, y_pred, output_dict=False)\n",
    "    logger.info(\"\\n\" + report)\n",
    "    with open(save_path, \"w\") as f:\n",
    "        f.write(report)\n",
    "    logger.info(f\"Classification report saved to {save_path}\")\n",
    "\n",
    "    # Generate confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot()\n",
    "    plt.savefig(os.path.join(os.path.dirname(save_path), 'confusion_matrix.png'))\n",
    "    logger.info(f\"Confusion matrix saved to {os.path.join(os.path.dirname(save_path), 'confusion_matrix.png')}\")\n",
    "    plt.close()\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def save_model(model, model_name, save_dir=\"../../data/model\"):\n",
    "    \"\"\"\n",
    "    Save the trained model to disk.\n",
    "\n",
    "    Parameters:\n",
    "    - model: Trained model to save.\n",
    "    - model_name: Name of the model for saving.\n",
    "    - save_dir: Directory to save the model.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.debug(f\"Entering save_model with model_name='{model_name}', save_dir='{save_dir}'\")\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        logger.debug(f\"Ensured that the directory '{save_dir}' exists.\")\n",
    "        model_path = os.path.join(save_dir, f\"{model_name}_model.pkl\")\n",
    "        logger.debug(f\"Model will be saved to '{model_path}'\")\n",
    "        joblib.dump(model, model_path)\n",
    "        logger.info(f\"Model saved to {model_path}\")\n",
    "\n",
    "        # Confirm the model file exists\n",
    "        if os.path.isfile(model_path):\n",
    "            logger.debug(f\"Confirmed model file exists at '{model_path}'\")\n",
    "        else:\n",
    "            logger.error(f\"Model file was not found after saving at '{model_path}'\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to save model '{model_name}' to '{save_dir}': {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def load_model(model_name, save_dir=\"../../data/model\"):\n",
    "    \"\"\"\n",
    "    Load the trained model from disk.\n",
    "\n",
    "    Parameters:\n",
    "    - model_name: Name of the model to load.\n",
    "    - save_dir: Directory where the model is saved.\n",
    "\n",
    "    Returns:\n",
    "    - model: Loaded trained model.\n",
    "    \"\"\"\n",
    "    model_path = os.path.join(save_dir, f\"{model_name}_model.pkl\")\n",
    "    model = joblib.load(model_path)\n",
    "    logger.info(f\"Model loaded from {model_path}\")\n",
    "    return model\n",
    "\n",
    "# Plot decision boundary\n",
    "def plot_decision_boundary(model, X, y, title, use_pca=True):\n",
    "    \"\"\"\n",
    "    Plot decision boundaries for the model.\n",
    "\n",
    "    Parameters:\n",
    "    - model: Trained model to visualize.\n",
    "    - X: Feature data (test set).\n",
    "    - y: Target labels.\n",
    "    - title: Title for the plot.\n",
    "    - use_pca: If True, applies PCA for dimensionality reduction if X has >2 features.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Original X shape: {X.shape}\")\n",
    "    if X.shape[1] > 2 and use_pca:\n",
    "        logger.info(\"X has more than 2 features, applying PCA for visualization.\")\n",
    "        try:\n",
    "            pca = PCA(n_components=2)\n",
    "            X_2d = pca.fit_transform(X)\n",
    "            explained_variance = pca.explained_variance_ratio_\n",
    "            logger.info(f\"PCA explained variance ratios: {explained_variance}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"PCA transformation failed: {e}\")\n",
    "            raise e\n",
    "    elif X.shape[1] > 2:\n",
    "        logger.error(\"Cannot plot decision boundary for more than 2D without PCA.\")\n",
    "        raise ValueError(\"Cannot plot decision boundary for more than 2D without PCA.\")\n",
    "    else:\n",
    "        logger.info(\"X has 2 or fewer features, using original features for plotting.\")\n",
    "        X_2d = X\n",
    "\n",
    "    logger.info(f\"Transformed X shape for plotting: {X_2d.shape}\")\n",
    "\n",
    "    # Create mesh grid\n",
    "    try:\n",
    "        x_min, x_max = X_2d[:, 0].min() - 1, X_2d[:, 0].max() + 1\n",
    "        y_min, y_max = X_2d[:, 1].min() - 1, X_2d[:, 1].max() + 1\n",
    "        xx, yy = np.meshgrid(\n",
    "            np.arange(x_min, x_max, 0.01),\n",
    "            np.arange(y_min, y_max, 0.01)\n",
    "        )\n",
    "        logger.info(f\"Mesh grid created with shape xx: {xx.shape}, yy: {yy.shape}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Mesh grid creation failed: {e}\")\n",
    "        raise e\n",
    "\n",
    "    # Flatten the grid arrays and combine into a single array\n",
    "    try:\n",
    "        grid_points_2d = np.c_[xx.ravel(), yy.ravel()]\n",
    "        logger.info(f\"Grid points in 2D PCA space shape: {grid_points_2d.shape}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Grid points preparation failed: {e}\")\n",
    "        raise e\n",
    "\n",
    "    if X.shape[1] > 2 and use_pca:\n",
    "        # Inverse transform the grid points back to the original feature space\n",
    "        try:\n",
    "            logger.info(\"Inverse transforming grid points back to original feature space for prediction.\")\n",
    "            grid_points_original = pca.inverse_transform(grid_points_2d)\n",
    "            logger.info(f\"Grid points in original feature space shape: {grid_points_original.shape}\")\n",
    "            # Predict on the grid points in original feature space\n",
    "            Z = model.predict(grid_points_original)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error predicting decision boundary: {e}\")\n",
    "            return\n",
    "    else:\n",
    "        # For 2D data, use grid points directly for prediction\n",
    "        grid_points_original = grid_points_2d\n",
    "        try:\n",
    "            Z = model.predict(grid_points_original)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error predicting decision boundary: {e}\")\n",
    "            return\n",
    "\n",
    "    try:\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        logger.info(f\"Decision boundary predictions reshaped to: {Z.shape}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Reshaping predictions failed: {e}\")\n",
    "        raise e\n",
    "\n",
    "    # Plot the decision boundary\n",
    "    try:\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.RdYlBu)\n",
    "        plt.scatter(X_2d[:, 0], X_2d[:, 1], c=y, edgecolor=\"k\", cmap=plt.cm.RdYlBu)\n",
    "        plt.title(title)\n",
    "        plt.xlabel(\"Principal Component 1\" if use_pca and X.shape[1] > 2 else \"Feature 1\")\n",
    "        plt.ylabel(\"Principal Component 2\" if use_pca and X.shape[1] > 2 else \"Feature 2\")\n",
    "        plt.show()\n",
    "        logger.info(\"Decision boundary plot displayed successfully.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Plotting failed: {e}\")\n",
    "        raise e\n",
    "\n",
    "\n",
    "\n",
    "# Hyperparameter tuning for Random Forest\n",
    "def tune_random_forest(X_train, y_train, scoring_metric=\"neg_log_loss\"):\n",
    "    logger.info(\"Starting hyperparameter tuning for Random Forest...\")\n",
    "    param_space = {\n",
    "        \"n_estimators\": Integer(10, 500),\n",
    "        \"max_depth\": Integer(2, 50),\n",
    "        \"min_samples_split\": Integer(2, 20),\n",
    "        \"min_samples_leaf\": Integer(1, 10),\n",
    "        \"max_features\": Categorical([\"sqrt\", \"log2\", None]),\n",
    "        \"bootstrap\": Categorical([True, False]),\n",
    "        \"criterion\": Categorical([\"gini\", \"entropy\"]),\n",
    "    }\n",
    "    logger.info(f\"Parameter space: {param_space}\")\n",
    "\n",
    "    search = BayesSearchCV(\n",
    "        RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "        param_space,\n",
    "        n_iter=60,\n",
    "        scoring=scoring_metric, #accuracy, neg_log_loss\n",
    "        cv=cv,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "    search.fit(X_train, y_train)\n",
    "    logger.info(f\"Best parameters found: {search.best_params_}\")\n",
    "    logger.info(f\"Best cross-validation score: {search.best_score_}\")\n",
    "    return search.best_params_, search.best_score_, search.best_estimator_\n",
    "\n",
    "# Hyperparameter tuning for XGBoost\n",
    "def tune_xgboost(X_train, y_train, scoring_metric=\"neg_log_loss\"):\n",
    "    logger.info(\"Starting hyperparameter tuning for XGBoost...\")\n",
    "    param_space = {\n",
    "        'learning_rate': Real(0.01, 0.3, prior='log-uniform'),\n",
    "        'n_estimators': Integer(100, 500),\n",
    "        'max_depth': Integer(3, 15),\n",
    "        'min_child_weight': Integer(1, 10),\n",
    "        'gamma': Real(0, 5),\n",
    "        'subsample': Real(0.5, 1.0),\n",
    "        'colsample_bytree': Real(0.5, 1.0),\n",
    "        'reg_alpha': Real(1e-8, 1.0, prior='log-uniform'),\n",
    "        'reg_lambda': Real(1e-8, 1.0, prior='log-uniform'),\n",
    "    }\n",
    "    logger.info(f\"Parameter space: {param_space}\")\n",
    "\n",
    "    search = BayesSearchCV(\n",
    "        XGBClassifier(eval_metric=\"logloss\", random_state=42, n_jobs=-1),\n",
    "        param_space,\n",
    "        n_iter=60,\n",
    "        scoring=scoring_metric,\n",
    "        cv=cv,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "    search.fit(X_train, y_train)\n",
    "    logger.info(f\"Best parameters found: {search.best_params_}\")\n",
    "    logger.info(f\"Best cross-validation score: {search.best_score_}\")\n",
    "    return search.best_params_, search.best_score_, search.best_estimator_\n",
    "\n",
    "# Hyperparameter tuning for Decision Tree\n",
    "def tune_decision_tree(X_train, y_train, scoring_metric=\"neg_log_loss\"):\n",
    "    logger.info(\"Starting hyperparameter tuning for Decision Tree...\")\n",
    "    param_space = {\n",
    "        \"max_depth\": Integer(2, 50),\n",
    "        \"min_samples_split\": Integer(2, 20),\n",
    "        \"min_samples_leaf\": Integer(1, 10),\n",
    "        \"criterion\": Categorical([\"gini\", \"entropy\"]),\n",
    "        \"splitter\": Categorical([\"best\", \"random\"]),\n",
    "    }\n",
    "    logger.info(f\"Parameter space: {param_space}\")\n",
    "\n",
    "    search = BayesSearchCV(\n",
    "        DecisionTreeClassifier(random_state=42),\n",
    "        param_space,\n",
    "        n_iter=60,\n",
    "        scoring=scoring_metric,\n",
    "        cv=cv,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "    search.fit(X_train, y_train)\n",
    "    logger.info(f\"Best parameters found: {search.best_params_}\")\n",
    "    logger.info(f\"Best cross-validation score: {search.best_score_}\")\n",
    "    return search.best_params_, search.best_score_, search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:23:39,263 [INFO] __main__: ✅ Configuration loaded from ../../ml-preprocessing-utils/data/dataset/test/preprocessor_config/preprocessor_config.yaml.\n",
      "2024-12-31 14:23:39,269 [INFO] __main__: ✅ Features and metadata loaded from ../../ml-preprocessing-utils/data/dataset/test/features_info/features_metadata.pkl\n",
      "2024-12-31 14:23:39,271 [INFO] __main__: 📥 Loading dataset from ../../data/processed/final_ml_dataset.csv...\n",
      "2024-12-31 14:23:39,279 [INFO] __main__: ✅ Dataset loaded from ../../data/processed/final_ml_dataset.csv\n",
      "2024-12-31 14:23:39,280 [INFO] __main__: ✅ Original dataset loaded successfully.\n",
      "2024-12-31 14:23:39,280 [INFO] __main__: 🔍 Filtering dataset for selected features...\n",
      "2024-12-31 14:23:39,281 [INFO] __main__: ✅ Dataset filtered based on selected features.\n",
      "2024-12-31 14:23:39,281 [INFO] __main__: ✅ Dataset filtered successfully.\n",
      "2024-12-31 14:23:39,282 [INFO] __main__: 📁 Separating columns into defined categories...\n",
      "2024-12-31 14:23:39,282 [INFO] __main__: ✅ Columns separated into defined categories.\n",
      "2024-12-31 14:23:39,282 [INFO] __main__: ✅ Features loaded and dataset filtered successfully.\n",
      "2024-12-31 14:23:39,283 [INFO] __main__: ✅ Filtered dataset and column assets loaded successfully via FeatureManager.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:23:39,283 [INFO] Starting: Final Preprocessing Pipeline in 'train' mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:23:39,283 [INFO] DataPreprocessor: Starting: Final Preprocessing Pipeline in 'train' mode.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:23:39,285 [INFO] Step: filter_columns\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:23:39,285 [INFO] DataPreprocessor: Step: filter_columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:23:39,287 [INFO] ✅ Filtered DataFrame to include only specified features. Shape: (125, 16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:23:39,287 [INFO] DataPreprocessor: ✅ Filtered DataFrame to include only specified features. Shape: (125, 16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:23:39,288 [INFO] ✅ Column filtering completed successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:23:39,288 [INFO] DataPreprocessor: ✅ Column filtering completed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:23:39,289 [INFO] Step: Split Dataset into Train and Test\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:23:39,289 [INFO] DataPreprocessor: Step: Split Dataset into Train and Test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:23:39,293 [INFO] Step: Handle Missing Values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:23:39,293 [INFO] DataPreprocessor: Step: Handle Missing Values\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:23:39,301 [INFO] Step: Test for Normality\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:23:39,301 [INFO] DataPreprocessor: Step: Test for Normality\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:23:39,306 [INFO] Step: Handle Outliers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:23:39,306 [INFO] DataPreprocessor: Step: Handle Outliers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:23:39,320 [INFO] Step: Generate Preprocessor Recommendations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:23:39,320 [INFO] DataPreprocessor: Step: Generate Preprocessor Recommendations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:23:39,327 [INFO] ✅ Preprocessor fitted on training data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:23:39,327 [INFO] DataPreprocessor: ✅ Preprocessor fitted on training data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:23:39,335 [INFO] Step: Implement SMOTE (Train Only)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:23:39,335 [INFO] DataPreprocessor: Step: Implement SMOTE (Train Only)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:23:39,336 [INFO] Class Distribution before SMOTE: {1: 52, 0: 27}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:23:39,336 [INFO] DataPreprocessor: Class Distribution before SMOTE: {1: 52, 0: 27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:23:39,337 [INFO] Imbalance Ratio (Minority/Majority): 0.5192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:23:39,337 [INFO] DataPreprocessor: Imbalance Ratio (Minority/Majority): 0.5192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:23:39,338 [INFO] Dataset contains both numerical and categorical features. Using SMOTENC.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:23:39,338 [INFO] DataPreprocessor: Dataset contains both numerical and categorical features. Using SMOTENC.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:23:39,344 [INFO] Applied SMOTENC. Resampled dataset shape: (104, 15)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:23:39,344 [INFO] DataPreprocessor: Applied SMOTENC. Resampled dataset shape: (104, 15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:23:39,345 [INFO] Step: Save Transformers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:23:39,345 [INFO] DataPreprocessor: Step: Save Transformers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:23:39,357 [INFO] Transformers saved at '../../ml-preprocessing-utils/data/dataset/test/transformers/transformers.pkl'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:23:39,357 [INFO] DataPreprocessor: Transformers saved at '../../ml-preprocessing-utils/data/dataset/test/transformers/transformers.pkl'.\n",
      "2024-12-31 14:23:39,358 [INFO] InverseTransform: ✅ Inverse transformation completed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:23:39,359 [INFO] ✅ Inverse transformations applied successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:23:39,359 [INFO] DataPreprocessor: ✅ Inverse transformations applied successfully.\n",
      "2024-12-31 14:23:39,360 [INFO] __main__: ✅ Preprocessing complete. X_train shape: (104, 15), X_test shape: (25, 15)\n",
      "2024-12-31 14:23:39,360 [INFO] __main__: Starting the Bayesian hyperparameter tuning process...\n",
      "2024-12-31 14:23:39,360 [INFO] __main__: 📌 Tuning hyperparameters for XGBoost...\n",
      "2024-12-31 14:23:39,361 [INFO] __main__: Starting hyperparameter tuning for XGBoost...\n",
      "2024-12-31 14:23:39,364 [INFO] __main__: Parameter space: {'learning_rate': Real(low=0.01, high=0.3, prior='log-uniform', transform='identity'), 'n_estimators': Integer(low=100, high=500, prior='uniform', transform='identity'), 'max_depth': Integer(low=3, high=15, prior='uniform', transform='identity'), 'min_child_weight': Integer(low=1, high=10, prior='uniform', transform='identity'), 'gamma': Real(low=0, high=5, prior='uniform', transform='identity'), 'subsample': Real(low=0.5, high=1.0, prior='uniform', transform='identity'), 'colsample_bytree': Real(low=0.5, high=1.0, prior='uniform', transform='identity'), 'reg_alpha': Real(low=1e-08, high=1.0, prior='log-uniform', transform='identity'), 'reg_lambda': Real(low=1e-08, high=1.0, prior='log-uniform', transform='identity')}\n",
      "2024-12-31 14:25:09,364 [INFO] __main__: Best parameters found: OrderedDict([('colsample_bytree', 0.5117319669372997), ('gamma', 0.0), ('learning_rate', 0.0321849510637896), ('max_depth', 3), ('min_child_weight', 1), ('n_estimators', 500), ('reg_alpha', 1.0), ('reg_lambda', 1.0), ('subsample', 0.8541148278142439)])\n",
      "2024-12-31 14:25:09,366 [INFO] __main__: Best cross-validation score: -0.48323462279926305\n",
      "2024-12-31 14:25:09,367 [INFO] __main__: ✅ XGBoost tuning done. Best Params: OrderedDict([('colsample_bytree', 0.5117319669372997), ('gamma', 0.0), ('learning_rate', 0.0321849510637896), ('max_depth', 3), ('min_child_weight', 1), ('n_estimators', 500), ('reg_alpha', 1.0), ('reg_lambda', 1.0), ('subsample', 0.8541148278142439)]), Best CV Score: -0.48323462279926305\n",
      "2024-12-31 14:25:09,367 [INFO] __main__: Evaluating model...\n",
      "2024-12-31 14:25:09,380 [INFO] __main__: Predicted probabilities: [0.28744018 0.88487947 0.98947924 0.05336478 0.96654296 0.16666678\n",
      " 0.88452595 0.5604174  0.75537395 0.90036345 0.9499962  0.8447243\n",
      " 0.9256647  0.9388823  0.2549677  0.966459   0.79955477 0.23803757\n",
      " 0.43638334 0.19004232 0.78478134 0.61416805 0.9817438  0.76857126\n",
      " 0.953545  ]\n",
      "2024-12-31 14:25:09,387 [INFO] __main__: Evaluation Metrics: {'accuracy': 0.76, 'precision': 0.76, 'recall': 0.76, 'f1_score': 0.76, 'roc_auc': 0.6587301587301587, 'log_loss': 0.6711150121950691}\n",
      "2024-12-31 14:25:09,391 [INFO] __main__: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.57      0.57         7\n",
      "           1       0.83      0.83      0.83        18\n",
      "\n",
      "    accuracy                           0.76        25\n",
      "   macro avg       0.70      0.70      0.70        25\n",
      "weighted avg       0.76      0.76      0.76        25\n",
      "\n",
      "2024-12-31 14:25:09,395 [INFO] __main__: Classification report saved to ../../ml-preprocessing-utils/data/dataset/test/models/classification_report.txt\n",
      "2024-12-31 14:25:09,493 [INFO] __main__: Confusion matrix saved to ../../ml-preprocessing-utils/data/dataset/test/models/confusion_matrix.png\n",
      "2024-12-31 14:25:09,494 [INFO] __main__: Original X shape: (25, 15)\n",
      "2024-12-31 14:25:09,494 [INFO] __main__: X has more than 2 features, applying PCA for visualization.\n",
      "2024-12-31 14:25:09,500 [INFO] __main__: PCA explained variance ratios: [0.45925915 0.23948802]\n",
      "2024-12-31 14:25:09,501 [INFO] __main__: Transformed X shape for plotting: (25, 2)\n",
      "2024-12-31 14:25:09,505 [INFO] __main__: Mesh grid created with shape xx: (1375, 1319), yy: (1375, 1319)\n",
      "2024-12-31 14:25:09,512 [INFO] __main__: Grid points in 2D PCA space shape: (1813625, 2)\n",
      "2024-12-31 14:25:09,512 [INFO] __main__: Inverse transforming grid points back to original feature space for prediction.\n",
      "2024-12-31 14:25:09,596 [INFO] __main__: Grid points in original feature space shape: (1813625, 15)\n",
      "2024-12-31 14:25:09,834 [INFO] __main__: Decision boundary predictions reshaped to: (1375, 1319)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:25:10,018 [INFO] __main__: Decision boundary plot displayed successfully.\n",
      "2024-12-31 14:25:10,019 [WARNING] __main__: Unsupported model: Random Forest. Skipping.\n",
      "2024-12-31 14:25:10,020 [WARNING] __main__: Unsupported model: Decision Tree. Skipping.\n",
      "2024-12-31 14:25:10,021 [INFO] __main__: ✅ Best model is XGBoost with Log Loss=0.6711150121950691\n",
      "2024-12-31 14:25:10,042 [INFO] __main__: Model saved to /workspaces/spl_freethrow_biomechanics_analysis_ml_prediction/ml-preprocessing-utils/data/dataset/test/models/XGBoost_model.pkl\n",
      "2024-12-31 14:25:10,047 [INFO] __main__: ✅ Model 'XGBoost_model.pkl' saved successfully in '/workspaces/spl_freethrow_biomechanics_analysis_ml_prediction/ml-preprocessing-utils/data/dataset/test/models'.\n",
      "2024-12-31 14:25:10,060 [INFO] __main__: ✅ Tuning results saved to ../../ml-preprocessing-utils/data/dataset/test/models/tuning_results.json.\n",
      "2024-12-31 14:25:10,061 [INFO] __main__: ✅ Training workflow completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# src/ml/train.py\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "from typing import Any, Dict\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import joblib  # Ensure joblib is imported\n",
    "\n",
    "# Local imports - Adjust the import paths based on your project structure\n",
    "# from feature_manager import FeatureManager\n",
    "# from data_preprocessor import DataPreprocessor\n",
    "# from train_utils.train_utils import (\n",
    "#     evaluate_model, save_model, load_model, plot_decision_boundary,\n",
    "#     tune_random_forest, tune_xgboost, tune_decision_tree\n",
    "# )\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(name)s: %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def load_config(config_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Load configuration from a YAML file.\n",
    "\n",
    "    Args:\n",
    "        config_path (str): Path to the configuration file.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Any]: Configuration dictionary.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(config_path, 'r') as f:\n",
    "            config = yaml.safe_load(f)\n",
    "        logger.info(f\"✅ Configuration loaded from {config_path}.\")\n",
    "        return config\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Failed to load configuration: {e}\")\n",
    "        raise\n",
    "\n",
    "def bayes_best_model_train(\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    "    X_test: pd.DataFrame,\n",
    "    y_test: pd.Series,\n",
    "    selection_metric: str,\n",
    "    model_save_dir: str,\n",
    "    classification_save_path: str,\n",
    "    tuning_results_save: str,\n",
    "    selected_models: Any,\n",
    "    use_pca: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    A streamlined function that:\n",
    "      1) Tunes and trains models using Bayesian optimization.\n",
    "      2) Evaluates the best model.\n",
    "      3) Saves the tuning results and best model.\n",
    "\n",
    "    Args:\n",
    "        X_train, y_train: Training features and labels.\n",
    "        X_test, y_test: Test features and labels.\n",
    "        selection_metric (str): Metric to select best model (e.g., \"Log Loss\", \"accuracy\").\n",
    "        model_save_dir (str): Directory to save the best model.\n",
    "        classification_save_path (str): Path to save classification report.\n",
    "        tuning_results_save (str): Path to save tuning results in JSON format.\n",
    "        selected_models (list|str): List of models (e.g. [\"XGBoost\", \"Random Forest\"]) or a single string.\n",
    "        use_pca (bool): If True, uses PCA for boundary plotting. Typically False for tree-based models.\n",
    "    \"\"\"\n",
    "    logger.info(\"Starting the Bayesian hyperparameter tuning process...\")\n",
    "\n",
    "    # Scoring metric selection\n",
    "    scoring_metric = \"neg_log_loss\" if selection_metric.lower() == \"log loss\" else \"accuracy\"\n",
    "\n",
    "    # Prepare model registry\n",
    "    model_registry = {\n",
    "        \"XGBoost\": tune_xgboost,\n",
    "        # \"Random Forest\": tune_random_forest,\n",
    "        # \"Decision Tree\": tune_decision_tree\n",
    "    }\n",
    "\n",
    "    # Normalize selected_models input\n",
    "    if isinstance(selected_models, str):\n",
    "        selected_models = [selected_models]\n",
    "    elif not selected_models:\n",
    "        selected_models = list(model_registry.keys())\n",
    "        logger.info(f\"No models specified. Using all available: {selected_models}\")\n",
    "\n",
    "    tuning_results = {}\n",
    "    best_model_name = None\n",
    "    best_model = None\n",
    "    best_metric_value = None\n",
    "\n",
    "    # Loop over requested models\n",
    "    for model_name in selected_models:\n",
    "        if model_name not in model_registry:\n",
    "            logger.warning(f\"Unsupported model: {model_name}. Skipping.\")\n",
    "            continue\n",
    "        try:\n",
    "            logger.info(f\"📌 Tuning hyperparameters for {model_name}...\")\n",
    "            tuner_func = model_registry[model_name]\n",
    "\n",
    "            best_params, best_score, best_estimator = tuner_func(\n",
    "                X_train, y_train, scoring_metric=scoring_metric\n",
    "            )\n",
    "            logger.info(f\"✅ {model_name} tuning done. Best Params: {best_params}, Best CV Score: {best_score}\")\n",
    "\n",
    "            # Evaluate on X_test\n",
    "            metrics = evaluate_model(best_estimator, X_test, y_test, save_path=classification_save_path)\n",
    "            metric_value = metrics.get(selection_metric.lower().replace(\" \", \"_\"))  # Adjust key\n",
    "\n",
    "            if metric_value is not None:\n",
    "                logger.debug(f\"Metric value for {selection_metric}: {metric_value}\")\n",
    "                if best_metric_value is None:\n",
    "                    best_metric_value = metric_value\n",
    "                    best_model_name = model_name\n",
    "                    best_model = best_estimator\n",
    "                    logger.debug(f\"Best model set to {best_model_name} with {selection_metric}={best_metric_value}\")\n",
    "                else:\n",
    "                    # For log loss, lower is better\n",
    "                    if selection_metric.lower() == \"log loss\" and metric_value < best_metric_value:\n",
    "                        best_metric_value = metric_value\n",
    "                        best_model_name = model_name\n",
    "                        best_model = best_estimator\n",
    "                        logger.debug(f\"Best model updated to {best_model_name} with {selection_metric}={best_metric_value}\")\n",
    "                    # For other metrics (accuracy, f1, etc.), higher is better\n",
    "                    elif selection_metric.lower() != \"log loss\" and metric_value > best_metric_value:\n",
    "                        best_metric_value = metric_value\n",
    "                        best_model_name = model_name\n",
    "                        best_model = best_estimator\n",
    "                        logger.debug(f\"Best model updated to {best_model_name} with {selection_metric}={best_metric_value}\")\n",
    "            else:\n",
    "                logger.debug(f\"Metric value for {selection_metric} is None. Best model not updated.\")\n",
    "\n",
    "            # Save partial results\n",
    "            tuning_results[model_name] = {\n",
    "                \"Best Params\": best_params,\n",
    "                \"Best CV Score\": best_score,\n",
    "                \"Evaluation Metrics\": metrics,\n",
    "            }\n",
    "\n",
    "            # Plot boundary (optional for tree-based with PCA)\n",
    "            try:\n",
    "                plot_decision_boundary(best_estimator, X_test, y_test, f\"{model_name} Decision Boundary\", use_pca=use_pca)\n",
    "            except ValueError as e:\n",
    "                logger.warning(f\"Skipping decision boundary plot for {model_name}: {e}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Error tuning {model_name}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Save best model\n",
    "    if best_model_name:\n",
    "        logger.info(f\"✅ Best model is {best_model_name} with {selection_metric}={best_metric_value}\")\n",
    "        try:\n",
    "            absolute_save_dir = os.path.abspath(model_save_dir)\n",
    "            logger.debug(f\"Absolute model save directory: {absolute_save_dir}\")\n",
    "            os.makedirs(absolute_save_dir, exist_ok=True)\n",
    "            logger.debug(f\"Ensured that the model save directory '{absolute_save_dir}' exists.\")\n",
    "\n",
    "            logger.debug(f\"Attempting to save model '{best_model_name}' to '{absolute_save_dir}'.\")\n",
    "            logger.debug(\"Calling save_model function...\")\n",
    "            save_model(best_model, best_model_name, save_dir=absolute_save_dir)\n",
    "            logger.debug(\"save_model function called successfully.\")\n",
    "            logger.info(f\"✅ Model '{best_model_name}_model.pkl' saved successfully in '{absolute_save_dir}'.\")\n",
    "\n",
    "            # Confirm the model file exists\n",
    "            model_file_path = os.path.join(absolute_save_dir, f\"{best_model_name}_model.pkl\")\n",
    "            if os.path.isfile(model_file_path):\n",
    "                logger.debug(f\"Model file exists: {model_file_path}\")\n",
    "            else:\n",
    "                logger.error(f\"Model file was not found after saving: {model_file_path}\")\n",
    "\n",
    "            # List directory contents\n",
    "            logger.debug(f\"Directory contents after saving: {os.listdir(absolute_save_dir)}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Failed to save best model {best_model_name}: {e}\")\n",
    "            raise  # Ensure the exception is propagated\n",
    "    else:\n",
    "        logger.warning(\"⚠️ No best model was selected. Tuning might have failed for all models.\")\n",
    "\n",
    "    # Save tuning results\n",
    "    try:\n",
    "        with open(tuning_results_save, \"w\") as f:\n",
    "            json.dump(tuning_results, f, indent=4)\n",
    "        logger.info(f\"✅ Tuning results saved to {tuning_results_save}.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Error saving tuning results: {e}\")\n",
    "\n",
    "def main():\n",
    "    # ----------------------------\n",
    "    # 1. Load Configuration\n",
    "    # ----------------------------\n",
    "    config = load_config('../../ml-preprocessing-utils/data/dataset/test/preprocessor_config/preprocessor_config.yaml')\n",
    "\n",
    "    # Global paths\n",
    "    model_save_dir = config.get('execution', {}).get('train', {}).get('model_save_path', '../../ml-preprocessing-utils/data/dataset/test/models')\n",
    "    tuning_results_save = os.path.join(model_save_dir, \"tuning_results.json\")\n",
    "    classification_save_path = os.path.join(model_save_dir, \"classification_report.txt\")\n",
    "    plots_dir = config.get('execution', {}).get('shared', {}).get('plot_output_dir', '../../ml-preprocessing-utils/data/dataset/test/plots')\n",
    "\n",
    "    # Extract model-related config\n",
    "    selected_models = config.get('models', {}).get('selected_models', [\"XGBoost\", \"Random Forest\", \"Decision Tree\"])\n",
    "    selection_metric = config.get('models', {}).get('selection_metric', \"Log Loss\")\n",
    "\n",
    "    # Extract Tree Based Classifier options from config\n",
    "    tree_classifier_options = config.get('models', {}).get('Tree Based Classifier', {})\n",
    "\n",
    "    # ----------------------------\n",
    "    # 2. Initialize and Use FeatureManager\n",
    "    # ----------------------------\n",
    "    # Initialize FeatureManager with the desired save_path\n",
    "    save_path = config.get('execution', {}).get('shared', {}).get('features_metadata_path', '../../ml-preprocessing-utils/data/dataset/test/features_info/features_metadata.pkl')\n",
    "    feature_manager = FeatureManager(save_path=save_path)\n",
    "    try:\n",
    "        # Load the dataset and column assets from the metadata\n",
    "        filtered_df, column_assets = feature_manager.load_features_and_dataset(debug=False)\n",
    "        logger.info(\"✅ Filtered dataset and column assets loaded successfully via FeatureManager.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Failed to load dataset and metadata: {e}\")\n",
    "        return\n",
    "\n",
    "    # ----------------------------\n",
    "    # 3. Initialize DataPreprocessor\n",
    "    # ----------------------------\n",
    "    # We assume a supervised classification use case: \"Tree Based Classifier\"\n",
    "    preprocessor = DataPreprocessor(\n",
    "        model_type=\"Tree Based Classifier\",\n",
    "        column_assets=column_assets,\n",
    "        mode='train',\n",
    "        options=tree_classifier_options,  # The options from config for \"Tree Based Classifier\"\n",
    "        debug=config.get('logging', {}).get('debug', False),  # or config-based\n",
    "        normalize_debug=config.get('execution', {}).get('train', {}).get('normalize_debug', False),\n",
    "        normalize_graphs_output=config.get('execution', {}).get('train', {}).get('normalize_graphs_output', False),\n",
    "        graphs_output_dir=config.get('execution', {}).get('shared', {}).get('plot_output_dir', '../../ml-preprocessing-utils/data/dataset/test/plots'),\n",
    "        transformers_dir=config.get('execution', {}).get('train', {}).get('save_transformers_path', '../../ml-preprocessing-utils/data/dataset/test/transformers')\n",
    "    )\n",
    "\n",
    "    # ----------------------------\n",
    "    # 4. Execute Preprocessing\n",
    "    # ----------------------------\n",
    "    try:\n",
    "        # Execute preprocessing by passing the entire filtered_df\n",
    "        X_train, X_test, y_train, y_test, recommendations, X_test_inverse = preprocessor.final_preprocessing(filtered_df)\n",
    "        \n",
    "        logger.info(f\"✅ Preprocessing complete. X_train shape: {X_train.shape}, X_test shape: {X_test.shape}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Error during preprocessing: {e}\")\n",
    "        return\n",
    "\n",
    "    # ----------------------------\n",
    "    # 5. Train & Tune the Model\n",
    "    # ----------------------------\n",
    "    try:\n",
    "        bayes_best_model_train(\n",
    "            X_train=X_train,\n",
    "            y_train=y_train,\n",
    "            X_test=X_test,\n",
    "            y_test=y_test,\n",
    "            selection_metric=selection_metric,\n",
    "            model_save_dir=model_save_dir,\n",
    "            classification_save_path=classification_save_path,\n",
    "            tuning_results_save=tuning_results_save,\n",
    "            selected_models=selected_models,\n",
    "            use_pca=True  \n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Model training/tuning failed: {e}\")\n",
    "        return\n",
    "\n",
    "    # ----------------------------\n",
    "    # 6. Load and Use the Saved Model (Optional)\n",
    "    # ----------------------------\n",
    "    # This step can be considered optional since predictions are handled in predict.py\n",
    "    # If you still need to verify the saved model here, you can do so\n",
    "    \"\"\"\n",
    "    load_model_name = \"XGBoost\"\n",
    "    model_file_path = os.path.join(os.path.abspath(model_save_dir), f\"{load_model_name}_model.pkl\")\n",
    "    \n",
    "    if os.path.isfile(model_file_path):\n",
    "        logger.info(f\"Loading model: {load_model_name} from {model_file_path}\")\n",
    "        try:\n",
    "            loaded_model = load_model(load_model_name, save_dir=model_save_dir)\n",
    "            predictions = loaded_model.predict(X_test)\n",
    "            logger.info(f\"Predictions from loaded model: {predictions}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Failed to load or use the model: {e}\")\n",
    "            return\n",
    "    else:\n",
    "        logger.error(f\"❌ Model file '{model_file_path}' does not exist. Cannot load the model.\")\n",
    "        return\n",
    "    \n",
    "    # Integrate predictions\n",
    "    try:\n",
    "        if X_test_inverse is not None:\n",
    "            # Insert predictions into the inverse-transformed DataFrame\n",
    "            X_test_inverse['Prediction'] = predictions\n",
    "            logger.debug(\"Predictions attached to inverse-transformed DataFrame.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Inverse transformation failed: {e}\")\n",
    "    \"\"\"\n",
    "\n",
    "    logger.info(\"✅ Training workflow completed successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:25:10,107 [INFO] __main__: ✅ Configuration loaded from ../../ml-preprocessing-utils/data/dataset/test/preprocessor_config/preprocessor_config.yaml.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:25:10,114 [INFO] predict_preprocessing: ✅ DataPreprocessor initialized in 'predict' mode.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:25:10,115 [INFO] Step: Load Transformers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:25:10,115 [INFO] DataPreprocessor: Step: Load Transformers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:25:10,123 [INFO] Transformers loaded successfully from '../../ml-preprocessing-utils/data/dataset/test/transformers/transformers.pkl'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:25:10,123 [INFO] DataPreprocessor: Transformers loaded successfully from '../../ml-preprocessing-utils/data/dataset/test/transformers/transformers.pkl'.\n",
      "2024-12-31 14:25:10,124 [INFO] predict_preprocessing: ✅ Transformers loaded successfully.\n",
      "2024-12-31 14:25:10,138 [INFO] predict_preprocessing: ✅ Trained model loaded from '../../ml-preprocessing-utils/data/dataset/test/models/XGBoost_model.pkl'.\n",
      "2024-12-31 14:25:10,152 [INFO] predict_preprocessing: ✅ Prediction input data loaded from '../../ml-preprocessing-utils/data/dataset/test/data/final_ml_dataset.csv'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:25:10,156 [INFO] Data transformed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:25:10,156 [INFO] DataPreprocessor: Data transformed.\n",
      "2024-12-31 14:25:10,157 [INFO] predict_preprocessing: ✅ New data preprocessed successfully.\n",
      "2024-12-31 14:25:10,166 [INFO] predict_preprocessing: ✅ Predictions made successfully on new data.\n",
      "2024-12-31 14:25:10,171 [INFO] predict_preprocessing: ✅ Predicted probabilities added to the dataset.\n",
      "2024-12-31 14:25:10,172 [INFO] predict_preprocessing: ✅ Predictions integrated into the dataset successfully.\n",
      "2024-12-31 14:25:10,175 [INFO] InverseTransform: ✅ Inverse transformation completed successfully.\n",
      "2024-12-31 14:25:10,176 [INFO] predict_preprocessing: ✅ Inverse transformations applied successfully.\n",
      "2024-12-31 14:25:10,176 [INFO] predict_preprocessing: Inverse-transformed X_new_inverse.shape=(125, 15)\n",
      "2024-12-31 14:25:10,191 [INFO] predict_preprocessing: ✅ Predictions saved to '../../ml-preprocessing-utils/data/dataset/test/data/predictions.csv'.\n",
      "\n",
      "Inverse Transformed Prediction DataFrame with Predictions:\n",
      "   release_ball_direction_x  release_ball_direction_z  \\\n",
      "0                  0.377012                  0.926203   \n",
      "1                  0.417644                  0.901906   \n",
      "2                  0.383086                  0.922353   \n",
      "3                  0.292054                  0.956396   \n",
      "4                  0.389917                  0.918894   \n",
      "\n",
      "   release_ball_direction_y  elbow_release_angle  elbow_max_angle  \\\n",
      "0                  0.002969            72.325830       106.272118   \n",
      "1                 -0.110176            58.430068       101.798798   \n",
      "2                 -0.050096            73.883728       106.989633   \n",
      "3                  0.003209            71.424835       104.160865   \n",
      "4                 -0.059987            73.619348       103.832024   \n",
      "\n",
      "   wrist_release_angle  wrist_max_angle  knee_release_angle  knee_max_angle  \\\n",
      "0            28.102765        35.918406           32.646276       63.541007   \n",
      "1            32.766626        38.693790           33.834026       65.565635   \n",
      "2            26.220943        38.998089           33.550293       65.257346   \n",
      "3            22.649881        36.353038           29.568453       64.652573   \n",
      "4            21.754498        34.532455           32.856306       64.165568   \n",
      "\n",
      "   release_ball_speed  calculated_release_angle  release_ball_velocity_x  \\\n",
      "0            9.907618                 62.959206                 3.735294   \n",
      "1           11.826803                 64.964999                 4.939394   \n",
      "2           10.283314                 60.314694                 3.939394   \n",
      "3            9.164302                 63.418903                 2.676471   \n",
      "4           11.113489                 60.900817                 4.333333   \n",
      "\n",
      "   release_ball_velocity_y  release_ball_velocity_z  \\\n",
      "0                 0.029412                 9.176471   \n",
      "1                -1.303030                10.666667   \n",
      "2                -0.515152                 9.484848   \n",
      "3                 0.029412                 8.764706   \n",
      "4                -0.666667                10.212121   \n",
      "\n",
      "  player_estimated_hand_length_cm_category  Prediction  \n",
      "0                                   Medium           0  \n",
      "1                                   Medium           1  \n",
      "2                                   Medium           0  \n",
      "3                                   Medium           0  \n",
      "4                                   Medium           0  \n",
      "2024-12-31 14:25:10,194 [INFO] predict_preprocessing: ✅ Predict mode executed successfully.\n"
     ]
    }
   ],
   "source": [
    "# src/freethrow_predictions/ml/predict.py\n",
    "\n",
    "import logging\n",
    "import os\n",
    "from typing import Any, Dict\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "# Local imports\n",
    "# from data_preprocessor import DataPreprocessor  # Ensure correct import path\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(name)s: %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def load_config(config_path: str) -> Dict[str, Any]:\n",
    "    try:\n",
    "        with open(config_path, 'r') as f:\n",
    "            config = yaml.safe_load(f)\n",
    "        logger.info(f\"✅ Configuration loaded from {config_path}.\")\n",
    "        return config\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Failed to load configuration: {e}\")\n",
    "        raise\n",
    "\n",
    "def load_dataset(path: str) -> pd.DataFrame:\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Dataset not found at {path}\")\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "def main():\n",
    "    # ----------------------------\n",
    "    # 1. Load Configuration\n",
    "    # ----------------------------\n",
    "    config_path = '../../ml-preprocessing-utils/data/dataset/test/preprocessor_config/preprocessor_config.yaml'\n",
    "    try:\n",
    "        config = load_config(config_path)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Cannot proceed without configuration: {e}\")\n",
    "        return\n",
    "\n",
    "    # ----------------------------\n",
    "    # 2. Configure Logging\n",
    "    # ----------------------------\n",
    "    logger_config = config.get('logging', {})\n",
    "    logger_level = logger_config.get('level', 'INFO').upper()\n",
    "    logger_format = logger_config.get('format', '%(asctime)s [%(levelname)s] %(message)s')\n",
    "    debug_flag = config.get('logging', {}).get('debug', False)\n",
    "    logging.getLogger().setLevel(logging.DEBUG if debug_flag else getattr(logging, logger_level, logging.INFO))\n",
    "    logger = logging.getLogger('predict_preprocessing')\n",
    "\n",
    "    # ----------------------------\n",
    "    # 3. Extract Feature Assets and Execution Parameters for Predict\n",
    "    # ----------------------------\n",
    "    features_config = config.get('features', {})\n",
    "    column_assets = {\n",
    "        'y_variable': features_config.get('y_variable', []),  # Not used in predict\n",
    "        'ordinal_categoricals': features_config.get('ordinal_categoricals', []),\n",
    "        'nominal_categoricals': features_config.get('nominal_categoricals', []),\n",
    "        'numericals': features_config.get('numericals', [])\n",
    "    }\n",
    "\n",
    "    execution_predict = config.get('execution', {}).get('predict', {})\n",
    "    prediction_input_path = execution_predict.get('prediction_input_path', '')\n",
    "    load_transformers_path = execution_predict.get('load_transformers_path', '')\n",
    "    trained_model_path = execution_predict.get('trained_model_path', '')\n",
    "    predictions_output_path = execution_predict.get('predictions_output_path', './predictions')\n",
    "    normalize_debug = execution_predict.get('normalize_debug', False)\n",
    "    normalize_graphs_output = execution_predict.get('normalize_graphs_output', False)\n",
    "\n",
    "    # Validate essential paths\n",
    "    if not prediction_input_path:\n",
    "        logger.error(\"❌ 'prediction_input_path' for predict mode is not specified in the configuration.\")\n",
    "        return\n",
    "    if not os.path.exists(prediction_input_path):\n",
    "        logger.error(f\"❌ Prediction input dataset not found at {prediction_input_path}.\")\n",
    "        return\n",
    "    if not load_transformers_path:\n",
    "        logger.error(\"❌ 'load_transformers_path' for predict mode is not specified in the configuration.\")\n",
    "        return\n",
    "    if not os.path.exists(load_transformers_path):\n",
    "        logger.error(f\"❌ Transformers file not found at {load_transformers_path}.\")\n",
    "        return\n",
    "    if not trained_model_path:\n",
    "        logger.error(\"❌ 'trained_model_path' for predict mode is not specified in the configuration.\")\n",
    "        return\n",
    "    if not os.path.exists(trained_model_path):\n",
    "        logger.error(f\"❌ Trained model not found at {trained_model_path}.\")\n",
    "        return\n",
    "\n",
    "    # ----------------------------\n",
    "    # 4. Initialize DataPreprocessor in Predict Mode\n",
    "    # ----------------------------\n",
    "    try:\n",
    "        preprocessor = DataPreprocessor(\n",
    "            model_type=config.get('model_type', 'Tree Based Classifier'),\n",
    "            column_assets=column_assets,\n",
    "            mode='predict',\n",
    "            options=config.get('models', {}).get(config.get('model_type', 'Tree Based Classifier'), {}),\n",
    "            debug=debug_flag,\n",
    "            normalize_debug=normalize_debug,\n",
    "            normalize_graphs_output=normalize_graphs_output,\n",
    "            graphs_output_dir=config.get('execution', {}).get('shared', {}).get('plot_output_dir', './plots'),\n",
    "            transformers_dir=os.path.dirname(load_transformers_path)\n",
    "        )\n",
    "        logger.info(\"✅ DataPreprocessor initialized in 'predict' mode.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Failed to initialize DataPreprocessor: {e}\")\n",
    "        return\n",
    "\n",
    "    # ----------------------------\n",
    "    # 5. Load Trained Transformers\n",
    "    # ----------------------------\n",
    "    try:\n",
    "        transformers = preprocessor.load_transformers()\n",
    "        logger.info(\"✅ Transformers loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Failed to load transformers: {e}\")\n",
    "        return\n",
    "\n",
    "    # ----------------------------\n",
    "    # 6. Load Trained Model\n",
    "    # ----------------------------\n",
    "    try:\n",
    "        model = joblib.load(trained_model_path)\n",
    "        logger.info(f\"✅ Trained model loaded from '{trained_model_path}'.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Failed to load trained model: {e}\")\n",
    "        return\n",
    "\n",
    "    # ----------------------------\n",
    "    # 7. Load Prediction Input Data\n",
    "    # ----------------------------\n",
    "    try:\n",
    "        X_new = load_dataset(prediction_input_path)\n",
    "        logger.info(f\"✅ Prediction input data loaded from '{prediction_input_path}'.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Failed to load prediction input data: {e}\")\n",
    "        return\n",
    "\n",
    "    # ----------------------------\n",
    "    # 8. Preprocess the New Data\n",
    "    # ----------------------------\n",
    "    try:\n",
    "        X_new_preprocessed = preprocessor.transform(X_new)\n",
    "        logger.info(\"✅ New data preprocessed successfully.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Preprocessing of new data failed: {e}\")\n",
    "        return\n",
    "\n",
    "    # ----------------------------\n",
    "    # 9. Make Predictions\n",
    "    # ----------------------------\n",
    "    try:\n",
    "        predictions = model.predict(X_new_preprocessed)\n",
    "        logger.info(\"✅ Predictions made successfully on new data.\")\n",
    "\n",
    "        # If the model supports predict_proba, get probabilities\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            y_proba = model.predict_proba(X_new_preprocessed)[:, 1]\n",
    "            X_new['Prediction_Probability'] = y_proba\n",
    "            logger.info(\"✅ Predicted probabilities added to the dataset.\")\n",
    "        else:\n",
    "            logger.info(\"⚠️ Model does not support probability predictions.\")\n",
    "\n",
    "        # Add predictions to the original prediction dataset\n",
    "        X_new['Prediction'] = predictions\n",
    "        logger.info(\"✅ Predictions integrated into the dataset successfully.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Failed to make predictions: {e}\")\n",
    "        return\n",
    "\n",
    "    # ----------------------------\n",
    "    # 10. Inverse Transform the Data for Interpretability\n",
    "    # ----------------------------\n",
    "    try:\n",
    "        X_new_inverse = preprocessor.inverse_transform_data(X_new_preprocessed)\n",
    "        logger.info(\"✅ Inverse transformations applied successfully.\")\n",
    "        logger.info(f\"Inverse-transformed X_new_inverse.shape={X_new_inverse.shape}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Inverse transformations failed: {e}\")\n",
    "        # Proceed with predictions only\n",
    "        X_new_inverse = pd.DataFrame({'Prediction': predictions})\n",
    "        logger.warning(\"Inverse transformation was not applied. Proceeding with predictions only.\")\n",
    "\n",
    "    # Attach predictions to inverse-transformed DataFrame\n",
    "    try:\n",
    "        if X_new_inverse is not None and 'Prediction' not in X_new_inverse.columns:\n",
    "            X_new_inverse['Prediction'] = predictions\n",
    "            logger.debug(\"Predictions attached to inverse-transformed DataFrame.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Failed to attach predictions to inverse-transformed DataFrame: {e}\")\n",
    "        X_new_inverse = pd.DataFrame({'Prediction': predictions})  # Assign to X_new_inverse\n",
    "\n",
    "    # ----------------------------\n",
    "    # 11. Save Predictions\n",
    "    # ----------------------------\n",
    "    try:\n",
    "        os.makedirs(predictions_output_path, exist_ok=True)\n",
    "        X_new_inverse.to_csv(os.path.join(predictions_output_path, 'predictions.csv'), index=False)\n",
    "        logger.info(f\"✅ Predictions saved to '{predictions_output_path}/predictions.csv'.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Failed to save predictions: {e}\")\n",
    "        return\n",
    "\n",
    "    # ----------------------------\n",
    "    # 12. Display Predictions\n",
    "    # ----------------------------\n",
    "    try:\n",
    "        print(\"\\nInverse Transformed Prediction DataFrame with Predictions:\")\n",
    "        print(X_new_inverse.head())\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Error during displaying predictions: {e}\")\n",
    "        return\n",
    "\n",
    "    logger.info(\"✅ Predict mode executed successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile ../../src/freethrow_predictions/ml/shap/shap_utils.py\n",
    "\n",
    "import logging\n",
    "import shap\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "\n",
    "def setup_logging(debug: bool = False) -> logging.Logger:\n",
    "    \"\"\"\n",
    "    Configure and return a logger.\n",
    "\n",
    "    :param debug: If True, set logging level to DEBUG. Otherwise, INFO.\n",
    "    :return: Configured logger.\n",
    "    \"\"\"\n",
    "    level = logging.DEBUG if debug else logging.INFO\n",
    "    logging.basicConfig(level=level, \n",
    "                        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "                        datefmt='%Y-%m-%d %H:%M:%S')\n",
    "    logger = logging.getLogger(__name__)\n",
    "    return logger\n",
    "\n",
    "\n",
    "def compute_shap_values(model, X_transformed: pd.DataFrame, debug: bool = False, logger: logging.Logger = None):\n",
    "    \"\"\"\n",
    "    Compute SHAP values for the transformed data.\n",
    "\n",
    "    :param model: Trained machine learning model.\n",
    "    :param X_transformed: Transformed features used for prediction (pd.DataFrame).\n",
    "    :param debug: If True, enable detailed debug logs.\n",
    "    :param logger: Logger instance for logging.\n",
    "    :return: Tuple of (explainer, shap_values).\n",
    "    \"\"\"\n",
    "    if logger:\n",
    "        logger.info(\"Initializing SHAP explainer...\")\n",
    "    try:\n",
    "        explainer = shap.Explainer(model, X_transformed)\n",
    "        shap_values = explainer(X_transformed)\n",
    "        if logger:\n",
    "            logger.info(\"SHAP values computed successfully.\")\n",
    "            if debug:\n",
    "                logger.debug(f\"SHAP values shape: {shap_values.values.shape}\")\n",
    "    except Exception as e:\n",
    "        if logger:\n",
    "            logger.error(f\"SHAP value computation failed: {e}\")\n",
    "        raise\n",
    "    return explainer, shap_values\n",
    "\n",
    "\n",
    "def plot_shap_summary(shap_values, X_original: pd.DataFrame, save_path: str, debug: bool = False, \n",
    "                      logger: logging.Logger = None):\n",
    "    \"\"\"\n",
    "    Generate and save a SHAP summary plot.\n",
    "\n",
    "    :param shap_values: SHAP values computed for the dataset.\n",
    "    :param X_original: Original (untransformed) feature DataFrame.\n",
    "    :param save_path: Full file path to save the plot.\n",
    "    :param debug: If True, enable detailed debug logs.\n",
    "    :param logger: Logger instance for logging.\n",
    "    \"\"\"\n",
    "    if logger:\n",
    "        logger.info(\"Generating SHAP summary plot...\")\n",
    "    try:\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        shap.summary_plot(shap_values, X_original, show=False)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        if logger and debug:\n",
    "            logger.debug(f\"SHAP summary plot saved to {save_path}\")\n",
    "        if logger:\n",
    "            logger.info(\"SHAP summary plot generated successfully.\")\n",
    "    except Exception as e:\n",
    "        if logger:\n",
    "            logger.error(f\"Failed to generate SHAP summary plot: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def plot_shap_dependence(shap_values, feature: str, X_original: pd.DataFrame, save_path: str, debug: bool = False, \n",
    "                         logger: logging.Logger = None):\n",
    "    \"\"\"\n",
    "    Generate and save a SHAP dependence plot for a specific feature.\n",
    "\n",
    "    :param shap_values: SHAP values computed for the dataset.\n",
    "    :param feature: Feature name to generate the dependence plot for.\n",
    "    :param X_original: Original (untransformed) feature DataFrame.\n",
    "    :param save_path: Full file path to save the plot.\n",
    "    :param debug: If True, enable detailed debug logs.\n",
    "    :param logger: Logger instance for logging.\n",
    "    \"\"\"\n",
    "    if logger:\n",
    "        logger.info(f\"Generating SHAP dependence plot for feature '{feature}'...\")\n",
    "    try:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        shap.dependence_plot(feature, shap_values.values, X_original, show=False)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        if logger and debug:\n",
    "            logger.debug(f\"SHAP dependence plot for '{feature}' saved to {save_path}\")\n",
    "        if logger:\n",
    "            logger.info(f\"SHAP dependence plot for feature '{feature}' generated successfully.\")\n",
    "    except Exception as e:\n",
    "        if logger:\n",
    "            logger.error(f\"Failed to generate SHAP dependence plot for feature '{feature}': {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def generate_global_recommendations(shap_values, X_original: pd.DataFrame, top_n: int = 5, debug: bool = False, \n",
    "                                    use_mad: bool = False, logger: logging.Logger = None) -> dict:\n",
    "    \"\"\"\n",
    "    Generate global recommendations based on SHAP values and feature distributions.\n",
    "\n",
    "    :param shap_values: SHAP values computed for the dataset.\n",
    "    :param X_original: Original (untransformed) feature DataFrame.\n",
    "    :param top_n: Number of top features to generate recommendations for.\n",
    "    :param debug: If True, enable detailed debug logs.\n",
    "    :param use_mad: If True, use Median Absolute Deviation for range definition.\n",
    "    :param logger: Logger instance for logging.\n",
    "    :return: recommendations: Dictionary mapping features to recommended value ranges, importance, and direction.\n",
    "    \"\"\"\n",
    "    if logger:\n",
    "        logger.info(\"Generating feature importance based on SHAP values...\")\n",
    "    try:\n",
    "        shap_df = pd.DataFrame(shap_values.values, columns=X_original.columns)\n",
    "        \n",
    "        # Calculate mean absolute SHAP values for importance\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'feature': X_original.columns,\n",
    "            'importance': np.abs(shap_df).mean(axis=0),\n",
    "            'mean_shap': shap_df.mean(axis=0)\n",
    "        }).sort_values(by='importance', ascending=False)\n",
    "        \n",
    "        if logger and debug:\n",
    "            logger.debug(f\"Feature importance (top {top_n}):\\n{feature_importance.head(top_n)}\")\n",
    "        \n",
    "        top_features = feature_importance.head(top_n)['feature'].tolist()\n",
    "        recommendations = {}\n",
    "        \n",
    "        for feature in top_features:\n",
    "            feature_values = X_original[feature]\n",
    "            \n",
    "            if use_mad:\n",
    "                # Use Median and MAD for robust statistics\n",
    "                median = feature_values.median()\n",
    "                mad = feature_values.mad()\n",
    "                lower_bound = median - 1.5 * mad\n",
    "                upper_bound = median + 1.5 * mad\n",
    "                range_str = f\"{lower_bound:.1f}–{upper_bound:.1f}\"\n",
    "            else:\n",
    "                # Default to Interquartile Range (IQR)\n",
    "                lower_bound = feature_values.quantile(0.25)\n",
    "                upper_bound = feature_values.quantile(0.75)\n",
    "                range_str = f\"{lower_bound:.1f}–{upper_bound:.1f}\"\n",
    "            \n",
    "            # Determine direction based on mean SHAP value\n",
    "            mean_shap = feature_importance.loc[feature_importance['feature'] == feature, 'mean_shap'].values[0]\n",
    "            direction = 'positive' if mean_shap > 0 else 'negative'\n",
    "            \n",
    "            recommendations[feature] = {\n",
    "                'range': range_str,\n",
    "                'importance': round(feature_importance.loc[feature_importance['feature'] == feature, 'importance'].values[0], 4),  # Rounded for readability\n",
    "                'direction': direction\n",
    "            }\n",
    "            if logger and debug:\n",
    "                logger.debug(f\"Recommendation for {feature}: Range={range_str}, Importance={feature_importance.loc[feature_importance['feature'] == feature, 'importance'].values[0]}, Direction={direction}\")\n",
    "        \n",
    "        if logger and debug:\n",
    "            logger.debug(f\"Final Recommendations with Importance and Direction: {recommendations}\")\n",
    "        return recommendations\n",
    "    except Exception as e:\n",
    "        if logger:\n",
    "            logger.error(f\"Failed to generate global recommendations: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def generate_individual_feedback(trial: pd.Series, shap_values_trial: np.ndarray, feature_metadata: dict = None, \n",
    "                                 logger: logging.Logger = None) -> dict:\n",
    "    \"\"\"\n",
    "    Generate specific feedback for a single trial based on its SHAP values and feature metadata.\n",
    "\n",
    "    Args:\n",
    "        trial (pd.Series): A single trial's data.\n",
    "        shap_values_trial (np.ndarray): SHAP values for the trial.\n",
    "        feature_metadata (dict, optional): Additional metadata for features (e.g., units).\n",
    "        logger (logging.Logger, optional): Logger instance for logging.\n",
    "\n",
    "    Returns:\n",
    "        dict: Feedback messages for each feature.\n",
    "    \"\"\"\n",
    "    feedback = {}\n",
    "    feature_names = trial.index.tolist()\n",
    "\n",
    "    for feature, shap_value in zip(feature_names, shap_values_trial):\n",
    "        if shap_value > 0:\n",
    "            adjustment = \"maintain or increase\"\n",
    "            direction = \"positively\"\n",
    "        elif shap_value < 0:\n",
    "            adjustment = \"decrease\"\n",
    "            direction = \"positively\"\n",
    "        else:\n",
    "            feedback[feature] = f\"{feature.replace('_', ' ').capitalize()} has no impact on the prediction.\"\n",
    "            continue\n",
    "\n",
    "        # Map SHAP values to meaningful adjustment magnitudes\n",
    "        # Example: 10% of the current feature value\n",
    "        current_value = trial[feature]\n",
    "        adjustment_factor = 0.1\n",
    "        adjustment_amount = adjustment_factor * abs(current_value)\n",
    "\n",
    "        # Incorporate feature metadata if available\n",
    "        if feature_metadata and feature in feature_metadata:\n",
    "            unit = feature_metadata[feature].get('unit', '')\n",
    "            adjustment_str = f\"{adjustment_amount:.2f} {unit}\" if unit else f\"{adjustment_amount:.2f}\"\n",
    "        else:\n",
    "            adjustment_str = f\"{adjustment_amount:.2f}\"\n",
    "\n",
    "        # Construct feedback message\n",
    "        feedback_message = (\n",
    "            f\"Consider to {adjustment} '{feature.replace('_', ' ')}' by approximately {adjustment_str} \"\n",
    "            f\"to {direction} influence the result.\"\n",
    "        )\n",
    "        feedback[feature] = feedback_message\n",
    "\n",
    "    return feedback\n",
    "\n",
    "\n",
    "def compute_individual_shap_values(explainer, X_transformed: pd.DataFrame, trial_index: int, \n",
    "                                   logger: logging.Logger = None):\n",
    "    \"\"\"\n",
    "    Compute SHAP values for a single trial.\n",
    "\n",
    "    :param explainer: SHAP explainer object.\n",
    "    :param X_transformed: Transformed features used for prediction.\n",
    "    :param trial_index: Index of the trial.\n",
    "    :param logger: Logger instance.\n",
    "    :return: shap_values for the trial.\n",
    "    \"\"\"\n",
    "    if logger:\n",
    "        logger.info(f\"Computing SHAP values for trial at index {trial_index}...\")\n",
    "    try:\n",
    "        trial = X_transformed.iloc[[trial_index]]\n",
    "        shap_values = explainer(trial)\n",
    "        if logger:\n",
    "            logger.debug(f\"SHAP values for trial {trial_index} computed successfully.\")\n",
    "    except Exception as e:\n",
    "        if logger:\n",
    "            logger.error(f\"Failed to compute SHAP values for trial {trial_index}: {e}\")\n",
    "        raise\n",
    "    return shap_values\n",
    "\n",
    "\n",
    "def plot_individual_shap_force(shap_explainer, shap_values, X_original: pd.DataFrame, trial_index: int, \n",
    "                               save_path: str, logger: logging.Logger = None):\n",
    "    \"\"\"\n",
    "    Generate and save a SHAP force plot for a specific trial.\n",
    "\n",
    "    :param shap_explainer: SHAP explainer object.\n",
    "    :param shap_values: SHAP values for the trial.\n",
    "    :param X_original: Original feature DataFrame.\n",
    "    :param trial_index: Index of the trial.\n",
    "    :param save_path: Full file path to save the force plot.\n",
    "    :param logger: Logger instance.\n",
    "    \"\"\"\n",
    "    if logger:\n",
    "        logger.info(f\"Generating SHAP force plot for trial {trial_index}...\")\n",
    "    try:\n",
    "        shap_plot = shap.force_plot(\n",
    "            shap_explainer.expected_value, \n",
    "            shap_values.values[0], \n",
    "            X_original.iloc[trial_index],\n",
    "            matplotlib=False\n",
    "        )\n",
    "        shap.save_html(save_path, shap_plot)\n",
    "        if logger:\n",
    "            logger.debug(f\"SHAP force plot saved to {save_path}\")\n",
    "            logger.info(f\"SHAP force plot for trial {trial_index} generated successfully.\")\n",
    "    except Exception as e:\n",
    "        if logger:\n",
    "            logger.error(f\"Failed to generate SHAP force plot for trial {trial_index}: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def extract_force_plot_values(shap_values, trial_index: int, logger: logging.Logger = None) -> dict:\n",
    "    \"\"\"\n",
    "    Extract SHAP values and feature contributions for a specific trial.\n",
    "\n",
    "    Args:\n",
    "        shap_values (shap.Explanation): SHAP values object.\n",
    "        trial_index (int): Index of the trial.\n",
    "        logger (logging.Logger, optional): Logger instance.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary of feature contributions.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        shap_values_instance = shap_values.values[trial_index]\n",
    "        features_instance = shap_values.data[trial_index]\n",
    "        feature_contributions = dict(zip(shap_values.feature_names, shap_values_instance))\n",
    "        if logger and logger.isEnabledFor(logging.DEBUG):\n",
    "            logger.debug(f\"SHAP values for trial {trial_index}: {feature_contributions}\")\n",
    "        return feature_contributions\n",
    "    except Exception as e:\n",
    "        if logger:\n",
    "            logger.error(f\"Error extracting SHAP values for trial {trial_index}: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def save_shap_values(shap_values, save_path: str, logger: logging.Logger = None):\n",
    "    \"\"\"\n",
    "    Save SHAP values to a file using pickle.\n",
    "\n",
    "    :param shap_values: SHAP values object to save.\n",
    "    :param save_path: File path to save the SHAP values.\n",
    "    :param logger: Logger instance.\n",
    "    \"\"\"\n",
    "    if logger:\n",
    "        logger.info(f\"Saving SHAP values to {save_path}...\")\n",
    "    try:\n",
    "        with open(save_path, \"wb\") as f:\n",
    "            pickle.dump(shap_values, f)\n",
    "        if logger:\n",
    "            logger.info(f\"SHAP values saved successfully to {save_path}.\")\n",
    "    except Exception as e:\n",
    "        if logger:\n",
    "            logger.error(f\"Failed to save SHAP values: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def load_shap_values(load_path: str, logger: logging.Logger = None):\n",
    "    \"\"\"\n",
    "    Load SHAP values from a pickle file.\n",
    "\n",
    "    :param load_path: File path to load the SHAP values from.\n",
    "    :param logger: Logger instance.\n",
    "    :return: Loaded SHAP values object.\n",
    "    \"\"\"\n",
    "    if logger:\n",
    "        logger.info(f\"Loading SHAP values from {load_path}...\")\n",
    "    try:\n",
    "        with open(load_path, \"rb\") as f:\n",
    "            shap_values = pickle.load(f)\n",
    "        if logger:\n",
    "            logger.info(f\"SHAP values loaded successfully from {load_path}.\")\n",
    "    except Exception as e:\n",
    "        if logger:\n",
    "            logger.error(f\"Failed to load SHAP values: {e}\")\n",
    "        raise\n",
    "    return shap_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-30 22:35:04,310 [INFO] __main__: ✅ DataPreprocessor initialized in 'predict' mode.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-30 22:35:04,311 [INFO] Step: Load Transformers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-30 22:35:04,311 [INFO] DataPreprocessor: Step: Load Transformers\n",
      "2024-12-30 22:35:04,318 [INFO] __main__: ✅ Transformers loaded successfully.\n",
      "2024-12-30 22:35:04,335 [INFO] __main__: ✅ Trained model loaded from '../../ml-preprocessing-utils/data/dataset/test/models/XGBoost_model.pkl'.\n",
      "2024-12-30 22:35:04,347 [INFO] __main__: ✅ Prediction input data loaded from '../../ml-preprocessing-utils/data/dataset/test/data/final_ml_dataset.csv'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-30 22:35:04,351 [INFO] Data transformed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-30 22:35:04,351 [INFO] DataPreprocessor: Data transformed.\n",
      "2024-12-30 22:35:04,352 [INFO] __main__: ✅ New data preprocessed successfully.\n",
      "2024-12-30 22:35:04,369 [INFO] __main__: ✅ Predictions made successfully on new data.\n",
      "2024-12-30 22:35:04,391 [INFO] __main__: ✅ Predicted probabilities obtained.\n",
      "2024-12-30 22:35:04,396 [INFO] __main__: ✅ Predictions integrated into the dataset successfully.\n",
      "2024-12-30 22:35:04,397 [INFO] __main__: ✅ Prediction probabilities added to the dataset.\n",
      "2024-12-30 22:35:04,399 [INFO] InverseTransform: ✅ Inverse transformation completed successfully.\n",
      "2024-12-30 22:35:04,400 [INFO] __main__: ✅ Inverse transformations applied successfully.\n",
      "2024-12-30 22:35:04,400 [INFO] __main__: Inverse-transformed X_new_inverse.shape=(125, 15)\n",
      "2024-12-30 22:35:04,414 [INFO] __main__: ✅ Predictions saved to '../../ml-preprocessing-utils/data/dataset/test/data/predictions.csv'.\n",
      "2024-12-30 22:35:04,414 [INFO] __main__: Initializing SHAP explainer...\n",
      "2024-12-30 22:35:04,699 [INFO] __main__: SHAP values computed successfully.\n",
      "2024-12-30 22:35:04,700 [INFO] __main__: ✅ SHAP values computed successfully.\n",
      "2024-12-30 22:35:04,702 [INFO] __main__: Generating SHAP summary plot...\n",
      "2024-12-30 22:35:04,704 [ERROR] __main__: Failed to generate SHAP summary plot: The shape of the shap_values matrix does not match the shape of the provided data matrix.\n",
      "2024-12-30 22:35:04,704 [ERROR] __main__: ❌ Failed to generate SHAP summary plot: The shape of the shap_values matrix does not match the shape of the provided data matrix.\n",
      "2024-12-30 22:35:04,705 [INFO] __main__: Generating feature importance based on SHAP values...\n",
      "2024-12-30 22:35:04,705 [ERROR] __main__: Failed to generate global recommendations: Shape of passed values is (125, 15), indices imply (125, 142)\n",
      "2024-12-30 22:35:04,706 [ERROR] __main__: ❌ Failed to generate SHAP dependence plots: Shape of passed values is (125, 15), indices imply (125, 142)\n",
      "2024-12-30 22:35:04,707 [INFO] __main__: Saving SHAP values to ./shap_outputs/shap_values.pkl...\n",
      "2024-12-30 22:35:04,709 [INFO] __main__: SHAP values saved successfully to ./shap_outputs/shap_values.pkl.\n",
      "2024-12-30 22:35:04,710 [INFO] __main__: ✅ SHAP values saved to './shap_outputs/shap_values.pkl'.\n",
      "2024-12-30 22:35:04,713 [ERROR] __main__: ❌ Failed to save global recommendations: local variable 'recommendations' referenced before assignment\n",
      "\n",
      "Inverse Transformed Prediction DataFrame with Predictions:\n",
      "   release_ball_direction_x  release_ball_direction_z  \\\n",
      "0                  0.377012                  0.926203   \n",
      "1                  0.417644                  0.901906   \n",
      "2                  0.383086                  0.922353   \n",
      "3                  0.292054                  0.956396   \n",
      "4                  0.389917                  0.918894   \n",
      "\n",
      "   release_ball_direction_y  elbow_release_angle  elbow_max_angle  \\\n",
      "0                  0.002969            72.325830       106.272118   \n",
      "1                 -0.110176            58.430068       101.798798   \n",
      "2                 -0.050096            73.883728       106.989633   \n",
      "3                  0.003209            71.424835       104.160865   \n",
      "4                 -0.059987            73.619348       103.832024   \n",
      "\n",
      "   wrist_release_angle  wrist_max_angle  knee_release_angle  knee_max_angle  \\\n",
      "0            28.102765        35.918406           32.646276       63.541007   \n",
      "1            32.766626        38.693790           33.834026       65.565635   \n",
      "2            26.220943        38.998089           33.550293       65.257346   \n",
      "3            22.649881        36.353038           29.568453       64.652573   \n",
      "4            21.754498        34.532455           32.856306       64.165568   \n",
      "\n",
      "   release_ball_speed  calculated_release_angle  release_ball_velocity_x  \\\n",
      "0            9.907618                 62.959206                 3.735294   \n",
      "1           11.826803                 64.964999                 4.939394   \n",
      "2           10.283314                 60.314694                 3.939394   \n",
      "3            9.164302                 63.418903                 2.676471   \n",
      "4           11.113489                 60.900817                 4.333333   \n",
      "\n",
      "   release_ball_velocity_y  release_ball_velocity_z  \\\n",
      "0                 0.029412                 9.176471   \n",
      "1                -1.303030                10.666667   \n",
      "2                -0.515152                 9.484848   \n",
      "3                 0.029412                 8.764706   \n",
      "4                -0.666667                10.212121   \n",
      "\n",
      "  player_estimated_hand_length_cm_category  \n",
      "0                                   Medium  \n",
      "1                                   Medium  \n",
      "2                                   Medium  \n",
      "3                                   Medium  \n",
      "4                                   Medium  \n",
      "2024-12-30 22:35:04,718 [INFO] __main__: ✅ Predict pipeline executed successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# src/freethrow_predictions/ml/predict.py\n",
    "\n",
    "import logging\n",
    "import os\n",
    "from typing import Any, Dict, Tuple\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "# Local imports\n",
    "# from shap_utils import (\n",
    "#     setup_logging,\n",
    "#     compute_shap_values,\n",
    "#     plot_shap_summary,\n",
    "#     plot_shap_dependence,\n",
    "#     generate_global_recommendations,\n",
    "#     generate_individual_feedback,\n",
    "#     save_shap_values,\n",
    "#     load_shap_values\n",
    "# )\n",
    "# from classification_processors.datapreprocessor_class import DataPreprocessor\n",
    "# from classification_processors.inverse_preprocessor_class import InversePreprocessor\n",
    "\n",
    "def load_config(config_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Load the YAML configuration file.\n",
    "\n",
    "    :param config_path: Path to the configuration file.\n",
    "    :return: Configuration as a dictionary.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(config_path, 'r') as f:\n",
    "            config = yaml.safe_load(f)\n",
    "        return config\n",
    "    except Exception as e:\n",
    "        raise FileNotFoundError(f\"Failed to load configuration file: {e}\")\n",
    "\n",
    "def load_dataset(path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load the dataset from a CSV file.\n",
    "\n",
    "    :param path: Path to the CSV file.\n",
    "    :return: DataFrame containing the dataset.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Dataset not found at {path}\")\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "def main():\n",
    "    # ----------------------------\n",
    "    # 1. Load Configuration\n",
    "    # ----------------------------\n",
    "    config_path = '../../ml-preprocessing-utils/data/dataset/test/preprocessor_config/preprocessor_config.yaml'\n",
    "    try:\n",
    "        config = load_config(config_path)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"❌ Cannot proceed without configuration: {e}\")\n",
    "        return\n",
    "\n",
    "    # ----------------------------\n",
    "    # 2. Configure Logging\n",
    "    # ----------------------------\n",
    "    debug_flag = config.get('logging', {}).get('debug', False)\n",
    "    logger = setup_logging(debug=debug_flag)\n",
    "\n",
    "    # ----------------------------\n",
    "    # 3. Extract Feature Assets and Execution Parameters for Predict\n",
    "    # ----------------------------\n",
    "    features_config = config.get('features', {})\n",
    "    column_assets = {\n",
    "        'y_variable': features_config.get('y_variable', []),  # Not used in predict\n",
    "        'ordinal_categoricals': features_config.get('ordinal_categoricals', []),\n",
    "        'nominal_categoricals': features_config.get('nominal_categoricals', []),\n",
    "        'numericals': features_config.get('numericals', [])\n",
    "    }\n",
    "\n",
    "    execution_predict = config.get('execution', {}).get('predict', {})\n",
    "    prediction_input_path = execution_predict.get('prediction_input_path', '')\n",
    "    load_transformers_path = execution_predict.get('load_transformers_path', '')\n",
    "    trained_model_path = execution_predict.get('trained_model_path', '')\n",
    "    predictions_output_path = execution_predict.get('predictions_output_path', './predictions')\n",
    "    normalize_debug = execution_predict.get('normalize_debug', False)\n",
    "    normalize_graphs_output = execution_predict.get('normalize_graphs_output', False)\n",
    "    shap_output_path = execution_predict.get('shap_output_path', './shap_outputs')\n",
    "    recommendations_output_path = execution_predict.get('recommendations_output_path', './recommendations')\n",
    "    specific_feedback_output_path = execution_predict.get('specific_feedback_output_path', './feedback')\n",
    "\n",
    "    # Validate essential paths\n",
    "    essential_paths = {\n",
    "        'prediction_input_path': prediction_input_path,\n",
    "        'load_transformers_path': load_transformers_path,\n",
    "        'trained_model_path': trained_model_path\n",
    "    }\n",
    "\n",
    "    for key, path in essential_paths.items():\n",
    "        if not path:\n",
    "            logger.error(f\"❌ '{key}' for predict mode is not specified in the configuration.\")\n",
    "            return\n",
    "        if not os.path.exists(path):\n",
    "            logger.error(f\"❌ {key} not found at {path}.\")\n",
    "            return\n",
    "\n",
    "    # ----------------------------\n",
    "    # 4. Initialize DataPreprocessor in Predict Mode\n",
    "    # ----------------------------\n",
    "    try:\n",
    "        preprocessor = DataPreprocessor(\n",
    "            model_type=config.get('model_type', 'Tree Based Classifier'),\n",
    "            column_assets=column_assets,\n",
    "            mode='predict',\n",
    "            options=config.get('models', {}).get(config.get('model_type', 'Tree Based Classifier'), {}),\n",
    "            debug=debug_flag,\n",
    "            normalize_debug=normalize_debug,\n",
    "            normalize_graphs_output=normalize_graphs_output,\n",
    "            graphs_output_dir=config.get('execution', {}).get('shared', {}).get('plot_output_dir', './plots'),\n",
    "            transformers_dir=os.path.dirname(load_transformers_path)\n",
    "        )\n",
    "        logger.info(\"✅ DataPreprocessor initialized in 'predict' mode.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Failed to initialize DataPreprocessor: {e}\")\n",
    "        return\n",
    "\n",
    "    # ----------------------------\n",
    "    # 5. Load Trained Transformers\n",
    "    # ----------------------------\n",
    "    try:\n",
    "        transformers = preprocessor.load_transformers()\n",
    "        logger.info(\"✅ Transformers loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Failed to load transformers: {e}\")\n",
    "        return\n",
    "\n",
    "    # ----------------------------\n",
    "    # 6. Load Trained Model\n",
    "    # ----------------------------\n",
    "    try:\n",
    "        model = joblib.load(trained_model_path)\n",
    "        logger.info(f\"✅ Trained model loaded from '{trained_model_path}'.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Failed to load trained model: {e}\")\n",
    "        return\n",
    "\n",
    "    # ----------------------------\n",
    "    # 7. Load Prediction Input Data\n",
    "    # ----------------------------\n",
    "    try:\n",
    "        X_new = load_dataset(prediction_input_path)\n",
    "        logger.info(f\"✅ Prediction input data loaded from '{prediction_input_path}'.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Failed to load prediction input data: {e}\")\n",
    "        return\n",
    "\n",
    "    # ----------------------------\n",
    "    # 8. Preprocess the New Data\n",
    "    # ----------------------------\n",
    "    try:\n",
    "        X_new_preprocessed = preprocessor.transform(X_new)\n",
    "        logger.info(\"✅ New data preprocessed successfully.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Preprocessing of new data failed: {e}\")\n",
    "        return\n",
    "\n",
    "    # ----------------------------\n",
    "    # 9. Make Predictions\n",
    "    # ----------------------------\n",
    "    try:\n",
    "        predictions = model.predict(X_new_preprocessed)\n",
    "        logger.info(\"✅ Predictions made successfully on new data.\")\n",
    "\n",
    "        # If the model supports predict_proba, get probabilities\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            prediction_probabilities = model.predict_proba(X_new_preprocessed)\n",
    "            logger.info(\"✅ Predicted probabilities obtained.\")\n",
    "        else:\n",
    "            logger.warning(\"⚠️ Model does not support probability predictions.\")\n",
    "            prediction_probabilities = None\n",
    "\n",
    "        # Add predictions to the original prediction dataset\n",
    "        X_new['Prediction'] = predictions\n",
    "        logger.info(\"✅ Predictions integrated into the dataset successfully.\")\n",
    "\n",
    "        if prediction_probabilities is not None:\n",
    "            classes = model.classes_\n",
    "            if prediction_probabilities.shape[1] == 2:\n",
    "                # Binary classification\n",
    "                X_new['Prediction_Probability'] = prediction_probabilities[:, 1]\n",
    "            else:\n",
    "                # Multiclass classification\n",
    "                for idx, class_label in enumerate(classes):\n",
    "                    X_new[f'Prediction_Probability_Class_{class_label}'] = prediction_probabilities[:, idx]\n",
    "            logger.info(\"✅ Prediction probabilities added to the dataset.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Failed to make predictions: {e}\")\n",
    "        return\n",
    "\n",
    "    # ----------------------------\n",
    "    # 10. Inverse Transform the Data for Interpretability\n",
    "    # ----------------------------\n",
    "    try:\n",
    "        X_new_inverse = preprocessor.inverse_transform_data(X_new_preprocessed)\n",
    "        logger.info(\"✅ Inverse transformations applied successfully.\")\n",
    "        logger.info(f\"Inverse-transformed X_new_inverse.shape={X_new_inverse.shape}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Inverse transformations failed: {e}\")\n",
    "        X_new_inverse = pd.DataFrame({'predictions': predictions})  # Correct variable assignment\n",
    "\n",
    "    # ----------------------------\n",
    "    # 11. Save Predictions\n",
    "    # ----------------------------\n",
    "    try:\n",
    "        os.makedirs(predictions_output_path, exist_ok=True)\n",
    "        final_output_path = os.path.join(predictions_output_path, 'predictions.csv')\n",
    "        X_new_inverse.to_csv(final_output_path, index=False)\n",
    "        logger.info(f\"✅ Predictions saved to '{final_output_path}'.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Failed to save predictions: {e}\")\n",
    "        return\n",
    "\n",
    "    # ----------------------------\n",
    "    # 12. Compute SHAP Values\n",
    "    # ----------------------------\n",
    "    try:\n",
    "        explainer, shap_values = compute_shap_values(model, X_new_preprocessed, debug=debug_flag, logger=logger)\n",
    "        logger.info(\"✅ SHAP values computed successfully.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Failed to compute SHAP values: {e}\")\n",
    "        return\n",
    "\n",
    "    # ----------------------------\n",
    "    # 13. Generate and Save SHAP Summary Plot\n",
    "    # ----------------------------\n",
    "    try:\n",
    "        os.makedirs(shap_output_path, exist_ok=True)\n",
    "        shap_summary_path = os.path.join(shap_output_path, 'shap_summary.png')\n",
    "        plot_shap_summary(\n",
    "            shap_values=shap_values,\n",
    "            X_original=X_new,\n",
    "            save_path=shap_summary_path,\n",
    "            debug=debug_flag,\n",
    "            logger=logger\n",
    "        )\n",
    "        logger.info(f\"✅ SHAP summary plot saved to '{shap_summary_path}'.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Failed to generate SHAP summary plot: {e}\")\n",
    "\n",
    "    # ----------------------------\n",
    "    # 14. Generate and Save SHAP Dependence Plots\n",
    "    # ----------------------------\n",
    "    try:\n",
    "        recommendations = generate_global_recommendations(\n",
    "            shap_values=shap_values,\n",
    "            X_original=X_new,\n",
    "            top_n=10,  # Adjust based on your preference\n",
    "            debug=debug_flag,\n",
    "            use_mad=False,  # Set to True if using MAD for range definitions\n",
    "            logger=logger\n",
    "        )\n",
    "        os.makedirs(shap_output_path, exist_ok=True)\n",
    "        for feature in recommendations.keys():\n",
    "            sanitized_feature = feature.replace(\" \", \"_\").replace(\"/\", \"_\")\n",
    "            shap_dependence_path = os.path.join(shap_output_path, f'shap_dependence_{sanitized_feature}.png')\n",
    "            plot_shap_dependence(\n",
    "                shap_values=shap_values,\n",
    "                feature=feature,\n",
    "                X_original=X_new,\n",
    "                save_path=shap_dependence_path,\n",
    "                debug=debug_flag,\n",
    "                logger=logger\n",
    "            )\n",
    "            logger.info(f\"✅ SHAP dependence plot for '{feature}' saved to '{shap_dependence_path}'.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Failed to generate SHAP dependence plots: {e}\")\n",
    "\n",
    "    # ----------------------------\n",
    "    # 15. Save SHAP Values\n",
    "    # ----------------------------\n",
    "    try:\n",
    "        shap_values_path = os.path.join(shap_output_path, 'shap_values.pkl')\n",
    "        save_shap_values(shap_values, shap_values_path, logger=logger)\n",
    "        logger.info(f\"✅ SHAP values saved to '{shap_values_path}'.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Failed to save SHAP values: {e}\")\n",
    "\n",
    "    # ----------------------------\n",
    "    # 16. Save Global Recommendations\n",
    "    # ----------------------------\n",
    "    try:\n",
    "        os.makedirs(recommendations_output_path, exist_ok=True)\n",
    "        recommendations_path = os.path.join(recommendations_output_path, 'global_shap_recommendations.json')\n",
    "        with open(recommendations_path, 'w') as f:\n",
    "            json.dump(recommendations, f, indent=4)\n",
    "        logger.info(f\"✅ Global SHAP recommendations saved to '{recommendations_path}'.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Failed to save global recommendations: {e}\")\n",
    "\n",
    "    # ----------------------------\n",
    "    # 17. Generate Specific Feedback (Optional)\n",
    "    # ----------------------------\n",
    "    if config.get('execution', {}).get('include_specific_feedback', False):\n",
    "        try:\n",
    "            feature_metadata = {}\n",
    "            for feature in X_new.columns:\n",
    "                if 'angle' in feature.lower():\n",
    "                    unit = 'degrees'\n",
    "                else:\n",
    "                    unit = 'units'  # Adjust based on your feature's unit\n",
    "                feature_metadata[feature] = {'unit': unit}\n",
    "\n",
    "            specific_feedback = generate_individual_feedback(\n",
    "                trial=X_new,\n",
    "                shap_values_trial=shap_values.values,\n",
    "                feature_metadata=feature_metadata,\n",
    "                logger=logger\n",
    "            )\n",
    "            X_new_inverse['specific_feedback'] = specific_feedback\n",
    "            logger.info(\"✅ Specific feedback generated for each trial.\")\n",
    "\n",
    "            # Save specific feedback\n",
    "            os.makedirs(specific_feedback_output_path, exist_ok=True)\n",
    "            feedback_path = os.path.join(specific_feedback_output_path, 'specific_feedback.json')\n",
    "            with open(feedback_path, 'w') as f:\n",
    "                json.dump(specific_feedback, f, indent=4)\n",
    "            logger.info(f\"✅ Specific feedback saved to '{feedback_path}'.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Failed to generate specific feedback: {e}\")\n",
    "\n",
    "    # ----------------------------\n",
    "    # 18. Final Output Display\n",
    "    # ----------------------------\n",
    "    try:\n",
    "        print(\"\\nInverse Transformed Prediction DataFrame with Predictions:\")\n",
    "        print(X_new_inverse.head())\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Error during displaying predictions: {e}\")\n",
    "\n",
    "    logger.info(\"✅ Predict pipeline executed successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science_ft_bio_predictions",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
