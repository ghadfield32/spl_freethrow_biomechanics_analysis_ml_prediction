{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main function used for testing prior to use in Streamlit\n",
    "Give the option of processing new data or selecting new features or reviewing the categoricals and how they are handled\n",
    "- processing the ml dataset\n",
    "  - option of new data to for processing the basketball data\n",
    "- selecting new features:\n",
    "  - options of how to handle categorical functions\n",
    "  - multi collinearity handling\n",
    "  - feature importance review \n",
    "  - then select the features\n",
    "- Preprocessing\n",
    "  - SMOTE selection if prompted (otherwise use the automation)\n",
    "    - option of selection or of the metrics that go into the automation of the SMOTE\n",
    "    - option of checking the benefit of the smote selected\n",
    "  - Great output of the selections chosen\n",
    "  - Option of including the basic outlier methods\n",
    "- Train:\n",
    "  - option of training which model or all\n",
    "  - option of adjusting the bayesian ranges for the models\n",
    "- Predict:\n",
    "  - options: model and anything else we can \n",
    "- Shap Dashboard:\n",
    "  - Options of which metrics to view for the graphs specifically and global recommendations for SHAP\n",
    "- Bayesian Dashboard separately to find the best metrics globally for their probabilities as a concept\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "\n",
    "#----------Loading and prepping the dataset-----------------\n",
    "from ml.load_and_feature_engineer_data import process_basketball_data\n",
    "\n",
    "# Define paths\n",
    "directory_path = \"../../../SPL-Open-Data/basketball/freethrow/data/P0001\"\n",
    "player_info_path = \"../../../SPL-Open-Data/basketball/freethrow/participant_information.json\"\n",
    "output_ml_path = \"../../../data/processed/final_ml_dataset.csv\"\n",
    "output_granular_path = \"../../../data/processed/final_granular_dataset.csv\"\n",
    "\n",
    "# Call the processing function to create final_ml_dataset and final_granular_dataset \n",
    "# Purpose: feature engineer on a per trial basis so we can see on one example we are getting them correctly\n",
    "process_basketball_data(\n",
    "    directory_path=directory_path,\n",
    "    player_info_path=player_info_path,\n",
    "    output_ml_path=output_ml_path,\n",
    "    output_granular_path=output_granular_path,\n",
    "    debug=True,  # Enable debug mode for detailed logs\n",
    "    log_level=logging.DEBUG,  # Set logging level to DEBUG\n",
    "    new_data=False  # Set to True to append, False to overwrite\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#---------- feature selection (on entire dataset)-----------------\n",
    "from ml.feature_selection.multicollinearity_checker import check_multicollinearity\n",
    "from ml.feature_selection.feature_importance_calculator import calculate_feature_importance\n",
    "\n",
    "\n",
    "file_path = \"../../data/processed/final_ml_dataset.csv\"\n",
    "final_ml_df = pd.read_csv(file_path)\n",
    "\n",
    "# Feature selection based on multi collinearity and random forest importance selection\n",
    "target_variable = 'result'\n",
    "correlation_threshold = 0.8\n",
    "debug = True\n",
    "\n",
    "# Remove columns to address collinearity\n",
    "drop_features = [\n",
    "    'trial_id', 'player_participant_id', 'landing_y', 'landing_x', 'entry_angle', 'shot_id',\n",
    "    'L_KNEE_avg_power', 'L_WRIST_energy_std', 'R_WRIST_energy_max', \n",
    "    'R_ANKLE_energy_mean', 'R_5THFINGER_energy_std', 'R_KNEE_avg_power', 'L_1STFINGER_max_power', \n",
    "    'L_5THFINGER_energy_max', 'L_WRIST_max_power', 'R_HIP_energy_std', 'L_1STFINGER_energy_max', \n",
    "    'R_ANKLE_energy_max', 'R_ELBOW_energy_max', 'R_ANKLE_energy_std', 'L_WRIST_energy_max', \n",
    "    'player_estimated_hand_length_cm', 'player_estimated_standing_reach_cm', \n",
    "    'player_estimated_wingspan_cm', 'player_weight__in_kg', 'L_KNEE_energy_std', 'L_HIP_energy_max', \n",
    "    'L_ANKLE_energy_max', 'L_WRIST_std_power', 'L_ELBOW_std_power', 'R_KNEE_max_power', \n",
    "    'L_ELBOW_avg_power', 'R_ELBOW_min_power', 'L_WRIST_min_power', 'R_HIP_energy_mean', \n",
    "    'L_ELBOW_energy_max', 'L_ELBOW_min_power', 'R_1STFINGER_min_power', 'L_ANKLE_min_power', \n",
    "    'L_1STFINGER_avg_power', 'R_ANKLE_std_power', 'R_5THFINGER_avg_power', 'L_1STFINGER_energy_mean', \n",
    "    'R_HIP_max_power', 'R_WRIST_avg_power', 'R_ELBOW_energy_mean', 'L_WRIST_avg_power', \n",
    "    'L_1STFINGER_std_power', 'L_KNEE_energy_max', 'L_WRIST_energy_mean', 'R_KNEE_energy_std', \n",
    "    'L_HIP_energy_std', 'L_KNEE_energy_mean', 'R_WRIST_energy_mean', 'L_ELBOW_max_power', \n",
    "    'R_WRIST_energy_std', 'L_ANKLE_std_power', 'L_HIP_energy_mean', 'L_ELBOW_energy_mean', \n",
    "    'R_HIP_avg_power', 'L_HIP_std_power', 'R_KNEE_std_power', 'L_ANKLE_energy_std', \n",
    "    'release_frame_time', 'R_ANKLE_avg_power', 'L_ANKLE_max_power', 'L_5THFINGER_energy_std', \n",
    "    'R_WRIST_min_power', 'R_1STFINGER_energy_mean', 'R_ELBOW_energy_std', 'R_HIP_std_power', \n",
    "    'R_KNEE_energy_max', 'R_WRIST_std_power', 'L_1STFINGER_energy_std', 'L_HIP_avg_power', \n",
    "    'R_5THFINGER_energy_mean', 'R_ANKLE_max_power', 'L_ANKLE_avg_power', 'R_5THFINGER_max_power', \n",
    "    'R_5THFINGER_energy_max', 'L_5THFINGER_min_power', 'L_ELBOW_energy_std', \n",
    "    'R_1STFINGER_energy_max', 'R_KNEE_min_power', 'R_1STFINGER_energy_std', \n",
    "    'R_5THFINGER_std_power', 'L_1STFINGER_min_power', 'R_ELBOW_max_power', 'L_HIP_min_power', \n",
    "    'L_5THFINGER_std_power', 'R_1STFINGER_max_power', 'R_KNEE_energy_mean', 'L_5THFINGER_avg_power', \n",
    "    'L_5THFINGER_max_power', 'R_HIP_min_power', 'L_KNEE_max_power', 'R_5THFINGER_min_power', \n",
    "    'R_1STFINGER_std_power', 'R_ELBOW_avg_power', 'L_ANKLE_energy_mean', 'R_ELBOW_std_power', \n",
    "    'L_5THFINGER_energy_mean', 'R_1STFINGER_avg_power', 'R_HIP_energy_max', 'L_KNEE_std_power',\n",
    "    'R_ANKLE_min_power', 'L_KNEE_min_power', 'L_HIP_max_power'\n",
    "]\n",
    "\n",
    "# Step 1: Check for multicollinearity\n",
    "print(\"\\nChecking for Multicollinearity...\")\n",
    "multicollinearity_df = check_multicollinearity(final_ml_df, threshold=correlation_threshold, debug=debug)\n",
    "\n",
    "# Step 2: Handle multicollinearity\n",
    "if not multicollinearity_df.empty:\n",
    "    for index, row in multicollinearity_df.iterrows():\n",
    "        feature1, feature2, correlation = row['Feature1'], row['Feature2'], row['Correlation']\n",
    "        print(f\"High correlation ({correlation}) between '{feature1}' and '{feature2}'.\")\n",
    "\n",
    "        # Drop or combine features based on criteria\n",
    "        # Example decision logic here...\n",
    "        # drop_features = ['trial_id', 'player_participant_id']\n",
    "        # # Drop the identified features from the dataset\n",
    "        # Drop the identified features from the dataset\n",
    "        final_ml_df = final_ml_df.drop(columns=drop_features, errors='ignore')\n",
    "\n",
    "        print(f\"Dropped {len(drop_features)} features: {', '.join(drop_features)}\")\n",
    "else:\n",
    "    print(\"No multicollinearity issues detected.\")\n",
    "\n",
    "# Step 3: Calculate feature importance\n",
    "print(\"\\nCalculating Feature Importance...\")\n",
    "feature_importances = calculate_feature_importance(\n",
    "    final_ml_df, target_variable=target_variable, n_estimators=100, random_state=42, debug=debug\n",
    ")\n",
    "\n",
    "print(\"\\nFinal Feature Importances:\")\n",
    "print(feature_importances.to_string(index=False))\n",
    "\n",
    "\n",
    "#Final Decisions: \n",
    "# Features recommended for dropping\n",
    "features_to_drop = [\n",
    "    'peak_height_relative'\n",
    "]\n",
    "print(f\"Dropped features (for redundancy or multicollinearity): {', '.join(features_to_drop)}\")\n",
    "\n",
    "# Final features to retain for classification\n",
    "final_keep_list = [\n",
    "    'release_ball_direction_x' ,'release_ball_direction_z', 'release_ball_direction_y',\n",
    "    'elbow_release_angle', 'elbow_max_angle',\n",
    "    'wrist_release_angle', 'wrist_max_angle',\n",
    "    'knee_release_angle', 'knee_max_angle',\n",
    "    'result', 'release_ball_speed',\n",
    "    'release_ball_velocity_x', 'release_ball_velocity_y','release_ball_velocity_z',\n",
    "    'calculated_release_angle'\n",
    "]\n",
    "# Apply the filter to keep only these columns\n",
    "final_ml_df_selected_features = final_ml_df[final_keep_list]\n",
    "print(f\"Retained {len(final_keep_list)} features: {', '.join(final_keep_list)}\")\n",
    "\n",
    "# Save feature names to a file\n",
    "with open('../../data/model/pipeline/final_ml_df_selected_features_columns.pkl', 'wb') as f:\n",
    "    pickle.dump(final_ml_df_selected_features.columns.tolist(), f)\n",
    "\n",
    "print(f\"Retained {len(final_keep_list)} features: {', '.join(final_keep_list)}\")\n",
    "\n",
    "#---------STOP: CHECK ON CATEGORICALS AND IF YOU WANT TO BIN, WILL BE REMINDED AFTER DATAPREPROCESSOR, DO IT HERE OR DO IT WITH FEATURE-ENGINEERING PACKAGE--------\n",
    "from ml.feature_selection.categorize_categoricals import transform_features_with_bins\n",
    "# # Example bin configuration\n",
    "category_bin_config = {\n",
    "    'player_height_in_meters': {\n",
    "        'bins': [0, 1.80, 2.00, np.inf],\n",
    "        'labels': [\"Short\", \"Average\", \"Tall\"]\n",
    "    },\n",
    "    'player_weight__in_kg': {\n",
    "        'bins': [0, 75, 95, np.inf],\n",
    "        'labels': [\"Lightweight\", \"Average\", \"Heavy\"]\n",
    "    },\n",
    "    'player_estimated_wingspan_cm': {\n",
    "        'bins': [0, 190, 220, np.inf],\n",
    "        'labels': [\"Small\", \"Medium\", \"Large\"]\n",
    "    },\n",
    "    'player_estimated_standing_reach_cm': {\n",
    "        'bins': [0, 230, 250, np.inf],\n",
    "        'labels': [\"Short\", \"Average\", \"Tall\"]\n",
    "    },\n",
    "    'player_estimated_hand_length_cm': {\n",
    "        'bins': [0, 20, 25, np.inf],\n",
    "        'labels': [\"Small\", \"Medium\", \"Large\"]\n",
    "    }\n",
    "}\n",
    "# Save the category bin configuration\n",
    "with open('../../data/model/pipeline/category_bin_config.pkl', 'wb') as f:\n",
    "    pickle.dump(category_bin_config, f)\n",
    "\n",
    "# Load the category bin configuration\n",
    "with open('../../data/model/pipeline/category_bin_config.pkl', 'rb') as f:\n",
    "    loaded_category_bin_config = pickle.load(f)\n",
    "\n",
    "file_path = \"../../data/processed/final_ml_dataset.csv\"\n",
    "#import ml dataset from spl_dataset_prep\n",
    "final_ml_df = pd.read_csv(file_path)\n",
    "\n",
    "\n",
    "# Step 1: Transform player features using the configuration\n",
    "categorized_columns_df = transform_features_with_bins(final_ml_df, loaded_category_bin_config, debug=debug)\n",
    "\n",
    "# Step 2: Combine the original DataFrame with the categorized columns\n",
    "final_ml_df_categoricals = pd.concat([final_ml_df, categorized_columns_df], axis=1)\n",
    "\n",
    "\n",
    "# from ml.feature_selection.data_loader_post_select_features import load_selected_features_data\n",
    "# # Example usage:\n",
    "# final_ml_df_selected_features = load_selected_features_data(\n",
    "#     features_path='../../data/model/pipeline/final_ml_df_selected_features_columns.pkl',\n",
    "#     dataset_path='../../data/processed/final_ml_dataset.csv',\n",
    "#     category_bin_config_path='../../data/model/pipeline/category_bin_config.pkl',\n",
    "#     y_variable='result',\n",
    "#     debug=False\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #-----------------Preprocessing (within Predict and Training so Commented Out)---------------------\n",
    "# #--Preprocessing--\n",
    "# # Additions \n",
    "#     # - add in the option to change the smote technique to the datapreprocessor if chosen to not automate\n",
    "# from ml.classification_preprocessor.datapreprocessor_class import DataPreprocessor\n",
    "\n",
    "# # File paths\n",
    "# features_path = '../../data/model/pipeline/final_ml_df_selected_features_columns.pkl'\n",
    "# dataset_path = \"../../data/processed/final_ml_dataset.csv\"\n",
    "# assets_path = '../../data/model/pipeline/preprocessing_assets.pkl'\n",
    "# category_bin_config_path = '../../data/model/pipeline/category_bin_config.pkl'\n",
    "\n",
    "# # Example 1: With train-test split\n",
    "# dp_split = DataPreprocessor(\n",
    "#     features_path=features_path,\n",
    "#     dataset_path=dataset_path,\n",
    "#     assets_path=assets_path,\n",
    "#     category_bin_config_path=category_bin_config_path,\n",
    "#     y_variable='result',\n",
    "#     perform_split=True,\n",
    "#     test_size=0.3,\n",
    "#     random_state=123,\n",
    "#     stratify=True,\n",
    "# )\n",
    "# (\n",
    "#     X_train_transformed,\n",
    "#     X_test_transformed,\n",
    "#     y_train_encoded,\n",
    "#     y_test_encoded,\n",
    "#     transformed_data_train,\n",
    "#     transformed_train_df,  # Include the additional output\n",
    "#     X_train,\n",
    "#     y_train,\n",
    "# ) = dp_split.run()\n",
    "\n",
    "\n",
    "# print(f\"X_train_transformed shape: {X_train_transformed.shape}\")\n",
    "# print(f\"X_test_transformed shape: {X_test_transformed.shape}\")\n",
    "# print(f\"Transformed Data Train: {transformed_data_train.keys()}\")\n",
    "# print(f\"Original X_train shape: {X_train.shape}\")\n",
    "# print(f\"Original y_train shape: {y_train.shape}\")\n",
    "\n",
    "# # Example 2: Without train-test split\n",
    "# dp_no_split = DataPreprocessor(\n",
    "#     features_path=features_path,\n",
    "#     dataset_path=dataset_path,\n",
    "#     assets_path=assets_path,\n",
    "#     category_bin_config_path=category_bin_config_path,\n",
    "#     y_variable='result',\n",
    "#     perform_split=False,\n",
    "# )\n",
    "# X_transformed, y_encoded, transformed_data, transformed_data_df, original_data, X, y = dp_no_split.run()\n",
    "\n",
    "# print(f\"X_transformed shape: {X_transformed.shape}\")\n",
    "# print(f\"Transformed Data: {transformed_data.keys()}\")\n",
    "# print(f\"Transformed Data: {transformed_data_df.columns}\")\n",
    "# print(f\"Original X shape: {X.shape}\")\n",
    "# print(f\"Original y shape: {y.shape}\")\n",
    "\n",
    "# #-------------Preprocessing with Optimization Ranges------------\n",
    "# dp = DataPreprocessor(\n",
    "#     features_path=features_path,\n",
    "#     dataset_path=dataset_path,\n",
    "#     assets_path=assets_path,\n",
    "#     category_bin_config_path=category_bin_config_path,\n",
    "#     y_variable=\"result\",\n",
    "#     optimization_columns=[\"knee_max_angle\", \"wrist_max_angle\", \"elbow_max_angle\"],\n",
    "#     perform_split=False,\n",
    "# )\n",
    "\n",
    "# results = dp.run(return_optimization_ranges=True)\n",
    "# X_transformed, y_encoded, transformed_data, transformed_data_df, original_data, X, y, optimization_ranges, optimization_transformed_ranges = results\n",
    "\n",
    "# print(f\"Optimization Ranges: {optimization_ranges}\")\n",
    "# print(f\"optimization_transformed_ranges: {optimization_transformed_ranges}\")\n",
    "# print(f\"original_data shape: {original_data.shape}\")\n",
    "# print(f\"X shape: {X.shape}\")\n",
    "# print(f\"X_transformed shape: {X_transformed.shape}\")\n",
    "\n",
    "# #-----------------Inverse Preprocessing(within Predict and Training so Commented Out)---------------------\n",
    "# from ml.classification_processors.inverse_preprocessor_class import InversePreprocessor\n",
    "\n",
    "\n",
    "\n",
    "# # Example 1: With train-test split\n",
    "# dp_split = DataPreprocessor(\n",
    "#     features_path=features_path,\n",
    "#     dataset_path=dataset_path,\n",
    "#     assets_path=assets_path,\n",
    "#     category_bin_config_path=category_bin_config_path,\n",
    "#     y_variable='result',\n",
    "#     perform_split=True,\n",
    "#     test_size=0.3,\n",
    "#     random_state=123,\n",
    "#     stratify=True,\n",
    "# )\n",
    "# (\n",
    "#     X_train_transformed,\n",
    "#     X_test_transformed,\n",
    "#     y_train_encoded,\n",
    "#     y_test_encoded,\n",
    "#     transformed_data_train,\n",
    "#     transformed_train_df,  # Include the additional output\n",
    "#     X_train,\n",
    "#     y_train,\n",
    "# ) = dp_split.run()\n",
    "\n",
    "\n",
    "# print(f\"X_train_transformed shape: {X_train_transformed.shape}\")\n",
    "# print(f\"X_test_transformed shape: {X_test_transformed.shape}\")\n",
    "# print(f\"Transformed Data Train: {transformed_data_train.keys()}\")\n",
    "# print(f\"Original X_train shape: {X_train.shape}\")\n",
    "# print(f\"Original y_train shape: {y_train.shape}\")\n",
    "\n",
    "\n",
    "# # Initialize InversePreprocessor with assets_path (no need to preload assets manually)\n",
    "# inverse_transformer = InversePreprocessor(\n",
    "#     assets_path=assets_path,  # Assets will be loaded automatically\n",
    "#     debug=True\n",
    "# )\n",
    "\n",
    "# # Perform inverse transformation and combine with targets\n",
    "# final_dataset = inverse_transformer.transform(\n",
    "#     original_data=X_train,\n",
    "#     transformed_data=transformed_data_train,\n",
    "#     y_encoded=y_train_encoded\n",
    "# )\n",
    "\n",
    "# # Example: Append specified columns from original_data to final_dataset\n",
    "# columns_to_append = ['player_height_in_meters', 'player_weight__in_kg']  # Example columns\n",
    "# final_dataset = inverse_transformer.append_columns_from_original(\n",
    "#     final_dataset=final_dataset,\n",
    "#     original_data=X_train,\n",
    "#     columns_to_append=columns_to_append,\n",
    "#     debug=True\n",
    "# )\n",
    "\n",
    "# # Display the resulting dataset\n",
    "# print(\"[Final Dataset]:\")\n",
    "# print(final_dataset.head())\n",
    "# print(final_dataset.shape)\n",
    "\n",
    "# # ---------------Example 2: Without train-test split------------------\n",
    "# dp_no_split = DataPreprocessor(\n",
    "#     features_path=features_path,\n",
    "#     dataset_path=dataset_path,\n",
    "#     assets_path=assets_path,\n",
    "#     category_bin_config_path=category_bin_config_path,\n",
    "#     y_variable='result',\n",
    "#     perform_split=False,\n",
    "# )\n",
    "# X_transformed, y_encoded, transformed_data, transformed_data_df, original_data, X, y = dp_no_split.run()\n",
    "\n",
    "# print(f\"X_transformed shape: {X_transformed.shape}\")\n",
    "# print(f\"Transformed Data: {transformed_data.keys()}\")\n",
    "# print(f\"Transformed Data: {transformed_data_df.columns}\")\n",
    "# print(f\"Original X shape: {X.shape}\")\n",
    "# print(f\"Original y shape: {y.shape}\")\n",
    "\n",
    "# # Inverse transformation for Example 2: Without train-test split\n",
    "# print(\"\\n[Example 2: Without Train-Test Split]\")\n",
    "# inverse_transformer = InversePreprocessor(\n",
    "#     assets_path=assets_path,  # Assets will be loaded automatically\n",
    "#     debug=True\n",
    "# )\n",
    "\n",
    "# # Perform inverse transformation and combine with targets\n",
    "# final_dataset_no_split = inverse_transformer.transform(\n",
    "#     original_data=X,\n",
    "#     transformed_data=transformed_data,\n",
    "#     y_encoded=y_encoded\n",
    "# )\n",
    "\n",
    "# # Example: Append specified columns from original_data to final_dataset_no_split\n",
    "# columns_to_append_no_split = ['trial_id']  # Example columns\n",
    "# final_dataset_no_split = inverse_transformer.append_columns_from_original(\n",
    "#     final_dataset=final_dataset_no_split,\n",
    "#     original_data=original_data,\n",
    "#     columns_to_append=columns_to_append_no_split,\n",
    "#     debug=True\n",
    "# )\n",
    "\n",
    "# # Display the resulting dataset\n",
    "# print(\"[Final Dataset without Train-Test Split]:\")\n",
    "# print(f\"Final Dataset shape (No Split): {final_dataset_no_split.shape}\")\n",
    "# print(f\"Original Dataset shape (With Optimization Ranges): {original_data.shape}\")\n",
    "\n",
    "\n",
    "# #-------------example 3: Preprocessing with Optimization Ranges------------\n",
    "# dp = DataPreprocessor(\n",
    "#     features_path=features_path,\n",
    "#     dataset_path=dataset_path,\n",
    "#     assets_path=assets_path,\n",
    "#     category_bin_config_path=category_bin_config_path,\n",
    "#     y_variable=\"result\",\n",
    "#     optimization_columns=[\"knee_max_angle\", \"wrist_max_angle\", \"elbow_max_angle\"],\n",
    "#     perform_split=False,\n",
    "# )\n",
    "\n",
    "# results = dp.run(return_optimization_ranges=True)\n",
    "# X_transformed, y_encoded, transformed_data, transformed_data_df, original_data, X, y, optimization_ranges, optimization_transformed_ranges = results\n",
    "\n",
    "# print(f\"Optimization Ranges: {optimization_ranges}\")\n",
    "# print(f\"optimization_transformed_ranges: {optimization_transformed_ranges}\")\n",
    "\n",
    "# # Inverse transformation for Example 3: Preprocessing with Optimization Ranges\n",
    "# print(\"\\n[Example 3: Preprocessing with Optimization Ranges]\")\n",
    "# inverse_transformer = InversePreprocessor(\n",
    "#     assets_path=assets_path,  # Assets will be loaded automatically\n",
    "#     debug=True\n",
    "# )\n",
    "\n",
    "# # Perform inverse transformation and combine with targets\n",
    "# final_dataset_with_optimization = inverse_transformer.transform(\n",
    "#     original_data=X,\n",
    "#     transformed_data=transformed_data,\n",
    "#     y_encoded=y_encoded\n",
    "# )\n",
    "\n",
    "# # Example: Append specified columns from original_data to final_dataset_with_optimization\n",
    "# columns_to_append_with_opt = [\"trial_id\"]  # Example columns\n",
    "# final_dataset_with_optimization = inverse_transformer.append_columns_from_original(\n",
    "#     final_dataset=final_dataset_with_optimization,\n",
    "#     original_data=original_data,\n",
    "#     columns_to_append=columns_to_append_with_opt,\n",
    "#     debug=True\n",
    "# )\n",
    "\n",
    "# # Inverse transform optimization parameters\n",
    "# inverse_optimization_params = inverse_transformer.inverse_transform_optimization_params(\n",
    "#     params=pd.DataFrame(optimization_transformed_ranges),\n",
    "#     optimization_columns=[\"knee_max_angle\", \"wrist_max_angle\", \"elbow_max_angle\"]\n",
    "# )\n",
    "\n",
    "# # Display the resulting datasets\n",
    "# print(f\"Final Dataset shape (With Optimization Ranges): {final_dataset_with_optimization.shape}\")\n",
    "# print(f\"Original Dataset shape (With Optimization Ranges): {original_data.shape}\")\n",
    "\n",
    "# print(\"\\n[Inverse-Transformed Optimization Parameters]:\")\n",
    "# print(inverse_optimization_params)\n",
    "# print(f\"Inverse-Transformed Optimization Parameters shape: {inverse_optimization_params.shape}\")\n",
    "\n",
    "#-----------------mlflow functions---------------------\n",
    "from ml.mlflow.mlflow_logger import MLflowLogger\n",
    "    \n",
    "#-----------------Training---------------------\n",
    "\n",
    "\n",
    "\n",
    "#-----------------Predicting---------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#-----------------Shap Dashboard---------------------\n",
    "#-----------------Experimental: Generalized Bayesian Optimized Metrics Dashboard---------------------\n",
    "#-----------------Animation---------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eliminate repeated code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_script.py\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from ml.load_and_feature_engineer_data import process_basketball_data\n",
    "from ml.feature_selection.categorize_categoricals import transform_features_with_bins\n",
    "from ml.feature_selection.multicollinearchk_featimportancechk import (\n",
    "    check_multicollinearity,\n",
    "    calculate_feature_importance\n",
    ")\n",
    "from ml.feature_selection.data_loader_post_select_features import load_selected_features_data\n",
    "from ml.classification_preprocessor.smote_automation import (\n",
    "    check_dataset_for_smote,\n",
    "    apply_smote\n",
    ")\n",
    "from ml.classification_preprocessor.datapreprocessor_class import DataPreprocessor\n",
    "from ml.classification_processors.inverse_preprocessor_class import InversePreprocessor\n",
    "from ml.mlflow.mlflow_logger import MLflowLogger\n",
    "from ml.train import bayes_best_model_train\n",
    "from shap.shap_utils import compute_shap_values, plot_shap_summary\n",
    "from utils.naming_utils import generate_output_filename\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def process_ml_dataset(base_dir, participant_info_path, processed_data_dir, pipeline_assets_dir, debug=False):\n",
    "    \"\"\"\n",
    "    Process the ML dataset by loading and preprocessing basketball data.\n",
    "\n",
    "    Parameters:\n",
    "        base_dir (str): Directory path to the raw basketball data.\n",
    "        participant_info_path (str): Path to the participant information JSON file.\n",
    "        processed_data_dir (str): Directory to save processed data.\n",
    "        pipeline_assets_dir (str): Directory to save pipeline assets.\n",
    "        debug (bool): Enable detailed logging and validation outputs.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Paths to the processed ML and granular datasets.\n",
    "    \"\"\"\n",
    "    # Generate dynamic filenames\n",
    "    output_ml_filename = generate_output_filename(\"final_ml_dataset\")\n",
    "    output_granular_filename = generate_output_filename(\"final_granular_dataset\")\n",
    "    output_ml_path = os.path.join(processed_data_dir, output_ml_filename)\n",
    "    output_granular_path = os.path.join(processed_data_dir, output_granular_filename)\n",
    "\n",
    "    # Process basketball data\n",
    "    process_basketball_data(\n",
    "        directory_path=base_dir,\n",
    "        player_info_path=participant_info_path,\n",
    "        output_ml_path=output_ml_path,\n",
    "        output_granular_path=output_granular_path,\n",
    "        debug=debug,\n",
    "        log_level=logging.DEBUG if debug else logging.INFO,\n",
    "        new_data=False  # Set to True to append, False to overwrite\n",
    "    )\n",
    "\n",
    "    if debug:\n",
    "        logger.debug(f\"Processed ML Dataset saved at: {output_ml_path}\")\n",
    "        logger.debug(f\"Processed Granular Dataset saved at: {output_granular_path}\")\n",
    "    else:\n",
    "        print(\"Step [Process ML Dataset] completed.\")\n",
    "\n",
    "    return output_ml_path, output_granular_path\n",
    "\n",
    "def select_new_features(processed_ml_path, pipeline_assets_dir, debug=False):\n",
    "    \"\"\"\n",
    "    Select new features by handling categoricals, checking multicollinearity, and reviewing feature importance.\n",
    "\n",
    "    Parameters:\n",
    "        processed_ml_path (str): Path to the processed ML dataset CSV.\n",
    "        pipeline_assets_dir (str): Directory to save pipeline assets.\n",
    "        debug (bool): Enable detailed logging and validation outputs.\n",
    "\n",
    "    Returns:\n",
    "        str: Path to the selected features pickle file.\n",
    "    \"\"\"\n",
    "    # Define bin configuration\n",
    "    category_bin_config = {\n",
    "        'player_height_in_meters': {\n",
    "            'bins': [0, 1.80, 2.00, np.inf],\n",
    "            'labels': [\"Short\", \"Average\", \"Tall\"]\n",
    "        },\n",
    "        'player_weight__in_kg': {\n",
    "            'bins': [0, 75, 95, np.inf],\n",
    "            'labels': [\"Lightweight\", \"Average\", \"Heavy\"]\n",
    "        },\n",
    "        'player_estimated_wingspan_cm': {\n",
    "            'bins': [0, 190, 220, np.inf],\n",
    "            'labels': [\"Small\", \"Medium\", \"Large\"]\n",
    "        },\n",
    "        'player_estimated_standing_reach_cm': {\n",
    "            'bins': [0, 230, 250, np.inf],\n",
    "            'labels': [\"Short\", \"Average\", \"Tall\"]\n",
    "        },\n",
    "        'player_estimated_hand_length_cm': {\n",
    "            'bins': [0, 20, 25, np.inf],\n",
    "            'labels': [\"Small\", \"Medium\", \"Large\"]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Save bin configuration with dynamic naming\n",
    "    bin_config_filename = generate_output_filename(\"category_bin_config\")\n",
    "    bin_config_path = os.path.join(pipeline_assets_dir, bin_config_filename)\n",
    "    with open(bin_config_path, 'wb') as f:\n",
    "        pickle.dump(category_bin_config, f)\n",
    "    logger.info(f\"Category bin configuration saved as {bin_config_filename}\")\n",
    "\n",
    "    # Load bin configuration\n",
    "    with open(bin_config_path, 'rb') as f:\n",
    "        loaded_category_bin_config = pickle.load(f)\n",
    "    logger.debug(\"Category bin configuration loaded.\")\n",
    "\n",
    "    # Load the processed ML dataset\n",
    "    final_ml_df = pd.read_csv(processed_ml_path)\n",
    "    logger.debug(f\"ML dataset loaded with shape: {final_ml_df.shape}\")\n",
    "\n",
    "    # Transform features with bins\n",
    "    categorized_columns_df = transform_features_with_bins(\n",
    "        final_ml_df, loaded_category_bin_config, debug=debug\n",
    "    )\n",
    "\n",
    "    # Combine original DataFrame with categorized columns\n",
    "    final_ml_df_categoricals = pd.concat([final_ml_df, categorized_columns_df], axis=1)\n",
    "    logger.debug(f\"Combined DataFrame shape: {final_ml_df_categoricals.shape}\")\n",
    "\n",
    "    # Multicollinearity and Feature Importance\n",
    "    target_variable = 'result'\n",
    "    correlation_threshold = 0.8\n",
    "\n",
    "    multicollinearity_df = check_multicollinearity(\n",
    "        final_ml_df_categoricals, threshold=correlation_threshold, debug=debug\n",
    "    )\n",
    "\n",
    "    # Define features to drop based on analysis\n",
    "    drop_features = []\n",
    "\n",
    "    if not multicollinearity_df.empty:\n",
    "        for index, row in multicollinearity_df.iterrows():\n",
    "            feature1, feature2, correlation = row['Feature1'], row['Feature2'], row['Correlation']\n",
    "            logger.debug(f\"High correlation ({correlation}) between '{feature1}' and '{feature2}'.\")\n",
    "            # Decision logic to drop features (e.g., drop the second feature)\n",
    "            drop_features.append(feature2)\n",
    "        # Remove duplicates\n",
    "        drop_features = list(set(drop_features))\n",
    "        final_ml_df_categoricals.drop(columns=drop_features, errors='ignore', inplace=True)\n",
    "        logger.info(f\"Dropped {len(drop_features)} features due to multicollinearity: {', '.join(drop_features)}\")\n",
    "    else:\n",
    "        logger.info(\"No multicollinearity issues detected.\")\n",
    "\n",
    "    # Calculate feature importance\n",
    "    feature_importances = calculate_feature_importance(\n",
    "        final_ml_df_categoricals, target_variable=target_variable, n_estimators=100,\n",
    "        random_state=42, debug=debug\n",
    "    )\n",
    "    logger.debug(\"Feature importance calculated.\")\n",
    "\n",
    "    if debug:\n",
    "        print(\"\\nFinal Feature Importances:\")\n",
    "        print(feature_importances.to_string(index=False))\n",
    "    else:\n",
    "        print(\"Step [Feature Selection] completed.\")\n",
    "\n",
    "    # Final feature selection\n",
    "    final_keep_list = [\n",
    "        'release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y',\n",
    "        'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle',\n",
    "        'knee_release_angle', 'knee_max_angle', 'result', 'release_ball_speed',\n",
    "        'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z',\n",
    "        'calculated_release_angle'\n",
    "    ]\n",
    "\n",
    "    final_ml_df_selected_features = final_ml_df_categoricals[final_keep_list]\n",
    "    logger.info(f\"Retained {len(final_keep_list)} features for classification.\")\n",
    "\n",
    "    # Save selected feature names with dynamic naming\n",
    "    selected_features_filename = generate_output_filename(\"final_ml_df_selected_features_columns\", \"selected_features\", \"pkl\")\n",
    "    selected_features_path = os.path.join(pipeline_assets_dir, selected_features_filename)\n",
    "    with open(selected_features_path, 'wb') as f:\n",
    "        pickle.dump(final_ml_df_selected_features.columns.tolist(), f)\n",
    "    logger.info(f\"Selected feature names saved as {selected_features_filename}\")\n",
    "\n",
    "    return selected_features_path\n",
    "\n",
    "def preprocess_data(selected_features_path, processed_ml_path, pipeline_assets_dir, debug=False):\n",
    "    \"\"\"\n",
    "    Preprocess the data by handling class imbalance with SMOTE and applying outlier detection.\n",
    "\n",
    "    Parameters:\n",
    "        selected_features_path (str): Path to the selected features pickle file.\n",
    "        processed_ml_path (str): Path to the processed ML dataset CSV.\n",
    "        pipeline_assets_dir (str): Directory to save pipeline assets.\n",
    "        debug (bool): Enable detailed logging and validation outputs.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Preprocessed training and testing data.\n",
    "    \"\"\"\n",
    "    # Load selected features data\n",
    "    final_ml_df_selected_features = load_selected_features_data(\n",
    "        features_path=selected_features_path,\n",
    "        dataset_path=processed_ml_path,\n",
    "        category_bin_config_path=os.path.join(pipeline_assets_dir, generate_output_filename(\"category_bin_config\")),\n",
    "        y_variable='result',\n",
    "        debug=debug\n",
    "    )\n",
    "\n",
    "    # Initial Dataset Info\n",
    "    logger.info(\"[Initial Dataset Info]\")\n",
    "    logger.info(f\"Columns to work with: {final_ml_df_selected_features.columns.tolist()}\")\n",
    "\n",
    "    # Split dataset into features (X) and target (y)\n",
    "    X = final_ml_df_selected_features.drop(columns=['result'])\n",
    "    y = final_ml_df_selected_features['result']\n",
    "\n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    logger.info(\"[Train-Test Split]\")\n",
    "    logger.info(f\"X_train Shape: {X_train.shape}\")\n",
    "    logger.info(f\"X_test Shape: {X_test.shape}\")\n",
    "    logger.info(f\"y_train Shape: {y_train.shape}\")\n",
    "    logger.info(f\"y_test Shape: {y_test.shape}\")\n",
    "\n",
    "    # Analyze and apply SMOTE\n",
    "    smote_analysis = check_dataset_for_smote(X_train, y_train, debug=debug)\n",
    "    logger.info(f\"SMOTE Analysis Recommendations: {smote_analysis['recommendations']}\")\n",
    "\n",
    "    X_resampled, y_resampled, smote_used = apply_smote(\n",
    "        X_train, y_train, smote_analysis[\"recommendations\"], debug=debug\n",
    "    )\n",
    "    logger.info(f\"Applied SMOTE Variant: {smote_used}\")\n",
    "    logger.info(f\"Resampled Class Distribution: {Counter(y_resampled)}\")\n",
    "\n",
    "    # Save SMOTE technique used\n",
    "    logging.info(f\"SMOTE Technique Used: {smote_used}\")\n",
    "\n",
    "    # Optionally include outlier detection methods here\n",
    "    # (Implementation depends on specific requirements)\n",
    "\n",
    "    if debug:\n",
    "        print(\"Preprocessing completed with SMOTE applied.\")\n",
    "    else:\n",
    "        print(\"Step [Preprocessing] completed.\")\n",
    "\n",
    "    return X_resampled, X_test, y_resampled, y_test\n",
    "\n",
    "def train_model(selected_features_path, processed_ml_path, pipeline_assets_dir, model_save_dir, tuning_results_save,\n",
    "                classification_save_path, debug=False):\n",
    "    \"\"\"\n",
    "    Train machine learning models with hyperparameter tuning.\n",
    "\n",
    "    Parameters:\n",
    "        selected_features_path (str): Path to the selected features pickle file.\n",
    "        processed_ml_path (str): Path to the processed ML dataset CSV.\n",
    "        pipeline_assets_dir (str): Directory containing pipeline assets.\n",
    "        model_save_dir (str): Directory to save trained models.\n",
    "        tuning_results_save (str): Path to save hyperparameter tuning results.\n",
    "        classification_save_path (str): Path to save classification reports.\n",
    "        debug (bool): Enable detailed logging and validation outputs.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Initialize MLflow Logger\n",
    "    mlflow_logger = MLflowLogger(\n",
    "        tracking_uri=\"file:///absolute/path/to/mlruns\",  # Ensure this path exists\n",
    "        experiment_name=\"Model_Tuning_and_Evaluation\",\n",
    "        enable_mlflow=True\n",
    "    )\n",
    "\n",
    "    # Data Preprocessing\n",
    "    dp = DataPreprocessor(\n",
    "        features_path=selected_features_path,\n",
    "        dataset_path=processed_ml_path,\n",
    "        assets_path=os.path.join(pipeline_assets_dir, generate_output_filename(\"preprocessing_assets\")),\n",
    "        category_bin_config_path=os.path.join(pipeline_assets_dir, generate_output_filename(\"category_bin_config\")),\n",
    "        y_variable='result',\n",
    "        perform_split=True,\n",
    "        test_size=0.3,\n",
    "        random_state=123,\n",
    "        stratify=True,\n",
    "        optimization_columns=[\"knee_max_angle\", \"wrist_max_angle\", \"elbow_max_angle\"],\n",
    "        debug=debug\n",
    "    )\n",
    "\n",
    "    results = dp.run(return_optimization_ranges=True)\n",
    "    X_train_transformed, X_test_transformed, y_train_encoded, y_test_encoded, _, _, _, _, optimization_ranges, _ = results\n",
    "    logger.debug(\"Data preprocessing for model training completed.\")\n",
    "\n",
    "    # Model Training and Hyperparameter Tuning\n",
    "    logger.info(\"Starting hyperparameter tuning and evaluation...\")\n",
    "    bayes_best_model_train(\n",
    "        X_train=X_train_transformed,\n",
    "        y_train=y_train_encoded,\n",
    "        X_test=X_test_transformed,\n",
    "        y_test=y_test_encoded,\n",
    "        selection_metric=\"Log Loss\",\n",
    "        use_pca=True,\n",
    "        save_dir=tuning_results_save,\n",
    "        model_save_dir=model_save_dir,\n",
    "        classification_save_path=classification_save_path,\n",
    "        selected_models=\"XGBoost\",  # Can choose decision tree or random forest, or set to None to find best of all 3\n",
    "        mlflow_logger=mlflow_logger\n",
    "    )\n",
    "\n",
    "    if debug:\n",
    "        print(\"Model training and hyperparameter tuning completed.\")\n",
    "    else:\n",
    "        print(\"Step [Train Model] completed.\")\n",
    "\n",
    "def predict_and_evaluate(model_save_dir, pipeline_assets_dir, processed_ml_path, debug=False):\n",
    "    \"\"\"\n",
    "    Make predictions using the trained model and perform inverse transformations for interpretability.\n",
    "\n",
    "    Parameters:\n",
    "        model_save_dir (str): Directory where models are saved.\n",
    "        pipeline_assets_dir (str): Directory containing pipeline assets.\n",
    "        processed_ml_path (str): Path to the processed ML dataset CSV.\n",
    "        debug (bool): Enable detailed logging and validation outputs.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Load selected features\n",
    "    selected_features_filename = generate_output_filename(\"final_ml_df_selected_features_columns\", \"selected_features\", \"pkl\")\n",
    "    selected_features_path = os.path.join(pipeline_assets_dir, selected_features_filename)\n",
    "\n",
    "    # Load the processed ML dataset\n",
    "    final_ml_df_selected_features = load_selected_features_data(\n",
    "        features_path=selected_features_path,\n",
    "        dataset_path=processed_ml_path,\n",
    "        category_bin_config_path=os.path.join(pipeline_assets_dir, generate_output_filename(\"category_bin_config\")),\n",
    "        y_variable='result',\n",
    "        debug=debug\n",
    "    )\n",
    "\n",
    "    # Split dataset into features (X) and target (y)\n",
    "    X = final_ml_df_selected_features.drop(columns=['result'])\n",
    "    y = final_ml_df_selected_features['result']\n",
    "\n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    # Load the trained model\n",
    "    load_model_name = \"XGBoost\"\n",
    "    logger.info(f\"Loading model: {load_model_name}\")\n",
    "    loaded_model = load_model(load_model_name, save_dir=model_save_dir)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = loaded_model.predict(X_test)\n",
    "    logger.info(f\"Predictions from loaded model: {y_pred}\")\n",
    "\n",
    "    # Initialize Inverse Preprocessor\n",
    "    preprocessing_assets_filename = generate_output_filename(\"preprocessing_assets\")\n",
    "    preprocessing_assets_path = os.path.join(pipeline_assets_dir, preprocessing_assets_filename)\n",
    "    inverse_transformer = InversePreprocessor(\n",
    "        assets_path=preprocessing_assets_path,  # Assets will be loaded automatically\n",
    "        debug=debug\n",
    "    )\n",
    "\n",
    "    # Perform inverse transformation and combine with targets\n",
    "    final_dataset = inverse_transformer.transform(\n",
    "        original_data=X_train,\n",
    "        transformed_data=None,  # Adjust as per your pipeline\n",
    "        y_encoded=y_train  # Adjust encoding as necessary\n",
    "    )\n",
    "\n",
    "    # Append specified columns\n",
    "    columns_to_append = ['player_height_in_meters', 'player_weight__in_kg']\n",
    "    final_dataset = inverse_transformer.append_columns_from_original(\n",
    "        final_dataset=final_dataset,\n",
    "        original_data=X_train,\n",
    "        columns_to_append=columns_to_append,\n",
    "        debug=debug\n",
    "    )\n",
    "\n",
    "    # Display the resulting dataset\n",
    "    print(\"[Final Dataset]:\")\n",
    "    print(final_dataset.head())\n",
    "    print(final_dataset.shape)\n",
    "\n",
    "    # Compute SHAP values and plot summary\n",
    "    shap_values = compute_shap_values(loaded_model, X_test)\n",
    "    plot_shap_summary(shap_values, X_test)\n",
    "\n",
    "    if debug:\n",
    "        print(\"Prediction and evaluation completed.\")\n",
    "    else:\n",
    "        print(\"Step [Predict and Evaluate] completed.\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to execute the machine learning pipeline.\n",
    "    \"\"\"\n",
    "    # Define base directories\n",
    "    base_dir = \"../../../SPL-Open-Data/basketball/freethrow/data/P0001\"\n",
    "    participant_info_path = \"../../../SPL-Open-Data/basketball/freethrow/participant_information.json\"\n",
    "    processed_data_dir = \"../../data/processed/\"\n",
    "    pipeline_assets_dir = '../../data/model/pipeline/'\n",
    "    model_save_dir = \"../../data/model\"\n",
    "    tuning_results_save = \"../../data/model/tuning_results/tuning_results.json\"\n",
    "    classification_save_path = \"../../data/model/classification_reports/classification_reports.txt\"\n",
    "\n",
    "    # Set debug mode\n",
    "    debug_mode = True  # Set to False to minimize outputs\n",
    "\n",
    "    # Step 1: Process ML Dataset\n",
    "    processed_ml_path, processed_granular_path = process_ml_dataset(\n",
    "        base_dir=base_dir,\n",
    "        participant_info_path=participant_info_path,\n",
    "        processed_data_dir=processed_data_dir,\n",
    "        pipeline_assets_dir=pipeline_assets_dir,\n",
    "        debug=debug_mode\n",
    "    )\n",
    "\n",
    "    # Step 2: Select New Features\n",
    "    selected_features_path = select_new_features(\n",
    "        processed_ml_path=processed_ml_path,\n",
    "        pipeline_assets_dir=pipeline_assets_dir,\n",
    "        debug=debug_mode\n",
    "    )\n",
    "\n",
    "    # Step 3: Preprocess Data\n",
    "    X_resampled, X_test, y_resampled, y_test = preprocess_data(\n",
    "        selected_features_path=selected_features_path,\n",
    "        processed_ml_path=processed_ml_path,\n",
    "        pipeline_assets_dir=pipeline_assets_dir,\n",
    "        debug=debug_mode\n",
    "    )\n",
    "\n",
    "    # Step 4: Train Model\n",
    "    train_model(\n",
    "        selected_features_path=selected_features_path,\n",
    "        processed_ml_path=processed_ml_path,\n",
    "        pipeline_assets_dir=pipeline_assets_dir,\n",
    "        model_save_dir=model_save_dir,\n",
    "        tuning_results_save=tuning_results_save,\n",
    "        classification_save_path=classification_save_path,\n",
    "        debug=debug_mode\n",
    "    )\n",
    "\n",
    "    # Step 5: Predict and Evaluate\n",
    "    predict_and_evaluate(\n",
    "        model_save_dir=model_save_dir,\n",
    "        pipeline_assets_dir=pipeline_assets_dir,\n",
    "        processed_ml_path=processed_ml_path,\n",
    "        debug=debug_mode\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science_ft_bio_predictions",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
