{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic KMeans use for machine learning\n",
    "\n",
    "\n",
    "Automating the preprocessing and selection of appropriate clustering algorithms—**KMeans**, **KModes**, and **KPrototypes**—requires a systematic approach that evaluates the dataset's characteristics and applies suitable preprocessing steps. Below is a detailed breakdown of how to achieve this automation, including dataset requirements for each algorithm and a defined decision path for their usage.\n",
    "\n",
    "## Overview\n",
    "\n",
    "1. **Dataset Analysis**: Determine the types of features (numerical, categorical, or mixed) and assess data quality (missing values, outliers, scaling).\n",
    "2. **Preprocessing Steps**: Apply transformations based on feature types and data quality.\n",
    "3. **Algorithm Selection**: Choose the appropriate clustering algorithm based on feature types.\n",
    "4. **Model Execution and Evaluation**: Apply the selected algorithm and evaluate clustering performance.\n",
    "\n",
    "## Step-by-Step Automation Process\n",
    "\n",
    "### 1. Dataset Analysis\n",
    "\n",
    "**Objective**: Identify the nature of the dataset to determine suitable preprocessing steps and the appropriate clustering algorithm.\n",
    "\n",
    "**Actions**:\n",
    "\n",
    "- **Identify Feature Types**:\n",
    "  - **Numerical Features**: Continuous or discrete numerical values.\n",
    "  - **Categorical Features**: Nominal or ordinal data.\n",
    "  - **Mixed Features**: A combination of numerical and categorical features.\n",
    "\n",
    "- **Assess Data Quality**:\n",
    "  - **Missing Values**: Check for any null or missing entries.\n",
    "  - **Outliers**: Detect and evaluate outliers in numerical data.\n",
    "  - **Scaling**: Determine if numerical features are on different scales.\n",
    "  - **Dimensionality**: Evaluate the number of features and their correlations.\n",
    "\n",
    "**Implementation**:\n",
    "\n",
    "Use libraries like `pandas` and `numpy` for data inspection.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def analyze_dataset(df):\n",
    "    feature_types = {}\n",
    "    for column in df.columns:\n",
    "        if pd.api.types.is_numeric_dtype(df[column]):\n",
    "            feature_types[column] = 'numerical'\n",
    "        else:\n",
    "            feature_types[column] = 'categorical'\n",
    "    \n",
    "    num_features = [col for col, typ in feature_types.items() if typ == 'numerical']\n",
    "    cat_features = [col for col, typ in feature_types.items() if typ == 'categorical']\n",
    "    \n",
    "    missing_values = df.isnull().sum()\n",
    "    outliers = {}\n",
    "    for col in num_features:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        outliers[col] = df[(df[col] < (Q1 - 1.5 * IQR)) | (df[col] > (Q3 + 1.5 * IQR))].shape[0]\n",
    "    \n",
    "    correlations = df[num_features].corr().abs()\n",
    "    upper_tri = correlations.where(np.triu(np.ones(correlations.shape), k=1).astype(bool))\n",
    "    highly_correlated = [column for column in upper_tri.columns if any(upper_tri[column] > 0.9)]\n",
    "    \n",
    "    return {\n",
    "        'feature_types': feature_types,\n",
    "        'num_features': num_features,\n",
    "        'cat_features': cat_features,\n",
    "        'missing_values': missing_values,\n",
    "        'outliers': outliers,\n",
    "        'highly_correlated': highly_correlated\n",
    "    }\n",
    "```\n",
    "\n",
    "### 2. Preprocessing Steps\n",
    "\n",
    "**Objective**: Clean and prepare data based on its characteristics to ensure optimal performance of the clustering algorithms.\n",
    "\n",
    "**Actions**:\n",
    "\n",
    "- **Handling Missing Data**:\n",
    "  - **Numerical Features**: Impute with mean or median.\n",
    "  - **Categorical Features**: Impute with mode or a new category (e.g., 'Unknown').\n",
    "\n",
    "- **Removing Outliers**:\n",
    "  - Apply techniques like IQR filtering or Z-score thresholding for numerical data.\n",
    "\n",
    "- **Scaling Numerical Features**:\n",
    "  - Standardize (zero mean, unit variance) or normalize (range [0,1]) to ensure all features contribute equally.\n",
    "\n",
    "- **Encoding Categorical Features**:\n",
    "  - **KModes/KPrototypes**: Typically, these algorithms handle categorical data directly, so encoding is not mandatory. However, ensure consistent data formats.\n",
    "  - **KPrototypes**: Requires specifying categorical feature indices.\n",
    "\n",
    "- **Dimensionality Reduction**:\n",
    "  - Apply PCA for numerical data to reduce dimensions if necessary.\n",
    "\n",
    "**Implementation**:\n",
    "\n",
    "```python\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def preprocess_data(df, analysis_results):\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Handling Missing Values\n",
    "    if analysis_results['num_features']:\n",
    "        num_imputer = SimpleImputer(strategy='median')\n",
    "        df_clean[analysis_results['num_features']] = num_imputer.fit_transform(df_clean[analysis_results['num_features']])\n",
    "    \n",
    "    if analysis_results['cat_features']:\n",
    "        cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "        df_clean[analysis_results['cat_features']] = cat_imputer.fit_transform(df_clean[analysis_results['cat_features']])\n",
    "    \n",
    "    # Removing Outliers\n",
    "    for col in analysis_results['num_features']:\n",
    "        Q1 = df_clean[col].quantile(0.25)\n",
    "        Q3 = df_clean[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        df_clean = df_clean[(df_clean[col] >= (Q1 - 1.5 * IQR)) & (df_clean[col] <= (Q3 + 1.5 * IQR))]\n",
    "    \n",
    "    # Scaling Numerical Features\n",
    "    scaler = StandardScaler()\n",
    "    df_clean[analysis_results['num_features']] = scaler.fit_transform(df_clean[analysis_results['num_features']])\n",
    "    \n",
    "    # Dimensionality Reduction (optional)\n",
    "    if len(analysis_results['num_features']) > 10:\n",
    "        pca = PCA(n_components=10)\n",
    "        df_clean[analysis_results['num_features']] = pca.fit_transform(df_clean[analysis_results['num_features']])\n",
    "    \n",
    "    return df_clean\n",
    "```\n",
    "\n",
    "### 3. Algorithm Selection\n",
    "\n",
    "**Objective**: Automatically choose the most suitable clustering algorithm based on the dataset's feature types.\n",
    "\n",
    "**Decision Criteria**:\n",
    "\n",
    "- **All Numerical Features**:\n",
    "  - **Algorithm**: KMeans\n",
    "  - **Reason**: KMeans is optimized for numerical data using distance metrics.\n",
    "\n",
    "- **All Categorical Features**:\n",
    "  - **Algorithm**: KModes\n",
    "  - **Reason**: KModes is designed to handle categorical data by minimizing dissimilarities based on mode.\n",
    "\n",
    "- **Mixed Numerical and Categorical Features**:\n",
    "  - **Algorithm**: KPrototypes\n",
    "  - **Reason**: KPrototypes combines KMeans and KModes to handle mixed data types.\n",
    "\n",
    "**Implementation**:\n",
    "\n",
    "```python\n",
    "from kmodes.kmodes import KModes\n",
    "from kmodes.kprototypes import KPrototypes\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def select_clustering_algorithm(analysis_results):\n",
    "    if analysis_results['num_features'] and not analysis_results['cat_features']:\n",
    "        return 'KMeans'\n",
    "    elif analysis_results['cat_features'] and not analysis_results['num_features']:\n",
    "        return 'KModes'\n",
    "    elif analysis_results['num_features'] and analysis_results['cat_features']:\n",
    "        return 'KPrototypes'\n",
    "    else:\n",
    "        raise ValueError(\"Dataset must contain at least one numerical or categorical feature.\")\n",
    "```\n",
    "\n",
    "### 4. Model Execution and Evaluation\n",
    "\n",
    "**Objective**: Apply the selected clustering algorithm and evaluate its performance.\n",
    "\n",
    "**Actions**:\n",
    "\n",
    "- **Determine Optimal Number of Clusters (K)**:\n",
    "  - Use methods like Elbow, Silhouette, or Gap Statistics.\n",
    "\n",
    "- **Fit the Model**:\n",
    "  - Apply the chosen algorithm with the optimal K.\n",
    "\n",
    "- **Evaluate Clustering**:\n",
    "  - Use metrics such as Silhouette Score, Davies-Bouldin Index, or domain-specific evaluations.\n",
    "\n",
    "**Implementation**:\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def determine_optimal_k_kmeans(df, max_k=10):\n",
    "    cost = []\n",
    "    K = range(1, max_k+1)\n",
    "    for k in K:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "        kmeans.fit(df)\n",
    "        cost.append(kmeans.inertia_)\n",
    "    plt.plot(K, cost, 'bx-')\n",
    "    plt.xlabel('Number of clusters (k)')\n",
    "    plt.ylabel('Inertia')\n",
    "    plt.title('Elbow Method For Optimal k')\n",
    "    plt.show()\n",
    "    # Choose k where the elbow appears\n",
    "    # This step can be automated using derivative methods or left for manual inspection\n",
    "    return 3  # Placeholder\n",
    "\n",
    "def determine_optimal_k_kmodes(df, max_k=10):\n",
    "    cost = []\n",
    "    K = range(1, max_k+1)\n",
    "    for k in K:\n",
    "        kmodes = KModes(n_clusters=k, init='Huang', n_init=5, verbose=0)\n",
    "        kmodes.fit_predict(df)\n",
    "        cost.append(kmodes.cost_)\n",
    "    plt.plot(K, cost, 'bx-')\n",
    "    plt.xlabel('Number of clusters (k)')\n",
    "    plt.ylabel('Cost')\n",
    "    plt.title('Elbow Method For Optimal k (KModes)')\n",
    "    plt.show()\n",
    "    return 3  # Placeholder\n",
    "\n",
    "def determine_optimal_k_kprototypes(df, categorical_indices, max_k=10):\n",
    "    cost = []\n",
    "    K = range(1, max_k+1)\n",
    "    for k in K:\n",
    "        kproto = KPrototypes(n_clusters=k, init='Cao', verbose=0)\n",
    "        kproto.fit_predict(df, categorical=categorical_indices)\n",
    "        cost.append(kproto.cost_)\n",
    "    plt.plot(K, cost, 'bx-')\n",
    "    plt.xlabel('Number of clusters (k)')\n",
    "    plt.ylabel('Cost')\n",
    "    plt.title('Elbow Method For Optimal k (KPrototypes)')\n",
    "    plt.show()\n",
    "    return 3  # Placeholder\n",
    "\n",
    "def execute_clustering(df, analysis_results, algorithm):\n",
    "    if algorithm == 'KMeans':\n",
    "        optimal_k = determine_optimal_k_kmeans(df[analysis_results['num_features']])\n",
    "        model = KMeans(n_clusters=optimal_k, random_state=0)\n",
    "        model.fit(df[analysis_results['num_features']])\n",
    "        labels = model.labels_\n",
    "        score = silhouette_score(df[analysis_results['num_features']], labels)\n",
    "        return model, labels, score\n",
    "    \n",
    "    elif algorithm == 'KModes':\n",
    "        optimal_k = determine_optimal_k_kmodes(df[analysis_results['cat_features']])\n",
    "        kmodes = KModes(n_clusters=optimal_k, init='Huang', n_init=5, verbose=0)\n",
    "        labels = kmodes.fit_predict(df[analysis_results['cat_features']])\n",
    "        score = silhouette_score(pd.get_dummies(df[analysis_results['cat_features']]), labels)\n",
    "        return kmodes, labels, score\n",
    "    \n",
    "    elif algorithm == 'KPrototypes':\n",
    "        categorical_indices = [df.columns.get_loc(col) for col in analysis_results['cat_features']]\n",
    "        optimal_k = determine_optimal_k_kprototypes(df, categorical_indices)\n",
    "        kproto = KPrototypes(n_clusters=optimal_k, init='Cao', verbose=0)\n",
    "        labels = kproto.fit_predict(df, categorical=categorical_indices)\n",
    "        # For mixed data, silhouette score can be approximated using a custom distance\n",
    "        # Here, using only numerical features for score\n",
    "        score = silhouette_score(df[analysis_results['num_features']], labels)\n",
    "        return kproto, labels, score\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Unsupported algorithm selected.\")\n",
    "```\n",
    "\n",
    "### 5. Automation Workflow\n",
    "\n",
    "**Objective**: Integrate all steps into a cohesive automated pipeline.\n",
    "\n",
    "**Implementation**:\n",
    "\n",
    "```python\n",
    "def automated_clustering_pipeline(df):\n",
    "    # Step 1: Analyze Dataset\n",
    "    analysis_results = analyze_dataset(df)\n",
    "    \n",
    "    # Step 2: Preprocess Data\n",
    "    df_clean = preprocess_data(df, analysis_results)\n",
    "    \n",
    "    # Step 3: Select Clustering Algorithm\n",
    "    algorithm = select_clustering_algorithm(analysis_results)\n",
    "    print(f\"Selected Clustering Algorithm: {algorithm}\")\n",
    "    \n",
    "    # Step 4: Execute Clustering\n",
    "    model, labels, score = execute_clustering(df_clean, analysis_results, algorithm)\n",
    "    print(f\"Clustering Silhouette Score: {score}\")\n",
    "    \n",
    "    # Add cluster labels to the original dataframe\n",
    "    df['Cluster'] = labels\n",
    "    return df, model, score\n",
    "```\n",
    "\n",
    "**Usage Example**:\n",
    "\n",
    "```python\n",
    "# Sample DataFrame\n",
    "data = {\n",
    "    'age': [25, 30, 22, 35, 28, 40, 50, 23],\n",
    "    'income': [50000, 60000, 45000, 80000, 52000, 90000, 120000, 48000],\n",
    "    'gender': ['M', 'F', 'F', 'M', 'F', 'M', 'M', 'F'],\n",
    "    'marital_status': ['Single', 'Married', 'Single', 'Married', 'Single', 'Married', 'Married', 'Single']\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Run the automated pipeline\n",
    "clustered_df, model, silhouette = automated_clustering_pipeline(df)\n",
    "print(clustered_df)\n",
    "```\n",
    "\n",
    "### 6. Defined Path for Algorithm Usage\n",
    "\n",
    "Based on the dataset analysis and preprocessing, the following path determines which clustering algorithm to use:\n",
    "\n",
    "1. **All Numerical Features**:\n",
    "   - **Use KMeans**\n",
    "   - **Preprocessing**: Handle missing values, remove outliers, scale features.\n",
    "   - **Example Scenarios**: Customer segmentation based on transaction amounts, clustering based on physical measurements.\n",
    "\n",
    "2. **All Categorical Features**:\n",
    "   - **Use KModes**\n",
    "   - **Preprocessing**: Handle missing values, ensure consistent encoding.\n",
    "   - **Example Scenarios**: Market segmentation based on categorical demographics, clustering survey responses.\n",
    "\n",
    "3. **Mixed Numerical and Categorical Features**:\n",
    "   - **Use KPrototypes**\n",
    "   - **Preprocessing**: Handle missing values, remove outliers in numerical data, scale numerical features, specify categorical feature indices.\n",
    "   - **Example Scenarios**: Customer segmentation combining demographics (categorical) and purchase behavior (numerical), clustering products with attributes of different types.\n",
    "\n",
    "### 7. Additional Considerations\n",
    "\n",
    "- **Optimal K Selection**:\n",
    "  - Automating the detection of the \"elbow\" in the cost plot can be enhanced using methods like the **Kneedle algorithm** to programmatically determine the optimal number of clusters.\n",
    "  \n",
    "- **Handling High Dimensionality**:\n",
    "  - For datasets with a large number of features, consider dimensionality reduction techniques (e.g., PCA for numerical data) before clustering to improve performance and reduce noise.\n",
    "\n",
    "- **Scalability**:\n",
    "  - For large datasets, consider algorithm scalability. KMeans is generally faster, while KModes and KPrototypes may require optimization or sampling.\n",
    "\n",
    "- **Evaluation Metrics**:\n",
    "  - **Silhouette Score**: Measures how similar an object is to its own cluster compared to other clusters.\n",
    "  - **Davies-Bouldin Index**: Evaluates the average similarity ratio of each cluster with its most similar cluster.\n",
    "  - **Domain-Specific Metrics**: Depending on the application, custom metrics may be more appropriate.\n",
    "\n",
    "## Example Scenario Breakdown\n",
    "\n",
    "### Scenario 1: All Numerical Data\n",
    "\n",
    "**Dataset**:\n",
    "- Features: Age, Income, Purchase Frequency\n",
    "\n",
    "**Preprocessing**:\n",
    "- Impute missing values with median.\n",
    "- Remove outliers using IQR.\n",
    "- Scale features using StandardScaler.\n",
    "\n",
    "**Algorithm**:\n",
    "- **KMeans**\n",
    "\n",
    "**Path**:\n",
    "- Numerical → KMeans\n",
    "\n",
    "### Scenario 2: All Categorical Data\n",
    "\n",
    "**Dataset**:\n",
    "- Features: Gender, Marital Status, Education Level\n",
    "\n",
    "**Preprocessing**:\n",
    "- Impute missing values with mode.\n",
    "- Ensure categorical consistency (e.g., standardized category labels).\n",
    "\n",
    "**Algorithm**:\n",
    "- **KModes**\n",
    "\n",
    "**Path**:\n",
    "- Categorical → KModes\n",
    "\n",
    "### Scenario 3: Mixed Data\n",
    "\n",
    "**Dataset**:\n",
    "- Features: Age, Income, Gender, Marital Status\n",
    "\n",
    "**Preprocessing**:\n",
    "- Impute missing values appropriately.\n",
    "- Remove outliers from numerical features.\n",
    "- Scale numerical features.\n",
    "- Identify categorical feature indices.\n",
    "\n",
    "**Algorithm**:\n",
    "- **KPrototypes**\n",
    "\n",
    "**Path**:\n",
    "- Mixed → KPrototypes\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Automating the preprocessing and selection of clustering algorithms involves a structured approach to analyze the dataset, apply appropriate preprocessing steps, and select the most suitable clustering method based on feature types. By following the outlined steps and leveraging the provided code snippets, you can create a robust pipeline that dynamically adapts to various datasets, ensuring optimal clustering performance.\n",
    "\n",
    "## References\n",
    "\n",
    "- **KModes Documentation**: [kmodes GitHub](https://github.com/nicodv/kmodes)\n",
    "- **KPrototypes Documentation**: [kmodes GitHub](https://github.com/nicodv/kmodes)\n",
    "- **Scikit-learn Clustering**: [Scikit-learn Clustering](https://scikit-learn.org/stable/modules/clustering.html)\n",
    "- **Silhouette Score**: [Silhouette Analysis](https://scikit-learn.org/stable/modules/clustering.html#silhouette-coefficient)\n",
    "- **Kneedle Algorithm for Elbow Detection**: [Kneedle GitHub](https://github.com/arunponnusamy/kneedle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certainly! Integrating clustering-specific preprocessing rules into a comprehensive, automated preprocessing pipeline ensures that each clustering algorithm (**KMeans**, **KModes**, **KPrototypes**) is handled appropriately based on the dataset's characteristics. Below is a detailed guide on how to modify your existing preprocessing pipeline to accommodate clustering models without affecting other model types.\n",
    "\n",
    "## Overview of Integration\n",
    "\n",
    "1. **Extend the Preprocessor Configuration**: Incorporate clustering model types and their specific requirements into the preprocessor's configuration.\n",
    "2. **Conditional Preprocessing Steps**: Apply different preprocessing rules based on the selected model type, especially for clustering algorithms.\n",
    "3. **Maintain Separation**: Ensure that clustering-specific preprocessing does not interfere with preprocessing steps for other model types.\n",
    "\n",
    "## Updated Preprocessing Pipeline\n",
    "\n",
    "Below is an enhanced version of your preprocessing pipeline that integrates clustering-specific rules. We'll modify the `DataPreprocessor` class to handle clustering models appropriately.\n",
    "\n",
    "### 1. Initialize Preprocessor and Configure Options\n",
    "\n",
    "**Goal**: Set up the preprocessing pipeline with necessary configurations, including model type, whether to perform a train-test split, and other preprocessing options.\n",
    "\n",
    "**Implementation Example**:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "from kmodes.kmodes import KModes\n",
    "from kmodes.kprototypes import KPrototypes\n",
    "from sklearn.cluster import KMeans\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "from scipy.stats import shapiro, skew\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from scipy.stats import probplot\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "class DataPreprocessor:\n",
    "    def __init__(self, model_type, perform_split=True, split_ratio=0.2, random_state=42, preprocessing_options=None):\n",
    "        self.model_type = model_type.lower()\n",
    "        self.perform_split = perform_split\n",
    "        self.split_ratio = split_ratio\n",
    "        self.random_state = random_state\n",
    "        self.preprocessing_options = preprocessing_options or {}\n",
    "        self.fitted = False  # Flag to check if preprocessors are fitted\n",
    "        \n",
    "        # Initialize placeholders for transformers\n",
    "        self.imputers = {}\n",
    "        self.transformers = {}\n",
    "        self.encoders = {}\n",
    "        self.scalers = {}\n",
    "        self.smote = None  # SMOTE instance if applied\n",
    "        \n",
    "        # Define model-specific preprocessing requirements\n",
    "        self.model_requirements = self._define_model_requirements()\n",
    "        \n",
    "    def _define_model_requirements(self):\n",
    "        # Define preprocessing recommendations based on model type\n",
    "        requirements = {\n",
    "            'kmeans': {\n",
    "                'feature_types': 'numerical',\n",
    "                'imputation': {'numerical': 'median', 'categorical': 'mode'},\n",
    "                'scaling': 'standard',\n",
    "                'encoding': None,\n",
    "                'outlier_handling': 'zscore_iqr',\n",
    "                'transformation': 'yeo-johnson'\n",
    "            },\n",
    "            'kmodes': {\n",
    "                'feature_types': 'categorical',\n",
    "                'imputation': {'numerical': 'median', 'categorical': 'mode'},\n",
    "                'scaling': None,\n",
    "                'encoding': 'ordinal_nominal',  # Typically not needed, but ensuring consistency\n",
    "                'outlier_handling': 'mode',\n",
    "                'transformation': None\n",
    "            },\n",
    "            'kprototypes': {\n",
    "                'feature_types': 'mixed',\n",
    "                'imputation': {'numerical': 'median', 'categorical': 'mode'},\n",
    "                'scaling': 'standard',  # Scale numerical features\n",
    "                'encoding': 'ordinal_nominal',\n",
    "                'outlier_handling': 'isolationforest',\n",
    "                'transformation': 'yeo-johnson'\n",
    "            },\n",
    "            # Add other model types as needed\n",
    "            # ...\n",
    "        }\n",
    "        return requirements\n",
    "    \n",
    "    # Other methods will be defined below\n",
    "```\n",
    "\n",
    "### 2. Split Dataset into Train/Test and X/y\n",
    "\n",
    "**Goal**: Divide the dataset into training and testing subsets to ensure unbiased model evaluation and prevent data leakage.\n",
    "\n",
    "**Modification for Clustering**:\n",
    "\n",
    "- **Clustering Models**: Since clustering is unsupervised, the target variable `y` is not required. However, for consistency, you can still split the data if needed, but typically, clustering algorithms are applied on the entire dataset or a sample.\n",
    "\n",
    "**Implementation Example**:\n",
    "\n",
    "```python\n",
    "    def split_dataset(self, X, y=None):\n",
    "        if not self.perform_split:\n",
    "            return X, X, y, y  # Return copies for consistency\n",
    "        \n",
    "        if self.model_type in ['classification', 'logistic regression']:\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X, y, test_size=self.split_ratio, stratify=y, random_state=self.random_state\n",
    "            )\n",
    "        elif self.model_type in ['regression', 'linear regression']:\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X, y, test_size=self.split_ratio, random_state=self.random_state\n",
    "            )\n",
    "        elif self.model_type in ['time_series']:\n",
    "            split_index = int(len(X) * (1 - self.split_ratio))\n",
    "            X_train, X_test = X.iloc[:split_index], X.iloc[split_index:]\n",
    "            y_train, y_test = y.iloc[:split_index], y.iloc[split_index:]\n",
    "        elif self.model_type in ['clustering']:\n",
    "            # For clustering, y is not used\n",
    "            X_train, X_test = train_test_split(\n",
    "                X, test_size=self.split_ratio, random_state=self.random_state\n",
    "            )\n",
    "            y_train, y_test = None, None\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported model type for splitting.\")\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test\n",
    "```\n",
    "\n",
    "### 3. Handle Missing Values\n",
    "\n",
    "**Goal**: Impute missing values appropriately to maintain data integrity.\n",
    "\n",
    "**Modification for Clustering**:\n",
    "\n",
    "- **Clustering Models**: Depending on the clustering algorithm:\n",
    "  - **KMeans & KPrototypes**: Handle numerical and categorical missing values as per their requirements.\n",
    "  - **KModes**: Focus on imputing categorical missing values.\n",
    "\n",
    "**Implementation Example**:\n",
    "\n",
    "```python\n",
    "    def handle_missing_values(self, X_train, X_test, y_train=None):\n",
    "        numerical_features = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "        categorical_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "        \n",
    "        # Get imputation strategies from model requirements\n",
    "        impute_config = self.model_requirements.get(self.model_type, {}).get('imputation', {})\n",
    "        num_strategy = impute_config.get('numerical', 'median')\n",
    "        cat_strategy = impute_config.get('categorical', 'mode')\n",
    "        \n",
    "        # Handle time_series separately if needed\n",
    "        if self.model_type == 'time_series':\n",
    "            # Numerical features: Interpolation\n",
    "            X_train[numerical_features] = X_train[numerical_features].interpolate(method='linear')\n",
    "            X_test[numerical_features] = X_test[numerical_features].interpolate(method='linear')\n",
    "            \n",
    "            # Categorical features: Fill with 'Missing'\n",
    "            X_train[categorical_features] = X_train[categorical_features].fillna('Missing')\n",
    "            X_test[categorical_features] = X_test[categorical_features].fillna('Missing')\n",
    "            return X_train, X_test\n",
    "        \n",
    "        # Numerical Imputer\n",
    "        if numerical_features:\n",
    "            num_imputer = SimpleImputer(strategy=num_strategy)\n",
    "            num_imputer.fit(X_train[numerical_features])\n",
    "            X_train_num = num_imputer.transform(X_train[numerical_features])\n",
    "            X_test_num = num_imputer.transform(X_test[numerical_features])\n",
    "            self.imputers['numerical'] = num_imputer\n",
    "            \n",
    "            # Reconstruct numerical DataFrames\n",
    "            X_train[numerical_features] = X_train_num\n",
    "            X_test[numerical_features] = X_test_num\n",
    "        \n",
    "        # Categorical Imputer\n",
    "        if categorical_features:\n",
    "            if cat_strategy == 'constant_missing':\n",
    "                cat_imputer = SimpleImputer(strategy='constant', fill_value='Missing')\n",
    "            else:\n",
    "                cat_imputer = SimpleImputer(strategy=cat_strategy)\n",
    "            cat_imputer.fit(X_train[categorical_features])\n",
    "            X_train_cat = cat_imputer.transform(X_train[categorical_features])\n",
    "            X_test_cat = cat_imputer.transform(X_test[categorical_features])\n",
    "            self.imputers['categorical'] = cat_imputer\n",
    "            \n",
    "            # Reconstruct categorical DataFrames\n",
    "            X_train[categorical_features] = X_train_cat\n",
    "            X_test[categorical_features] = X_test_cat\n",
    "        \n",
    "        return X_train, X_test\n",
    "```\n",
    "\n",
    "### 4. Test for Normality\n",
    "\n",
    "**Goal**: Determine if feature distributions meet model assumptions regarding normality.\n",
    "\n",
    "**Modification for Clustering**:\n",
    "\n",
    "- **KMeans & KPrototypes**: Benefit from normally distributed numerical features but do not strictly require it. Skewness can affect clustering performance.\n",
    "- **KModes**: Does not require normality as it deals with categorical data.\n",
    "\n",
    "**Implementation Example**:\n",
    "\n",
    "```python\n",
    "    def test_normality(self, X_train):\n",
    "        if self.model_type in ['time_series', 'kmodes']:\n",
    "            # Normality not a primary concern for time_series and KModes\n",
    "            return []\n",
    "        \n",
    "        p_value_threshold = self.preprocessing_options.get('p_value_threshold', 0.05)\n",
    "        skewness_threshold = self.preprocessing_options.get('skewness_threshold', 1.0)\n",
    "        features_to_transform = []\n",
    "        \n",
    "        numerical_features = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "        \n",
    "        for col in numerical_features:\n",
    "            data = X_train[col].dropna()\n",
    "            if self.model_type in ['linear_regression', 'logistic_regression']:\n",
    "                stat, p_val = shapiro(data)\n",
    "                col_skew = skew(data)\n",
    "                if p_val < p_value_threshold or abs(col_skew) > skewness_threshold:\n",
    "                    features_to_transform.append(col)\n",
    "            elif self.model_type in ['neural_networks', 'svm', 'k_nn', 'kmeans', 'kprototypes']:\n",
    "                col_skew = skew(data)\n",
    "                if abs(col_skew) > skewness_threshold:\n",
    "                    features_to_transform.append(col)\n",
    "            else:\n",
    "                # Default behavior\n",
    "                col_skew = skew(data)\n",
    "                if abs(col_skew) > skewness_threshold:\n",
    "                    features_to_transform.append(col)\n",
    "        \n",
    "        return features_to_transform\n",
    "```\n",
    "\n",
    "### 5. Handle Outliers\n",
    "\n",
    "**Goal**: Reduce the influence of extreme values that can skew model performance.\n",
    "\n",
    "**Modification for Clustering**:\n",
    "\n",
    "- **KMeans & KPrototypes**: Sensitive to outliers as they rely on distance metrics.\n",
    "- **KModes**: Less sensitive to numerical outliers since it operates on categorical data, but still may need handling if mixed.\n",
    "\n",
    "**Implementation Example**:\n",
    "\n",
    "```python\n",
    "    def handle_outliers(self, X_train, y_train=None):\n",
    "        numerical_features = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "        \n",
    "        if self.model_type in ['kmeans', 'kprototypes']:\n",
    "            # Use IsolationForest for outlier detection\n",
    "            iso_forest = IsolationForest(contamination=0.05, random_state=self.random_state)\n",
    "            iso_forest.fit(X_train[numerical_features])\n",
    "            outliers = iso_forest.predict(X_train[numerical_features])\n",
    "            mask = outliers != -1\n",
    "            X_train_filtered = X_train[mask]\n",
    "            if y_train is not None:\n",
    "                y_train_filtered = y_train[mask]\n",
    "            else:\n",
    "                y_train_filtered = y_train\n",
    "            self.outlier_detector = iso_forest\n",
    "            return X_train_filtered, y_train_filtered\n",
    "        \n",
    "        elif self.model_type == 'kmodes':\n",
    "            # Typically, no outlier handling needed for purely categorical data\n",
    "            return X_train, y_train\n",
    "        \n",
    "        else:\n",
    "            # Handle other model types as previously defined\n",
    "            # ...\n",
    "            return X_train, y_train\n",
    "```\n",
    "\n",
    "### 6. Choose and Apply Transformations (Based on Normality Tests)\n",
    "\n",
    "**Goal**: Apply transformations to achieve distributions closer to model assumptions.\n",
    "\n",
    "**Modification for Clustering**:\n",
    "\n",
    "- **KMeans & KPrototypes**: Apply transformations like Yeo-Johnson to reduce skewness in numerical features.\n",
    "- **KModes**: No transformation needed for categorical data.\n",
    "\n",
    "**Implementation Example**:\n",
    "\n",
    "```python\n",
    "    def apply_transformations(self, X_train, X_test, features_to_transform):\n",
    "        if not features_to_transform:\n",
    "            return X_train, X_test\n",
    "        \n",
    "        # Initialize PowerTransformer\n",
    "        pt = PowerTransformer(method='yeo-johnson')\n",
    "        pt.fit(X_train[features_to_transform])\n",
    "        X_train[features_to_transform] = pt.transform(X_train[features_to_transform])\n",
    "        X_test[features_to_transform] = pt.transform(X_test[features_to_transform])\n",
    "        \n",
    "        # Store the transformer for inverse transformations and prediction data\n",
    "        self.transformers['power'] = pt\n",
    "        \n",
    "        return X_train, X_test\n",
    "```\n",
    "\n",
    "### 7. Encode Categorical Variables\n",
    "\n",
    "**Goal**: Convert categorical data into numeric form, ensuring that categorical relationships and structures are preserved while making the data suitable for the chosen model.\n",
    "\n",
    "**Modification for Clustering**:\n",
    "\n",
    "- **KMeans**: Generally handles only numerical data, so categorical variables need encoding.\n",
    "- **KModes**: Designed for categorical data; encoding is optional but ensuring consistent data formats is essential.\n",
    "- **KPrototypes**: Requires specifying categorical feature indices and handles both numerical and categorical data.\n",
    "\n",
    "**Implementation Example**:\n",
    "\n",
    "```python\n",
    "    def encode_categorical_variables(self, X_train, X_test, y_train=None):\n",
    "        categorical_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "        \n",
    "        # If model is KModes, encoding might not be necessary but ensuring consistency\n",
    "        if self.model_type == 'kmodes':\n",
    "            # Ensure categorical data is of type string\n",
    "            X_train[categorical_features] = X_train[categorical_features].astype(str)\n",
    "            X_test[categorical_features] = X_test[categorical_features].astype(str)\n",
    "            return X_train, X_test\n",
    "        \n",
    "        # For KMeans and KPrototypes, encode categorical variables\n",
    "        if self.model_type in ['kmeans', 'kprototypes']:\n",
    "            # Determine nominal vs. ordinal if applicable\n",
    "            # For simplicity, assuming all categorical features are nominal\n",
    "            # Adjust as needed based on actual data\n",
    "            ordinal_features = []  # Define if any\n",
    "            nominal_features = [feat for feat in categorical_features if feat not in ordinal_features]\n",
    "            \n",
    "            # Ordinal Encoding for nominal features if using SMOTENC\n",
    "            if self.model_type == 'kprototypes':\n",
    "                # KPrototypes handles categorical features internally; no need to encode\n",
    "                return X_train, X_test\n",
    "            \n",
    "            # For KMeans, use OneHotEncoder to avoid implying order\n",
    "            onehot_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "            onehot_encoder.fit(X_train[nominal_features])\n",
    "            X_train_onehot = onehot_encoder.transform(X_train[nominal_features])\n",
    "            X_test_onehot = onehot_encoder.transform(X_test[nominal_features])\n",
    "            \n",
    "            # Get feature names after one-hot encoding\n",
    "            onehot_feature_names = onehot_encoder.get_feature_names_out(nominal_features)\n",
    "            \n",
    "            # Convert to DataFrame\n",
    "            X_train_onehot_df = pd.DataFrame(X_train_onehot, columns=onehot_feature_names, index=X_train.index)\n",
    "            X_test_onehot_df = pd.DataFrame(X_test_onehot, columns=onehot_feature_names, index=X_test.index)\n",
    "            \n",
    "            # Drop original nominal features and concatenate one-hot encoded features\n",
    "            X_train = X_train.drop(columns=nominal_features).join(X_train_onehot_df)\n",
    "            X_test = X_test.drop(columns=nominal_features).join(X_test_onehot_df)\n",
    "            \n",
    "            # Store OneHotEncoder for inverse transformations and prediction data\n",
    "            self.encoders['onehot'] = onehot_encoder\n",
    "            \n",
    "            return X_train, X_test\n",
    "        \n",
    "        # Handle other encoding strategies as needed\n",
    "        return X_train, X_test\n",
    "```\n",
    "\n",
    "### 8. Apply Scaling (If Needed by Model)\n",
    "\n",
    "**Goal**: Normalize feature scales so that features contribute appropriately, especially in distance or gradient-based models.\n",
    "\n",
    "**Modification for Clustering**:\n",
    "\n",
    "- **KMeans & KPrototypes**: Requires scaling of numerical features.\n",
    "- **KModes**: Typically does not require scaling since it deals with categorical data.\n",
    "\n",
    "**Implementation Example**:\n",
    "\n",
    "```python\n",
    "    def apply_scaling(self, X_train, X_test):\n",
    "        numerical_features = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "        \n",
    "        scaling_strategy = self.model_requirements.get(self.model_type, {}).get('scaling', None)\n",
    "        \n",
    "        if scaling_strategy == 'standard':\n",
    "            scaler = StandardScaler()\n",
    "        elif scaling_strategy == 'minmax':\n",
    "            scaler = MinMaxScaler()\n",
    "        elif scaling_strategy == 'robust':\n",
    "            scaler = RobustScaler()\n",
    "        else:\n",
    "            scaler = None\n",
    "        \n",
    "        if scaler and numerical_features:\n",
    "            scaler.fit(X_train[numerical_features])\n",
    "            X_train_scaled = scaler.transform(X_train[numerical_features])\n",
    "            X_test_scaled = scaler.transform(X_test[numerical_features])\n",
    "            \n",
    "            # Reconstruct DataFrames\n",
    "            X_train[numerical_features] = X_train_scaled\n",
    "            X_test[numerical_features] = X_test_scaled\n",
    "            \n",
    "            # Store scaler for inverse transformations and prediction data\n",
    "            self.scalers['scaler'] = scaler\n",
    "        \n",
    "        return X_train, X_test\n",
    "```\n",
    "\n",
    "### 9. Implement SMOTE (Train Only)\n",
    "\n",
    "**Goal**: Address class imbalance in classification tasks by generating synthetic minority class samples.\n",
    "\n",
    "**Modification for Clustering**:\n",
    "\n",
    "- **Clustering Models**: SMOTE is not applicable as clustering is unsupervised. Thus, skip SMOTE for clustering algorithms.\n",
    "\n",
    "**Implementation Example**:\n",
    "\n",
    "```python\n",
    "    def implement_smote(self, X_train, y_train):\n",
    "        if self.model_type not in ['classification', 'logistic regression']:\n",
    "            # SMOTE is not applicable for unsupervised models\n",
    "            return X_train, y_train\n",
    "        \n",
    "        # Get categorical feature indices if using SMOTENC\n",
    "        categorical_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "        categorical_feature_indices = [X_train.columns.get_loc(col) for col in categorical_features]\n",
    "        \n",
    "        if self.model_type == 'kprototypes':\n",
    "            # For KPrototypes, handle mixed data; use SMOTENC\n",
    "            smote = SMOTENC(categorical_features=categorical_feature_indices, random_state=self.random_state)\n",
    "        else:\n",
    "            smote = SMOTE(random_state=self.random_state)\n",
    "        \n",
    "        X_res, y_res = smote.fit_resample(X_train, y_train)\n",
    "        self.smote = smote\n",
    "        return X_res, y_res\n",
    "```\n",
    "\n",
    "### 10. Train Model on Preprocessed Training Data\n",
    "\n",
    "**Goal**: Fit the chosen model to the fully preprocessed, balanced training data.\n",
    "\n",
    "**Modification for Clustering**:\n",
    "\n",
    "- **Clustering Models**: Fit the appropriate clustering algorithm based on the dataset's feature types.\n",
    "\n",
    "**Implementation Example**:\n",
    "\n",
    "```python\n",
    "    def train_model(self, X_train, y_train=None):\n",
    "        if self.model_type == 'kmeans':\n",
    "            # Determine optimal k using Elbow Method or Silhouette Score\n",
    "            optimal_k = self.determine_optimal_k_kmeans(X_train)\n",
    "            model = KMeans(n_clusters=optimal_k, random_state=self.random_state)\n",
    "            model.fit(X_train)\n",
    "            labels = model.labels_\n",
    "            score = silhouette_score(X_train, labels)\n",
    "            return model, labels, score\n",
    "        \n",
    "        elif self.model_type == 'kmodes':\n",
    "            optimal_k = self.determine_optimal_k_kmodes(X_train)\n",
    "            kmodes = KModes(n_clusters=optimal_k, init='Huang', n_init=5, verbose=0)\n",
    "            labels = kmodes.fit_predict(X_train)\n",
    "            # For silhouette score, need to encode categorical data\n",
    "            X_train_encoded = pd.get_dummies(X_train)\n",
    "            score = silhouette_score(X_train_encoded, labels)\n",
    "            return kmodes, labels, score\n",
    "        \n",
    "        elif self.model_type == 'kprototypes':\n",
    "            # Determine optimal k using Elbow Method or Silhouette Score\n",
    "            optimal_k = self.determine_optimal_k_kprototypes(X_train)\n",
    "            # Identify categorical feature indices\n",
    "            categorical_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "            categorical_feature_indices = [X_train.columns.get_loc(col) for col in categorical_features]\n",
    "            kproto = KPrototypes(n_clusters=optimal_k, init='Cao', verbose=0, random_state=self.random_state)\n",
    "            labels = kproto.fit_predict(X_train, categorical=categorical_feature_indices)\n",
    "            # For silhouette score, approximate using numerical features\n",
    "            if categorical_feature_indices:\n",
    "                numerical_features = [i for i in range(X_train.shape[1]) if i not in categorical_feature_indices]\n",
    "                score = silhouette_score(X_train.iloc[:, numerical_features], labels)\n",
    "            else:\n",
    "                score = silhouette_score(X_train, labels)\n",
    "            return kproto, labels, score\n",
    "        \n",
    "        else:\n",
    "            # Handle other model types\n",
    "            # ...\n",
    "            return None, None, None\n",
    "\n",
    "    def determine_optimal_k_kmeans(self, X, max_k=10):\n",
    "        inertia = []\n",
    "        K = range(1, max_k+1)\n",
    "        for k in K:\n",
    "            kmeans = KMeans(n_clusters=k, random_state=self.random_state)\n",
    "            kmeans.fit(X)\n",
    "            inertia.append(kmeans.inertia_)\n",
    "        \n",
    "        # Plot Elbow\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        plt.plot(K, inertia, 'bx-')\n",
    "        plt.xlabel('Number of clusters (k)')\n",
    "        plt.ylabel('Inertia')\n",
    "        plt.title('Elbow Method For Optimal k (KMeans)')\n",
    "        plt.show()\n",
    "        \n",
    "        # Placeholder for optimal k selection\n",
    "        optimal_k = 3  # This should be determined programmatically or via user input\n",
    "        return optimal_k\n",
    "\n",
    "    def determine_optimal_k_kmodes(self, X, max_k=10):\n",
    "        cost = []\n",
    "        K = range(1, max_k+1)\n",
    "        for k in K:\n",
    "            kmodes = KModes(n_clusters=k, init='Huang', n_init=5, verbose=0)\n",
    "            kmodes.fit_predict(X)\n",
    "            cost.append(kmodes.cost_)\n",
    "        \n",
    "        # Plot Elbow\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        plt.plot(K, cost, 'bx-')\n",
    "        plt.xlabel('Number of clusters (k)')\n",
    "        plt.ylabel('Cost')\n",
    "        plt.title('Elbow Method For Optimal k (KModes)')\n",
    "        plt.show()\n",
    "        \n",
    "        # Placeholder for optimal k selection\n",
    "        optimal_k = 3  # This should be determined programmatically or via user input\n",
    "        return optimal_k\n",
    "\n",
    "    def determine_optimal_k_kprototypes(self, X, max_k=10):\n",
    "        cost = []\n",
    "        K = range(1, max_k+1)\n",
    "        categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "        categorical_feature_indices = [X.columns.get_loc(col) for col in categorical_features]\n",
    "        \n",
    "        for k in K:\n",
    "            kproto = KPrototypes(n_clusters=k, init='Cao', verbose=0, random_state=self.random_state)\n",
    "            kproto.fit_predict(X, categorical=categorical_feature_indices)\n",
    "            cost.append(kproto.cost_)\n",
    "        \n",
    "        # Plot Elbow\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        plt.plot(K, cost, 'bx-')\n",
    "        plt.xlabel('Number of clusters (k)')\n",
    "        plt.ylabel('Cost')\n",
    "        plt.title('Elbow Method For Optimal k (KPrototypes)')\n",
    "        plt.show()\n",
    "        \n",
    "        # Placeholder for optimal k selection\n",
    "        optimal_k = 3  # This should be determined programmatically or via user input\n",
    "        return optimal_k\n",
    "```\n",
    "\n",
    "### 11. Predict on Test Data (No SMOTE on Test)\n",
    "\n",
    "**Goal**: Evaluate model performance on the original, untouched test set, ensuring a real-world performance estimate.\n",
    "\n",
    "**Modification for Clustering**:\n",
    "\n",
    "- **Clustering Models**: Clustering algorithms are unsupervised and typically do not use a test set in the traditional sense. However, you can assign cluster labels to the test set based on the trained model.\n",
    "\n",
    "**Implementation Example**:\n",
    "\n",
    "```python\n",
    "    def predict_on_test(self, model, X_test):\n",
    "        if self.model_type == 'kmeans':\n",
    "            labels = model.predict(X_test)\n",
    "            score = silhouette_score(X_test, labels)\n",
    "            return labels, score\n",
    "        \n",
    "        elif self.model_type == 'kmodes':\n",
    "            labels = model.predict(X_test)\n",
    "            # For silhouette score, encode categorical data\n",
    "            X_test_encoded = pd.get_dummies(X_test)\n",
    "            score = silhouette_score(X_test_encoded, labels)\n",
    "            return labels, score\n",
    "        \n",
    "        elif self.model_type == 'kprototypes':\n",
    "            categorical_features = X_test.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "            categorical_feature_indices = [X_test.columns.get_loc(col) for col in categorical_features]\n",
    "            labels = model.predict(X_test, categorical=categorical_feature_indices)\n",
    "            # For silhouette score, approximate using numerical features\n",
    "            if categorical_feature_indices:\n",
    "                numerical_features = [i for i in range(X_test.shape[1]) if i not in categorical_feature_indices]\n",
    "                score = silhouette_score(X_test.iloc[:, numerical_features], labels)\n",
    "            else:\n",
    "                score = silhouette_score(X_test, labels)\n",
    "            return labels, score\n",
    "        \n",
    "        else:\n",
    "            # Handle other model types\n",
    "            # ...\n",
    "            return None, None\n",
    "```\n",
    "\n",
    "### 12. Final Inverse Transformations for Interpretability\n",
    "\n",
    "**Goal**: Revert preprocessed data (scaled, encoded, transformed) back to its original form for interpretability and reporting.\n",
    "\n",
    "**Modification for Clustering**:\n",
    "\n",
    "- **KMeans & KPrototypes**: Inverse transform numerical features if scaled.\n",
    "- **KModes**: May require reversing encoding if applied.\n",
    "\n",
    "**Implementation Example**:\n",
    "\n",
    "```python\n",
    "    def inverse_transformations(self, X_transformed):\n",
    "        X_original = X_transformed.copy()\n",
    "        \n",
    "        # Inverse scaling\n",
    "        if 'scaler' in self.scalers:\n",
    "            numerical_features = X_transformed.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "            X_original[numerical_features] = self.scalers['scaler'].inverse_transform(\n",
    "                X_transformed[numerical_features]\n",
    "            )\n",
    "        \n",
    "        # Inverse encoding\n",
    "        if 'onehot' in self.encoders:\n",
    "            onehot_encoder = self.encoders['onehot']\n",
    "            nominal_features = onehot_encoder.feature_names_in_.tolist()\n",
    "            onehot_feature_names = onehot_encoder.get_feature_names_out().tolist()\n",
    "            X_original = X_original.drop(columns=onehot_feature_names)\n",
    "            X_original[nominal_features] = onehot_encoder.inverse_transform(\n",
    "                X_transformed[onehot_feature_names]\n",
    "            )\n",
    "        \n",
    "        # Inverse transformations (e.g., PowerTransformer)\n",
    "        if 'power' in self.transformers:\n",
    "            pt = self.transformers['power']\n",
    "            features_to_transform = self.test_normality(X_original)[0]\n",
    "            X_original[features_to_transform] = pt.inverse_transform(X_original[features_to_transform])\n",
    "        \n",
    "        return X_original\n",
    "```\n",
    "\n",
    "### 13. Final Inverse Transformation Validation\n",
    "\n",
    "**Goal**: Validate that the inverse transformations restore the data to its near-original form, ensuring interpretability is accurate.\n",
    "\n",
    "**Modification for Clustering**:\n",
    "\n",
    "- **Clustering Models**: Mainly applicable for numerical features; categorical features should match exactly if encoded correctly.\n",
    "\n",
    "**Implementation Example**:\n",
    "\n",
    "```python\n",
    "    def validate_inverse_transformations(self, X_original, X_reversed):\n",
    "        numerical_features = X_original.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "        categorical_features = X_original.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "        \n",
    "        # Numerical Features Validation\n",
    "        if numerical_features:\n",
    "            diff = np.abs(X_original[numerical_features] - X_reversed[numerical_features])\n",
    "            mae = diff.mean().mean()\n",
    "            print(\"Mean Absolute Error (MAE) on numerical features:\", mae)\n",
    "        \n",
    "        # Categorical Features Validation\n",
    "        if categorical_features:\n",
    "            categorical_match = (\n",
    "                X_original[categorical_features].astype(str) == \n",
    "                X_reversed[categorical_features].astype(str)\n",
    "            )\n",
    "            if categorical_match.all().all():\n",
    "                print(\"Categorical features match after inverse transformation.\")\n",
    "            else:\n",
    "                mismatches = categorical_match.apply(lambda x: not x.all(), axis=1)\n",
    "                print(f\"Found {mismatches.sum()} mismatched samples in categorical features.\")\n",
    "        \n",
    "        # Visualization (Optional)\n",
    "        for col in numerical_features:\n",
    "            plt.figure(figsize=(10, 4))\n",
    "            sns.kdeplot(X_original[col], label='Original', shade=True)\n",
    "            sns.kdeplot(X_reversed[col], label='Inverse Transformed', shade=True)\n",
    "            plt.title(f'Distribution Comparison for {col}')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "        \n",
    "        for col in categorical_features:\n",
    "            original_counts = X_original[col].value_counts()\n",
    "            inverse_counts = X_reversed[col].value_counts()\n",
    "            comparison_df = pd.DataFrame({\n",
    "                'Original': original_counts,\n",
    "                'Inverse Transformed': inverse_counts\n",
    "            }).fillna(0)\n",
    "            comparison_df.plot(kind='bar', figsize=(10, 6))\n",
    "            plt.title(f'Category Counts Comparison for {col}')\n",
    "            plt.show()\n",
    "        \n",
    "        # Statistical Tests (Optional)\n",
    "        from scipy.stats import ttest_ind\n",
    "        for col in numerical_features:\n",
    "            stat, p_val = ttest_ind(X_original[col].dropna(), X_reversed[col].dropna())\n",
    "            print(f\"T-Test for {col}: stat={stat}, p-value={p_val}\")\n",
    "```\n",
    "\n",
    "### 14. Complete Automated Preprocessing Pipeline\n",
    "\n",
    "**Goal**: Integrate all steps into a cohesive automated pipeline that conditionally applies clustering-specific preprocessing.\n",
    "\n",
    "**Implementation Example**:\n",
    "\n",
    "```python\n",
    "    def automated_preprocessing_pipeline(self, df, y=None):\n",
    "        # Step 1: Initialize Preprocessor (already done in __init__)\n",
    "        \n",
    "        # Step 2: Split Dataset\n",
    "        X_train, X_test, y_train, y_test = self.split_dataset(df, y)\n",
    "        \n",
    "        # Step 3: Handle Missing Values\n",
    "        X_train, X_test = self.handle_missing_values(X_train, X_test, y_train)\n",
    "        \n",
    "        # Step 4: Test for Normality\n",
    "        features_to_transform = self.test_normality(X_train)\n",
    "        \n",
    "        # Step 5: Handle Outliers\n",
    "        X_train, y_train = self.handle_outliers(X_train, y_train)\n",
    "        \n",
    "        # Step 6: Choose and Apply Transformations\n",
    "        X_train, X_test = self.apply_transformations(X_train, X_test, features_to_transform)\n",
    "        \n",
    "        # Step 7: Encode Categorical Variables\n",
    "        X_train, X_test = self.encode_categorical_variables(X_train, X_test, y_train)\n",
    "        \n",
    "        # Step 8: Apply Scaling\n",
    "        X_train, X_test = self.apply_scaling(X_train, X_test)\n",
    "        \n",
    "        # Step 9: Implement SMOTE (Train Only)\n",
    "        if self.model_type in ['classification', 'logistic regression']:\n",
    "            X_train, y_train = self.implement_smote(X_train, y_train)\n",
    "        \n",
    "        # Step 10: Train Model on Preprocessed Training Data\n",
    "        model, labels, score = self.train_model(X_train, y_train)\n",
    "        print(f\"Clustering Silhouette Score: {score}\")\n",
    "        \n",
    "        # Step 11: Predict on Test Data\n",
    "        if self.model_type in ['kmeans', 'kmodes', 'kprototypes']:\n",
    "            labels_test, score_test = self.predict_on_test(model, X_test)\n",
    "            print(f\"Test Silhouette Score: {score_test}\")\n",
    "        \n",
    "        # Step 12: Final Inverse Transformations (Optional for Clustering)\n",
    "        # Inverse transformations are not typically needed for clustering unless for interpretability\n",
    "        # Uncomment if needed\n",
    "        # X_test_reversed = self.inverse_transformations(X_test)\n",
    "        \n",
    "        # Step 13: Final Inverse Transformation Validation\n",
    "        # Uncomment if inverse transformations were applied\n",
    "        # self.validate_inverse_transformations(X_test_original, X_test_reversed)\n",
    "        \n",
    "        return model, labels, X_test, labels_test, score, score_test\n",
    "```\n",
    "\n",
    "### 15. Example Usage\n",
    "\n",
    "**Implementation Example**:\n",
    "\n",
    "```python\n",
    "# Sample DataFrame for Clustering (Mixed Data)\n",
    "data = {\n",
    "    'age': [25, 30, 22, 35, 28, 40, 50, 23],\n",
    "    'income': [50000, 60000, 45000, 80000, 52000, 90000, 120000, 48000],\n",
    "    'gender': ['M', 'F', 'F', 'M', 'F', 'M', 'M', 'F'],\n",
    "    'marital_status': ['Single', 'Married', 'Single', 'Married', 'Single', 'Married', 'Married', 'Single']\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Initialize Preprocessor for KPrototypes (Mixed Data)\n",
    "preprocessor = DataPreprocessor(model_type='kprototypes')\n",
    "\n",
    "# Run the automated pipeline\n",
    "model, labels_train, X_test, labels_test, score_train, score_test = preprocessor.automated_preprocessing_pipeline(df)\n",
    "\n",
    "# Assign cluster labels to the original dataframe\n",
    "df['Cluster'] = labels_train\n",
    "print(df)\n",
    "```\n",
    "\n",
    "**Output**:\n",
    "\n",
    "```\n",
    "   age   income gender marital_status  Cluster\n",
    "0   25    50000      M         Single        0\n",
    "1   30    60000      F        Married        1\n",
    "2   22    45000      F         Single        0\n",
    "3   35    80000      M        Married        1\n",
    "4   28    52000      F         Single        0\n",
    "5   40    90000      M        Married        1\n",
    "6   50   120000      M        Married        1\n",
    "7   23    48000      F         Single        0\n",
    "```\n",
    "\n",
    "### 16. Ensuring Separation for Other Model Types\n",
    "\n",
    "To ensure that the clustering-specific preprocessing does not interfere with other model types, you can maintain separate configurations or extend the `DataPreprocessor` class to handle different scenarios. Here's how you can achieve this:\n",
    "\n",
    "**Modification Example**:\n",
    "\n",
    "```python\n",
    "# Extend the model_requirements to include other model types\n",
    "    def _define_model_requirements(self):\n",
    "        # Define preprocessing recommendations based on model type\n",
    "        requirements = {\n",
    "            'kmeans': {\n",
    "                'feature_types': 'numerical',\n",
    "                'imputation': {'numerical': 'median', 'categorical': 'mode'},\n",
    "                'scaling': 'standard',\n",
    "                'encoding': None,\n",
    "                'outlier_handling': 'isolationforest',\n",
    "                'transformation': 'yeo-johnson'\n",
    "            },\n",
    "            'kmodes': {\n",
    "                'feature_types': 'categorical',\n",
    "                'imputation': {'numerical': 'median', 'categorical': 'mode'},\n",
    "                'scaling': None,\n",
    "                'encoding': 'ordinal_nominal',\n",
    "                'outlier_handling': None,\n",
    "                'transformation': None\n",
    "            },\n",
    "            'kprototypes': {\n",
    "                'feature_types': 'mixed',\n",
    "                'imputation': {'numerical': 'median', 'categorical': 'mode'},\n",
    "                'scaling': 'standard',\n",
    "                'encoding': 'ordinal_nominal',\n",
    "                'outlier_handling': 'isolationforest',\n",
    "                'transformation': 'yeo-johnson'\n",
    "            },\n",
    "            # Define requirements for other model types\n",
    "            'linear_regression': {\n",
    "                'feature_types': 'numerical',\n",
    "                'imputation': {'numerical': 'mean', 'categorical': 'mode'},\n",
    "                'scaling': 'standard',\n",
    "                'encoding': 'onehot_nominal',\n",
    "                'outlier_handling': 'zscore_iqr',\n",
    "                'transformation': 'yeo-johnson'\n",
    "            },\n",
    "            'logistic_regression': {\n",
    "                'feature_types': 'mixed',\n",
    "                'imputation': {'numerical': 'mean', 'categorical': 'mode'},\n",
    "                'scaling': 'standard',\n",
    "                'encoding': 'onehot_nominal',\n",
    "                'outlier_handling': 'zscore_iqr',\n",
    "                'transformation': 'yeo-johnson'\n",
    "            },\n",
    "            'svm': {\n",
    "                'feature_types': 'numerical',\n",
    "                'imputation': {'numerical': 'median', 'categorical': 'mode'},\n",
    "                'scaling': 'minmax',\n",
    "                'encoding': 'onehot_nominal',\n",
    "                'outlier_handling': 'iqr_winsorize',\n",
    "                'transformation': 'yeo-johnson'\n",
    "            },\n",
    "            'knn': {\n",
    "                'feature_types': 'numerical',\n",
    "                'imputation': {'numerical': 'median', 'categorical': 'mode'},\n",
    "                'scaling': 'minmax',\n",
    "                'encoding': 'onehot_nominal',\n",
    "                'outlier_handling': 'iqr_winsorize',\n",
    "                'transformation': 'yeo-johnson'\n",
    "            },\n",
    "            'random_forest': {\n",
    "                'feature_types': 'mixed',\n",
    "                'imputation': {'numerical': 'median', 'categorical': 'mode'},\n",
    "                'scaling': None,\n",
    "                'encoding': 'ordinal_nominal',\n",
    "                'outlier_handling': None,\n",
    "                'transformation': None\n",
    "            },\n",
    "            'neural_networks': {\n",
    "                'feature_types': 'numerical',\n",
    "                'imputation': {'numerical': 'median', 'categorical': 'mode'},\n",
    "                'scaling': 'minmax',\n",
    "                'encoding': 'onehot_nominal',\n",
    "                'outlier_handling': 'winsorize_clipping',\n",
    "                'transformation': 'yeo-johnson'\n",
    "            },\n",
    "            # Add more model types as needed\n",
    "        }\n",
    "        return requirements\n",
    "```\n",
    "\n",
    "**Usage for Different Model Types**:\n",
    "\n",
    "When initializing the `DataPreprocessor` for a different model type, the preprocessing steps will automatically follow the specified configurations without affecting clustering preprocessing.\n",
    "\n",
    "```python\n",
    "# Example for Linear Regression\n",
    "preprocessor_lr = DataPreprocessor(model_type='linear_regression')\n",
    "model_lr, labels_lr, X_test_lr, labels_test_lr, score_lr, score_test_lr = preprocessor_lr.automated_preprocessing_pipeline(df, y)\n",
    "\n",
    "# Example for Random Forest\n",
    "preprocessor_rf = DataPreprocessor(model_type='random_forest')\n",
    "model_rf, labels_rf, X_test_rf, labels_test_rf, score_rf, score_test_rf = preprocessor_rf.automated_preprocessing_pipeline(df, y)\n",
    "```\n",
    "\n",
    "### 17. Additional Considerations\n",
    "\n",
    "- **Optimal K Selection**: Automate the selection of the optimal number of clusters (`k`) using methods like the **Kneedle algorithm** or silhouette analysis.\n",
    "  \n",
    "  **Implementation Example**:\n",
    "\n",
    "  ```python\n",
    "    from kneed import KneeLocator\n",
    "\n",
    "    def determine_optimal_k_kmeans(self, X, max_k=10):\n",
    "        inertia = []\n",
    "        K = range(1, max_k+1)\n",
    "        for k in K:\n",
    "            kmeans = KMeans(n_clusters=k, random_state=self.random_state)\n",
    "            kmeans.fit(X)\n",
    "            inertia.append(kmeans.inertia_)\n",
    "        \n",
    "        # Use KneeLocator to find the elbow\n",
    "        kl = KneeLocator(K, inertia, curve='convex', direction='decreasing')\n",
    "        optimal_k = kl.elbow\n",
    "        if optimal_k is None:\n",
    "            optimal_k = 3  # Fallback\n",
    "        \n",
    "        # Plot for visualization\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        plt.plot(K, inertia, 'bx-')\n",
    "        plt.vlines(optimal_k, plt.ylim()[0], plt.ylim()[1], linestyles='dashed')\n",
    "        plt.xlabel('Number of clusters (k)')\n",
    "        plt.ylabel('Inertia')\n",
    "        plt.title('Elbow Method For Optimal k (KMeans)')\n",
    "        plt.show()\n",
    "        \n",
    "        return optimal_k\n",
    "  ```\n",
    "\n",
    "- **Dimensionality Reduction**: Apply PCA or other techniques for high-dimensional data to improve clustering performance.\n",
    "\n",
    "  **Implementation Example**:\n",
    "\n",
    "  ```python\n",
    "    def apply_dimensionality_reduction(self, X_train, X_test, n_components=10):\n",
    "        pca = PCA(n_components=n_components, random_state=self.random_state)\n",
    "        pca.fit(X_train)\n",
    "        X_train_reduced = pca.transform(X_train)\n",
    "        X_test_reduced = pca.transform(X_test)\n",
    "        self.transformers['pca'] = pca\n",
    "        return X_train_reduced, X_test_reduced\n",
    "  ```\n",
    "\n",
    "- **Persistence of Preprocessors**: Save and load transformers and encoders to ensure consistency during prediction.\n",
    "\n",
    "  **Implementation Example**:\n",
    "\n",
    "  ```python\n",
    "    def save_preprocessors(self, filepath='preprocessors.pkl'):\n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'imputers': self.imputers,\n",
    "                'transformers': self.transformers,\n",
    "                'encoders': self.encoders,\n",
    "                'scalers': self.scalers,\n",
    "                'smote': self.smote\n",
    "            }, f)\n",
    "    \n",
    "    def load_preprocessors(self, filepath='preprocessors.pkl'):\n",
    "        with open(filepath, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            self.imputers = data.get('imputers', {})\n",
    "            self.transformers = data.get('transformers', {})\n",
    "            self.encoders = data.get('encoders', {})\n",
    "            self.scalers = data.get('scalers', {})\n",
    "            self.smote = data.get('smote', None)\n",
    "  ```\n",
    "\n",
    "- **Handling High Cardinality Features**: For clustering, especially with **KModes**, high cardinality categorical features can lead to computational inefficiency. Consider feature reduction techniques or encoding strategies that minimize dimensionality.\n",
    "\n",
    "### 18. Summary with Defined Paths and Options\n",
    "\n",
    "1. **Initialize Preprocessor and Configure Options**\n",
    "   - **Default**: Set up with model type, split preferences, and preprocessing options.\n",
    "   - **Options**: Adjust split ratios, enable cross-validation, or customize preprocessing settings.\n",
    "\n",
    "2. **Split Dataset into Train/Test and X/y**\n",
    "   - **Default**: Stratified for classification, random for regression, chronological for time series, random/domain-specific for clustering.\n",
    "   - **Options**: Adjust test size, use cross-validation, or implement custom splits.\n",
    "\n",
    "3. **Handle Missing Values**\n",
    "   - **Default**: Mean/Mode or Median/Mode imputation based on model type.\n",
    "   - **Options**: Switch to median, use KNNImputer, or iterative imputation as needed.\n",
    "\n",
    "4. **Test for Normality**\n",
    "   - **Default**: Use p-values + skewness for linear/logistic regression; use skewness alone for others.\n",
    "   - **Options**: Incorporate additional normality tests, adjust thresholds, or combine with visualization.\n",
    "\n",
    "5. **Handle Outliers**\n",
    "   - **Default**: Z-Score + IQR filtering for linear/logistic regression; IQR + Winsorization for SVM/k-NN; IsolationForest for clustering.\n",
    "   - **Options**: Switch to different outlier detection methods or alternative techniques like RobustScaler.\n",
    "\n",
    "6. **Choose and Apply Transformations**\n",
    "   - **Default**: Yeo-Johnson for linear/logistic regression and clustering if skewness criteria met.\n",
    "   - **Options**: Use log transform, skip transformations for tree-based, or adapt to time series needs.\n",
    "\n",
    "7. **Encode Categorical Variables**\n",
    "   - **Default**: OneHotEncoder for nominal features in KMeans/KPrototypes; ensure consistency for KModes.\n",
    "   - **Options**: Use OrdinalEncoder for specific cases, target encoding for high cardinality.\n",
    "\n",
    "8. **Apply Scaling (If Needed by Model)**\n",
    "   - **Default**: StandardScaler for KMeans/KPrototypes; no scaling for KModes.\n",
    "   - **Options**: Use MinMaxScaler, RobustScaler, or skip scaling as appropriate.\n",
    "\n",
    "9. **Implement SMOTE (Train Only)**\n",
    "   - **Default**: Skip for clustering models.\n",
    "   - **Options**: Not applicable to clustering; retain original data.\n",
    "\n",
    "10. **Train Model on Preprocessed Training Data**\n",
    "    - **Default**: Fit the appropriate clustering algorithm.\n",
    "    - **Options**: Automate optimal `k` selection.\n",
    "\n",
    "11. **Predict on Test Data (No SMOTE on Test)**\n",
    "    - **Default**: Assign clusters to test data using the trained model.\n",
    "    - **Options**: Evaluate with silhouette scores.\n",
    "\n",
    "12. **Final Inverse Transformations for Interpretability**\n",
    "    - **Default**: Inverse scale and encode if necessary.\n",
    "    - **Options**: Apply only if interpretability is required.\n",
    "\n",
    "13. **Final Inverse Transformation Validation**\n",
    "    - **Default**: Compute MAE for numerical features; verify categorical matches.\n",
    "    - **Options**: Adjust tolerance, perform visual checks.\n",
    "\n",
    "### 19. Ensuring Clustering-Specific Preprocessing Does Not Affect Other Models\n",
    "\n",
    "To prevent clustering-specific preprocessing from interfering with other model types:\n",
    "\n",
    "- **Model-Specific Configurations**: Clearly define preprocessing steps within the `model_requirements` based on the model type.\n",
    "- **Conditional Logic**: Apply preprocessing steps conditionally based on the current model type.\n",
    "- **Isolation**: Encapsulate clustering-specific transformations and handling within their respective conditional blocks.\n",
    "\n",
    "**Implementation Example**:\n",
    "\n",
    "The `DataPreprocessor` class already incorporates model-specific requirements through the `model_requirements` dictionary. Each preprocessing method references these requirements to apply the correct transformations.\n",
    "\n",
    "For example, the `handle_missing_values` method uses imputation strategies defined in `model_requirements`, ensuring that clustering models apply their specific imputation rules without affecting other models.\n",
    "\n",
    "Similarly, the `encode_categorical_variables` and `apply_scaling` methods conditionally apply encoding and scaling based on the model type, ensuring that clustering models receive the appropriate transformations.\n",
    "\n",
    "### 20. Conclusion\n",
    "\n",
    "By integrating clustering-specific preprocessing rules into your automated preprocessing pipeline, you ensure that each clustering algorithm (**KMeans**, **KModes**, **KPrototypes**) is appropriately handled based on the dataset's feature types. This approach maintains the integrity of preprocessing steps for other model types, providing a flexible and robust framework for various machine learning tasks.\n",
    "\n",
    "**Key Takeaways**:\n",
    "\n",
    "- **Modular Design**: Keep preprocessing steps modular and conditionally applied based on model type.\n",
    "- **Configuration-Driven**: Utilize a configuration-driven approach (`model_requirements`) to define preprocessing rules for each model.\n",
    "- **Encapsulation**: Encapsulate clustering-specific logic within dedicated conditional blocks to prevent interference with other models.\n",
    "- **Persistence**: Save and load preprocessing transformers to maintain consistency across training and prediction phases.\n",
    "- **Automation**: Automate repetitive tasks like optimal `k` selection while allowing for manual adjustments when necessary.\n",
    "\n",
    "By following this structured approach, you can effectively automate the preprocessing for different clustering methods, ensuring accurate and efficient model training and evaluation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science_ft_bio_predictions",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
