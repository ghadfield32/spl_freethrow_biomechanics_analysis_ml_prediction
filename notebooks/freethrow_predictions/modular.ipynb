{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base repo of data: https://github.com/mlsedigital/SPL-open-data\n",
    "\n",
    "Great xyz data dict: https://www.inpredictable.com/2021/01/nba-player-shooting-motions-data-dump.html\n",
    "\n",
    "\n",
    "## Ultimate Goals:\n",
    "* Find optimal shooting form/release point\n",
    "  * Include the feedback system based on optimal shooting form and release point\n",
    "  * ### Use shooting meter from NBA 2k to show good vs bad releases and body movements\n",
    "  * ^use the motions that:\n",
    "    * produce good results (a make or enough kinetic energy for a near make)\n",
    "    * are within certain boundaries where the player is comfortable AND in right shooting form\n",
    "    * are within certain ranges (this inch to this inch is the best motion area for this player, etc.)\n",
    "    * Compared to Popular Shooters to make recommendations to shoot more like someone like Klay Thompson or Buddy Hield (see if the data from the xyz data dictionary has good shooters to make recommendations to shoot like them)\n",
    "* Exhaustion levels and optimal energy max and min\n",
    "* Shot outcome prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Finding Optimal Shooting Form/Release Point\n",
    "\n",
    "    Detailed Analysis of Body Movements: Include analysis of body kinematics (positions and velocities of limbs) at the time of release. Use joint coordinates to extract features like elbow angle, shoulder rotation, and wrist flexion at the release frame.\n",
    "    Machine Learning for Form Classification: Train a model to classify \"good\" vs. \"bad\" shooting forms using successful shot data (makes or near-makes) and compare them against unsuccessful shots.\n",
    "    Feedback Mechanism: Implement a feedback system that suggests adjustments based on the comparison of current shooting mechanics to historical optimal ones.\n",
    "\n",
    "2. Shooting Meter Simulation (NBA 2K-Style)\n",
    "\n",
    "    Visual Feedback System: Develop a visualization tool that overlays a “meter” on shot video frames, indicating the quality of the release in real-time. This can be based on a scoring function derived from the shooting form features and ball dynamics.\n",
    "    Comfort Zone Identification: Use clustering algorithms (e.g., K-means) to identify comfortable ranges of motion based on historical shooting data for individual players.\n",
    "\n",
    "3. Shot Outcome Prediction\n",
    "\n",
    "    Feature Engineering for Outcome Modeling: Include additional features such as:\n",
    "        Kinetic Energy Calculation: KE=12mv2KE=21​mv2 to see if the ball had sufficient energy for a make.\n",
    "        Entry Angle and Trajectory Analysis: Analyze whether the angle at which the ball approaches the hoop aligns with optimal scoring trajectories.\n",
    "    Model Development: Build a machine learning model (e.g., logistic regression, XGBoost) that predicts shot outcomes based on ball speed, entry angle, release point, and body dynamics.\n",
    "    Training with Data Augmentation: Use synthetic data generation to include a wide variety of shot scenarios.\n",
    "\n",
    "4. Exhaustion Levels and Energy Management\n",
    "\n",
    "    Tracking Player Movements: Use the coordinates of major joints (e.g., knees, hips) to estimate a player's exertion level using metrics like the average vertical displacement over time.\n",
    "    Velocity and Acceleration Patterns: Monitor changes in the velocity and acceleration of the player's body parts throughout a game to detect fatigue.\n",
    "    Feature Integration: Create features such as average speed and distance covered leading up to the shot to include in the predictive model.\n",
    "\n",
    "5. shot simulator: \n",
    "    if by these metrics we can simulate the shot to a nearby hoop. We can set the hoop in a set location (or when using spatial, we'd pick the location), and set it up to virtualize the experience with yolo/opencv where when you make those motions to show shooting motion, you can show where the ball might go (even without a ball in hand)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Dataset Pipeline:\n",
    "    - categorize data\n",
    "    - multicollinearity/feature importances = feature selection\n",
    "    - preprocessing suggestions, datasets, preprocessing with strict guidelines\n",
    "    - ml model selection\n",
    "    - ml model testing\n",
    "    - ml data inverse transform\n",
    "    - ml model prediction\n",
    "    - live prediction and re-calculate the dataset formulas for when new data attaches\n",
    "    - get optimal ranges for the angles (knee/wrist/elbow) and input them into the meters\n",
    "    - input the meters into a video for optimal angles\n",
    "    - add in ml classification with re-calculation\n",
    "    - live camera feed with this? \n",
    "      - streamlit example of how this works with li\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ml/feature_selection/multicollinearity_checker.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ml/feature_selection/multicollinearity_checker.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def check_multicollinearity(df, threshold=0.8, debug=False):\n",
    "    \"\"\"\n",
    "    Identifies pairs of features with correlation above the specified threshold.\n",
    "    Args:\n",
    "        df (DataFrame): DataFrame containing numerical features.\n",
    "        threshold (float): Correlation coefficient threshold.\n",
    "        debug (bool): If True, prints debugging information.\n",
    "    Returns:\n",
    "        DataFrame: Pairs of features with high correlation.\n",
    "    \"\"\"\n",
    "    # Select only numerical columns\n",
    "    numeric_df = df.select_dtypes(include=[np.number])\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"Computing correlation matrix for {len(numeric_df.columns)} numerical features...\")\n",
    "\n",
    "    corr_matrix = numeric_df.corr().abs()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "    # Identify highly correlated features\n",
    "    highly_correlated = [\n",
    "        (column, idx, upper.loc[column, idx])\n",
    "        for column in upper.columns\n",
    "        for idx in upper.index\n",
    "        if (upper.loc[column, idx] > threshold)\n",
    "    ]\n",
    "\n",
    "    multicollinearity_df = pd.DataFrame(highly_correlated, columns=['Feature1', 'Feature2', 'Correlation'])\n",
    "\n",
    "    if debug:\n",
    "        if not multicollinearity_df.empty:\n",
    "            print(f\"Found {len(multicollinearity_df)} pairs of highly correlated features:\")\n",
    "            print(multicollinearity_df)\n",
    "        else:\n",
    "            print(\"No highly correlated feature pairs found.\")\n",
    "\n",
    "    return multicollinearity_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import pickle\n",
    "    # Load the category bin configuration\n",
    "    with open('../../data/model/pipeline/category_bin_config.pkl', 'rb') as f:\n",
    "        loaded_category_bin_config = pickle.load(f)\n",
    "\n",
    "    file_path = \"../../data/processed/final_ml_dataset.csv\"\n",
    "    #import ml dataset from spl_dataset_prep\n",
    "    final_ml_df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Feature selection based on multi collinearity and random forest importance selection\n",
    "    target_variable = 'result'\n",
    "    correlation_threshold = 0.8\n",
    "    debug = True\n",
    "\n",
    "    # Remove columns to address collinearity\n",
    "    drop_features = [ 'L_KNEE_min_power', 'L_HIP_max_power']\n",
    "    \n",
    "    # Step 1: Check for multicollinearity\n",
    "    print(\"\\nChecking for Multicollinearity...\")\n",
    "    multicollinearity_df = check_multicollinearity(final_ml_df, threshold=correlation_threshold, debug=debug)\n",
    "\n",
    "    # Step 2: Handle multicollinearity\n",
    "    if not multicollinearity_df.empty:\n",
    "        for index, row in multicollinearity_df.iterrows():\n",
    "            feature1, feature2, correlation = row['Feature1'], row['Feature2'], row['Correlation']\n",
    "            print(f\"High correlation ({correlation}) between '{feature1}' and '{feature2}'.\")\n",
    "    else:\n",
    "        print(\"No multicollinearity issues detected.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ml/feature_selection/feature_importance_calculator.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ml/feature_selection/feature_importance_calculator.py\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "import logging\n",
    "import pickle\n",
    "from typing import Optional, List, Dict, Any, Tuple, Union\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "def calculate_feature_importance(\n",
    "    df: pd.DataFrame,\n",
    "    target_variable: Union[str, List[str]],\n",
    "    n_estimators: int = 100,\n",
    "    random_state: int = 42,\n",
    "    debug: bool = False\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculates feature importance using a Random Forest model.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): Input DataFrame.\n",
    "        target_variable (str or list of str): Target column name or a list with a single target column.\n",
    "        n_estimators (int): Number of trees in the forest.\n",
    "        random_state (int): Random seed.\n",
    "        debug (bool): If True, prints debugging information.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: Feature importances.\n",
    "    \n",
    "    Note:\n",
    "        If target_variable is passed as a list, it must contain only one element.\n",
    "    \"\"\"\n",
    "    # Normalize the target variable: if provided as a list, extract the single string.\n",
    "    if isinstance(target_variable, list):\n",
    "        if len(target_variable) == 1:\n",
    "            target = target_variable[0]\n",
    "        else:\n",
    "            raise ValueError(\"calculate_feature_importance supports only a single target variable.\")\n",
    "    else:\n",
    "        target = target_variable\n",
    "\n",
    "    # Drop the target column from predictors and extract the target variable column.\n",
    "    X = df.drop(columns=[target])\n",
    "    y = df[target]\n",
    "    \n",
    "    # If y is a DataFrame (e.g. if someone mistakenly passed a list with more than one column),\n",
    "    # you might need to extract a Series. Here, we assume y is a Series.\n",
    "    if isinstance(y, pd.DataFrame):\n",
    "        # If a single-column DataFrame, convert to Series.\n",
    "        if y.shape[1] == 1:\n",
    "            y = y.iloc[:, 0]\n",
    "        else:\n",
    "            raise ValueError(\"The target variable DataFrame should contain only one column.\")\n",
    "\n",
    "    # Encode target variable if necessary\n",
    "    if y.dtype == 'object' or str(y.dtype) == 'category':\n",
    "        if debug:\n",
    "            print(f\"Target variable '{target}' is categorical. Encoding labels.\")\n",
    "        le = LabelEncoder()\n",
    "        y = le.fit_transform(y)\n",
    "\n",
    "    # Separate categorical and numerical features\n",
    "    categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    numeric_cols = X.select_dtypes(exclude=['object', 'category']).columns.tolist()\n",
    "\n",
    "    if debug:\n",
    "        print(f\"Categorical columns: {categorical_cols}\")\n",
    "        print(f\"Numerical columns: {numeric_cols}\")\n",
    "\n",
    "    # Encode categorical features if present\n",
    "    if categorical_cols:\n",
    "        ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "        X_encoded = ohe.fit_transform(X[categorical_cols])\n",
    "        X_encoded_df = pd.DataFrame(\n",
    "            X_encoded,\n",
    "            columns=ohe.get_feature_names_out(categorical_cols),\n",
    "            index=X.index\n",
    "        )\n",
    "        X = pd.concat([X[numeric_cols], X_encoded_df], axis=1)\n",
    "\n",
    "    # Select the model based on the type of y\n",
    "    model = (\n",
    "        RandomForestRegressor(n_estimators=n_estimators, random_state=random_state)\n",
    "        if y.dtype in ['int64', 'float64'] else\n",
    "        RandomForestClassifier(n_estimators=n_estimators, random_state=random_state)\n",
    "    )\n",
    "\n",
    "    if debug:\n",
    "        print(f\"Training Random Forest model with {n_estimators} estimators...\")\n",
    "\n",
    "    model.fit(X, y)\n",
    "\n",
    "    # Calculate feature importances and return them as a DataFrame.\n",
    "    feature_importances = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Importance': model.feature_importances_\n",
    "    }).sort_values(by='Importance', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    if debug:\n",
    "        print(\"Feature Importances:\")\n",
    "        print(feature_importances)\n",
    "\n",
    "    return feature_importances\n",
    "\n",
    "\n",
    "def manage_features(\n",
    "    mode: str,\n",
    "    features_df: Optional[pd.DataFrame] = None,\n",
    "    ordinal_categoricals: Optional[List[str]] = None,\n",
    "    nominal_categoricals: Optional[List[str]] = None,\n",
    "    numericals: Optional[List[str]] = None,\n",
    "    y_variable: Optional[Union[str, List[str]]] = None,\n",
    "    paths: Optional[Dict[str, str]] = None,\n",
    "    base_dir: Optional[Union[str, Path]] = None\n",
    ") -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Save or load features and metadata.\n",
    "\n",
    "    Parameters:\n",
    "        mode (str): \"save\" or \"load\".\n",
    "        features_df (pd.DataFrame, optional): DataFrame containing features (required for \"save\").\n",
    "        ordinal_categoricals (list, optional): List of ordinal categorical features.\n",
    "        nominal_categoricals (list, optional): List of nominal categorical features.\n",
    "        numericals (list, optional): List of numerical features.\n",
    "        y_variable (str or list of str, optional): Target variable.\n",
    "        paths (dict, optional): Dictionary mapping item keys to file names.\n",
    "        base_dir (str or Path, optional): Base directory where files should be saved or loaded from.\n",
    "\n",
    "    Returns:\n",
    "        For \"load\" mode, returns a dictionary of loaded items; for \"save\" mode, returns None.\n",
    "    \"\"\"\n",
    "    # Ensure base_dir is a Path object and resolve it\n",
    "    if base_dir is None:\n",
    "        base_dir = Path.cwd()\n",
    "    else:\n",
    "        base_dir = Path(base_dir).resolve()\n",
    "\n",
    "    # Set default file names joined with the base directory.\n",
    "    default_paths = {\n",
    "        'features': str(base_dir / 'final_ml_df_selected_features_columns.pkl'),\n",
    "        'ordinal_categoricals': str(base_dir / 'ordinal_categoricals.pkl'),\n",
    "        'nominal_categoricals': str(base_dir / 'nominal_categoricals.pkl'),\n",
    "        'numericals': str(base_dir / 'numericals.pkl'),\n",
    "        'y_variable': str(base_dir / 'y_variable.pkl')\n",
    "    }\n",
    "\n",
    "    # If a paths dictionary is provided, update the defaults.\n",
    "    if paths:\n",
    "        default_paths.update(paths)\n",
    "\n",
    "    try:\n",
    "        if mode == 'save':\n",
    "            if features_df is None:\n",
    "                raise ValueError(\"features_df must be provided in 'save' mode.\")\n",
    "\n",
    "            data_to_save = {\n",
    "                'features': features_df.columns.tolist(),\n",
    "                'ordinal_categoricals': ordinal_categoricals,\n",
    "                'nominal_categoricals': nominal_categoricals,\n",
    "                'numericals': numericals,\n",
    "                'y_variable': y_variable\n",
    "            }\n",
    "\n",
    "            # Create necessary directories and save each item.\n",
    "            for key, file_path in default_paths.items():\n",
    "                file_path_obj = Path(file_path)\n",
    "                file_path_obj.parent.mkdir(parents=True, exist_ok=True)\n",
    "                with open(file_path_obj, 'wb') as f:\n",
    "                    pickle.dump(data_to_save[key], f)\n",
    "                print(f\"✅ {key.replace('_', ' ').capitalize()} saved to {file_path_obj}\")\n",
    "\n",
    "        elif mode == 'load':\n",
    "            loaded_data = {}\n",
    "            for key, file_path in default_paths.items():\n",
    "                file_path_obj = Path(file_path)\n",
    "                if not file_path_obj.exists():\n",
    "                    print(f\"❌ {key.replace('_', ' ').capitalize()} file not found at {file_path_obj}\")\n",
    "                    loaded_data[key] = None\n",
    "                else:\n",
    "                    with open(file_path_obj, 'rb') as f:\n",
    "                        loaded_data[key] = pickle.load(f)\n",
    "                    print(f\"✅ {key.replace('_', ' ').capitalize()} loaded from {file_path_obj}\")\n",
    "            return loaded_data\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Mode should be either 'save' or 'load'.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during '{mode}' operation: {e}\")\n",
    "        if mode == 'load':\n",
    "            return {key: None for key in default_paths.keys()}\n",
    "        \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # For testing purposes: load dataset and run feature importance.\n",
    "\n",
    "    final_ml_features_path = str(Path(\"../../data/preprocessor\") / \"features_info\" / \"final_ml_df_selected_features_columns.pkl\")\n",
    "    # (Other paths omitted for brevity.)\n",
    "\n",
    "    file_path = \"../../data/processed/final_ml_dataset.csv\"\n",
    "    final_ml_df = pd.read_csv(file_path)\n",
    "\n",
    "    # For demonstration, set the target variable as a list.\n",
    "    target_variable = ['result']\n",
    "    correlation_threshold = 0.8\n",
    "    debug = True\n",
    "\n",
    "    # (Assume check_multicollinearity is defined elsewhere.)\n",
    "    from ml.feature_selection.multicollinearity_checker import check_multicollinearity\n",
    "\n",
    "    print(\"\\nChecking for Multicollinearity...\")\n",
    "    multicollinearity_df = check_multicollinearity(final_ml_df, threshold=correlation_threshold, debug=debug)\n",
    "\n",
    "    if multicollinearity_df.empty:\n",
    "        print(\"No multicollinearity issues detected.\")\n",
    "    else:\n",
    "        for _, row in multicollinearity_df.iterrows():\n",
    "            print(f\"High correlation ({row['Correlation']}) between '{row['Feature1']}' and '{row['Feature2']}'.\")\n",
    "\n",
    "    # Calculate feature importance.\n",
    "    print(\"\\nCalculating Feature Importance...\")\n",
    "    feature_importances = calculate_feature_importance(\n",
    "        final_ml_df, target_variable=target_variable, n_estimators=100, random_state=42, debug=debug\n",
    "    )\n",
    "    print(\"\\nFinal Feature Importances:\")\n",
    "    print(feature_importances.to_string(index=False))\n",
    "\n",
    "    \n",
    "    #Final Decisions: \n",
    "    # Features recommended for dropping\n",
    "    features_to_drop = [\n",
    "        'peak_height_relative'\n",
    "    ]\n",
    "    print(f\"Dropped features (for redundancy or multicollinearity): {', '.join(features_to_drop)}\")\n",
    "    \n",
    "\n",
    "    # Define categories and column names\n",
    "    ordinal_categoricals = []\n",
    "    nominal_categoricals = [] #'player_estimated_hand_length_cm_category'\n",
    "    numericals = [        'release_ball_direction_x' ,'release_ball_direction_z', 'release_ball_direction_y',\n",
    "        'elbow_release_angle', 'elbow_max_angle',\n",
    "        'wrist_release_angle', 'wrist_max_angle',\n",
    "        'knee_release_angle', 'knee_max_angle',\n",
    "        'release_ball_speed', 'calculated_release_angle',\n",
    "        'release_ball_velocity_x', 'release_ball_velocity_y','release_ball_velocity_z']\n",
    "    y_variable = ['result']\n",
    "    final_keep_list = ordinal_categoricals + nominal_categoricals + numericals + y_variable\n",
    "    \n",
    "    # Apply the filter to keep only these columns\n",
    "    final_ml_df_selected_features = final_ml_df[final_keep_list]\n",
    "    print(f\"Retained {len(final_keep_list)} features: {', '.join(final_keep_list)}\")\n",
    "\n",
    "    # Save feature names to a file\n",
    "    with open(final_ml_features_path, 'wb') as f:\n",
    "        pickle.dump(final_ml_df_selected_features.columns.tolist(), f)\n",
    "\n",
    "    print(f\"Retained {len(final_keep_list)} features: {', '.join(final_keep_list)}\")\n",
    "\n",
    "    # You might also load your config and then get base_dir from config.paths.processed_data_dir joined with config.paths.data_dir\n",
    "    base_dir = Path(\"../../data\") / \"preprocessor\" / \"features_info\"\n",
    "    paths = {\n",
    "        \"features\": str(base_dir / \"final_ml_df_selected_features_columns.pkl\"),\n",
    "        \"ordinal_categoricals\": str(base_dir / \"ordinal_categoricals.pkl\"),\n",
    "        'nominal_categoricals': str(base_dir /'nominal_categoricals.pkl'),\n",
    "        'numericals': str(base_dir /'numericals.pkl'),\n",
    "        'y_variable': str(base_dir /'y_variable.pkl')\n",
    "    }\n",
    "\n",
    "    # Save features and metadata\n",
    "    manage_features(\n",
    "        mode='save',\n",
    "        features_df=final_ml_df_selected_features,\n",
    "        ordinal_categoricals=ordinal_categoricals,\n",
    "        nominal_categoricals=nominal_categoricals,\n",
    "        numericals=numericals,\n",
    "        y_variable=y_variable,\n",
    "        paths=paths,\n",
    "        base_dir=base_dir\n",
    "    )\n",
    "\n",
    "    # Later, load features and metadata\n",
    "    loaded = manage_features(\n",
    "        mode='load',\n",
    "        paths=paths,\n",
    "        base_dir=base_dir\n",
    "    )\n",
    "    print(\"\\n📥 Loaded Data from:\", base_dir)\n",
    "    print(\"\\n📥 Loaded Data:\")\n",
    "    print(\"Features:\", loaded.get('features'))\n",
    "    print(\"Ordinal Categoricals:\", loaded.get('ordinal_categoricals'))\n",
    "    print(\"Nominal Categoricals:\", loaded.get('nominal_categoricals'))\n",
    "    print(\"Numericals:\", loaded.get('numericals'))\n",
    "    print(\"Y Variable:\", loaded.get('y_variable'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://chatgpt.com/c/67583ce6-82ac-8012-b1d7-c415f658caa0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ml/config/config_models.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ml/config/config_models.py\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Dict, Optional\n",
    "from omegaconf import OmegaConf\n",
    "from pathlib import Path\n",
    "class FeaturesConfig(BaseModel):\n",
    "    ordinal_categoricals: List[str] = []\n",
    "    nominal_categoricals: List[str] = []\n",
    "    numericals: List[str]\n",
    "    y_variable: List[str]\n",
    "\n",
    "class PathsConfig(BaseModel):\n",
    "    data_dir: str = \"../../data/preprocessor\"  # Unified base for preprocessor outputs\n",
    "    raw_data: str = \"../processed/final_ml_dataset.csv\"\n",
    "    processed_data_dir: str = \"processed\"\n",
    "    features_metadata_file: str = \"features_info/final_ml_df_selected_features_columns.pkl\"\n",
    "    predictions_output_dir: str = \"../../data/predictions\"\n",
    "    config_file: str = \"../../data/model/preprocessor_config/preprocessor_config.yaml\"\n",
    "    log_dir: str = \"../../data/preprocessor/logs\"\n",
    "    model_save_base_dir: str = \"../../data/model\"\n",
    "    transformers_save_base_dir: str = \"../../data/preprocessor/transformers\"\n",
    "    plots_output_dir: str = \"../../data/preprocessor/plots\"\n",
    "    training_output_dir: str = \"../../data/preprocessor/training_output\"\n",
    "    log_file: Optional[str] = \"../../data/preprocessor/prediction.log\"\n",
    "\n",
    "\n",
    "class ModelsConfig(BaseModel):\n",
    "    selected_models: List[str] = Field(default_factory=lambda: [\"XGBoost\", \"Random Forest\", \"Decision Tree, CatBoost\"])\n",
    "    selection_metric: str = \"Log Loss\"\n",
    "    Tree_Based_Classifier: Optional[Dict] = {}\n",
    "\n",
    "class LoggingConfig(BaseModel):\n",
    "    level: str = \"INFO\"\n",
    "    debug: bool = False\n",
    "\n",
    "class AppConfig(BaseModel):\n",
    "    # Make sure this is now a plain list\n",
    "    model_types: List[str]\n",
    "    model_sub_types: Dict[str, List[str]]\n",
    "    features: FeaturesConfig\n",
    "    paths: PathsConfig\n",
    "    models: ModelsConfig\n",
    "    logging: LoggingConfig = LoggingConfig()   # default provided if missing\n",
    "    execution: Optional[Dict] = {}\n",
    "\n",
    "\n",
    "def load_config(config_path: Path) -> AppConfig:\n",
    "    cfg = OmegaConf.load(config_path)\n",
    "    cfg_dict = OmegaConf.to_container(cfg, resolve=True)\n",
    "    print(\"DEBUG: Loaded configuration dictionary:\", cfg_dict)\n",
    "    app_config = AppConfig(**cfg_dict)\n",
    "    print(f\"[Config Loader] ✅ Successfully loaded configuration from {config_path}\")\n",
    "    return app_config\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import json\n",
    "    config = load_config(Path('../../data/model/preprocessor_config/preprocessor_config.yaml'))\n",
    "    print(json.dumps(config.model_dump(), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ml/config/config_loader.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ml/config/config_loader.py\n",
    "from pathlib import Path\n",
    "from omegaconf import OmegaConf\n",
    "from ml.config.config_models import AppConfig\n",
    "\n",
    "def load_config(config_path: Path) -> AppConfig:\n",
    "    \"\"\"\n",
    "    Load the configuration from a YAML file using OmegaConf and convert it into a typed AppConfig object.\n",
    "    \n",
    "    Args:\n",
    "        config_path (Path): Path to the YAML configuration file.\n",
    "    \n",
    "    Returns:\n",
    "        AppConfig: A validated configuration instance.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the YAML configuration with OmegaConf\n",
    "        cfg = OmegaConf.load(config_path)\n",
    "        # Convert the DictConfig to a regular dict (resolving variables, if any)\n",
    "        cfg_dict = OmegaConf.to_container(cfg, resolve=True)\n",
    "        # Use the Pydantic model for validation and type safety.\n",
    "        app_config = AppConfig(**cfg_dict)\n",
    "        print(f\"[Config Loader] ✅ Successfully loaded configuration from {config_path}\")\n",
    "        return app_config\n",
    "    except Exception as e:\n",
    "        print(f\"[Config Loader] ❌ Failed to load configuration: {e}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ml/train_utils/train_utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ml/train_utils/train_utils.py\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA \n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    roc_auc_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    log_loss,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import logging\n",
    "import json\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Main function with MLflow integration\n",
    "\n",
    "import joblib\n",
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "# Local imports\n",
    "from ml.feature_selection.data_loader_post_select_features import load_selected_features_data\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, save_path=\"classification_report.txt\"):\n",
    "    \"\"\"\n",
    "    Evaluate the model and log performance metrics.\n",
    "\n",
    "    Parameters:\n",
    "    - model: Trained model to evaluate.\n",
    "    - X_test: Test features.\n",
    "    - y_test: True labels for the test data.\n",
    "    - save_path: Path to save the classification report.\n",
    "\n",
    "    Returns:\n",
    "    - metrics: Dictionary containing evaluation metrics.\n",
    "    \"\"\"\n",
    "    logger.info(\"Evaluating model...\")\n",
    "    y_pred = model.predict(X_test)\n",
    "    logger.info(f\"Predictions: {y_pred}\")\n",
    "    \n",
    "    # Check if the model supports probability predictions\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "        logger.info(f\"Predicted probabilities: {y_proba}\")\n",
    "    else:\n",
    "        y_proba = None\n",
    "        logger.info(\"Model does not support probability predictions.\")\n",
    "\n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"Precision\": precision_score(y_test, y_pred, average=\"weighted\", zero_division=0),\n",
    "        \"Recall\": recall_score(y_test, y_pred, average=\"weighted\", zero_division=0),\n",
    "        \"F1 Score\": f1_score(y_test, y_pred, average=\"weighted\", zero_division=0),\n",
    "        \"ROC AUC\": roc_auc_score(y_test, y_proba) if y_proba is not None else None,\n",
    "        \"Log Loss\": log_loss(y_test, y_proba) if y_proba is not None else None,\n",
    "    }\n",
    "\n",
    "    # Log metrics\n",
    "    logger.info(f\"Evaluation Metrics: {metrics}\")\n",
    "\n",
    "    # Generate and save classification report\n",
    "    report = classification_report(y_test, y_pred, output_dict=False)\n",
    "    logger.info(\"\\n\" + report)\n",
    "    with open(save_path, \"w\") as f:\n",
    "        f.write(report)\n",
    "    logger.info(f\"Classification report saved to {save_path}\")\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def save_model(model, model_name, save_dir=\"../../data/model\"):\n",
    "    \"\"\"\n",
    "    Save the trained model and preprocessing steps to disk.\n",
    "\n",
    "    Parameters:\n",
    "    - model: Trained model to save.\n",
    "    - model_name: Name of the model for saving.\n",
    "    - preprocessing_steps: Dictionary of preprocessing objects (e.g., encoders, scalers).\n",
    "    - save_dir: Directory to save the model and preprocessing steps.\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    model_path = os.path.join(save_dir, f\"{model_name}_model.pkl\")\n",
    "\n",
    "    # Save the model\n",
    "    joblib.dump(model, model_path)\n",
    "    logger.info(f\"Model saved to {model_path}\")\n",
    "\n",
    "\n",
    "def load_model(model_name, save_dir=\"../../data/model\"):\n",
    "    \"\"\"\n",
    "    Load the trained model from disk.\n",
    "\n",
    "    Parameters:\n",
    "    - model_name: Name of the model to load.\n",
    "    - save_dir: Directory where the model is saved.\n",
    "\n",
    "    Returns:\n",
    "    - model: Loaded trained model.\n",
    "    \"\"\"\n",
    "    model_path = os.path.join(save_dir, f\"{model_name}_model.pkl\")\n",
    "    model = joblib.load(model_path)\n",
    "    logger.info(f\"Model loaded from {model_path}\")\n",
    "    return model\n",
    "\n",
    "\n",
    "# Plot decision boundary\n",
    "def plot_decision_boundary(model, X, y, title, use_pca=True):\n",
    "    \"\"\"\n",
    "    Plot decision boundaries for the model.\n",
    "\n",
    "    Parameters:\n",
    "    - model: Trained model to visualize.\n",
    "    - X: Feature data (test set).\n",
    "    - y: Target labels.\n",
    "    - title: Title for the plot.\n",
    "    - use_pca: If True, applies PCA for dimensionality reduction if X has >2 features.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Original X shape: {X.shape}\")\n",
    "    if X.shape[1] > 2 and use_pca:\n",
    "        logger.info(\"X has more than 2 features, applying PCA for visualization.\")\n",
    "        pca = PCA(n_components=2)\n",
    "        X_2d = pca.fit_transform(X)\n",
    "        explained_variance = pca.explained_variance_ratio_\n",
    "        logger.info(f\"PCA explained variance ratios: {explained_variance}\")\n",
    "    elif X.shape[1] > 2:\n",
    "        logger.error(\"Cannot plot decision boundary for more than 2D without PCA.\")\n",
    "        raise ValueError(\"Cannot plot decision boundary for more than 2D without PCA.\")\n",
    "    else:\n",
    "        logger.info(\"X has 2 or fewer features, using original features for plotting.\")\n",
    "        X_2d = X\n",
    "\n",
    "    logger.info(f\"Transformed X shape for plotting: {X_2d.shape}\")\n",
    "\n",
    "    # Create mesh grid\n",
    "    x_min, x_max = X_2d[:, 0].min() - 1, X_2d[:, 0].max() + 1\n",
    "    y_min, y_max = X_2d[:, 1].min() - 1, X_2d[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(\n",
    "        np.arange(x_min, x_max, 0.01),\n",
    "        np.arange(y_min, y_max, 0.01)\n",
    "    )\n",
    "    logger.info(f\"Mesh grid created with shape xx: {xx.shape}, yy: {yy.shape}\")\n",
    "\n",
    "    # Flatten the grid arrays and combine into a single array\n",
    "    grid_points_2d = np.c_[xx.ravel(), yy.ravel()]\n",
    "    logger.info(f\"Grid points in 2D PCA space shape: {grid_points_2d.shape}\")\n",
    "\n",
    "    if X.shape[1] > 2 and use_pca:\n",
    "        # Inverse transform the grid points back to the original feature space\n",
    "        logger.info(\"Inverse transforming grid points back to original feature space for prediction.\")\n",
    "        grid_points_original = pca.inverse_transform(grid_points_2d)\n",
    "        logger.info(f\"Grid points in original feature space shape: {grid_points_original.shape}\")\n",
    "        # Predict on the grid points in original feature space\n",
    "        try:\n",
    "            Z = model.predict(grid_points_original)\n",
    "        except ValueError as e:\n",
    "            logger.error(f\"Error predicting decision boundary: {e}\")\n",
    "            return\n",
    "    else:\n",
    "        # For 2D data, use grid points directly for prediction\n",
    "        grid_points_original = grid_points_2d\n",
    "        Z = model.predict(grid_points_original)\n",
    "\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    logger.info(f\"Decision boundary predictions reshaped to: {Z.shape}\")\n",
    "\n",
    "    # Plot the decision boundary\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.RdYlBu)\n",
    "    plt.scatter(X_2d[:, 0], X_2d[:, 1], c=y, edgecolor=\"k\", cmap=plt.cm.RdYlBu)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Principal Component 1\" if use_pca and X.shape[1] > 2 else \"Feature 1\")\n",
    "    plt.ylabel(\"Principal Component 2\" if use_pca and X.shape[1] > 2 else \"Feature 2\")\n",
    "    plt.show()\n",
    "\n",
    "# Hyperparameter tuning for Random Forest\n",
    "def tune_random_forest(X_train, y_train, scoring_metric=\"neg_log_loss\"):\n",
    "    logger.info(\"Starting hyperparameter tuning for Random Forest...\")\n",
    "    param_space = {\n",
    "        \"n_estimators\": Integer(10, 500),\n",
    "        \"max_depth\": Integer(2, 50),\n",
    "        \"min_samples_split\": Integer(2, 20),\n",
    "        \"min_samples_leaf\": Integer(1, 10),\n",
    "        \"max_features\": Categorical([\"sqrt\", \"log2\", None]),\n",
    "        \"bootstrap\": Categorical([True, False]),\n",
    "        \"criterion\": Categorical([\"gini\", \"entropy\"]),\n",
    "    }\n",
    "    logger.info(f\"Parameter space: {param_space}\")\n",
    "\n",
    "    search = BayesSearchCV(\n",
    "        RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "        param_space,\n",
    "        n_iter=60,\n",
    "        scoring=scoring_metric, #accuracy, neg_log_loss\n",
    "        cv=cv,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "    search.fit(X_train, y_train)\n",
    "    logger.info(f\"Best parameters found: {search.best_params_}\")\n",
    "    logger.info(f\"Best cross-validation score: {search.best_score_}\")\n",
    "    return search.best_params_, search.best_score_, search.best_estimator_\n",
    "\n",
    "# Hyperparameter tuning for XGBoost\n",
    "def tune_xgboost(X_train, y_train, scoring_metric=\"neg_log_loss\"):\n",
    "    logger.info(\"Starting hyperparameter tuning for XGBoost...\")\n",
    "    param_space = {\n",
    "        'learning_rate': Real(0.01, 0.3, prior='log-uniform'),\n",
    "        'n_estimators': Integer(100, 500),\n",
    "        'max_depth': Integer(3, 15),\n",
    "        'min_child_weight': Integer(1, 10),\n",
    "        'gamma': Real(0, 5),\n",
    "        'subsample': Real(0.5, 1.0),\n",
    "        'colsample_bytree': Real(0.5, 1.0),\n",
    "        'reg_alpha': Real(1e-8, 1.0, prior='log-uniform'),\n",
    "        'reg_lambda': Real(1e-8, 1.0, prior='log-uniform'),\n",
    "    }\n",
    "    logger.info(f\"Parameter space: {param_space}\")\n",
    "\n",
    "    search = BayesSearchCV(\n",
    "        XGBClassifier(eval_metric=\"logloss\", random_state=42, n_jobs=-1),\n",
    "        param_space,\n",
    "        n_iter=60,\n",
    "        scoring=scoring_metric,\n",
    "        cv=cv,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "    search.fit(X_train, y_train)\n",
    "    logger.info(f\"Best parameters found: {search.best_params_}\")\n",
    "    logger.info(f\"Best cross-validation score: {search.best_score_}\")\n",
    "    return search.best_params_, search.best_score_, search.best_estimator_\n",
    "\n",
    "# Hyperparameter tuning for Decision Tree\n",
    "def tune_decision_tree(X_train, y_train, scoring_metric=\"neg_log_loss\"):\n",
    "    logger.info(\"Starting hyperparameter tuning for Decision Tree...\")\n",
    "    param_space = {\n",
    "        \"max_depth\": Integer(2, 50),\n",
    "        \"min_samples_split\": Integer(2, 20),\n",
    "        \"min_samples_leaf\": Integer(1, 10),\n",
    "        \"criterion\": Categorical([\"gini\", \"entropy\"]),\n",
    "        \"splitter\": Categorical([\"best\", \"random\"]),\n",
    "    }\n",
    "    logger.info(f\"Parameter space: {param_space}\")\n",
    "\n",
    "    search = BayesSearchCV(\n",
    "        DecisionTreeClassifier(random_state=42),\n",
    "        param_space,\n",
    "        n_iter=60,\n",
    "        scoring=scoring_metric,\n",
    "        cv=cv,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "    search.fit(X_train, y_train)\n",
    "    logger.info(f\"Best parameters found: {search.best_params_}\")\n",
    "    logger.info(f\"Best cross-validation score: {search.best_score_}\")\n",
    "    return search.best_params_, search.best_score_, search.best_estimator_\n",
    "\n",
    "# Hyperparameter tuning for CatBoost\n",
    "def tune_catboost(X_train, y_train, scoring_metric=\"neg_log_loss\"):\n",
    "    logger.info(\"Starting hyperparameter tuning for CatBoost...\")\n",
    "    param_space = {\n",
    "        'learning_rate': Real(0.01, 0.3, prior='log-uniform'),\n",
    "        'iterations': Integer(100, 500),\n",
    "        'depth': Integer(3, 10),\n",
    "        'l2_leaf_reg': Real(1, 10),\n",
    "        'bagging_temperature': Real(0, 1),\n",
    "        'border_count': Integer(32, 255)\n",
    "    }\n",
    "    logger.info(f\"Parameter space for CatBoost: {param_space}\")\n",
    "\n",
    "    # Note: CatBoostClassifier might print a lot of output by default.\n",
    "    # We disable verbose by setting verbose=0.\n",
    "    search = BayesSearchCV(\n",
    "        CatBoostClassifier(random_state=42, thread_count=-1, verbose=0),\n",
    "        param_space,\n",
    "        n_iter=60,\n",
    "        scoring=scoring_metric,\n",
    "        cv=cv,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "    search.fit(X_train, y_train)\n",
    "    logger.info(f\"Best parameters found for CatBoost: {search.best_params_}\")\n",
    "    logger.info(f\"Best cross-validation score for CatBoost: {search.best_score_}\")\n",
    "    return search.best_params_, search.best_score_, search.best_estimator_\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def bayes_best_model_train(\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    "    X_test: pd.DataFrame,\n",
    "    y_test: pd.Series,\n",
    "    selection_metric: str,\n",
    "    model_save_dir: Path,\n",
    "    classification_save_path: Path,\n",
    "    tuning_results_save: Path,\n",
    "    selected_models: Any,\n",
    "    use_pca: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Streamlined function for model tuning, evaluation, and saving the best model.\n",
    "    \"\"\"\n",
    "    logger.info(\"Starting the Bayesian hyperparameter tuning process...\")\n",
    "\n",
    "    # Scoring metric selection\n",
    "    scoring_metric = \"neg_log_loss\" if selection_metric.lower() == \"log loss\" else \"accuracy\"\n",
    "\n",
    "    # Prepare model registry\n",
    "    model_registry = {\n",
    "        \"XGBoost\": tune_xgboost,\n",
    "        \"Random Forest\": tune_random_forest,\n",
    "        \"Decision Tree\": tune_decision_tree,\n",
    "        \"CatBoost\": tune_catboost\n",
    "    }\n",
    "\n",
    "    # Normalize selected_models input\n",
    "    if isinstance(selected_models, str):\n",
    "        selected_models = [selected_models]\n",
    "    elif not selected_models:\n",
    "        selected_models = list(model_registry.keys())\n",
    "        logger.info(f\"No models specified. Using all available: {selected_models}\")\n",
    "\n",
    "    tuning_results = {}\n",
    "    best_model_name = None\n",
    "    best_model = None\n",
    "    best_metric_value = None\n",
    "\n",
    "    # Ensure model_save_dir exists\n",
    "    model_save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    logger.debug(f\"Ensured that the model save directory '{model_save_dir}' exists.\")\n",
    "\n",
    "    # Define metric key mapping\n",
    "    metric_key_mapping = {\n",
    "        \"log loss\": \"Log Loss\",\n",
    "        \"accuracy\": \"Accuracy\",\n",
    "        \"precision\": \"Precision\",\n",
    "        \"recall\": \"Recall\",\n",
    "        \"f1 score\": \"F1 Score\",\n",
    "        \"roc auc\": \"ROC AUC\"\n",
    "    }\n",
    "\n",
    "    # Loop over requested models\n",
    "    for model_name in selected_models:\n",
    "        if model_name not in model_registry:\n",
    "            logger.warning(f\"Unsupported model: {model_name}. Skipping.\")\n",
    "            continue\n",
    "        try:\n",
    "            logger.info(f\"📌 Tuning hyperparameters for {model_name}...\")\n",
    "            tuner_func = model_registry[model_name]\n",
    "\n",
    "            best_params, best_score, best_estimator = tuner_func(\n",
    "                X_train, y_train, scoring_metric=scoring_metric\n",
    "            )\n",
    "            logger.info(f\"✅ {model_name} tuning done. Best Params: {best_params}, Best CV Score: {best_score}\")\n",
    "\n",
    "            # Evaluate on X_test\n",
    "            metrics = evaluate_model(best_estimator, X_test, y_test, save_path=classification_save_path)\n",
    "            metric_key = metric_key_mapping.get(selection_metric.lower(), selection_metric)\n",
    "            metric_value = metrics.get(metric_key)\n",
    "\n",
    "            # Debugging\n",
    "            logger.debug(f\"Selection Metric Key: {metric_key}\")\n",
    "            logger.debug(f\"Available Metrics: {metrics.keys()}\")\n",
    "\n",
    "            if metric_value is not None:\n",
    "                logger.debug(f\"Metric value for {selection_metric}: {metric_value}\")\n",
    "                if best_metric_value is None:\n",
    "                    best_metric_value = metric_value\n",
    "                    best_model_name = model_name\n",
    "                    best_model = best_estimator\n",
    "                    logger.debug(f\"Best model set to {best_model_name} with {selection_metric}={best_metric_value}\")\n",
    "                else:\n",
    "                    # For log loss, lower is better\n",
    "                    if selection_metric.lower() == \"log loss\" and metric_value < best_metric_value:\n",
    "                        best_metric_value = metric_value\n",
    "                        best_model_name = model_name\n",
    "                        best_model = best_estimator\n",
    "                        logger.debug(f\"Best model updated to {best_model_name} with {selection_metric}={best_metric_value}\")\n",
    "                    # For other metrics (accuracy, f1, etc.), higher is better\n",
    "                    elif selection_metric.lower() != \"log loss\" and metric_value > best_metric_value:\n",
    "                        best_metric_value = metric_value\n",
    "                        best_model_name = model_name\n",
    "                        best_model = best_estimator\n",
    "                        logger.debug(f\"Best model updated to {best_model_name} with {selection_metric}={best_metric_value}\")\n",
    "            else:\n",
    "                logger.debug(f\"Metric value for {selection_metric} is None. Best model not updated.\")\n",
    "\n",
    "            # Save partial results\n",
    "            tuning_results[model_name] = {\n",
    "                \"Best Params\": best_params,\n",
    "                \"Best CV Score\": best_score,\n",
    "                \"Evaluation Metrics\": metrics,\n",
    "            }\n",
    "\n",
    "            # Plot boundary (optional for tree-based with PCA)\n",
    "            try:\n",
    "                plot_decision_boundary(best_estimator, X_test, y_test, f\"{model_name} Decision Boundary\", use_pca=use_pca)\n",
    "            except ValueError as e:\n",
    "                logger.warning(f\"Skipping decision boundary plot for {model_name}: {e}\")\n",
    "\n",
    "            # Add feature importance plots for XGBoost\n",
    "            if model_name.lower() == \"xgboost\":\n",
    "                logger.info(\"Generating feature importance plots for XGBoost...\")\n",
    "                try:\n",
    "                    xgb.plot_importance(best_model, importance_type=\"weight\")\n",
    "                    plt.title(\"Feature Importance by Weight\")\n",
    "                    plt.show()\n",
    "\n",
    "                    xgb.plot_importance(best_model, importance_type=\"cover\")\n",
    "                    plt.title(\"Feature Importance by Cover\")\n",
    "                    plt.show()\n",
    "\n",
    "                    xgb.plot_importance(best_model, importance_type=\"gain\")\n",
    "                    plt.title(\"Feature Importance by Gain\")\n",
    "                    plt.show()\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error generating feature importance plots: {e}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Error tuning {model_name}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Save best model information\n",
    "    if best_model_name:\n",
    "        logger.info(f\"✅ Best model is {best_model_name} with {selection_metric}={best_metric_value}\")\n",
    "        try:\n",
    "            save_model(best_model, best_model_name, save_dir=model_save_dir)\n",
    "            logger.info(f\"✅ Model '{best_model_name}' saved successfully in '{model_save_dir}'.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Failed to save best model {best_model_name}: {e}\")\n",
    "            raise  # Ensure the exception is propagated\n",
    "\n",
    "        # Add Best Model info to tuning_results\n",
    "        tuning_results[\"Best Model\"] = {\n",
    "            \"model_name\": best_model_name,\n",
    "            \"metric_value\": best_metric_value,\n",
    "            \"path\": str(Path(model_save_dir) / best_model_name.replace(\" \", \"_\") / 'trained_model.pkl')\n",
    "        }\n",
    "    else:\n",
    "        logger.warning(\"⚠️ No best model was selected. Tuning might have failed for all models.\")\n",
    "\n",
    "    # Save tuning results\n",
    "    try:\n",
    "        with tuning_results_save.open(\"w\") as f:\n",
    "            json.dump(tuning_results, f, indent=4)\n",
    "        logger.info(f\"✅ Tuning results saved to {tuning_results_save}.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Error saving tuning results: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ml/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ml/train.py\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import joblib\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from ml.config.config_loader import load_config  # New import for config\n",
    "from ml.config.config_models import AppConfig\n",
    "from datapreprocessor import DataPreprocessor\n",
    "from ml.feature_selection.feature_importance_calculator import manage_features\n",
    "from ml.train_utils.train_utils import (bayes_best_model_train\n",
    ")\n",
    "\n",
    "# Setup basic logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"WE'RE IN THIS DIRECTORY =\", os.getcwd())\n",
    "print(\"WE'RE IN THIS sys.path =\", sys.path)\n",
    "\n",
    "def main():\n",
    "    # --- 1. Load Configuration via our new module ---\n",
    "    config_path = Path('../../data/model/preprocessor_config/preprocessor_config.yaml')\n",
    "    # load_config now returns an AppConfig instance (verified and typed)\n",
    "    config: AppConfig = load_config(config_path)\n",
    "    \n",
    "    # --- 2. Use Config Values in the Code ---\n",
    "    # Extract paths from configuration (using our typed model)\n",
    "    paths_config = config.paths\n",
    "    base_data_dir = Path(paths_config.data_dir).resolve()\n",
    "    raw_data_file = base_data_dir / paths_config.raw_data\n",
    "\n",
    "    # Output directories\n",
    "    log_dir = Path(paths_config.log_dir).resolve()\n",
    "    model_save_dir = Path(paths_config.model_save_base_dir).resolve()\n",
    "    transformers_save_dir = Path(paths_config.transformers_save_base_dir).resolve()\n",
    "    plots_output_dir = Path(paths_config.plots_output_dir).resolve()\n",
    "    training_output_dir = Path(paths_config.training_output_dir).resolve()\n",
    "\n",
    "    # Ensure output directories exist if needed\n",
    "    model_save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    CLASSIFICATION_REPORT_PATH = model_save_dir / \"classification_report.txt\"\n",
    "    TUNING_RESULTS_SAVE_PATH = model_save_dir / \"tuning_results.json\"\n",
    "\n",
    "    # Extract model settings from the config\n",
    "    selected_models = config.models.selected_models\n",
    "    print(f\"Selected Models: {selected_models}\")\n",
    "    selection_metric = config.models.selection_metric\n",
    "\n",
    "    # Load the dataset\n",
    "    try:\n",
    "        filtered_df = pd.read_csv(raw_data_file)\n",
    "        logger.info(f\"✅ Loaded dataset from {raw_data_file}. Shape: {filtered_df.shape}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Failed to load dataset: {e}\")\n",
    "        return\n",
    "    base_dir = Path(\"../../data\") / \"preprocessor\" / \"features_info\"\n",
    "    # Load feature metadata using manage_features\n",
    "    feature_paths = {\n",
    "        'features': '../../data/preprocessor/features_info/final_ml_df_selected_features_columns.pkl',\n",
    "        'ordinal_categoricals': '../../data/preprocessor/features_info/ordinal_categoricals.pkl',\n",
    "        'nominal_categoricals': '../../data/preprocessor/features_info/nominal_categoricals.pkl',\n",
    "        'numericals': '../../data/preprocessor/features_info/numericals.pkl',\n",
    "        'y_variable': '../../data/preprocessor/features_info/y_variable.pkl'\n",
    "    }\n",
    "    loaded = manage_features(mode='load', paths=feature_paths)\n",
    "    if loaded:\n",
    "        y_var = loaded.get('y_variable')\n",
    "    else:\n",
    "        logger.error(\"❌ Failed to load feature metadata.\")\n",
    "        return\n",
    "\n",
    "    # Initialize DataPreprocessor\n",
    "    preprocessor = DataPreprocessor(\n",
    "        model_type=\"Tree Based Classifier\",\n",
    "        y_variable=y_var,\n",
    "        ordinal_categoricals=loaded.get('ordinal_categoricals'),\n",
    "        nominal_categoricals=loaded.get('nominal_categoricals'),\n",
    "        numericals=loaded.get('numericals'),\n",
    "        mode='train',\n",
    "        debug=config.logging.debug,\n",
    "        normalize_debug=False,\n",
    "        normalize_graphs_output=False,\n",
    "        graphs_output_dir=plots_output_dir,\n",
    "        transformers_dir=transformers_save_dir\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        X_train, X_test, y_train, y_test, recommendations, X_test_inverse = preprocessor.final_preprocessing(filtered_df)\n",
    "        logger.info(f\"✅ Preprocessing complete. X_train shape: {X_train.shape}, X_test shape: {X_test.shape}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Error during preprocessing: {e}\")\n",
    "        return\n",
    "\n",
    "    # Proceed with training (tuning and model saving) using your existing function.\n",
    "    try:\n",
    "        bayes_best_model_train(\n",
    "            X_train=X_train,\n",
    "            y_train=y_train,\n",
    "            X_test=X_test,\n",
    "            y_test=y_test,\n",
    "            selection_metric=selection_metric,\n",
    "            model_save_dir=model_save_dir,\n",
    "            classification_save_path=CLASSIFICATION_REPORT_PATH,\n",
    "            tuning_results_save=TUNING_RESULTS_SAVE_PATH,\n",
    "            selected_models=selected_models,\n",
    "            use_pca=True  \n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Model training/tuning failed: {e}\")\n",
    "        return\n",
    "\n",
    "    logger.info(\"✅ Training workflow completed successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shap Functions for Prediction Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ml/predict/predict.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ml/predict/predict.py\n",
    "\n",
    "import pandas as pd\n",
    "import logging\n",
    "import os\n",
    "import joblib\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict\n",
    "\n",
    "# Local imports - Adjust based on your project structure\n",
    "from ml.train_utils.train_utils import load_model  # Ensure correct import path\n",
    "from datapreprocessor import DataPreprocessor  # Adjust as necessary\n",
    "\n",
    "# Import new configuration loader\n",
    "from ml.config.config_loader import load_config\n",
    "from ml.config.config_models import AppConfig  # To use as type annotation\n",
    "\n",
    "# Set up logger\n",
    "logger = logging.getLogger(\"PredictAndAttachLogger\")\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# Set up console and file handlers\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setLevel(logging.INFO)\n",
    "file_handler = logging.FileHandler(\"predictions.log\")\n",
    "file_handler.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n",
    "console_handler.setFormatter(formatter)\n",
    "file_handler.setFormatter(formatter)\n",
    "logger.addHandler(console_handler)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "def load_dataset(path: Path) -> pd.DataFrame:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Dataset not found at {path}\")\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "def predict_and_attach_predict_probs(trained_model, X_preprocessed, X_inversed):\n",
    "    # [Existing prediction code unchanged...]\n",
    "    try:\n",
    "        predictions = trained_model.predict(X_preprocessed)\n",
    "        logger.info(\"✅ Predictions made successfully.\")\n",
    "        logger.debug(f\"Predictions sample: {predictions[:5]}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Prediction failed: {e}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        prediction_probs = trained_model.predict_proba(X_preprocessed)\n",
    "        logger.info(\"✅ Prediction probabilities computed successfully.\")\n",
    "        logger.debug(f\"Prediction probabilities sample:\\n{prediction_probs[:2]}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Prediction probabilities computation failed: {e}\")\n",
    "        return\n",
    "\n",
    "    if hasattr(trained_model, 'classes_'):\n",
    "        class_labels = trained_model.classes_\n",
    "    else:\n",
    "        class_labels = [f'class_{i}' for i in range(prediction_probs.shape[1])]\n",
    "\n",
    "    try:\n",
    "        if X_inversed is not None:\n",
    "            if 'Prediction' not in X_inversed.columns:\n",
    "                X_inversed['Prediction'] = predictions\n",
    "                logger.debug(\"Predictions attached to inverse-transformed DataFrame.\")\n",
    "            if 'Prediction_Probabilities' not in X_inversed.columns:\n",
    "                X_inversed['Prediction_Probabilities'] = prediction_probs.tolist()\n",
    "                logger.debug(\"Prediction probabilities attached to inverse-transformed DataFrame.\")\n",
    "            for idx, label in enumerate(class_labels):\n",
    "                col_name = f'Probability_{label}'\n",
    "                X_inversed[col_name] = prediction_probs[:, idx]\n",
    "                logger.debug(f\"Attached column: {col_name}\")\n",
    "            X_inversed.drop(columns=['Prediction_Probabilities'], inplace=True)\n",
    "            logger.debug(f\"Final shape of X_inversed: {X_inversed.shape}\")\n",
    "            logger.debug(f\"Final columns in X_inversed: {X_inversed.columns.tolist()}\")\n",
    "        else:\n",
    "            logger.warning(\"X_inversed is None. Creating a new DataFrame with predictions.\")\n",
    "            data = {'Prediction': predictions, 'Prediction_Probabilities': prediction_probs.tolist()}\n",
    "            for idx, label in enumerate(class_labels):\n",
    "                col_name = f'Probability_{label}'\n",
    "                data[col_name] = prediction_probs[:, idx]\n",
    "            X_inversed = pd.DataFrame(data)\n",
    "            X_inversed.drop(columns=['Prediction_Probabilities'], inplace=True)\n",
    "            logger.debug(f\"Created new X_inversed DataFrame with shape: {X_inversed.shape}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Failed to attach predictions to inverse-transformed DataFrame: {e}\")\n",
    "        X_inversed = pd.DataFrame({'Prediction': predictions})\n",
    "        return\n",
    "\n",
    "    return predictions, prediction_probs, X_inversed\n",
    "\n",
    "def main():\n",
    "    # ----------------------------\n",
    "    # Step 1: Load Configuration\n",
    "    # ----------------------------\n",
    "    config_path = Path('../../data/model/preprocessor_config/preprocessor_config.yaml')\n",
    "    try:\n",
    "        config: AppConfig = load_config(config_path)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to load configuration: {e}\")\n",
    "        return\n",
    "\n",
    "    # ----------------------------\n",
    "    # Step 2: Extract Paths from Configuration\n",
    "    # ----------------------------\n",
    "    data_dir = Path(config.paths.data_dir).resolve()\n",
    "    raw_data_path = data_dir / config.paths.raw_data\n",
    "    processed_data_dir = data_dir / config.paths.processed_data_dir\n",
    "    transformers_dir = Path(config.paths.transformers_save_base_dir).resolve()\n",
    "    predictions_output_dir = Path(config.paths.predictions_output_dir).resolve()\n",
    "    log_dir = Path(config.paths.log_dir).resolve()\n",
    "    model_save_dir = Path(config.paths.model_save_base_dir).resolve()\n",
    "    log_file = config.paths.log_file\n",
    "\n",
    "    logger.info(\"✅ Starting prediction module.\")\n",
    "\n",
    "    # ----------------------------\n",
    "    # Step 3: Extract Feature Assets\n",
    "    # ----------------------------\n",
    "    features_config = config.features\n",
    "    column_assets = {\n",
    "        'y_variable': features_config.y_variable,\n",
    "        'ordinal_categoricals': features_config.ordinal_categoricals,\n",
    "        'nominal_categoricals': features_config.nominal_categoricals,\n",
    "        'numericals': features_config.numericals\n",
    "    }\n",
    "\n",
    "    # ----------------------------\n",
    "    # Step 4: Load Tuning Results to Find Best Model\n",
    "    # ----------------------------\n",
    "    tuning_results_path = model_save_dir / \"tuning_results.json\"\n",
    "    if not tuning_results_path.exists():\n",
    "        logger.error(f\"❌ Tuning results not found at '{tuning_results_path}'. Cannot determine the best model.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        with open(tuning_results_path, 'r') as f:\n",
    "            tuning_results = json.load(f)\n",
    "        best_model_info = tuning_results.get(\"Best Model\")\n",
    "        if not best_model_info:\n",
    "            logger.error(\"❌ Best model information not found in tuning results.\")\n",
    "            return\n",
    "        best_model_name = best_model_info.get(\"model_name\")\n",
    "        if not best_model_name:\n",
    "            logger.error(\"❌ Best model name not found in tuning results.\")\n",
    "            return\n",
    "        logger.info(f\"Best model identified: {best_model_name}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Failed to load tuning results: {e}\")\n",
    "        return\n",
    "\n",
    "    # ----------------------------\n",
    "    # Step 5: Load the Prediction Dataset\n",
    "    # ----------------------------\n",
    "    if not raw_data_path.exists():\n",
    "        logger.error(f\"❌ Prediction input dataset not found at '{raw_data_path}'.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        df_predict = pd.read_csv(raw_data_path)\n",
    "        logger.info(f\"✅ Prediction input data loaded from '{raw_data_path}'.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Failed to load prediction input data: {e}\")\n",
    "        return\n",
    "\n",
    "    # ----------------------------\n",
    "    # Step 6: Initialize DataPreprocessor\n",
    "    # ----------------------------\n",
    "    preprocessor = DataPreprocessor(\n",
    "        model_type=\"Tree Based Classifier\",  # Adjust if needed based on best_model_name\n",
    "        y_variable=column_assets.get('y_variable', []),\n",
    "        ordinal_categoricals=column_assets.get('ordinal_categoricals', []),\n",
    "        nominal_categoricals=column_assets.get('nominal_categoricals', []),\n",
    "        numericals=column_assets.get('numericals', []),\n",
    "        mode='predict',\n",
    "        options={},\n",
    "        debug=False,\n",
    "        normalize_debug=False,\n",
    "        normalize_graphs_output=False,\n",
    "        graphs_output_dir=Path(config.paths.plots_output_dir).resolve(),\n",
    "        transformers_dir=transformers_dir\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        X_preprocessed, recommendations, X_inversed = preprocessor.final_preprocessing(df_predict)\n",
    "        print(\"X_new_preprocessed type =\", type(X_preprocessed), \"X_new_inverse type =\", type(X_inversed))\n",
    "        logger.info(\"✅ Preprocessing completed successfully in predict mode.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Preprocessing failed in predict mode: {e}\")\n",
    "        return\n",
    "\n",
    "    # ----------------------------\n",
    "    # Step 7: Load the Best Model\n",
    "    # ----------------------------\n",
    "    try:\n",
    "        trained_model = load_model(best_model_name, model_save_dir)\n",
    "        model_path = model_save_dir / best_model_name.replace(\" \", \"_\") / 'trained_model.pkl'\n",
    "        logger.info(f\"✅ Trained model loaded from '{model_path}'.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Failed to load the best model '{best_model_name}': {e}\")\n",
    "        return\n",
    "\n",
    "    # ----------------------------\n",
    "    # Step 8: Make Predictions and Attach Probabilities\n",
    "    # ----------------------------\n",
    "    predictions, prediction_probs, X_inversed = predict_and_attach_predict_probs(trained_model, X_preprocessed, X_inversed)\n",
    "    print(X_inversed)\n",
    "\n",
    "    # ----------------------------\n",
    "    # Step 9: Save Predictions\n",
    "    # ----------------------------\n",
    "    try:\n",
    "        predictions_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        predictions_filename = predictions_output_dir / f'predictions_{best_model_name.replace(\" \", \"_\")}.csv'\n",
    "        X_inversed.to_csv(predictions_filename, index=False)\n",
    "        logger.info(f\"✅ Predictions saved to '{predictions_filename}'.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Failed to save predictions: {e}\")\n",
    "        return\n",
    "\n",
    "    logger.info(f\"✅ All prediction tasks completed successfully for model '{best_model_name}'.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ml/shap/shap_utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ml/shap/shap_utils.py\n",
    "\n",
    "import logging\n",
    "import shap\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def compute_shap_values(model, X, debug: bool = False, logger: logging.Logger = None):\n",
    "    if logger:\n",
    "        logger.info(\"Initializing SHAP explainer...\")\n",
    "\n",
    "    try:\n",
    "        explainer = shap.Explainer(model, X)\n",
    "        if logger and debug:\n",
    "            logger.debug(f\"SHAP Explainer initialized: {type(explainer)}\")\n",
    "            logger.debug(f\"Explainer details: {explainer}\")\n",
    "\n",
    "        shap_values = explainer(X)\n",
    "        if logger and debug:\n",
    "            logger.debug(f\"SHAP values computed: {type(shap_values)}\")\n",
    "            logger.debug(f\"Shape of shap_values: {shap_values.shape}\")\n",
    "            if hasattr(shap_values, 'values'):\n",
    "                logger.debug(f\"Sample SHAP values:\\n{shap_values.values[:2]}\")\n",
    "            if hasattr(shap_values, 'feature_names'):\n",
    "                logger.debug(f\"SHAP feature names: {shap_values.feature_names}\")\n",
    "\n",
    "        # Determine number of classes\n",
    "        n_classes = len(model.classes_)\n",
    "        logger.debug(f\"Number of classes in the model: {n_classes}\")\n",
    "\n",
    "        if shap_values.values.ndim == 3:\n",
    "            if n_classes > 1:\n",
    "                # For multi-class classification, select SHAP values for the positive class\n",
    "                shap_values_class = shap_values.values[:, :, 1]\n",
    "                logger.debug(f\"Extracted SHAP values for class 1: Shape {shap_values_class.shape}\")\n",
    "            else:\n",
    "                # For single-class models, retain SHAP values as is\n",
    "                shap_values_class = shap_values.values[:, :, 0]\n",
    "                logger.debug(f\"Extracted SHAP values for single class: Shape {shap_values_class.shape}\")\n",
    "        elif shap_values.values.ndim == 2:\n",
    "            if n_classes > 1:\n",
    "                # For binary classification, SHAP returns 2D array for the positive class\n",
    "                shap_values_class = shap_values.values\n",
    "                logger.debug(f\"Extracted SHAP values for positive class: Shape {shap_values_class.shape}\")\n",
    "            else:\n",
    "                shap_values_class = shap_values.values\n",
    "                logger.debug(f\"Extracted SHAP values for single class: Shape {shap_values_class.shape}\")\n",
    "        else:\n",
    "            logger.error(f\"Unexpected SHAP values dimensions: {shap_values.values.ndim}\")\n",
    "            raise ValueError(\"Unexpected SHAP values dimensions.\")\n",
    "\n",
    "        return explainer, shap_values_class\n",
    "    except Exception as e:\n",
    "        if logger:\n",
    "            logger.error(f\"Failed to compute SHAP values: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def plot_shap_summary(shap_values, X_original: pd.DataFrame, save_path: str, debug: bool = False, \n",
    "                     logger: logging.Logger = None):\n",
    "    \"\"\"\n",
    "    Generate and save a SHAP summary plot.\n",
    "\n",
    "    :param shap_values: SHAP values computed for the dataset.\n",
    "    :param X_original: Original (preprocessed) feature DataFrame.\n",
    "    :param save_path: Full file path to save the plot.\n",
    "    :param debug: If True, enable detailed debug logs.\n",
    "    :param logger: Logger instance for logging.\n",
    "    \"\"\"\n",
    "    if logger:\n",
    "        logger.info(\"Generating SHAP summary plot...\")\n",
    "        logger.debug(f\"Type of shap_values: {type(shap_values)}\")\n",
    "        logger.debug(f\"Shape of shap_values: {shap_values.shape}\")\n",
    "        \n",
    "        # Check if shap_values is a shap.Explanation object\n",
    "        if isinstance(shap_values, shap.Explanation):\n",
    "            logger.debug(f\"SHAP feature names: {shap_values.feature_names}\")\n",
    "        else:\n",
    "            logger.debug(\"shap_values is not a shap.Explanation object. Attempting to extract feature names.\")\n",
    "            if hasattr(shap_values, 'feature_names'):\n",
    "                logger.debug(f\"shap_values feature names: {shap_values.feature_names}\")\n",
    "            else:\n",
    "                logger.debug(\"Cannot extract feature names from shap_values.\")\n",
    "        \n",
    "        logger.debug(f\"Type of X_original: {type(X_original)}\")\n",
    "        logger.debug(f\"Shape of X_original: {X_original.shape}\")\n",
    "        logger.debug(f\"Columns in X_original: {X_original.columns.tolist()}\")\n",
    "        \n",
    "        # Verify column alignment\n",
    "        if isinstance(shap_values, shap.Explanation):\n",
    "            shap_feature_names = shap_values.feature_names\n",
    "        elif hasattr(shap_values, 'feature_names'):\n",
    "            shap_feature_names = shap_values.feature_names\n",
    "        else:\n",
    "            shap_feature_names = X_original.columns.tolist()  # Fallback\n",
    "        \n",
    "        if list(shap_feature_names) != list(X_original.columns):\n",
    "            logger.error(\"Column mismatch between SHAP values and X_original.\")\n",
    "            logger.error(f\"SHAP feature names ({len(shap_feature_names)}): {shap_feature_names}\")\n",
    "            logger.error(f\"X_original columns ({len(X_original.columns)}): {X_original.columns.tolist()}\")\n",
    "            raise ValueError(\"Column mismatch between SHAP values and X_original.\")\n",
    "        else:\n",
    "            logger.debug(\"Column alignment verified between SHAP values and X_original.\")\n",
    "    \n",
    "    try:\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        shap.summary_plot(shap_values, X_original, show=False)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        if logger and debug:\n",
    "            logger.debug(f\"SHAP summary plot saved to {save_path}\")\n",
    "        if logger:\n",
    "            logger.info(\"SHAP summary plot generated successfully.\")\n",
    "    except Exception as e:\n",
    "        if logger:\n",
    "            logger.error(f\"Failed to generate SHAP summary plot: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_shap_dependence(shap_values, feature: str, X_original: pd.DataFrame, save_path: str, debug: bool = False, \n",
    "                         logger: logging.Logger = None):\n",
    "    \"\"\"\n",
    "    Generate and save a SHAP dependence plot for a specific feature.\n",
    "\n",
    "    :param shap_values: SHAP values computed for the dataset.\n",
    "    :param feature: Feature name to generate the dependence plot for.\n",
    "    :param X_original: Original (untransformed) feature DataFrame.\n",
    "    :param save_path: Full file path to save the plot.\n",
    "    :param debug: If True, enable detailed debug logs.\n",
    "    :param logger: Logger instance for logging.\n",
    "    \"\"\"\n",
    "    if logger:\n",
    "        logger.info(f\"Generating SHAP dependence plot for feature '{feature}'...\")\n",
    "    try:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        # shap.dependence_plot(feature, shap_values.values, X_original, show=False)\n",
    "        shap.dependence_plot(feature, shap_values, X_original, show=False)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        if logger and debug:\n",
    "            logger.debug(f\"SHAP dependence plot for '{feature}' saved to {save_path}\")\n",
    "        if logger:\n",
    "            logger.info(f\"SHAP dependence plot for feature '{feature}' generated successfully.\")\n",
    "    except Exception as e:\n",
    "        if logger:\n",
    "            logger.error(f\"Failed to generate SHAP dependence plot for feature '{feature}': {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def generate_global_recommendations(shap_values, X_original: pd.DataFrame, top_n: int = 5, debug: bool = False, \n",
    "                                    use_mad: bool = False, logger: logging.Logger = None) -> dict:\n",
    "    \"\"\"\n",
    "    Generate global recommendations based on SHAP values and feature distributions.\n",
    "\n",
    "    :param shap_values: SHAP values computed for the dataset.\n",
    "    :param X_original: Original (untransformed) feature DataFrame.\n",
    "    :param top_n: Number of top features to generate recommendations for.\n",
    "    :param debug: If True, enable detailed debug logs.\n",
    "    :param use_mad: If True, use Median Absolute Deviation for range definition.\n",
    "    :param logger: Logger instance for logging.\n",
    "    :return: recommendations: Dictionary mapping features to recommended value ranges, importance, and direction.\n",
    "    \"\"\"\n",
    "    if logger:\n",
    "        logger.info(\"Generating feature importance based on SHAP values...\")\n",
    "    try:\n",
    "        # shap_df = pd.DataFrame(shap_values.values, columns=X_original.columns)\n",
    "        shap_df = pd.DataFrame(shap_values, columns=X_original.columns)\n",
    "\n",
    "        # Calculate mean absolute SHAP values for importance\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'feature': X_original.columns,\n",
    "            'importance': np.abs(shap_df).mean(axis=0),\n",
    "            'mean_shap': shap_df.mean(axis=0)\n",
    "        }).sort_values(by='importance', ascending=False)\n",
    "        \n",
    "        if logger and debug:\n",
    "            logger.debug(f\"Feature importance (top {top_n}):\\n{feature_importance.head(top_n)}\")\n",
    "        \n",
    "        top_features = feature_importance.head(top_n)['feature'].tolist()\n",
    "        recommendations = {}\n",
    "        \n",
    "        for feature in top_features:\n",
    "            feature_values = X_original[feature]\n",
    "            \n",
    "            if use_mad:\n",
    "                # Use Median and MAD for robust statistics\n",
    "                median = feature_values.median()\n",
    "                mad = feature_values.mad()\n",
    "                lower_bound = median - 1.5 * mad\n",
    "                upper_bound = median + 1.5 * mad\n",
    "                range_str = f\"{lower_bound:.1f}–{upper_bound:.1f}\"\n",
    "            else:\n",
    "                # Default to Interquartile Range (IQR)\n",
    "                lower_bound = feature_values.quantile(0.25)\n",
    "                upper_bound = feature_values.quantile(0.75)\n",
    "                range_str = f\"{lower_bound:.1f}–{upper_bound:.1f}\"\n",
    "            \n",
    "            # Determine direction based on mean SHAP value\n",
    "            mean_shap = feature_importance.loc[feature_importance['feature'] == feature, 'mean_shap'].values[0]\n",
    "            direction = 'positive' if mean_shap > 0 else 'negative'\n",
    "            \n",
    "            recommendations[feature] = {\n",
    "                'range': range_str,\n",
    "                'importance': round(feature_importance.loc[feature_importance['feature'] == feature, 'importance'].values[0], 4),  # Rounded for readability\n",
    "                'direction': direction\n",
    "            }\n",
    "            if logger and debug:\n",
    "                logger.debug(f\"Recommendation for {feature}: Range={range_str}, Importance={feature_importance.loc[feature_importance['feature'] == feature, 'importance'].values[0]}, Direction={direction}\")\n",
    "        \n",
    "        if logger and debug:\n",
    "            logger.debug(f\"Final Recommendations with Importance and Direction: {recommendations}\")\n",
    "        return recommendations\n",
    "    except Exception as e:\n",
    "        if logger:\n",
    "            logger.error(f\"Failed to generate global recommendations: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def generate_individual_feedback(trial: pd.Series, shap_values_trial: np.ndarray,\n",
    "                                 feature_metadata: dict = None, logger: logging.Logger = None) -> dict:\n",
    "    \"\"\"\n",
    "    Generate individual feedback for a single trial and break it out into several pieces for each feature.\n",
    "    \n",
    "    For each feature, three keys will be generated:\n",
    "      1. shap_{sign}_direction_{feature}: A recommendation such as \"decrease\" or \"maintain or increase\"\n",
    "      2. shap_importance_{feature}: The absolute SHAP value (rounded) for that feature in this trial\n",
    "      3. shap_{sign}_unit_change_{feature}: The computed unit–change (with unit) for that feature.\n",
    "    \n",
    "    :param trial: A pandas Series containing the trial's feature values.\n",
    "    :param shap_values_trial: A numpy array with the SHAP values for the trial.\n",
    "    :param feature_metadata: A dictionary with metadata per feature (e.g. units).\n",
    "    :param logger: Optional logger for debugging.\n",
    "    :return: A dictionary containing the feedback.\n",
    "    \"\"\"\n",
    "    feedback = {}\n",
    "    \n",
    "    # Loop over each feature and its associated SHAP value.\n",
    "    for feature, shap_val in zip(trial.index.tolist(), shap_values_trial):\n",
    "        # Choose the sign and suggestion based on the SHAP value.\n",
    "        if shap_val > 0:\n",
    "            sign = \"positive\"\n",
    "            suggestion = \"increase\"\n",
    "        elif shap_val < 0:\n",
    "            sign = \"negative\"\n",
    "            suggestion = \"decrease\"\n",
    "        else:\n",
    "            # If SHAP is zero, mark it as no impact.\n",
    "            feedback[f\"shap_{feature}_impact\"] = \"no impact\"\n",
    "            continue\n",
    "\n",
    "        # Compute the adjustment amount as (e.g.) 10% of the current value's absolute.\n",
    "        current_value = trial[feature]\n",
    "        adjustment_factor = 0.1\n",
    "        unit_change_value = adjustment_factor * abs(current_value)\n",
    "        \n",
    "        # Retrieve unit from feature_metadata, if available.\n",
    "        unit = \"\"\n",
    "        if feature_metadata and feature in feature_metadata:\n",
    "            unit = feature_metadata[feature].get('unit', '')\n",
    "        # Format the unit change as a string.\n",
    "        unit_change_str = f\"{unit_change_value:.2f} {unit}\".strip()\n",
    "\n",
    "        # The importance for the feature in this trial is simply the absolute value of the SHAP value.\n",
    "        importance = abs(shap_val)\n",
    "        \n",
    "        # Create key names.\n",
    "        key_direction = f\"shap_direction_{feature}\"\n",
    "        key_importance = f\"shap_importance_{feature}\"\n",
    "        key_unit_change = f\"shap_unit_change_{feature}\"\n",
    "        \n",
    "        # Set values.\n",
    "        feedback[key_direction] = suggestion\n",
    "        feedback[key_importance] = round(importance, 4)\n",
    "        feedback[key_unit_change] = unit_change_str\n",
    "\n",
    "        if logger and logger.isEnabledFor(logging.DEBUG):\n",
    "            logger.debug(f\"For feature '{feature}': {key_direction}='{suggestion}', \"\n",
    "                         f\"{key_importance}={round(importance, 4)}, {key_unit_change}='{unit_change_str}'\")\n",
    "    return feedback\n",
    "\n",
    "\n",
    "\n",
    "def expand_specific_feedback(df: pd.DataFrame, logger: Optional[logging.Logger] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Expand the 'specific_feedback' column from dictionaries to separate columns.\n",
    "\n",
    "    :param df: Original DataFrame containing the 'specific_feedback' column.\n",
    "    :param logger: Optional logger for debugging.\n",
    "    :return: Expanded DataFrame with separate feedback columns.\n",
    "    \"\"\"\n",
    "    if 'specific_feedback' not in df.columns:\n",
    "        logger.error(\"'specific_feedback' column not found in DataFrame.\")\n",
    "        raise KeyError(\"'specific_feedback' column not found.\")\n",
    "    \n",
    "    logger.info(\"Expanding 'specific_feedback' into separate columns.\")\n",
    "    try:\n",
    "        feedback_df = df['specific_feedback'].apply(pd.Series)\n",
    "        logger.debug(f\"Feedback DataFrame shape after expansion: {feedback_df.shape}\")\n",
    "        \n",
    "        # Optional: Handle missing values\n",
    "        feedback_df.fillna('No feedback available', inplace=True)\n",
    "        \n",
    "        # Merge with original DataFrame\n",
    "        df_expanded = pd.concat([df.drop(columns=['specific_feedback']), feedback_df], axis=1)\n",
    "        logger.info(\"'specific_feedback' expanded successfully.\")\n",
    "        return df_expanded\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to expand 'specific_feedback': {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compute_individual_shap_values(explainer, X_transformed: pd.DataFrame, trial_index: int, \n",
    "                                   logger: logging.Logger = None):\n",
    "    \"\"\"\n",
    "    Compute SHAP values for a single trial.\n",
    "\n",
    "    :param explainer: SHAP explainer object.\n",
    "    :param X_transformed: Transformed features used for prediction.\n",
    "    :param trial_index: Index of the trial.\n",
    "    :param logger: Logger instance.\n",
    "    :return: shap_values for the trial.\n",
    "    \"\"\"\n",
    "    if logger:\n",
    "        logger.info(f\"Computing SHAP values for trial at index {trial_index}...\")\n",
    "    try:\n",
    "        trial = X_transformed.iloc[[trial_index]]\n",
    "        shap_values = explainer(trial)\n",
    "        if logger:\n",
    "            logger.debug(f\"SHAP values for trial {trial_index} computed successfully.\")\n",
    "    except Exception as e:\n",
    "        if logger:\n",
    "            logger.error(f\"Failed to compute SHAP values for trial {trial_index}: {e}\")\n",
    "        raise\n",
    "    return shap_values\n",
    "\n",
    "\n",
    "def plot_individual_shap_force(shap_explainer, shap_values, X_original: pd.DataFrame, trial_index: int, \n",
    "                               save_path: str, logger: logging.Logger = None):\n",
    "    \"\"\"\n",
    "    Generate and save a SHAP force plot for a specific trial.\n",
    "\n",
    "    :param shap_explainer: SHAP explainer object.\n",
    "    :param shap_values: SHAP values for the trial.\n",
    "    :param X_original: Original feature DataFrame.\n",
    "    :param trial_index: Index of the trial.\n",
    "    :param save_path: Full file path to save the force plot.\n",
    "    :param logger: Logger instance.\n",
    "    \"\"\"\n",
    "    if logger:\n",
    "        logger.info(f\"Generating SHAP force plot for trial {trial_index}...\")\n",
    "    try:\n",
    "        shap_plot = shap.force_plot(\n",
    "            shap_explainer.expected_value, \n",
    "            shap_values.values[0], \n",
    "            X_original.iloc[trial_index],\n",
    "            matplotlib=False\n",
    "        )\n",
    "        shap.save_html(save_path, shap_plot)\n",
    "        if logger:\n",
    "            logger.debug(f\"SHAP force plot saved to {save_path}\")\n",
    "            logger.info(f\"SHAP force plot for trial {trial_index} generated successfully.\")\n",
    "    except Exception as e:\n",
    "        if logger:\n",
    "            logger.error(f\"Failed to generate SHAP force plot for trial {trial_index}: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def extract_force_plot_values(shap_values, trial_index: int, logger: logging.Logger = None) -> dict:\n",
    "    \"\"\"\n",
    "    Extract SHAP values and feature contributions for a specific trial.\n",
    "\n",
    "    Args:\n",
    "        shap_values (shap.Explanation): SHAP values object.\n",
    "        trial_index (int): Index of the trial.\n",
    "        logger (logging.Logger, optional): Logger instance.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary of feature contributions.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        shap_values_instance = shap_values.values[trial_index]\n",
    "        features_instance = shap_values.data[trial_index]\n",
    "        feature_contributions = dict(zip(shap_values.feature_names, shap_values_instance))\n",
    "        if logger and logger.isEnabledFor(logging.DEBUG):\n",
    "            logger.debug(f\"SHAP values for trial {trial_index}: {feature_contributions}\")\n",
    "        return feature_contributions\n",
    "    except Exception as e:\n",
    "        if logger:\n",
    "            logger.error(f\"Error extracting SHAP values for trial {trial_index}: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def save_shap_values(shap_values, save_path: str, logger: logging.Logger = None):\n",
    "    \"\"\"\n",
    "    Save SHAP values to a file using pickle.\n",
    "\n",
    "    :param shap_values: SHAP values object to save.\n",
    "    :param save_path: File path to save the SHAP values.\n",
    "    :param logger: Logger instance.\n",
    "    \"\"\"\n",
    "    if logger:\n",
    "        logger.info(f\"Saving SHAP values to {save_path}...\")\n",
    "    try:\n",
    "        with open(save_path, \"wb\") as f:\n",
    "            pickle.dump(shap_values, f)\n",
    "        if logger:\n",
    "            logger.info(f\"SHAP values saved successfully to {save_path}.\")\n",
    "    except Exception as e:\n",
    "        if logger:\n",
    "            logger.error(f\"Failed to save SHAP values: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def load_shap_values(load_path: str, logger: logging.Logger = None):\n",
    "    \"\"\"\n",
    "    Load SHAP values from a pickle file.\n",
    "\n",
    "    :param load_path: File path to load the SHAP values from.\n",
    "    :param logger: Logger instance.\n",
    "    :return: Loaded SHAP values object.\n",
    "    \"\"\"\n",
    "    if logger:\n",
    "        logger.info(f\"Loading SHAP values from {load_path}...\")\n",
    "    try:\n",
    "        with open(load_path, \"rb\") as f:\n",
    "            shap_values = pickle.load(f)\n",
    "        if logger:\n",
    "            logger.info(f\"SHAP values loaded successfully from {load_path}.\")\n",
    "    except Exception as e:\n",
    "        if logger:\n",
    "            logger.error(f\"Failed to load SHAP values: {e}\")\n",
    "        raise\n",
    "    return shap_values\n",
    "\n",
    "def get_shap_row(shap_values, df: pd.DataFrame, trial_id: Any, logger: Optional[logging.Logger] = None):\n",
    "    \"\"\"\n",
    "    Retrieve the SHAP values by converting a string-based trial ID\n",
    "    to its integer position (row index) in the DataFrame.\n",
    "\n",
    "    :param shap_values: The array-like SHAP values (often shap.Explanation or np.ndarray).\n",
    "    :param df: A pandas DataFrame indexed by trial IDs.\n",
    "    :param trial_id: The ID (string or other) we want to map to a row position.\n",
    "    :param logger: Optional logger for debug or warning messages.\n",
    "    :return: The 1D array of SHAP values for the specified row, or None if not found.\n",
    "    \"\"\"\n",
    "    if trial_id not in df.index:\n",
    "        if logger:\n",
    "            logger.warning(f\"Trial ID '{trial_id}' not found in df.index: {df.index.tolist()}\")\n",
    "        return None\n",
    "\n",
    "    # Convert the string-based index (e.g., \"T0123\") to its integer position\n",
    "    pos = df.index.get_loc(trial_id)\n",
    "    if logger and logger.isEnabledFor(logging.DEBUG):\n",
    "        logger.debug(f\"Mapped trial ID '{trial_id}' to position {pos} in the DataFrame.\")\n",
    "\n",
    "    return shap_values[pos]\n",
    "\n",
    "\n",
    "def plot_shap_force(shap_explainer, shap_values, X_original, trial_index, \n",
    "                    save_path: Path, debug: bool = False, logger: Optional[logging.Logger] = None):\n",
    "    try:\n",
    "        # Retrieve the correct SHAP row and trial features\n",
    "        if isinstance(trial_index, int):\n",
    "            shap_value = shap_values[trial_index]\n",
    "            trial_features = X_original.iloc[[trial_index]]  # Pass as DataFrame (1-row)\n",
    "            if logger and debug:\n",
    "                logger.debug(f\"Using integer index {trial_index} for force plot.\")\n",
    "        else:\n",
    "            shap_value = get_shap_row(shap_values, X_original, trial_index, logger=logger)\n",
    "            if shap_value is None:\n",
    "                if logger:\n",
    "                    logger.warning(f\"SHAP row not found for trial '{trial_index}'.\")\n",
    "                return\n",
    "            trial_features = X_original.loc[[trial_index]]  # keep as DataFrame\n",
    "\n",
    "        # Determine the expected value and ensure it is a scalar (or select the first element if iterable)\n",
    "        if hasattr(shap_explainer.expected_value, '__iter__'):\n",
    "            base_value = shap_explainer.expected_value[0]\n",
    "            if logger and debug:\n",
    "                logger.debug(f\"Expected value is iterable; using {base_value}.\")\n",
    "        else:\n",
    "            base_value = shap_explainer.expected_value\n",
    "            if logger and debug:\n",
    "                logger.debug(f\"Expected value is scalar: {base_value}\")\n",
    "\n",
    "        # **Key change:** Use matplotlib=False so that the returned object is an interactive Visualizer\n",
    "        shap_plot = shap.force_plot(\n",
    "            base_value, \n",
    "            shap_value, \n",
    "            trial_features,\n",
    "            matplotlib=False  # Ensures an interactive (HTML/JS) plot is returned\n",
    "        )\n",
    "        \n",
    "        # Save the interactive plot as an HTML file.\n",
    "        # (It’s a best practice to convert save_path to string)\n",
    "        shap.save_html(str(save_path), shap_plot)\n",
    "        if logger and debug:\n",
    "            logger.debug(f\"Interactive SHAP force plot saved to {save_path}.\")\n",
    "        if logger:\n",
    "            logger.info(f\"✅ Interactive SHAP force plot generated for trial {trial_index}.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        if logger:\n",
    "            logger.error(f\"❌ Failed to generate SHAP force plot for trial {trial_index}: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# Below is an updated version of shap_utils.py that includes additional functionality.\n",
    "# This functionality helps compute a percentile-based error limit (unit_change_error_limit)\n",
    "# for each feature (metric), then uses that limit alongside the computed shap_unit_change_{metric}\n",
    "# and shap_direction_{metric} to determine a trial-based feedback label such as \"early\", \"good\",\n",
    "# or \"late\".\n",
    "#\n",
    "# The new function introduced at the end is `compute_feedback_with_thresholds`. Inside it, we:\n",
    "# 1) Collect a data distribution for each metric from the entire dataset (or a relevant subset).\n",
    "# 2) Compute a chosen percentile (like the 90th percentile) for each metric to determine an error limit.\n",
    "# 3) Retrieve shap_unit_change_{metric} and shap_direction_{metric} from the feedback.\n",
    "#    - If \"increase\" -> the \"goal\" is current_value + shap_unit_change.\n",
    "#    - If \"decrease\" -> the \"goal\" is current_value - shap_unit_change.\n",
    "# 4) Compare the difference (or ratio) between the goal and the actual distribution of that metric to see\n",
    "#    if the difference is within or beyond the error limit.\n",
    "# 5) Mark trial-based feedback as \"early\", \"good\", or \"late\" accordingly.\n",
    "#\n",
    "# NOTE: This is sample logic and can be adapted to match your exact domain definitions.\n",
    "#       Additional logging or debugging can be added as needed.\n",
    "\n",
    "import logging\n",
    "import shap\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Other functions omitted for brevity...\n",
    "\n",
    "###########################################\n",
    "# NEW FUNCTION: compute_feedback_with_thresholds\n",
    "###########################################\n",
    "\n",
    "def compute_feedback_with_thresholds(\n",
    "    df: pd.DataFrame,\n",
    "    features: List[str],\n",
    "    percentile: float = 90,\n",
    "    logger: Optional[logging.Logger] = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function demonstrates how to add percentile-based thresholds for each metric.\n",
    "\n",
    "    Steps:\n",
    "    1) For each metric in 'features', calculate the chosen percentile (e.g. 90th) across all trials.\n",
    "       This is our 'unit_change_error_limit' for that metric.\n",
    "    2) We look for the shap_direction_{metric} and shap_unit_change_{metric} columns in df.\n",
    "       If they say \"increase\" or \"decrease\", we can compute the 'goal' metric as:\n",
    "         - If direction is 'increase':  goal = actual_value + shap_unit_change (numerical interpretation)\n",
    "         - If direction is 'decrease':  goal = actual_value - shap_unit_change\n",
    "    3) Compare the difference between this 'goal' and the actual_value to see if it is within\n",
    "       the 'unit_change_error_limit'. Then assign feedback: 'early', 'good', or 'late'.\n",
    "       (The user can define the exact rules for these categories. We'll show an example.)\n",
    "\n",
    "    Returns:\n",
    "       The same DataFrame 'df', but with new columns:\n",
    "         - {feature}_threshold\n",
    "         - shap_feedback_{feature}\n",
    "       which store the threshold used and the final feedback label.\n",
    "    \"\"\"\n",
    "    df_out = df.copy()\n",
    "\n",
    "    # 1) Compute the percentile for each feature across the dataset.\n",
    "    thresholds = {}\n",
    "    for metric in features:\n",
    "        if metric not in df_out.columns:\n",
    "            if logger:\n",
    "                logger.warning(f\"Metric '{metric}' not found in df columns, skipping.\")\n",
    "            continue\n",
    "        threshold_value = np.percentile(df_out[metric].dropna(), percentile)\n",
    "        thresholds[metric] = threshold_value\n",
    "        if logger:\n",
    "            logger.info(f\"{percentile}th percentile for '{metric}' is {threshold_value:.3f}.\")\n",
    "\n",
    "    # 2) For each metric, retrieve direction & unit_change columns if they exist.\n",
    "    for metric in features:\n",
    "        dir_col = f\"shap_direction_{metric}\"\n",
    "        uc_col = f\"shap_unit_change_{metric}\"\n",
    "\n",
    "        threshold_col = f\"{metric}_threshold\"\n",
    "        feedback_col = f\"shap_feedback_{metric}\"\n",
    "\n",
    "        # We'll store the threshold for reference.\n",
    "        if metric in thresholds:\n",
    "            df_out[threshold_col] = thresholds[metric]\n",
    "        else:\n",
    "            df_out[threshold_col] = np.nan\n",
    "\n",
    "        # If direction or unit change columns are absent, skip.\n",
    "        if dir_col not in df_out.columns or uc_col not in df_out.columns:\n",
    "            if logger:\n",
    "                logger.debug(f\"Either {dir_col} or {uc_col} not found in df columns. Skipping feedback.\")\n",
    "            df_out[feedback_col] = \"No feedback\"\n",
    "            continue\n",
    "\n",
    "        # We'll parse the numeric portion from shap_unit_change_{metric} because that might be something like '0.45 meters'.\n",
    "        # Let's define a helper to parse the numeric part.\n",
    "        def parse_value_with_unit(value_str: Any) -> float:\n",
    "            if isinstance(value_str, (int, float)):\n",
    "                return float(value_str)\n",
    "            try:\n",
    "                # example: \"0.45 meters\", split by space, parse first item\n",
    "                parts = str(value_str).split()\n",
    "                return float(parts[0])\n",
    "            except:\n",
    "                return 0.0\n",
    "\n",
    "        # Now define an inline function to compute feedback row by row.\n",
    "        def compute_row_feedback(row):\n",
    "            # If we have no actual metric col in the row, skip.\n",
    "            if pd.isnull(row.get(metric, np.nan)):\n",
    "                return \"No actual metric\"\n",
    "\n",
    "            actual_val = row[metric]\n",
    "            direction = row[dir_col]\n",
    "            # shap_unit_change_{metric} might be numeric or str with unit, parse.\n",
    "            shap_delta = parse_value_with_unit(row[uc_col])\n",
    "\n",
    "            # compute \"goal\"\n",
    "            if direction == \"increase\":\n",
    "                goal_val = actual_val + shap_delta\n",
    "            elif direction == \"decrease\":\n",
    "                goal_val = actual_val - shap_delta\n",
    "            else:\n",
    "                # e.g. 'no feedback available' or something else.\n",
    "                return \"No direction\"\n",
    "\n",
    "            # difference from goal\n",
    "            diff = abs(goal_val - actual_val)\n",
    "\n",
    "            # compare with threshold\n",
    "            limit = thresholds.get(metric, np.nan)\n",
    "            if pd.isnull(limit) or limit == 0:\n",
    "                # fallback if threshold not found.\n",
    "                return \"No threshold\"\n",
    "\n",
    "            # Define rules based on percentiles:\n",
    "            if diff <= 0.05 * limit:\n",
    "                return \"good\"\n",
    "            elif diff > 0.05 * limit and goal_val > actual_val:\n",
    "                return \"early\"\n",
    "            elif diff > 0.05 * limit and goal_val < actual_val:\n",
    "                return \"late\"\n",
    "            else:\n",
    "                return \"No feedback\"\n",
    "\n",
    "        df_out[feedback_col] = df_out.apply(compute_row_feedback, axis=1)\n",
    "\n",
    "    return df_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Config Loader] ✅ Successfully loaded configuration from ..\\..\\data\\model\\preprocessor_config\\preprocessor_config.yaml\n",
      "Configuration loaded successfully from ..\\..\\data\\model\\preprocessor_config\\preprocessor_config.yaml.\n",
      "2025-02-06 22:19:20,421 - __main__ - INFO - Starting prediction module (unified predict_and_shap).\n",
      "Columns in input data: ['result', 'landing_x', 'landing_y', 'entry_angle', 'L_ANKLE_min_power', 'L_ANKLE_max_power', 'L_ANKLE_avg_power', 'L_ANKLE_std_power', 'R_ANKLE_min_power', 'R_ANKLE_max_power', 'R_ANKLE_avg_power', 'R_ANKLE_std_power', 'L_KNEE_min_power', 'L_KNEE_max_power', 'L_KNEE_avg_power', 'L_KNEE_std_power', 'R_KNEE_min_power', 'R_KNEE_max_power', 'R_KNEE_avg_power', 'R_KNEE_std_power', 'L_HIP_min_power', 'L_HIP_max_power', 'L_HIP_avg_power', 'L_HIP_std_power', 'R_HIP_min_power', 'R_HIP_max_power', 'R_HIP_avg_power', 'R_HIP_std_power', 'L_ELBOW_min_power', 'L_ELBOW_max_power', 'L_ELBOW_avg_power', 'L_ELBOW_std_power', 'R_ELBOW_min_power', 'R_ELBOW_max_power', 'R_ELBOW_avg_power', 'R_ELBOW_std_power', 'L_WRIST_min_power', 'L_WRIST_max_power', 'L_WRIST_avg_power', 'L_WRIST_std_power', 'R_WRIST_min_power', 'R_WRIST_max_power', 'R_WRIST_avg_power', 'R_WRIST_std_power', 'L_1STFINGER_min_power', 'L_1STFINGER_max_power', 'L_1STFINGER_avg_power', 'L_1STFINGER_std_power', 'L_5THFINGER_min_power', 'L_5THFINGER_max_power', 'L_5THFINGER_avg_power', 'L_5THFINGER_std_power', 'R_1STFINGER_min_power', 'R_1STFINGER_max_power', 'R_1STFINGER_avg_power', 'R_1STFINGER_std_power', 'R_5THFINGER_min_power', 'R_5THFINGER_max_power', 'R_5THFINGER_avg_power', 'R_5THFINGER_std_power', 'elbow_max_angle', 'elbow_release_angle', 'wrist_max_angle', 'wrist_release_angle', 'knee_max_angle', 'knee_release_angle', 'release_ball_speed', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'release_ball_direction_x', 'release_ball_direction_y', 'release_ball_direction_z', 'release_ball_x', 'release_ball_y', 'release_ball_z', 'release_frame_time', 'release_angle', 'time_to_peak', 'peak_height_relative', 'player_participant_id', 'player_height_in_meters', 'player_weight__in_kg', 'player_dominant_hand', 'player_estimated_wingspan_cm', 'player_estimated_standing_reach_cm', 'player_estimated_hand_length_cm', 'trial_id', 'shot_id', 'initial_release_angle', 'calculated_release_angle', 'optimal_release_angle', 'angle_difference', 'L_ANKLE_energy_mean', 'R_ANKLE_energy_mean', 'L_KNEE_energy_mean', 'R_KNEE_energy_mean', 'L_HIP_energy_mean', 'R_HIP_energy_mean', 'L_ELBOW_energy_mean', 'R_ELBOW_energy_mean', 'L_WRIST_energy_mean', 'R_WRIST_energy_mean', 'L_1STFINGER_energy_mean', 'R_1STFINGER_energy_mean', 'L_5THFINGER_energy_mean', 'R_5THFINGER_energy_mean', 'L_ANKLE_energy_max', 'R_ANKLE_energy_max', 'L_KNEE_energy_max', 'R_KNEE_energy_max', 'L_HIP_energy_max', 'R_HIP_energy_max', 'L_ELBOW_energy_max', 'R_ELBOW_energy_max', 'L_WRIST_energy_max', 'R_WRIST_energy_max', 'L_1STFINGER_energy_max', 'R_1STFINGER_energy_max', 'L_5THFINGER_energy_max', 'R_5THFINGER_energy_max', 'L_ANKLE_energy_std', 'R_ANKLE_energy_std', 'L_KNEE_energy_std', 'R_KNEE_energy_std', 'L_HIP_energy_std', 'R_HIP_energy_std', 'L_ELBOW_energy_std', 'R_ELBOW_energy_std', 'L_WRIST_energy_std', 'R_WRIST_energy_std', 'L_1STFINGER_energy_std', 'R_1STFINGER_energy_std', 'L_5THFINGER_energy_std', 'R_5THFINGER_energy_std', 'player_height_in_meters_category', 'player_weight__in_kg_category', 'player_estimated_wingspan_cm_category', 'player_estimated_standing_reach_cm_category', 'player_estimated_hand_length_cm_category']\n",
      "2025-02-06 22:19:20,427 - __main__ - INFO - Prediction input data loaded from C:\\Users\\ghadf\\vscode_projects\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\data\\preprocessor\\..\\..\\data\\processed\\final_ml_dataset.csv.\n",
      "2025-02-06 22:19:20,428 - __main__ - INFO - Best model identified: XGBoost\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-06 22:19:20,430 [INFO] Starting: Final Preprocessing Pipeline in 'predict' mode.\n",
      "2025-02-06 22:19:20,430 - INFO - Starting: Final Preprocessing Pipeline in 'predict' mode.\n",
      "2025-02-06 22:19:20,430 [INFO] Step: filter_columns\n",
      "2025-02-06 22:19:20,430 - INFO - Step: filter_columns\n",
      "2025-02-06 22:19:20,432 [INFO] ✅ Filtered DataFrame to include only specified features. Shape: (125, 14)\n",
      "2025-02-06 22:19:20,432 - INFO - ✅ Filtered DataFrame to include only specified features. Shape: (125, 14)\n",
      "2025-02-06 22:19:20,432 [DEBUG] Selected Features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z']\n",
      "2025-02-06 22:19:20,432 - DEBUG - Selected Features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z']\n",
      "2025-02-06 22:19:20,433 [INFO] ✅ Column filtering completed successfully.\n",
      "2025-02-06 22:19:20,433 - INFO - ✅ Column filtering completed successfully.\n",
      "2025-02-06 22:19:20,434 [INFO] Step: Preprocess Predict\n",
      "2025-02-06 22:19:20,434 - INFO - Step: Preprocess Predict\n",
      "2025-02-06 22:19:20,435 [DEBUG] Initial columns in prediction data: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z']\n",
      "2025-02-06 22:19:20,435 - DEBUG - Initial columns in prediction data: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z']\n",
      "2025-02-06 22:19:20,436 [DEBUG] Initial number of features: 14\n",
      "2025-02-06 22:19:20,436 - DEBUG - Initial number of features: 14\n",
      "2025-02-06 22:19:20,436 [INFO] Step: Load Transformers\n",
      "2025-02-06 22:19:20,436 - INFO - Step: Load Transformers\n",
      "2025-02-06 22:19:20,437 [DEBUG] Loading transformers from: C:\\Users\\ghadf\\vscode_projects\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\data\\preprocessor\\transformers\\transformers.pkl\n",
      "2025-02-06 22:19:20,437 - DEBUG - Loading transformers from: C:\\Users\\ghadf\\vscode_projects\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\data\\preprocessor\\transformers\\transformers.pkl\n",
      "2025-02-06 22:19:20,440 [DEBUG] Pipeline loaded. Ready to transform new data.\n",
      "2025-02-06 22:19:20,440 - DEBUG - Pipeline loaded. Ready to transform new data.\n",
      "2025-02-06 22:19:20,441 [INFO] Transformers loaded successfully from 'C:\\Users\\ghadf\\vscode_projects\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\data\\preprocessor\\transformers\\transformers.pkl'.\n",
      "2025-02-06 22:19:20,441 - INFO - Transformers loaded successfully from 'C:\\Users\\ghadf\\vscode_projects\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\data\\preprocessor\\transformers\\transformers.pkl'.\n",
      "2025-02-06 22:19:20,442 [DEBUG] Transformers loaded successfully.\n",
      "2025-02-06 22:19:20,442 - DEBUG - Transformers loaded successfully.\n",
      "2025-02-06 22:19:20,443 [INFO] Step: filter_columns\n",
      "2025-02-06 22:19:20,443 - INFO - Step: filter_columns\n",
      "2025-02-06 22:19:20,444 [INFO] ✅ Filtered DataFrame to include only specified features. Shape: (125, 14)\n",
      "2025-02-06 22:19:20,444 - INFO - ✅ Filtered DataFrame to include only specified features. Shape: (125, 14)\n",
      "2025-02-06 22:19:20,445 [DEBUG] Selected Features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z']\n",
      "2025-02-06 22:19:20,445 - DEBUG - Selected Features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z']\n",
      "2025-02-06 22:19:20,446 [DEBUG] Columns after filtering: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z']\n",
      "2025-02-06 22:19:20,446 - DEBUG - Columns after filtering: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z']\n",
      "2025-02-06 22:19:20,447 [DEBUG] Number of features after filtering: 14\n",
      "2025-02-06 22:19:20,447 - DEBUG - Number of features after filtering: 14\n",
      "2025-02-06 22:19:20,448 [INFO] Step: Handle Missing Values\n",
      "2025-02-06 22:19:20,448 - INFO - Step: Handle Missing Values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Features loaded from C:\\Users\\ghadf\\vscode_projects\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\data\\preprocessor\\features_info\\final_ml_df_selected_features_columns.pkl\n",
      "✅ Ordinal categoricals loaded from ..\\..\\data\\preprocessor\\features_info\\ordinal_categoricals.pkl\n",
      "✅ Nominal categoricals loaded from ..\\..\\data\\preprocessor\\features_info\\nominal_categoricals.pkl\n",
      "✅ Numericals loaded from ..\\..\\data\\preprocessor\\features_info\\numericals.pkl\n",
      "✅ Y variable loaded from ..\\..\\data\\preprocessor\\features_info\\y_variable.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-06 22:19:20,534 [DEBUG] Columns after handling missing values: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z']\n",
      "2025-02-06 22:19:20,534 - DEBUG - Columns after handling missing values: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z']\n",
      "2025-02-06 22:19:20,535 [DEBUG] Number of features after handling missing values: 14\n",
      "2025-02-06 22:19:20,535 - DEBUG - Number of features after handling missing values: 14\n",
      "2025-02-06 22:19:20,536 [DEBUG] Expected raw features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z']\n",
      "2025-02-06 22:19:20,536 - DEBUG - Expected raw features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z']\n",
      "2025-02-06 22:19:20,536 [DEBUG] Provided features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z']\n",
      "2025-02-06 22:19:20,536 - DEBUG - Provided features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z']\n",
      "2025-02-06 22:19:20,539 [DEBUG] Reordered columns to match the pipeline's raw feature expectations.\n",
      "2025-02-06 22:19:20,539 - DEBUG - Reordered columns to match the pipeline's raw feature expectations.\n",
      "2025-02-06 22:19:20,541 [DEBUG] Transformed data shape: (125, 14)\n",
      "2025-02-06 22:19:20,541 - DEBUG - Transformed data shape: (125, 14)\n",
      "2025-02-06 22:19:20,542 [DEBUG] Derived feature names from pipeline: ['num__release_ball_direction_x', 'num__release_ball_direction_z', 'num__release_ball_direction_y', 'num__elbow_release_angle', 'num__elbow_max_angle', 'num__wrist_release_angle', 'num__wrist_max_angle', 'num__knee_release_angle', 'num__knee_max_angle', 'num__release_ball_speed', 'num__calculated_release_angle', 'num__release_ball_velocity_x', 'num__release_ball_velocity_y', 'num__release_ball_velocity_z']\n",
      "2025-02-06 22:19:20,542 - DEBUG - Derived feature names from pipeline: ['num__release_ball_direction_x', 'num__release_ball_direction_z', 'num__release_ball_direction_y', 'num__elbow_release_angle', 'num__elbow_max_angle', 'num__wrist_release_angle', 'num__wrist_max_angle', 'num__knee_release_angle', 'num__knee_max_angle', 'num__release_ball_speed', 'num__calculated_release_angle', 'num__release_ball_velocity_x', 'num__release_ball_velocity_y', 'num__release_ball_velocity_z']\n",
      "2025-02-06 22:19:20,543 [DEBUG] X_preprocessed_df columns: ['num__release_ball_direction_x', 'num__release_ball_direction_z', 'num__release_ball_direction_y', 'num__elbow_release_angle', 'num__elbow_max_angle', 'num__wrist_release_angle', 'num__wrist_max_angle', 'num__knee_release_angle', 'num__knee_max_angle', 'num__release_ball_speed', 'num__calculated_release_angle', 'num__release_ball_velocity_x', 'num__release_ball_velocity_y', 'num__release_ball_velocity_z']\n",
      "2025-02-06 22:19:20,543 - DEBUG - X_preprocessed_df columns: ['num__release_ball_direction_x', 'num__release_ball_direction_z', 'num__release_ball_direction_y', 'num__elbow_release_angle', 'num__elbow_max_angle', 'num__wrist_release_angle', 'num__wrist_max_angle', 'num__knee_release_angle', 'num__knee_max_angle', 'num__release_ball_speed', 'num__calculated_release_angle', 'num__release_ball_velocity_x', 'num__release_ball_velocity_y', 'num__release_ball_velocity_z']\n",
      "2025-02-06 22:19:20,547 [DEBUG] Sample of X_preprocessed_df:\n",
      "   num__release_ball_direction_x  num__release_ball_direction_z  \\\n",
      "0                       0.256808                      -0.014333   \n",
      "1                       0.653985                      -0.770471   \n",
      "2                       0.316179                      -0.134154   \n",
      "3                      -0.573663                       0.925278   \n",
      "4                       0.382947                      -0.241799   \n",
      "\n",
      "   num__release_ball_direction_y  num__elbow_release_angle  \\\n",
      "0                       1.145467                  1.044847   \n",
      "1                      -0.889734                 -1.374648   \n",
      "2                       0.190965                  1.316104   \n",
      "3                       1.149799                  0.887968   \n",
      "4                       0.013044                  1.270071   \n",
      "\n",
      "   num__elbow_max_angle  num__wrist_release_angle  num__wrist_max_angle  \\\n",
      "0              1.095488                  0.424441             -0.778748   \n",
      "1             -1.250958                  1.913313              0.102943   \n",
      "2              1.471855                 -0.176303              0.199613   \n",
      "3             -0.011954                 -1.316314             -0.640673   \n",
      "4             -0.184445                 -1.602153             -1.219041   \n",
      "\n",
      "   num__knee_release_angle  num__knee_max_angle  num__release_ball_speed  \\\n",
      "0                 0.668492             0.797754                -0.199124   \n",
      "1                 1.092750             1.846483                 0.922120   \n",
      "2                 0.991403             1.686793                 0.020369   \n",
      "3                -0.430889             1.373530                -0.633392   \n",
      "4                 0.743514             1.121268                 0.505381   \n",
      "\n",
      "   num__calculated_release_angle  num__release_ball_velocity_x  \\\n",
      "0                       0.353184                     -0.010464   \n",
      "1                       1.052441                      0.777058   \n",
      "2                      -0.568742                      0.123024   \n",
      "3                       0.513443                     -0.702970   \n",
      "4                      -0.364409                      0.380674   \n",
      "\n",
      "   num__release_ball_velocity_y  num__release_ball_velocity_z  \n",
      "0                      1.217131                     -0.207739  \n",
      "1                     -1.333284                      0.940890  \n",
      "2                      0.174788                      0.029956  \n",
      "3                      1.217131                     -0.525123  \n",
      "4                     -0.115226                      0.590530  \n",
      "2025-02-06 22:19:20,547 - DEBUG - Sample of X_preprocessed_df:\n",
      "   num__release_ball_direction_x  num__release_ball_direction_z  \\\n",
      "0                       0.256808                      -0.014333   \n",
      "1                       0.653985                      -0.770471   \n",
      "2                       0.316179                      -0.134154   \n",
      "3                      -0.573663                       0.925278   \n",
      "4                       0.382947                      -0.241799   \n",
      "\n",
      "   num__release_ball_direction_y  num__elbow_release_angle  \\\n",
      "0                       1.145467                  1.044847   \n",
      "1                      -0.889734                 -1.374648   \n",
      "2                       0.190965                  1.316104   \n",
      "3                       1.149799                  0.887968   \n",
      "4                       0.013044                  1.270071   \n",
      "\n",
      "   num__elbow_max_angle  num__wrist_release_angle  num__wrist_max_angle  \\\n",
      "0              1.095488                  0.424441             -0.778748   \n",
      "1             -1.250958                  1.913313              0.102943   \n",
      "2              1.471855                 -0.176303              0.199613   \n",
      "3             -0.011954                 -1.316314             -0.640673   \n",
      "4             -0.184445                 -1.602153             -1.219041   \n",
      "\n",
      "   num__knee_release_angle  num__knee_max_angle  num__release_ball_speed  \\\n",
      "0                 0.668492             0.797754                -0.199124   \n",
      "1                 1.092750             1.846483                 0.922120   \n",
      "2                 0.991403             1.686793                 0.020369   \n",
      "3                -0.430889             1.373530                -0.633392   \n",
      "4                 0.743514             1.121268                 0.505381   \n",
      "\n",
      "   num__calculated_release_angle  num__release_ball_velocity_x  \\\n",
      "0                       0.353184                     -0.010464   \n",
      "1                       1.052441                      0.777058   \n",
      "2                      -0.568742                      0.123024   \n",
      "3                       0.513443                     -0.702970   \n",
      "4                      -0.364409                      0.380674   \n",
      "\n",
      "   num__release_ball_velocity_y  num__release_ball_velocity_z  \n",
      "0                      1.217131                     -0.207739  \n",
      "1                     -1.333284                      0.940890  \n",
      "2                      0.174788                      0.029956  \n",
      "3                      1.217131                     -0.525123  \n",
      "4                     -0.115226                      0.590530  \n",
      "2025-02-06 22:19:20,548 [DEBUG] [DEBUG] Original data shape before inverse transform: (125, 14)\n",
      "2025-02-06 22:19:20,548 - DEBUG - [DEBUG] Original data shape before inverse transform: (125, 14)\n",
      "2025-02-06 22:19:20,548 - DEBUG - [DEBUG Inverse] Starting inverse transformation. Input shape: (125, 14)\n",
      "2025-02-06 22:19:20,549 - DEBUG - [DEBUG Inverse] pipeline.transformers_: ['num']\n",
      "2025-02-06 22:19:20,549 - DEBUG - [DEBUG Inverse] Transformer 'num' has features ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z'], slicing X_transformed from 0 to 14 => shape (125, 14)\n",
      "2025-02-06 22:19:20,549 - DEBUG - [DEBUG Inverse] Found scaler for numeric features ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z']. Inverting...\n",
      "2025-02-06 22:19:20,551 - DEBUG - [DEBUG Inverse] Inverse DataFrame shape: (125, 14)\n",
      "2025-02-06 22:19:20,554 - DEBUG - [DEBUG Inverse] Sample:\n",
      "   release_ball_direction_x  release_ball_direction_z  \\\n",
      "0                  0.377012                  0.926203   \n",
      "1                  0.417644                  0.901906   \n",
      "2                  0.383086                  0.922353   \n",
      "3                  0.292054                  0.956396   \n",
      "4                  0.389917                  0.918894   \n",
      "\n",
      "   release_ball_direction_y  elbow_release_angle  elbow_max_angle  \\\n",
      "0                  0.002969            72.325830       106.272118   \n",
      "1                 -0.110176            58.430068       101.798798   \n",
      "2                 -0.050096            73.883728       106.989633   \n",
      "3                  0.003209            71.424835       104.160865   \n",
      "4                 -0.059987            73.619348       103.832024   \n",
      "\n",
      "   wrist_release_angle  wrist_max_angle  knee_release_angle  knee_max_angle  \\\n",
      "0            28.102765        35.918406           32.646276       63.541007   \n",
      "1            32.766626        38.693790           33.834026       65.565635   \n",
      "2            26.220943        38.998089           33.550293       65.257346   \n",
      "3            22.649881        36.353038           29.568453       64.652573   \n",
      "4            21.754498        34.532455           32.856306       64.165568   \n",
      "\n",
      "   release_ball_speed  calculated_release_angle  release_ball_velocity_x  \\\n",
      "0            9.907618                 62.959206                 3.735294   \n",
      "1           11.826803                 64.964999                 4.939394   \n",
      "2           10.283314                 60.314694                 3.939394   \n",
      "3            9.164302                 63.418903                 2.676471   \n",
      "4           11.113489                 60.900817                 4.333333   \n",
      "\n",
      "   release_ball_velocity_y  release_ball_velocity_z  \n",
      "0                 0.029412                 9.176471  \n",
      "1                -1.303030                10.666667  \n",
      "2                -0.515152                 9.484848  \n",
      "3                 0.029412                 8.764706  \n",
      "4                -0.666667                10.212121  \n",
      "2025-02-06 22:19:20,554 [DEBUG] [DEBUG] Inversed data shape: (125, 14)\n",
      "2025-02-06 22:19:20,554 - DEBUG - [DEBUG] Inversed data shape: (125, 14)\n",
      "2025-02-06 22:19:20,555 [INFO] Step: Generate Preprocessor Recommendations\n",
      "2025-02-06 22:19:20,555 - INFO - Step: Generate Preprocessor Recommendations\n",
      "2025-02-06 22:19:20,556 [INFO] Preprocessing Recommendations generated.\n",
      "2025-02-06 22:19:20,556 - INFO - Preprocessing Recommendations generated.\n",
      "2025-02-06 22:19:20,557 [INFO] Step 'Generate Preprocessor Recommendations' completed: Recommendations generated.\n",
      "2025-02-06 22:19:20,557 - INFO - Step 'Generate Preprocessor Recommendations' completed: Recommendations generated.\n",
      "2025-02-06 22:19:20,557 [DEBUG] Generated preprocessing recommendations.\n",
      "2025-02-06 22:19:20,557 - DEBUG - Generated preprocessing recommendations.\n",
      "2025-02-06 22:19:20,558 [INFO] ✅ Preprocessing completed successfully in predict mode.\n",
      "2025-02-06 22:19:20,558 - INFO - ✅ Preprocessing completed successfully in predict mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-06 22:19:20,559 - __main__ - INFO - Preprocessing completed successfully in predict mode.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-06 22:19:20,572 - INFO - Model loaded from C:\\Users\\ghadf\\vscode_projects\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\data\\model\\XGBoost_model.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial IDs are unique.\n",
      "2025-02-06 22:19:20,573 - __main__ - INFO - Trained model loaded from 'C:\\Users\\ghadf\\vscode_projects\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\data\\model\\XGBoost\\trained_model.pkl'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-06 22:19:20,580 - PredictAndAttachLogger - INFO - ✅ Predictions made successfully.\n",
      "2025-02-06 22:19:20,580 - PredictAndAttachLogger - INFO - ✅ Predictions made successfully.\n",
      "2025-02-06 22:19:20,580 - PredictAndAttachLogger - INFO - ✅ Predictions made successfully.\n",
      "2025-02-06 22:19:20,580 - INFO - ✅ Predictions made successfully.\n",
      "2025-02-06 22:19:20,583 - DEBUG - Predictions sample: [0 1 0 0 1]\n",
      "2025-02-06 22:19:20,589 - PredictAndAttachLogger - INFO - ✅ Prediction probabilities computed successfully.\n",
      "2025-02-06 22:19:20,589 - PredictAndAttachLogger - INFO - ✅ Prediction probabilities computed successfully.\n",
      "2025-02-06 22:19:20,589 - PredictAndAttachLogger - INFO - ✅ Prediction probabilities computed successfully.\n",
      "2025-02-06 22:19:20,589 - INFO - ✅ Prediction probabilities computed successfully.\n",
      "2025-02-06 22:19:20,592 - DEBUG - Prediction probabilities sample:\n",
      "[[0.9108718  0.08912821]\n",
      " [0.08208466 0.91791534]]\n",
      "2025-02-06 22:19:20,593 - DEBUG - Predictions attached to inverse-transformed DataFrame.\n",
      "2025-02-06 22:19:20,594 - DEBUG - Prediction probabilities attached to inverse-transformed DataFrame.\n",
      "2025-02-06 22:19:20,595 - DEBUG - Attached column: Probability_0\n",
      "2025-02-06 22:19:20,596 - DEBUG - Attached column: Probability_1\n",
      "2025-02-06 22:19:20,597 - DEBUG - Final shape of X_inversed: (125, 17)\n",
      "2025-02-06 22:19:20,599 - DEBUG - Final columns in X_inversed: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'elbow_release_angle', 'elbow_max_angle', 'wrist_release_angle', 'wrist_max_angle', 'knee_release_angle', 'knee_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'Prediction', 'Probability_0', 'Probability_1']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-06 22:19:20,600 - __main__ - INFO - Predictions generated and attached to the dataset.\n",
      "2025-02-06 22:19:20,602 - __main__ - INFO - Using 'trial_id' as the index for both preprocessed and inverse-transformed data.\n",
      "2025-02-06 22:19:20,602 - __main__ - INFO - Initializing SHAP explainer...\n",
      "2025-02-06 22:19:20,849 - __main__ - INFO - SHAP values computed successfully.\n",
      "2025-02-06 22:19:20,850 - __main__ - INFO - Generating SHAP summary plot...\n",
      "2025-02-06 22:19:21,113 - __main__ - INFO - SHAP summary plot generated successfully.\n",
      "2025-02-06 22:19:21,114 - __main__ - INFO - SHAP summary plot saved at C:\\Users\\ghadf\\vscode_projects\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\data\\predictions\\shap_results\\shap_summary.png.\n",
      "2025-02-06 22:19:21,114 - __main__ - INFO - Generating feature importance based on SHAP values...\n",
      "2025-02-06 22:19:21,127 - __main__ - INFO - Generating SHAP dependence plot for feature 'num__release_ball_direction_y'...\n",
      "2025-02-06 22:19:21,265 - __main__ - INFO - SHAP dependence plot for feature 'num__release_ball_direction_y' generated successfully.\n",
      "2025-02-06 22:19:21,265 - __main__ - INFO - Generating SHAP dependence plot for feature 'num__elbow_max_angle'...\n",
      "2025-02-06 22:19:21,395 - __main__ - INFO - SHAP dependence plot for feature 'num__elbow_max_angle' generated successfully.\n",
      "2025-02-06 22:19:21,395 - __main__ - INFO - Generating SHAP dependence plot for feature 'num__wrist_release_angle'...\n",
      "2025-02-06 22:19:21,521 - __main__ - INFO - SHAP dependence plot for feature 'num__wrist_release_angle' generated successfully.\n",
      "2025-02-06 22:19:21,522 - __main__ - INFO - Generating SHAP dependence plot for feature 'num__elbow_release_angle'...\n",
      "2025-02-06 22:19:21,652 - __main__ - INFO - SHAP dependence plot for feature 'num__elbow_release_angle' generated successfully.\n",
      "2025-02-06 22:19:21,652 - __main__ - INFO - Generating SHAP dependence plot for feature 'num__knee_max_angle'...\n",
      "2025-02-06 22:19:21,795 - __main__ - INFO - SHAP dependence plot for feature 'num__knee_max_angle' generated successfully.\n",
      "2025-02-06 22:19:21,796 - __main__ - INFO - Generating SHAP dependence plot for feature 'num__wrist_max_angle'...\n",
      "2025-02-06 22:19:21,934 - __main__ - INFO - SHAP dependence plot for feature 'num__wrist_max_angle' generated successfully.\n",
      "2025-02-06 22:19:21,934 - __main__ - INFO - Generating SHAP dependence plot for feature 'num__calculated_release_angle'...\n",
      "2025-02-06 22:19:22,069 - __main__ - INFO - SHAP dependence plot for feature 'num__calculated_release_angle' generated successfully.\n",
      "2025-02-06 22:19:22,070 - __main__ - INFO - Generating SHAP dependence plot for feature 'num__release_ball_speed'...\n",
      "2025-02-06 22:19:22,279 - __main__ - INFO - SHAP dependence plot for feature 'num__release_ball_speed' generated successfully.\n",
      "2025-02-06 22:19:22,279 - __main__ - INFO - Generating SHAP dependence plot for feature 'num__release_ball_velocity_x'...\n",
      "2025-02-06 22:19:22,410 - __main__ - INFO - SHAP dependence plot for feature 'num__release_ball_velocity_x' generated successfully.\n",
      "2025-02-06 22:19:22,411 - __main__ - INFO - Generating SHAP dependence plot for feature 'num__knee_release_angle'...\n",
      "2025-02-06 22:19:22,540 - __main__ - INFO - SHAP dependence plot for feature 'num__knee_release_angle' generated successfully.\n",
      "2025-02-06 22:19:22,540 - __main__ - INFO - Generating SHAP dependence plot for feature 'num__release_ball_velocity_z'...\n",
      "2025-02-06 22:19:22,675 - __main__ - INFO - SHAP dependence plot for feature 'num__release_ball_velocity_z' generated successfully.\n",
      "2025-02-06 22:19:22,675 - __main__ - INFO - Generating SHAP dependence plot for feature 'num__release_ball_direction_z'...\n",
      "2025-02-06 22:19:22,813 - __main__ - INFO - SHAP dependence plot for feature 'num__release_ball_direction_z' generated successfully.\n",
      "2025-02-06 22:19:22,813 - __main__ - INFO - Generating SHAP dependence plot for feature 'num__release_ball_velocity_y'...\n",
      "2025-02-06 22:19:22,961 - __main__ - INFO - SHAP dependence plot for feature 'num__release_ball_velocity_y' generated successfully.\n",
      "2025-02-06 22:19:22,962 - __main__ - INFO - Generating SHAP dependence plot for feature 'num__release_ball_direction_x'...\n",
      "2025-02-06 22:19:23,091 - __main__ - INFO - SHAP dependence plot for feature 'num__release_ball_direction_x' generated successfully.\n",
      "2025-02-06 22:19:23,092 - __main__ - INFO - SHAP dependence plots saved at C:\\Users\\ghadf\\vscode_projects\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\data\\predictions\\shap_results\\shap_dependence_plots.\n",
      "2025-02-06 22:19:23,094 - __main__ - INFO - ✅ Interactive SHAP force plot generated for trial T0001.\n",
      "2025-02-06 22:19:23,095 - __main__ - INFO - SHAP force plots saved at C:\\Users\\ghadf\\vscode_projects\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\data\\predictions\\shap_results\\shap_force_plots.\n",
      "2025-02-06 22:19:23,095 - __main__ - INFO - shap_values is assumed to be a 2D array already.\n",
      "2025-02-06 22:19:23,097 - __main__ - WARNING - Error generating feedback for trial 'T0001': Incompatible indexer with Series\n",
      "2025-02-06 22:19:23,144 - __main__ - INFO - Individual feedback generated for the entire dataset.\n",
      "2025-02-06 22:19:23,144 - __main__ - INFO - Expanding 'specific_feedback' into separate columns.\n",
      "2025-02-06 22:19:23,161 - __main__ - INFO - 'specific_feedback' expanded successfully.\n",
      "2025-02-06 22:19:23,161 - __main__ - INFO - 'specific_feedback' column expanded into separate feedback columns.\n",
      "2025-02-06 22:19:23,165 - __main__ - INFO - Final dataset and global recommendations saved.\n",
      "2025-02-06 22:19:23,166 - __main__ - INFO - Predict+SHAP pipeline completed successfully.\n",
      "2025-02-06 22:19:23,166 - __main__ - INFO - Unified predict_and_shap function executed successfully.\n",
      "\n",
      "Final Predictions with SHAP annotations (preview):\n",
      "          release_ball_direction_x  release_ball_direction_z  \\\n",
      "trial_id                                                       \n",
      "T0001                     0.377012                  0.926203   \n",
      "T0002                     0.417644                  0.901906   \n",
      "T0003                     0.383086                  0.922353   \n",
      "T0004                     0.292054                  0.956396   \n",
      "T0005                     0.389917                  0.918894   \n",
      "\n",
      "          release_ball_direction_y  elbow_release_angle  elbow_max_angle  \\\n",
      "trial_id                                                                   \n",
      "T0001                     0.002969            72.325830       106.272118   \n",
      "T0002                    -0.110176            58.430068       101.798798   \n",
      "T0003                    -0.050096            73.883728       106.989633   \n",
      "T0004                     0.003209            71.424835       104.160865   \n",
      "T0005                    -0.059987            73.619348       103.832024   \n",
      "\n",
      "          wrist_release_angle  wrist_max_angle  knee_release_angle  \\\n",
      "trial_id                                                             \n",
      "T0001               28.102765        35.918406           32.646276   \n",
      "T0002               32.766626        38.693790           33.834026   \n",
      "T0003               26.220943        38.998089           33.550293   \n",
      "T0004               22.649881        36.353038           29.568453   \n",
      "T0005               21.754498        34.532455           32.856306   \n",
      "\n",
      "          knee_max_angle  release_ball_speed  ...  \\\n",
      "trial_id                                      ...   \n",
      "T0001          63.541007            9.907618  ...   \n",
      "T0002          65.565635           11.826803  ...   \n",
      "T0003          65.257346           10.283314  ...   \n",
      "T0004          64.652573            9.164302  ...   \n",
      "T0005          64.165568           11.113489  ...   \n",
      "\n",
      "          shap_unit_change_calculated_release_angle  \\\n",
      "trial_id                                              \n",
      "T0001                         No feedback available   \n",
      "T0002                                  6.50 degrees   \n",
      "T0003                                  6.03 degrees   \n",
      "T0004                                  6.34 degrees   \n",
      "T0005                                  6.09 degrees   \n",
      "\n",
      "          shap_direction_release_ball_velocity_x  \\\n",
      "trial_id                                           \n",
      "T0001                      No feedback available   \n",
      "T0002                                   decrease   \n",
      "T0003                                   increase   \n",
      "T0004                                   increase   \n",
      "T0005                                   increase   \n",
      "\n",
      "          shap_importance_release_ball_velocity_x  \\\n",
      "trial_id                                            \n",
      "T0001                       No feedback available   \n",
      "T0002                                      0.4251   \n",
      "T0003                                      0.0734   \n",
      "T0004                                      0.0886   \n",
      "T0005                                      0.2112   \n",
      "\n",
      "          shap_unit_change_release_ball_velocity_x  \\\n",
      "trial_id                                             \n",
      "T0001                        No feedback available   \n",
      "T0002                                  0.49 meters   \n",
      "T0003                                  0.39 meters   \n",
      "T0004                                  0.27 meters   \n",
      "T0005                                  0.43 meters   \n",
      "\n",
      "          shap_direction_release_ball_velocity_y  \\\n",
      "trial_id                                           \n",
      "T0001                      No feedback available   \n",
      "T0002                                   decrease   \n",
      "T0003                                   decrease   \n",
      "T0004                                   increase   \n",
      "T0005                                   decrease   \n",
      "\n",
      "          shap_importance_release_ball_velocity_y  \\\n",
      "trial_id                                            \n",
      "T0001                       No feedback available   \n",
      "T0002                                      0.0088   \n",
      "T0003                                      0.0502   \n",
      "T0004                                      0.1388   \n",
      "T0005                                      0.1161   \n",
      "\n",
      "          shap_unit_change_release_ball_velocity_y  \\\n",
      "trial_id                                             \n",
      "T0001                        No feedback available   \n",
      "T0002                                  0.13 meters   \n",
      "T0003                                  0.05 meters   \n",
      "T0004                                  0.00 meters   \n",
      "T0005                                  0.07 meters   \n",
      "\n",
      "         shap_direction_release_ball_velocity_z  \\\n",
      "trial_id                                          \n",
      "T0001                     No feedback available   \n",
      "T0002                                  decrease   \n",
      "T0003                                  increase   \n",
      "T0004                                  decrease   \n",
      "T0005                                  decrease   \n",
      "\n",
      "         shap_importance_release_ball_velocity_z  \\\n",
      "trial_id                                           \n",
      "T0001                      No feedback available   \n",
      "T0002                                     0.2892   \n",
      "T0003                                     0.1668   \n",
      "T0004                                     0.0379   \n",
      "T0005                                     0.2486   \n",
      "\n",
      "         shap_unit_change_release_ball_velocity_z  \n",
      "trial_id                                           \n",
      "T0001                       No feedback available  \n",
      "T0002                                 1.07 meters  \n",
      "T0003                                 0.95 meters  \n",
      "T0004                                 0.88 meters  \n",
      "T0005                                 1.02 meters  \n",
      "\n",
      "[5 rows x 59 columns]\n",
      "Feedback for trial T0002:\n",
      "Trial Specific Feedback:\n",
      "  shap_direction_release_ball_direction_x: increase\n",
      "  shap_importance_release_ball_direction_x: 0.0536\n",
      "  shap_unit_change_release_ball_direction_x: 0.04 meters\n",
      "  shap_direction_release_ball_direction_z: increase\n",
      "  shap_importance_release_ball_direction_z: 0.0791\n",
      "  shap_unit_change_release_ball_direction_z: 0.09 meters\n",
      "  shap_direction_release_ball_direction_y: increase\n",
      "  shap_importance_release_ball_direction_y: 1.6703\n",
      "  shap_unit_change_release_ball_direction_y: 0.01 meters\n",
      "  shap_direction_elbow_release_angle: decrease\n",
      "  shap_importance_elbow_release_angle: 0.1628\n",
      "  shap_unit_change_elbow_release_angle: 5.84 degrees\n",
      "  shap_direction_elbow_max_angle: increase\n",
      "  shap_importance_elbow_max_angle: 0.4481\n",
      "  shap_unit_change_elbow_max_angle: 10.18 degrees\n",
      "  shap_direction_wrist_release_angle: decrease\n",
      "  shap_importance_wrist_release_angle: 0.3107\n",
      "  shap_unit_change_wrist_release_angle: 3.28 degrees\n",
      "  shap_direction_wrist_max_angle: increase\n",
      "  shap_importance_wrist_max_angle: 0.3199\n",
      "  shap_unit_change_wrist_max_angle: 3.87 degrees\n",
      "  shap_direction_knee_release_angle: increase\n",
      "  shap_importance_knee_release_angle: 0.1061\n",
      "  shap_unit_change_knee_release_angle: 3.38 degrees\n",
      "  shap_direction_knee_max_angle: decrease\n",
      "  shap_importance_knee_max_angle: 0.2354\n",
      "  shap_unit_change_knee_max_angle: 6.56 degrees\n",
      "  shap_direction_release_ball_speed: decrease\n",
      "  shap_importance_release_ball_speed: 0.321\n",
      "  shap_unit_change_release_ball_speed: 1.18 meters\n",
      "  shap_direction_calculated_release_angle: increase\n",
      "  shap_importance_calculated_release_angle: 0.2969\n",
      "  shap_unit_change_calculated_release_angle: 6.50 degrees\n",
      "  shap_direction_release_ball_velocity_x: decrease\n",
      "  shap_importance_release_ball_velocity_x: 0.4251\n",
      "  shap_unit_change_release_ball_velocity_x: 0.49 meters\n",
      "  shap_direction_release_ball_velocity_y: decrease\n",
      "  shap_importance_release_ball_velocity_y: 0.0088\n",
      "  shap_unit_change_release_ball_velocity_y: 0.13 meters\n",
      "  shap_direction_release_ball_velocity_z: decrease\n",
      "  shap_importance_release_ball_velocity_z: 0.2892\n",
      "  shap_unit_change_release_ball_velocity_z: 1.07 meters\n",
      "All trials have complete feedback.\n",
      "\n",
      "Feedback for trial T0001:\n",
      "Trial Specific Feedback:\n",
      "  shap_direction_release_ball_direction_x: No feedback available\n",
      "  shap_importance_release_ball_direction_x: No feedback available\n",
      "  shap_unit_change_release_ball_direction_x: No feedback available\n",
      "  shap_direction_release_ball_direction_z: No feedback available\n",
      "  shap_importance_release_ball_direction_z: No feedback available\n",
      "  shap_unit_change_release_ball_direction_z: No feedback available\n",
      "  shap_direction_release_ball_direction_y: No feedback available\n",
      "  shap_importance_release_ball_direction_y: No feedback available\n",
      "  shap_unit_change_release_ball_direction_y: No feedback available\n",
      "  shap_direction_elbow_release_angle: No feedback available\n",
      "  shap_importance_elbow_release_angle: No feedback available\n",
      "  shap_unit_change_elbow_release_angle: No feedback available\n",
      "  shap_direction_elbow_max_angle: No feedback available\n",
      "  shap_importance_elbow_max_angle: No feedback available\n",
      "  shap_unit_change_elbow_max_angle: No feedback available\n",
      "  shap_direction_wrist_release_angle: No feedback available\n",
      "  shap_importance_wrist_release_angle: No feedback available\n",
      "  shap_unit_change_wrist_release_angle: No feedback available\n",
      "  shap_direction_wrist_max_angle: No feedback available\n",
      "  shap_importance_wrist_max_angle: No feedback available\n",
      "  shap_unit_change_wrist_max_angle: No feedback available\n",
      "  shap_direction_knee_release_angle: No feedback available\n",
      "  shap_importance_knee_release_angle: No feedback available\n",
      "  shap_unit_change_knee_release_angle: No feedback available\n",
      "  shap_direction_knee_max_angle: No feedback available\n",
      "  shap_importance_knee_max_angle: No feedback available\n",
      "  shap_unit_change_knee_max_angle: No feedback available\n",
      "  shap_direction_release_ball_speed: No feedback available\n",
      "  shap_importance_release_ball_speed: No feedback available\n",
      "  shap_unit_change_release_ball_speed: No feedback available\n",
      "  shap_direction_calculated_release_angle: No feedback available\n",
      "  shap_importance_calculated_release_angle: No feedback available\n",
      "  shap_unit_change_calculated_release_angle: No feedback available\n",
      "  shap_direction_release_ball_velocity_x: No feedback available\n",
      "  shap_importance_release_ball_velocity_x: No feedback available\n",
      "  shap_unit_change_release_ball_velocity_x: No feedback available\n",
      "  shap_direction_release_ball_velocity_y: No feedback available\n",
      "  shap_importance_release_ball_velocity_y: No feedback available\n",
      "  shap_unit_change_release_ball_velocity_y: No feedback available\n",
      "  shap_direction_release_ball_velocity_z: No feedback available\n",
      "  shap_importance_release_ball_velocity_z: No feedback available\n",
      "  shap_unit_change_release_ball_velocity_z: No feedback available\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ghadf\\vscode_projects\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\freethrow_predictions\\ml\\shap\\shap_utils.py:313: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'No feedback available' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  feedback_df.fillna('No feedback available', inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %%writefile ml/predict_with_shap_usage.py\n",
    "import pandas as pd\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import joblib\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import logging.config\n",
    "import ast\n",
    "\n",
    "# Import configuration loader and models\n",
    "from ml.config.config_loader import load_config\n",
    "from ml.config.config_models import AppConfig\n",
    "# Import other necessary modules\n",
    "from ml.train_utils.train_utils import load_model\n",
    "from ml.feature_selection.feature_importance_calculator import manage_features\n",
    "\n",
    "# Optionally, import SHAP helpers if needed:\n",
    "# Local imports for SHAP and model loading should be uncommented and adjusted as needed\n",
    "from ml.shap.shap_utils import (\n",
    "    compute_shap_values,\n",
    "    plot_shap_summary,\n",
    "    plot_shap_dependence,\n",
    "    generate_global_recommendations,\n",
    "    generate_individual_feedback,\n",
    "    plot_shap_force,\n",
    "    expand_specific_feedback\n",
    ")\n",
    "\n",
    "# Assume these are imported in the modules that call them\n",
    "from datapreprocessor import DataPreprocessor\n",
    "from ml.predict.predict import predict_and_attach_predict_probs\n",
    "\n",
    "\n",
    "def load_dataset(path: Path) -> pd.DataFrame:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Dataset not found at {path}\")\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "\n",
    "def setup_logging(config: AppConfig, log_file_path: Path) -> logging.Logger:\n",
    "    log_level = config.logging.level.upper()\n",
    "    # Option 1: Remove file handler entirely (for ease)\n",
    "    logging_config = {\n",
    "        'version': 1,\n",
    "        'disable_existing_loggers': False,\n",
    "        'formatters': {\n",
    "            'standard': {\n",
    "                'format': '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "            },\n",
    "        },\n",
    "        'handlers': {\n",
    "            'console': {\n",
    "                'class': 'logging.StreamHandler',\n",
    "                'level': log_level,\n",
    "                'formatter': 'standard',\n",
    "                'stream': 'ext://sys.stdout',\n",
    "            },\n",
    "        },\n",
    "        'loggers': {\n",
    "            '__main__': {\n",
    "                'handlers': ['console'],\n",
    "                'level': log_level,\n",
    "                'propagate': False\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "    logging.config.dictConfig(logging_config)\n",
    "    return logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def unpack_feedback(feedback: any) -> None:\n",
    "    if feedback is None:\n",
    "        print(\"No feedback available.\")\n",
    "        return\n",
    "\n",
    "    if isinstance(feedback, dict):\n",
    "        feedback_dict = feedback\n",
    "    elif isinstance(feedback, str):\n",
    "        try:\n",
    "            # Print raw feedback for debugging\n",
    "            print(\"Raw feedback string (repr):\", repr(feedback))\n",
    "            feedback_dict = ast.literal_eval(feedback)\n",
    "        except Exception as e:\n",
    "            print(f\"ast.literal_eval failed: {e}\")\n",
    "            return\n",
    "    else:\n",
    "        print(\"Feedback is not in a recognized format.\")\n",
    "        return\n",
    "\n",
    "    print(\"Trial Specific Feedback:\")\n",
    "    for metric, suggestion in feedback_dict.items():\n",
    "        print(f\"  {metric}: {suggestion}\")\n",
    "\n",
    "\n",
    "def predict_and_shap(\n",
    "      config: AppConfig,\n",
    "      df_input: pd.DataFrame,\n",
    "      save_dir: Path,\n",
    "      generate_summary_plot: bool = True,\n",
    "      generate_dependence_plots: bool = False,\n",
    "      generate_force_plots_or_feedback_indices: Optional[List[Any]] = None,\n",
    "      top_n_features: int = 10,\n",
    "      use_mad: bool = False,\n",
    "      generate_feedback: bool = False,\n",
    "      index_column: Optional[str] = None,\n",
    "      logger: Optional[logging.Logger] = None,\n",
    "      # Optional overrides for feature file paths:\n",
    "      features_file: Optional[Path] = None,\n",
    "      ordinal_file: Optional[Path] = None,\n",
    "      nominal_file: Optional[Path] = None,\n",
    "      numericals_file: Optional[Path] = None,\n",
    "      y_variable_file: Optional[Path] = None,\n",
    "      model_save_dir_override: Optional[Path] = None,\n",
    "      transformers_dir_override: Optional[Path] = None\n",
    ") -> Dict[str, Any]:\n",
    "    results = {}\n",
    "    # Use dot‑notation to access configuration values\n",
    "    data_dir = Path(config.paths.data_dir).resolve()\n",
    "    model_save_dir = Path(config.paths.model_save_base_dir).resolve() if model_save_dir_override is None else model_save_dir_override.resolve()\n",
    "    transformers_dir = Path(config.paths.transformers_save_base_dir).resolve() if transformers_dir_override is None else transformers_dir_override.resolve()\n",
    "    # Use the configuration for feature paths:\n",
    "    features_file = Path(config.paths.features_metadata_file) if features_file is None else features_file\n",
    "    # [Set other feature file defaults similarly]\n",
    "\n",
    "    # Load tuning results, select best model, etc.\n",
    "    tuning_results_path = model_save_dir / \"tuning_results.json\"\n",
    "    if not tuning_results_path.exists():\n",
    "        raise FileNotFoundError(f\"Tuning results not found at '{tuning_results_path}'.\")\n",
    "    with open(tuning_results_path, 'r') as f:\n",
    "        tuning_results = json.load(f)\n",
    "    best_model_info = tuning_results.get(\"Best Model\")\n",
    "    if not best_model_info:\n",
    "        raise ValueError(\"Best model information not found in tuning results.\")\n",
    "    best_model_name = best_model_info.get(\"model_name\")\n",
    "    if not best_model_name:\n",
    "        raise ValueError(\"Best model name not found in tuning results.\")\n",
    "    if logger:\n",
    "        logger.info(f\"Best model identified: {best_model_name}\")\n",
    "    model_path = model_save_dir / best_model_name.replace(' ', '_') / \"trained_model.pkl\"\n",
    "\n",
    "    # Load feature lists (via manage_features) and initialize DataPreprocessor\n",
    "    # Build the feature paths dictionary:\n",
    "    feature_paths = {\n",
    "        'features': features_file,\n",
    "        'ordinal_categoricals': ordinal_file,\n",
    "        'nominal_categoricals': nominal_file,\n",
    "        'numericals': numericals_file,\n",
    "        'y_variable': y_variable_file\n",
    "    }\n",
    "    try:\n",
    "        feature_lists = manage_features(mode='load', paths=feature_paths)\n",
    "        y_variable_list = feature_lists.get('y_variable', [])\n",
    "        ordinal_categoricals = feature_lists.get('ordinal_categoricals', [])\n",
    "        nominal_categoricals = feature_lists.get('nominal_categoricals', [])\n",
    "        numericals = feature_lists.get('numericals', [])\n",
    "        if logger:\n",
    "            logger.debug(f\"Loaded Feature Lists: y_variable={y_variable_list}, ordinal_categoricals={ordinal_categoricals}, nominal_categoricals={nominal_categoricals}, numericals={numericals}\")\n",
    "    except Exception as e:\n",
    "        if logger:\n",
    "            logger.warning(f\"Feature lists could not be loaded: {e}\")\n",
    "        y_variable_list, ordinal_categoricals, nominal_categoricals, numericals = [], [], [], []\n",
    "\n",
    "    preprocessor = DataPreprocessor(\n",
    "        model_type=\"Tree Based Classifier\",\n",
    "        y_variable=y_variable_list,\n",
    "        ordinal_categoricals=ordinal_categoricals,\n",
    "        nominal_categoricals=nominal_categoricals,\n",
    "        numericals=numericals,\n",
    "        mode='predict',\n",
    "        options={},\n",
    "        debug=True,\n",
    "        normalize_debug=False,\n",
    "        normalize_graphs_output=False,\n",
    "        graphs_output_dir=Path(config.paths.plots_output_dir).resolve(),\n",
    "        transformers_dir=transformers_dir\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        X_preprocessed, recommendations, X_inversed = preprocessor.final_preprocessing(df_input)\n",
    "        if logger:\n",
    "            logger.info(\"Preprocessing completed successfully in predict mode.\")\n",
    "    except Exception as e:\n",
    "        if logger:\n",
    "            logger.error(f\"Preprocessing failed: {e}\")\n",
    "        raise\n",
    "\n",
    "    duplicates = X_inversed.index.duplicated()\n",
    "    if duplicates.any():\n",
    "        print(\"Duplicate trial IDs found:\", X_inversed.index[duplicates].tolist())\n",
    "    else:\n",
    "        print(\"Trial IDs are unique.\")\n",
    "\n",
    "    try:\n",
    "        model = load_model(best_model_name, model_save_dir)\n",
    "        if logger:\n",
    "            logger.info(f\"Trained model loaded from '{model_path}'.\")\n",
    "    except Exception as e:\n",
    "        if logger:\n",
    "            logger.error(f\"Failed to load the best model '{best_model_name}': {e}\")\n",
    "        raise\n",
    "\n",
    "    try:\n",
    "        predictions, prediction_probs, X_inversed = predict_and_attach_predict_probs(model, X_preprocessed, X_inversed)\n",
    "        results['predictions'] = predictions\n",
    "        results['prediction_probs'] = prediction_probs\n",
    "        if logger:\n",
    "            logger.info(\"Predictions generated and attached to the dataset.\")\n",
    "    except Exception as e:\n",
    "        if logger:\n",
    "            logger.error(f\"Prediction failed: {e}\")\n",
    "        raise\n",
    "\n",
    "    if index_column is not None:\n",
    "        if index_column in df_input.columns:\n",
    "            X_inversed[index_column] = df_input[index_column].values\n",
    "            X_preprocessed[index_column] = df_input[index_column].values\n",
    "            X_inversed.set_index(index_column, inplace=True)\n",
    "            # if index_column in X_inversed.columns:\n",
    "            #     X_inversed.drop(index_column, axis=1, inplace=True)\n",
    "            X_preprocessed.set_index(index_column, inplace=True)\n",
    "            if logger:\n",
    "                logger.info(f\"Using '{index_column}' as the index for both preprocessed and inverse-transformed data.\")\n",
    "        else:\n",
    "            if logger:\n",
    "                logger.warning(f\"Specified index column '{index_column}' not found in df_input; using default index.\")\n",
    "\n",
    "    try:\n",
    "        explainer, shap_values = compute_shap_values(model, X_preprocessed, debug=config.logging.debug, logger=logger)\n",
    "        results['shap_values'] = shap_values\n",
    "        results['explainer'] = explainer\n",
    "        results['X_preprocessed'] = X_preprocessed\n",
    "        if logger:\n",
    "            logger.info(\"SHAP values computed successfully.\")\n",
    "            logger.debug(f\"Type of shap_values: {type(shap_values)}\")\n",
    "            if hasattr(shap_values, 'shape'):\n",
    "                logger.debug(f\"shap_values.shape: {shap_values.shape}\")\n",
    "            # Log a sample of shap_values\n",
    "            if isinstance(shap_values, (pd.DataFrame, pd.Series)):\n",
    "                logger.debug(f\"shap_values sample:\\n{shap_values.head()}\")\n",
    "            elif isinstance(shap_values, np.ndarray):\n",
    "                logger.debug(f\"shap_values sample:\\n{shap_values[:5]}\")\n",
    "    except Exception as e:\n",
    "        if logger:\n",
    "            logger.error(f\"SHAP computation failed: {e}\")\n",
    "        raise\n",
    "\n",
    "    if generate_summary_plot:\n",
    "        shap_summary_path = save_dir / \"shap_summary.png\"\n",
    "        try:\n",
    "            plot_shap_summary(shap_values, X_preprocessed, str(shap_summary_path), debug=config.logging.debug, logger=logger)\n",
    "            results['shap_summary_plot'] = str(shap_summary_path)\n",
    "            if logger:\n",
    "                logger.info(f\"SHAP summary plot saved at {shap_summary_path}.\")\n",
    "        except Exception as e:\n",
    "            if logger:\n",
    "                logger.error(f\"Failed to generate SHAP summary plot: {e}\")\n",
    "\n",
    "    # --- Generate SHAP Dependence Plots ---\n",
    "    recommendations_dict = {}\n",
    "    if generate_dependence_plots:\n",
    "        try:\n",
    "            recommendations_dict = generate_global_recommendations(\n",
    "                shap_values=shap_values,\n",
    "                X_original=X_preprocessed,\n",
    "                top_n=top_n_features,\n",
    "                use_mad=use_mad,\n",
    "                logger=logger\n",
    "            )\n",
    "            results['recommendations'] = recommendations_dict\n",
    "            shap_dependence_dir = save_dir / \"shap_dependence_plots\"\n",
    "            os.makedirs(shap_dependence_dir, exist_ok=True)\n",
    "            for feature in recommendations_dict.keys():\n",
    "                dep_path = shap_dependence_dir / f\"shap_dependence_{feature}.png\"\n",
    "                plot_shap_dependence(shap_values, feature, X_preprocessed, str(dep_path),\n",
    "                                     debug=config.logging.debug, logger=logger)\n",
    "            if logger:\n",
    "                logger.info(f\"SHAP dependence plots saved at {shap_dependence_dir}.\")\n",
    "        except Exception as e:\n",
    "            if logger:\n",
    "                logger.error(f\"Failed to generate SHAP dependence plots: {e}\")\n",
    "\n",
    "    # --- Optionally Generate SHAP Force Plots for Specified IDs ---\n",
    "    if generate_force_plots_or_feedback_indices is not None:\n",
    "        try:\n",
    "            force_plots_dir = save_dir / \"shap_force_plots\"\n",
    "            force_plots_dir.mkdir(exist_ok=True, parents=True)\n",
    "            for trial_id in generate_force_plots_or_feedback_indices:\n",
    "                force_path = force_plots_dir / f\"shap_force_plot_{trial_id}.html\"  # Unique filename per trial\n",
    "                plot_shap_force(explainer, shap_values, X_preprocessed, trial_id, force_path,\n",
    "                                logger=logger, debug=config.logging.debug)\n",
    "\n",
    "            if logger:\n",
    "                logger.info(f\"SHAP force plots saved at {force_plots_dir}.\")\n",
    "        except Exception as e:\n",
    "            if logger:\n",
    "                logger.error(f\"Failed to generate SHAP force plots: {e}\")\n",
    "\n",
    "    # --- Generate Individual Feedback for the Entire Dataset if Requested ---\n",
    "    if generate_feedback:\n",
    "        try:\n",
    "            feature_metadata = {}\n",
    "            for feature in X_inversed.columns:\n",
    "                unit = 'degrees' if 'angle' in feature.lower() else 'meters'\n",
    "                feature_metadata[feature] = {'unit': unit}\n",
    "            \n",
    "            feedback_dict = {}\n",
    "            adjusted_shap_values = shap_values\n",
    "            if isinstance(shap_values, list):\n",
    "                if len(shap_values) == 2:\n",
    "                    logger.info(\"Detected shap_values as a list of two arrays; using shap_values[1] for positive class.\")\n",
    "                    adjusted_shap_values = shap_values[1]\n",
    "                else:\n",
    "                    logger.warning(\"shap_values is a list but not of length 2. Using the first element.\")\n",
    "                    adjusted_shap_values = shap_values[0]\n",
    "            elif hasattr(shap_values, 'ndim') and shap_values.ndim == 3:\n",
    "                logger.info(f\"shap_values is a 3D array with shape {shap_values.shape}. Attempting extraction based on axis.\")\n",
    "                if shap_values.shape[0] == 2:\n",
    "                    adjusted_shap_values = shap_values[1, :, :]\n",
    "                elif shap_values.shape[1] == 2:\n",
    "                    adjusted_shap_values = shap_values[:, 1, :]\n",
    "                else:\n",
    "                    logger.warning(\"Unexpected shape for shap_values 3D array. Using the first slice along the first axis.\")\n",
    "                    adjusted_shap_values = shap_values[0, :, :]\n",
    "            else:\n",
    "                logger.info(\"shap_values is assumed to be a 2D array already.\")\n",
    "\n",
    "            logger.debug(f\"Type of adjusted_shap_values: {type(adjusted_shap_values)}; shape: {getattr(adjusted_shap_values, 'shape', 'N/A')}\")\n",
    "\n",
    "            # Check if the number of shap_values rows matches X_inversed\n",
    "            if hasattr(adjusted_shap_values, 'shape'):\n",
    "                if adjusted_shap_values.shape[0] != len(X_inversed):\n",
    "                    logger.warning(f\"Number of shap values ({adjusted_shap_values.shape[0]}) does not match number of trials ({len(X_inversed)}).\")\n",
    "                else:\n",
    "                    logger.debug(\"Number of shap values matches number of trials.\")\n",
    "            else:\n",
    "                logger.warning(f\"adjusted_shap_values does not have a 'shape' attribute.\")\n",
    "\n",
    "            # Reindex shap_values if they are a DataFrame or Series\n",
    "            if isinstance(adjusted_shap_values, (pd.DataFrame, pd.Series)):\n",
    "                adjusted_shap_values = adjusted_shap_values.reindex(X_inversed.index)\n",
    "                logger.debug(\"Reindexed adjusted_shap_values to match X_inversed index.\")\n",
    "\n",
    "            for trial_id in X_inversed.index:\n",
    "                try:\n",
    "                    # Specific Debugging for T0001\n",
    "                    if trial_id == 'T0001':\n",
    "                        logger.debug(\"---- Debugging Trial T0001 ----\")\n",
    "                        logger.debug(f\"Trial Features: {X_inversed.loc[trial_id].to_dict()}\")\n",
    "                    \n",
    "                    # Extract SHAP values using .loc\n",
    "                    if isinstance(adjusted_shap_values, pd.Series) or isinstance(adjusted_shap_values, pd.DataFrame):\n",
    "                        shap_values_trial = adjusted_shap_values.loc[trial_id]\n",
    "                        logger.debug(f\"SHAP values for trial_id={trial_id} accessed via .loc.\")\n",
    "                    elif isinstance(adjusted_shap_values, np.ndarray):\n",
    "                        pos = X_inversed.index.get_loc(trial_id)\n",
    "                        shap_values_trial = adjusted_shap_values[pos]\n",
    "                        logger.debug(f\"SHAP values for trial_id={trial_id} accessed via numpy indexing at pos={pos}.\")\n",
    "                    else:\n",
    "                        logger.warning(f\"Unsupported type for adjusted_shap_values: {type(adjusted_shap_values)}\")\n",
    "                        raise TypeError(f\"Unsupported type for adjusted_shap_values: {type(adjusted_shap_values)}\")\n",
    "\n",
    "                    # Extract trial features\n",
    "                    trial_features_raw = X_inversed.loc[[trial_id]]\n",
    "                    logger.debug(f\"Processing trial '{trial_id}': trial_features_raw shape={trial_features_raw.shape}, columns={trial_features_raw.columns.tolist()}\")\n",
    "\n",
    "                    trial_features = trial_features_raw.iloc[0]\n",
    "                    logger.debug(f\"Trial '{trial_id}' converted to Series; trial_features shape={trial_features.shape}, index={trial_features.index.tolist()}\")\n",
    "\n",
    "                    # Generate individual feedback\n",
    "                    fb = generate_individual_feedback(trial_features, shap_values_trial, feature_metadata, logger=logger)\n",
    "                    feedback_dict[trial_id] = fb\n",
    "                    X_inversed.at[trial_id, 'specific_feedback'] = fb\n",
    "                    logger.debug(f\"Feedback for trial '{trial_id}' generated successfully: {fb}\")\n",
    "\n",
    "                except Exception as err:\n",
    "                    logger.warning(f\"Error generating feedback for trial '{trial_id}': {err}\")\n",
    "                    # Ensure that even if feedback generation fails, 'specific_feedback' is populated to prevent NaN\n",
    "                    X_inversed.at[trial_id, 'specific_feedback'] = {}\n",
    "                    continue\n",
    "\n",
    "            results['individual_feedback'] = feedback_dict\n",
    "            if logger:\n",
    "                logger.info(\"Individual feedback generated for the entire dataset.\")\n",
    "\n",
    "            # --- Handle 'specific_feedback' Column Dtype Before Filling NaNs ---\n",
    "            X_inversed['specific_feedback'] = X_inversed['specific_feedback'].astype(object)\n",
    "            X_inversed.fillna('No feedback available', inplace=True)\n",
    "            logger.debug(\"'specific_feedback' column dtype set to object and NaNs filled with 'No feedback available'.\")\n",
    "\n",
    "            # --- Expand 'specific_feedback' into Separate Columns ---\n",
    "            try:\n",
    "                X_inversed = expand_specific_feedback(X_inversed, logger=logger)\n",
    "                results['final_dataset'] = X_inversed\n",
    "                if logger:\n",
    "                    logger.info(\"'specific_feedback' column expanded into separate feedback columns.\")\n",
    "            except Exception as e:\n",
    "                if logger:\n",
    "                    logger.error(f\"Failed to expand 'specific_feedback': {e}\")\n",
    "                raise\n",
    "\n",
    "        except Exception as e:\n",
    "            if logger:\n",
    "                logger.error(\"Failed to generate individual feedback: %s\", e)\n",
    "\n",
    "    # --- Save Final Dataset and Global Recommendations ---\n",
    "    try:\n",
    "        final_dataset_path = save_dir / \"final_predictions_with_shap.csv\"\n",
    "        X_inversed.to_csv(final_dataset_path, index=True)\n",
    "        results['final_dataset'] = str(final_dataset_path)\n",
    "        if recommendations_dict:\n",
    "            recs_path = save_dir / \"global_shap_recommendations.json\"\n",
    "            with open(recs_path, \"w\") as f:\n",
    "                json.dump(recommendations_dict, f, indent=4)\n",
    "            results['recommendations_file'] = str(recs_path)\n",
    "        if logger:\n",
    "            logger.info(\"Final dataset and global recommendations saved.\")\n",
    "    except Exception as e:\n",
    "        if logger:\n",
    "            logger.error(f\"Failed to save outputs: {e}\")\n",
    "        raise\n",
    "\n",
    "    if logger:\n",
    "        logger.info(\"Predict+SHAP pipeline completed successfully.\")\n",
    "    return results\n",
    "\n",
    "\n",
    "def main():\n",
    "    config_path = Path('../../data/model/preprocessor_config/preprocessor_config.yaml')\n",
    "    try:\n",
    "        config: AppConfig = load_config(config_path)\n",
    "        print(f\"Configuration loaded successfully from {config_path}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to load configuration: {e}\")\n",
    "        return\n",
    "\n",
    "    data_dir = Path(config.paths.data_dir).resolve()\n",
    "    raw_data_path = data_dir / config.paths.raw_data\n",
    "    predictions_output_path = Path(config.paths.predictions_output_dir).resolve()\n",
    "    predictions_output_path = predictions_output_path / \"shap_results\"\n",
    "\n",
    "    log_dir = Path(config.paths.log_dir).resolve()\n",
    "    log_file = config.paths.log_file\n",
    "\n",
    "    logger = setup_logging(config, log_dir / log_file)\n",
    "    logger.info(\"Starting prediction module (unified predict_and_shap).\")\n",
    "    logger.debug(f\"Paths: {config.paths}\")\n",
    "\n",
    "    try:\n",
    "        df_predict = load_dataset(raw_data_path)\n",
    "        print(\"Columns in input data:\", df_predict.columns.tolist())\n",
    "        logger.info(f\"Prediction input data loaded from {raw_data_path}.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load input data: {e}\")\n",
    "        return\n",
    "    base_dir = Path(\"../../data\") / \"preprocessor\" / \"features_info\"\n",
    "    try:\n",
    "        results = predict_and_shap(\n",
    "            config=config,\n",
    "            df_input=df_predict,\n",
    "            save_dir=predictions_output_path,\n",
    "            generate_summary_plot=True,\n",
    "            generate_dependence_plots=True,\n",
    "            generate_force_plots_or_feedback_indices=['T0001'],\n",
    "            top_n_features=len(df_predict.columns),\n",
    "            use_mad=False,\n",
    "            generate_feedback=True,\n",
    "            index_column=\"trial_id\",\n",
    "            logger=logger,\n",
    "            features_file = (Path(config.paths.data_dir) / config.paths.features_metadata_file).resolve(),\n",
    "            ordinal_file=Path(f'{base_dir}/ordinal_categoricals.pkl'),\n",
    "            nominal_file=Path(f'{base_dir}/nominal_categoricals.pkl'),\n",
    "            numericals_file=Path(f'{base_dir}/numericals.pkl'),\n",
    "            y_variable_file=Path(f'{base_dir}/y_variable.pkl'),\n",
    "            model_save_dir_override=Path(config.paths.model_save_base_dir),\n",
    "            transformers_dir_override=Path(config.paths.transformers_save_base_dir)\n",
    "        )\n",
    "        logger.info(\"Unified predict_and_shap function executed successfully.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unified predict_and_shap function failed: {e}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        print(\"\\nFinal Predictions with SHAP annotations (preview):\")\n",
    "        final_df = pd.read_csv(results['final_dataset'], index_col=0)\n",
    "        print(final_df.head())\n",
    "\n",
    "        # Debug: Print columns in final_df\n",
    "        logger.debug(f\"Final DataFrame columns: {final_df.columns.tolist()}\")\n",
    "\n",
    "        trial_id = 'T0002'\n",
    "        if trial_id in final_df.index:\n",
    "            # Select all 'shap_' columns\n",
    "            shap_columns = [col for col in final_df.columns if col.startswith('shap_')]\n",
    "            logger.debug(f\"'shap_' columns for feedback: {shap_columns}\")\n",
    "\n",
    "            # Ensure there are 'shap_' columns\n",
    "            if not shap_columns:\n",
    "                logger.error(\"No 'shap_' columns found in the final DataFrame.\")\n",
    "                print(\"No feedback columns found in the final DataFrame.\")\n",
    "            else:\n",
    "                feedback_entry = final_df.loc[trial_id, shap_columns].to_dict()\n",
    "                logger.debug(f\"Feedback entry for {trial_id}: {feedback_entry}\")\n",
    "\n",
    "                print(f\"Feedback for trial {trial_id}:\")\n",
    "                unpack_feedback(feedback_entry)\n",
    "        else:\n",
    "            print(f\"No feedback found for trial {trial_id}.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to display outputs: {e}\")\n",
    "\n",
    "    # Check for missing feedback across all 'shap_' columns\n",
    "    try:\n",
    "        shap_columns = [col for col in final_df.columns if col.startswith('shap_')]\n",
    "        if shap_columns:\n",
    "            null_feedback = final_df[final_df[shap_columns].isnull().any(axis=1)]\n",
    "            if not null_feedback.empty:\n",
    "                print(f\"Trials with null feedback: {null_feedback.index.tolist()}\")\n",
    "            else:\n",
    "                print(\"All trials have complete feedback.\")\n",
    "        else:\n",
    "            print(\"No 'shap_' columns found to check for feedback completeness.\")\n",
    "    except KeyError:\n",
    "        print(\"No 'shap_' columns found in the final DataFrame.\")\n",
    "\n",
    "    # Additional Debugging: Check Feedback for T0001\n",
    "    try:\n",
    "        trial_id = 'T0001'\n",
    "        if trial_id in final_df.index:\n",
    "            shap_columns = [col for col in final_df.columns if col.startswith('shap_')]\n",
    "            feedback_entry = final_df.loc[trial_id, shap_columns].to_dict()\n",
    "            logger.debug(f\"Feedback entry for {trial_id}: {feedback_entry}\")\n",
    "\n",
    "            print(f\"\\nFeedback for trial {trial_id}:\")\n",
    "            unpack_feedback(feedback_entry)\n",
    "        else:\n",
    "            print(f\"No feedback found for trial {trial_id}.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to display feedback for trial {trial_id}: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ml/predict/predict_with_shap_usage_example.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ml/predict/predict_with_shap_usage_example.py\n",
    "\n",
    "import pandas as pd\n",
    "import logging\n",
    "import os\n",
    "import yaml\n",
    "import joblib\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict\n",
    "\n",
    "from ml.predict.predict import predict_and_attach_predict_probs\n",
    "from datapreprocessor import DataPreprocessor\n",
    "from ml.feature_selection.feature_importance_calculator import manage_features  # New Import\n",
    "\n",
    "# Local imports for SHAP and model loading should be uncommented and adjusted as needed\n",
    "from ml.shap.shap_utils import (\n",
    "    compute_shap_values,\n",
    "    plot_shap_summary,\n",
    "    plot_shap_dependence,\n",
    "    generate_global_recommendations,\n",
    "    generate_individual_feedback\n",
    ")\n",
    "from ml.train_utils.train_utils import load_model  # Ensure 'train_utils.py' contains load_model\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def load_dataset(path: Path) -> pd.DataFrame:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Dataset not found at {path}\")\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "\n",
    "def load_config(config_path: Path) -> Dict[str, Any]:\n",
    "    if not config_path.exists():\n",
    "        raise FileNotFoundError(f\"Configuration file not found at {config_path}\")\n",
    "    with open(config_path, 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    return config\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    # ----------------------------\n",
    "    # Step 1: Load Configuration\n",
    "    # ----------------------------\n",
    "    config_path = Path('../../data/model/preprocessor_config/preprocessor_config.yaml')  # Adjust as needed\n",
    "    try:\n",
    "        config = load_config(config_path)\n",
    "        print(f\"Configuration loaded successfully from {config_path}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to load configuration: {e}\")\n",
    "        return  # Exit if config loading fails\n",
    "\n",
    "    # ----------------------------\n",
    "    # Step 2: Extract Paths from Configuration\n",
    "    # ----------------------------\n",
    "    paths = config.get('paths', {})\n",
    "    feature_paths = {\n",
    "        'features': Path('../../data/preprocessor/features_info/final_ml_df_selected_features_columns.pkl'),\n",
    "        'ordinal_categoricals': Path('../../data/preprocessor/features_info/ordinal_categoricals.pkl'),\n",
    "        'nominal_categoricals': Path('../../data/preprocessor/features_info/nominal_categoricals.pkl'),\n",
    "        'numericals': Path('../../data/preprocessor/features_info/numericals.pkl'),\n",
    "        'y_variable': Path('../../data/preprocessor/features_info/y_variable.pkl')\n",
    "    }\n",
    "\n",
    "    # Define other necessary paths\n",
    "    data_dir = Path(paths.get('data_dir', '../../data/processed')).resolve()\n",
    "    raw_data_path = data_dir / paths.get('raw_data', 'final_ml_dataset.csv')  # Corrected key\n",
    "    processed_data_dir = data_dir / paths.get('processed_data_dir', 'preprocessor/processed')\n",
    "    transformers_dir = Path(paths.get('transformers_save_base_dir', '../preprocessor/transformers')).resolve()  # Corrected key\n",
    "    predictions_output_path = Path(paths.get('predictions_output_dir', 'preprocessor/predictions')).resolve()\n",
    "    log_dir = Path(paths.get('log_dir', '../preprocessor/logs')).resolve()\n",
    "    model_save_dir = Path(paths.get('model_save_base_dir', '../preprocessor/models')).resolve()  # Corrected key\n",
    "    log_file = paths.get('log_file', 'prediction.log')  # Ensure this key exists in config\n",
    "\n",
    "    # ----------------------------\n",
    "    # Step 3: Setup Logging\n",
    "    # ----------------------------\n",
    "    logger.info(\"✅ Starting prediction module.\")\n",
    "    logger.debug(f\"Configuration paths extracted: {paths}\")\n",
    "\n",
    "    # ----------------------------\n",
    "    # Step 4: Load Feature Lists Using manage_features\n",
    "    # ----------------------------\n",
    "    try:\n",
    "        feature_lists = manage_features(\n",
    "            mode='load',\n",
    "            paths=feature_paths\n",
    "        )\n",
    "        y_variable = feature_lists.get('y_variable', [])\n",
    "        ordinal_categoricals = feature_lists.get('ordinal_categoricals', [])\n",
    "        nominal_categoricals = feature_lists.get('nominal_categoricals', [])\n",
    "        numericals = feature_lists.get('numericals', [])\n",
    "\n",
    "        logger.debug(f\"Loaded Feature Lists: y_variable={y_variable}, \"\n",
    "                     f\"ordinal_categoricals={ordinal_categoricals}, \"\n",
    "                     f\"nominal_categoricals={nominal_categoricals}, \"\n",
    "                     f\"numericals={numericals}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Failed to load feature lists: {e}\")\n",
    "        return\n",
    "\n",
    "    # ----------------------------\n",
    "    # Step 5: Load Tuning Results to Find Best Model\n",
    "    # ----------------------------\n",
    "    tuning_results_path = model_save_dir / \"tuning_results.json\"\n",
    "    if not tuning_results_path.exists():\n",
    "        logger.error(f\"❌ Tuning results not found at '{tuning_results_path}'. Cannot determine the best model.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        with open(tuning_results_path, 'r') as f:\n",
    "            tuning_results = json.load(f)\n",
    "        best_model_info = tuning_results.get(\"Best Model\")\n",
    "        if not best_model_info:\n",
    "            logger.error(\"❌ Best model information not found in tuning results.\")\n",
    "            return\n",
    "        best_model_name = best_model_info.get(\"model_name\")\n",
    "        if not best_model_name:\n",
    "            logger.error(\"❌ Best model name not found in tuning results.\")\n",
    "            return\n",
    "        logger.info(f\"Best model identified: {best_model_name}\")\n",
    "        logger.debug(f\"Best model details: {best_model_info}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Failed to load tuning results: {e}\")\n",
    "        return\n",
    "\n",
    "    # ----------------------------\n",
    "    # Step 6: Preprocess the Data\n",
    "    # ----------------------------\n",
    "    # Load Prediction Dataset\n",
    "    if not raw_data_path.exists():\n",
    "        logger.error(f\"❌ Prediction input dataset not found at '{raw_data_path}'.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        df_predict = load_dataset(raw_data_path)\n",
    "        logger.info(f\"✅ Prediction input data loaded from '{raw_data_path}'.\")\n",
    "        logger.debug(f\"Prediction input data shape: {df_predict.shape}\")\n",
    "        logger.debug(f\"Prediction input data columns: {df_predict.columns.tolist()}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Failed to load prediction input data: {e}\")\n",
    "        return\n",
    "\n",
    "    # Initialize DataPreprocessor with Loaded Feature Lists\n",
    "    preprocessor = DataPreprocessor(\n",
    "        model_type=\"Tree Based Classifier\",  # Adjust if model type varies\n",
    "        y_variable=y_variable,\n",
    "        ordinal_categoricals=ordinal_categoricals,\n",
    "        nominal_categoricals=nominal_categoricals,\n",
    "        numericals=numericals,\n",
    "        mode='predict',\n",
    "        options={},  # Populate based on specific requirements or configurations\n",
    "        debug=True,  # Enable debug mode for detailed logs\n",
    "        normalize_debug=False,  # As per requirements\n",
    "        normalize_graphs_output=False,  # As per requirements\n",
    "        graphs_output_dir=Path(paths.get('plots_output_dir', '../preprocessor/plots')).resolve(),\n",
    "        transformers_dir=transformers_dir\n",
    "    )\n",
    "\n",
    "    # Execute Preprocessing for Prediction\n",
    "    try:\n",
    "        X_preprocessed, recommendations, X_inversed = preprocessor.final_preprocessing(df_predict)\n",
    "        print(\"X_new_preprocessed type = \", type(X_preprocessed), \"X_new_inverse type = \", type(X_inversed))\n",
    "        logger.info(\"✅ Preprocessing completed successfully in predict mode.\")\n",
    "        logger.debug(f\"Shape of X_preprocessed: {X_preprocessed.shape}\")\n",
    "        logger.debug(f\"Columns in X_preprocessed: {X_preprocessed.columns.tolist()}\")\n",
    "        logger.debug(f\"Shape of X_inversed: {X_inversed.shape}\")\n",
    "        logger.debug(f\"Columns in X_inversed: {X_inversed.columns.tolist()}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Preprocessing failed in predict mode: {e}\")\n",
    "        return\n",
    "\n",
    "    # ----------------------------\n",
    "    # Step 7: Load the Best Model\n",
    "    # ----------------------------\n",
    "    try:\n",
    "        trained_model = load_model(best_model_name, model_save_dir)\n",
    "        model_path = model_save_dir / best_model_name.replace(' ', '_') / 'trained_model.pkl'\n",
    "        logger.info(f\"✅ Trained model loaded from '{model_path}'.\")\n",
    "        logger.debug(f\"Trained model type: {type(trained_model)}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Failed to load the best model '{best_model_name}': {e}\")\n",
    "        return\n",
    "\n",
    "    # ----------------------------\n",
    "    # Step 8: Make Predictions + Prediction Probabilities and add them to the inverse-transformed DataFrame\n",
    "    # ----------------------------\n",
    "    predictions, prediction_probs, X_inversed = predict_and_attach_predict_probs(trained_model, X_preprocessed, X_inversed)\n",
    "\n",
    "\n",
    "    # ----------------------------\n",
    "    # Step 12: Compute SHAP Values for Interpretability\n",
    "    # ----------------------------\n",
    "    try:\n",
    "        logger.info(\"📊 Computing SHAP values for interpretability...\")\n",
    "        explainer, shap_values = compute_shap_values(\n",
    "            trained_model, \n",
    "            X_preprocessed,  # Compute SHAP on preprocessed data\n",
    "            debug=config.get('logging', {}).get('debug', False), \n",
    "            logger=logger\n",
    "        )\n",
    "        logger.info(\"✅ SHAP values computed successfully.\")\n",
    "        \n",
    "        # Additional Debugging\n",
    "        logger.debug(f\"Type of shap_values: {type(shap_values)}\")\n",
    "        logger.debug(f\"Shape of shap_values: {shap_values.shape}\")\n",
    "        if hasattr(shap_values, 'feature_names'):\n",
    "            logger.debug(f\"SHAP feature names: {shap_values.feature_names}\")\n",
    "        else:\n",
    "            logger.debug(\"shap_values does not have 'feature_names' attribute.\")\n",
    "        \n",
    "        logger.debug(f\"Type of X_preprocessed: {type(X_preprocessed)}\")\n",
    "        logger.debug(f\"Shape of X_preprocessed: {X_preprocessed.shape}\")\n",
    "        logger.debug(f\"Columns in X_preprocessed: {X_preprocessed.columns.tolist()}\")\n",
    "        \n",
    "        # Check feature alignment\n",
    "        if hasattr(shap_values, 'feature_names'):\n",
    "            shap_feature_names = shap_values.feature_names\n",
    "        else:\n",
    "            shap_feature_names = X_preprocessed.columns.tolist()\n",
    "        \n",
    "        if list(shap_feature_names) != list(X_preprocessed.columns):\n",
    "            logger.error(\"Column mismatch between SHAP values and X_preprocessed.\")\n",
    "            logger.error(f\"SHAP feature names ({len(shap_feature_names)}): {shap_feature_names}\")\n",
    "            logger.error(f\"X_preprocessed columns ({len(X_preprocessed.columns)}): {X_preprocessed.columns.tolist()}\")\n",
    "            raise ValueError(\"Column mismatch between SHAP values and X_preprocessed.\")\n",
    "        else:\n",
    "            logger.debug(\"Column alignment verified between SHAP values and X_preprocessed.\")\n",
    "        \n",
    "        # Ensure all features are numeric\n",
    "        X_preprocessed = X_preprocessed.apply(pd.to_numeric, errors='coerce')\n",
    "        logger.debug(\"Converted all features in X_preprocessed to numeric types.\")\n",
    "        \n",
    "        # Check for non-numeric columns\n",
    "        non_numeric_cols = X_preprocessed.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "        if non_numeric_cols:\n",
    "            logger.error(f\"Non-numeric columns found in X_preprocessed: {non_numeric_cols}\")\n",
    "            raise ValueError(\"All features must be numeric for SHAP plotting.\")\n",
    "        else:\n",
    "            logger.debug(\"All features in X_preprocessed are numeric.\")\n",
    "        \n",
    "        # Check for missing values in shap_values\n",
    "        if hasattr(shap_values, 'values'):\n",
    "            if pd.isnull(shap_values.values).any():\n",
    "                logger.warning(\"SHAP values contain NaNs.\")\n",
    "            else:\n",
    "                logger.debug(\"SHAP values do not contain NaNs.\")\n",
    "        else:\n",
    "            logger.warning(\"shap_values do not have 'values' attribute.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Failed to compute SHAP values: {e}\")\n",
    "        return\n",
    "\n",
    "    # ----------------------------\n",
    "    # Step 13: Generate SHAP Summary Plot\n",
    "    # ----------------------------\n",
    "    try:\n",
    "        shap_summary_path = predictions_output_path / 'shap_summary.png'\n",
    "        logger.debug(f\"SHAP summary plot will be saved to: {shap_summary_path}\")\n",
    "        \n",
    "        # Pass X_preprocessed to SHAP plotting functions\n",
    "        plot_shap_summary(\n",
    "            shap_values=shap_values, \n",
    "            X_original=X_preprocessed,  # Use preprocessed data that matches SHAP values\n",
    "            save_path=shap_summary_path, \n",
    "            debug=config.get('logging', {}).get('debug', False), \n",
    "            logger=logger\n",
    "        )\n",
    "        logger.info(f\"✅ SHAP summary plot saved to '{shap_summary_path}'.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Failed to generate SHAP summary plot: {e}\")\n",
    "        return\n",
    "\n",
    "    # ----------------------------\n",
    "    # Step 14: Generate SHAP Dependence Plots for Top Features\n",
    "    # ----------------------------\n",
    "    try:\n",
    "        top_n = len(X_preprocessed.columns)  # Define how many top features you want\n",
    "        logger.debug(f\"Generating SHAP dependence plots for top {top_n} features.\")\n",
    "        recommendations = generate_global_recommendations(\n",
    "            shap_values=shap_values, \n",
    "            X_original=X_preprocessed,  # Use preprocessed data\n",
    "            top_n=top_n, \n",
    "            debug=config.get('logging', {}).get('debug', False), \n",
    "            use_mad=False,  # Set to True if using MAD for range definitions\n",
    "            logger=logger\n",
    "        )\n",
    "        shap_dependence_dir = predictions_output_path / 'shap_dependence_plots'\n",
    "        os.makedirs(shap_dependence_dir, exist_ok=True)\n",
    "        logger.debug(f\"SHAP dependence plots will be saved to: {shap_dependence_dir}\")\n",
    "        for feature in recommendations.keys():\n",
    "            shap_dependence_filename = f\"shap_dependence_{feature}.png\"\n",
    "            shap_dependence_path = shap_dependence_dir / shap_dependence_filename\n",
    "            logger.debug(f\"Generating SHAP dependence plot for feature '{feature}' at '{shap_dependence_path}'.\")\n",
    "            plot_shap_dependence(\n",
    "                shap_values=shap_values, \n",
    "                feature=feature, \n",
    "                X_original=X_preprocessed,  # Use preprocessed data\n",
    "                save_path=shap_dependence_path, \n",
    "                debug=config.get('logging', {}).get('debug', False), \n",
    "                logger=logger\n",
    "            )\n",
    "        logger.info(f\"✅ SHAP dependence plots saved to '{shap_dependence_dir}'.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Failed to generate SHAP dependence plots: {e}\")\n",
    "        return\n",
    "\n",
    "    # ----------------------------\n",
    "    # Step 15: Annotate Final Dataset with SHAP Recommendations\n",
    "    # ----------------------------\n",
    "    try:\n",
    "        logger.info(\"🔍 Annotating final dataset with SHAP recommendations...\")\n",
    "        feature_metadata = {}\n",
    "        for feature in X_preprocessed.columns:\n",
    "            if 'angle' in feature.lower():\n",
    "                unit = 'degrees'\n",
    "            else:\n",
    "                unit = 'meters'\n",
    "            feature_metadata[feature] = {'unit': unit}\n",
    "\n",
    "        specific_feedback_list = []\n",
    "        for idx in X_inversed.index:\n",
    "            trial = X_inversed.loc[idx]\n",
    "            shap_values_trial = shap_values[idx]  # Removed .values\n",
    "            feedback = generate_individual_feedback(trial, shap_values_trial, feature_metadata, logger=logger)\n",
    "            specific_feedback_list.append(feedback)\n",
    "            if config.get('logging', {}).get('debug', False):\n",
    "                logger.debug(f\"Generated feedback for trial {idx}: {feedback}\")\n",
    "        X_inversed['specific_feedback'] = specific_feedback_list\n",
    "\n",
    "        logger.info(\"✅ Specific feedback generated for all trials.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Failed to generate specific feedback: {e}\")\n",
    "        return\n",
    "\n",
    "    # ----------------------------\n",
    "    # Step 16: Save the Final Dataset with Predictions and SHAP Annotations\n",
    "    # ----------------------------\n",
    "    try:\n",
    "        os.makedirs(predictions_output_path, exist_ok=True)\n",
    "        final_output_path = predictions_output_path / 'final_predictions_with_shap.csv'\n",
    "        X_inversed.to_csv(final_output_path, index=False)\n",
    "        logger.info(f\"✅ Final dataset with predictions and SHAP annotations saved to '{final_output_path}'.\")\n",
    "        logger.debug(f\"Final dataset shape: {X_inversed.shape}\")\n",
    "        logger.debug(f\"Final dataset columns: {X_inversed.columns.tolist()}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Failed to save the final dataset: {e}\")\n",
    "        return\n",
    "\n",
    "    # ----------------------------\n",
    "    # Step 17: Save Global Recommendations as JSON\n",
    "    # ----------------------------\n",
    "    try:\n",
    "        recommendations_filename = \"global_shap_recommendations.json\"\n",
    "        recommendations_save_path = predictions_output_path / recommendations_filename\n",
    "        with open(recommendations_save_path, \"w\") as f:\n",
    "            json.dump(recommendations, f, indent=4)\n",
    "        logger.info(f\"✅ Global SHAP recommendations saved to '{recommendations_save_path}'.\")\n",
    "        logger.debug(f\"Global SHAP recommendations saved.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Failed to save global SHAP recommendations: {e}\")\n",
    "        return\n",
    "\n",
    "    # ----------------------------\n",
    "    # Step 18: Generate Specific Feedback for Each Trial\n",
    "    # ----------------------------\n",
    "    # Note: This step was moved above in the script for logical flow.\n",
    "\n",
    "    # ----------------------------\n",
    "    # Step 19: Display Minimal Outputs\n",
    "    # ----------------------------\n",
    "    try:\n",
    "        # Display the first few rows of the final dataset\n",
    "        print(\"\\nInverse Transformed Prediction DataFrame with Predictions and SHAP Annotations:\")\n",
    "        print(X_inversed.head())\n",
    "\n",
    "        # Display global recommendations\n",
    "        print(\"\\nGlobal SHAP Recommendations:\")\n",
    "        for feature, rec in recommendations.items():\n",
    "            print(f\"{feature}: Range={rec['range']}, Importance={rec['importance']}, Direction={rec['direction']}\")\n",
    "\n",
    "        # Display specific feedback for each trial\n",
    "        print(\"\\nSpecific Feedback for Each Trial:\")\n",
    "        for idx, row in X_inversed.iterrows():\n",
    "            print(f\"Trial {idx + 1}:\")\n",
    "            for feature, feedback in row['specific_feedback'].items():\n",
    "                print(f\"  - {feedback}\")\n",
    "            print()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Error during displaying outputs: {e}\")\n",
    "        return\n",
    "\n",
    "    logger.info(\"✅ Prediction pipeline with SHAP analysis completed successfully.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ml/bayesian_optimization/bayesian_optimized_metrics_EXAMPLE.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ml/bayesian_optimization/bayesian_optimized_metrics_EXAMPLE.py\n",
    "\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def prepare_data():\n",
    "    \"\"\"Prepare training data.\"\"\"\n",
    "    np.random.seed(42)\n",
    "    X_test = pd.DataFrame({\n",
    "        'knee_max_angle': np.random.uniform(40, 140, 200),\n",
    "        'wrist_max_angle': np.random.uniform(0, 90, 200),\n",
    "        'elbow_max_angle': np.random.uniform(30, 160, 200),\n",
    "    })\n",
    "    y_test = pd.Series(np.random.choice([0, 1], size=200))\n",
    "\n",
    "    features = ['knee_max_angle', 'wrist_max_angle', 'elbow_max_angle']\n",
    "    X_train = X_test[features]\n",
    "    y_train = y_test\n",
    "\n",
    "    # Debug: Check the dataset details\n",
    "    print(f\"X_train shape: {X_train.shape}\")\n",
    "    print(f\"X_train sample:\\n{X_train.head()}\")\n",
    "    print(f\"y_train sample:\\n{y_train.head()}\")\n",
    "    return X_train, y_train, features\n",
    "\n",
    "\n",
    "def train_model(X_train, y_train):\n",
    "    \"\"\"Train a Decision Tree Classifier.\"\"\"\n",
    "    clf = DecisionTreeClassifier()\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Debug: Check feature importances\n",
    "    print(f\"Feature importances: {clf.feature_importances_}\")\n",
    "    return clf\n",
    "\n",
    "\n",
    "def define_search_space(features):\n",
    "    \"\"\"Define the search space for optimization.\"\"\"\n",
    "    spaces = {\n",
    "        'knee_max_angle': Real(40, 140, name='knee_angle'),\n",
    "        'wrist_max_angle': Real(30, 90, name='wrist_angle'),\n",
    "        'elbow_max_angle': Real(30, 160, name='elbow_angle')\n",
    "    }\n",
    "    return [spaces[feature] for feature in features]\n",
    "\n",
    "\n",
    "def objective_function(clf):\n",
    "    \"\"\"Create an objective function for Bayesian Optimization.\"\"\"\n",
    "    def objective(params):\n",
    "        knee, wrist, elbow = params\n",
    "        input_df = pd.DataFrame([[knee, wrist, elbow]], \n",
    "                                columns=['knee_max_angle', 'wrist_max_angle', 'elbow_max_angle'])\n",
    "        success_prob = clf.predict_proba(input_df)[0, 1]\n",
    "        # Debug: Log evaluation details\n",
    "        print(f\"Evaluating: knee={knee:.2f}, wrist={wrist:.2f}, elbow={elbow:.2f}, success_prob={success_prob:.2f}\")\n",
    "        return -success_prob  # Negative for minimization\n",
    "    return objective\n",
    "\n",
    "\n",
    "def perform_optimization(objective, space):\n",
    "    \"\"\"Perform Bayesian Optimization.\"\"\"\n",
    "    res = gp_minimize(\n",
    "        func=objective,\n",
    "        dimensions=space,\n",
    "        n_calls=50,\n",
    "        n_random_starts=10,\n",
    "        random_state=42\n",
    "    )\n",
    "    # Debug: Log optimization result details\n",
    "    print(f\"Optimization result: {res}\")\n",
    "    return res\n",
    "\n",
    "\n",
    "def calculate_baselines(X_train):\n",
    "    \"\"\"Calculate baseline values for each feature.\"\"\"\n",
    "    baselines = {col: X_train[col].mean() for col in X_train.columns}\n",
    "    print(f\"Baseline values: {baselines}\")\n",
    "    return baselines\n",
    "\n",
    "\n",
    "def compare_results(features, baselines, results):\n",
    "    \"\"\"Compare optimal values with baselines.\"\"\"\n",
    "    print(\"\\nOptimization Results:\")\n",
    "    for feature, baseline, optimal in zip(features, baselines.values(), results.x):\n",
    "        difference = optimal - baseline\n",
    "        print(f\"{feature} - Optimal: {optimal:.2f}, Baseline: {baseline:.2f}, Difference: {difference:.2f}\")\n",
    "\n",
    "\n",
    "# Main workflow\n",
    "X_train, y_train, features = prepare_data()\n",
    "clf = train_model(X_train, y_train)\n",
    "space = define_search_space(features)\n",
    "objective = objective_function(clf)\n",
    "res = perform_optimization(objective, space)\n",
    "baselines = calculate_baselines(X_train)\n",
    "compare_results(features, baselines, res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ml/bayesian_optimization/bayesian_optimized_metrics.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ml/bayesian_optimization/bayesian_optimized_metrics.py\n",
    "# previously: %%writefile ../../src/freethrow_predictions/ml/bayes_optim_angles_xgboostpreds.py\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "# Import the early stopping callback from skopt\n",
    "from skopt.callbacks import DeltaYStopper\n",
    "\n",
    "# Configuration and model utilities\n",
    "from ml.config.config_loader import load_config\n",
    "from ml.config.config_models import AppConfig\n",
    "from ml.train_utils.train_utils import load_model\n",
    "\n",
    "# Updated DataPreprocessor (which now returns X_inversed as part of final_preprocessing)\n",
    "from datapreprocessor import DataPreprocessor\n",
    "from ml.feature_selection.feature_importance_calculator import manage_features\n",
    "\n",
    "# Simple helper for debug prints (logger removed)\n",
    "def log_debug(message, debug):\n",
    "    if debug:\n",
    "        print(message)\n",
    "\n",
    "def get_preprocessing_results(preprocessor, df, debug):\n",
    "    \"\"\"\n",
    "    Run preprocessing and return X_preprocessed, X_inversed.\n",
    "    We assume final_preprocessing returns: X_preprocessed, recommendations, X_inversed.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        X_preprocessed, recommendations, X_inversed = preprocessor.final_preprocessing(df)\n",
    "        log_debug(f\"[Debug] Preprocessing complete. X_preprocessed shape: {X_preprocessed.shape}\", debug)\n",
    "        return X_preprocessed, X_inversed\n",
    "    except Exception as e:\n",
    "        log_debug(f\"[Error] Preprocessing failed: {e}\", debug)\n",
    "        raise\n",
    "\n",
    "def compute_real_ranges(X_inversed, optimization_columns, debug):\n",
    "    \"\"\"\n",
    "    Compute the min and max for each optimization column from X_inversed (real domain).\n",
    "    \"\"\"\n",
    "    real_ranges = {}\n",
    "    for col in optimization_columns:\n",
    "        # Here we assume that the column names in X_inversed are the original names.\n",
    "        real_min = float(X_inversed[col].min())\n",
    "        real_max = float(X_inversed[col].max())\n",
    "        real_ranges[col] = (real_min, real_max)\n",
    "    log_debug(f\"[Debug] Computed optimization ranges (real domain): {real_ranges}\", debug)\n",
    "    return real_ranges\n",
    "\n",
    "def map_transformed_to_real(candidate_val, trans_range, real_range):\n",
    "    \"\"\"\n",
    "    Given a candidate value in the transformed domain, linearly map it into\n",
    "    the real domain using the provided ranges.\n",
    "    \n",
    "    Parameters:\n",
    "      - candidate_val: a value in the transformed domain\n",
    "      - trans_range: (min, max) tuple from X_preprocessed\n",
    "      - real_range: (min, max) tuple from X_inversed\n",
    "      \n",
    "    Returns:\n",
    "      The candidate value mapped into the real domain.\n",
    "    \"\"\"\n",
    "    trans_min, trans_max = trans_range\n",
    "    real_min, real_max = real_range\n",
    "    # Avoid division by zero by checking if trans_max == trans_min.\n",
    "    if trans_max == trans_min:\n",
    "        return real_min\n",
    "    # Linear mapping formula:\n",
    "    real_val = real_min + ((candidate_val - trans_min) / (trans_max - trans_min)) * (real_max - real_min)\n",
    "    return real_val\n",
    "\n",
    "\n",
    "def compute_optimization_ranges(X_transformed, optimization_columns, debug):\n",
    "    \"\"\"\n",
    "    Compute the min and max for each optimization column from X_transformed.\n",
    "    If a transformed feature has a prefix (e.g., 'num__'), use that column.\n",
    "    \"\"\"\n",
    "    ranges = {}\n",
    "    for col in optimization_columns:\n",
    "        # Determine the key in the transformed data\n",
    "        transformed_col = f\"num__{col}\" if f\"num__{col}\" in X_transformed.columns else col\n",
    "        ranges[col] = (float(X_transformed[transformed_col].min()), float(X_transformed[transformed_col].max()))\n",
    "    log_debug(f\"[Debug] Computed optimization ranges (transformed domain): {ranges}\", debug)\n",
    "    return ranges\n",
    "\n",
    "def define_search_space(opt_ranges, optimization_columns, debug):\n",
    "    \"\"\"\n",
    "    Define the search space for Bayesian optimization using ranges from X_transformed.\n",
    "    \"\"\"\n",
    "    missing = [col for col in optimization_columns if col not in opt_ranges]\n",
    "    if missing:\n",
    "        raise KeyError(f\"Missing columns in optimization ranges: {missing}\")\n",
    "    space = [Real(opt_ranges[col][0], opt_ranges[col][1], name=col) for col in optimization_columns]\n",
    "    log_debug(f\"[Debug] Defined search space: {space}\", debug)\n",
    "    return space\n",
    "\n",
    "\n",
    "\n",
    "def get_model_feature_order(model):\n",
    "    if hasattr(model, \"get_booster\"):\n",
    "        # XGBoost case\n",
    "        return model.get_booster().feature_names\n",
    "    elif hasattr(model, \"feature_names_\"):\n",
    "        # CatBoost case\n",
    "        return model.feature_names_\n",
    "    else:\n",
    "        raise AttributeError(\"The model does not have a known attribute for feature names.\")\n",
    "\n",
    "def objective(params, optimization_columns, X_preprocessed, model, debug):\n",
    "    # Create the baseline feature vector by taking the mean of X_preprocessed\n",
    "    feature_vector = X_preprocessed.mean(axis=0).copy()\n",
    "    \n",
    "    if debug:\n",
    "        print(\"[Debug] Columns in X_preprocessed.mean(axis=0):\", feature_vector.index.tolist())\n",
    "    \n",
    "    # Update features with candidate parameter values\n",
    "    for col, value in zip(optimization_columns, params):\n",
    "        transformed_col = f\"num__{col}\"\n",
    "        if transformed_col in feature_vector.index:\n",
    "            print(f\"[Debug] Updating {transformed_col} with value {value}\")\n",
    "            feature_vector[transformed_col] = value\n",
    "        else:\n",
    "            print(f\"[Debug] Warning: {transformed_col} not found in feature_vector!\")\n",
    "    \n",
    "    if debug:\n",
    "        print(\"[Debug] Columns in feature_vector after assignment:\", feature_vector.index.tolist())\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    feature_df = pd.DataFrame([feature_vector])\n",
    "    \n",
    "    # Get the expected feature order using our helper function\n",
    "    expected_feature_order = get_model_feature_order(model)\n",
    "    if debug:\n",
    "        print(\"[Debug] Expected feature order:\", expected_feature_order)\n",
    "        print(\"[Debug] Feature DataFrame columns before reindex:\", feature_df.columns.tolist())\n",
    "    \n",
    "    feature_df = feature_df.reindex(columns=expected_feature_order)\n",
    "    \n",
    "    if debug:\n",
    "        print(\"[Debug] Feature DataFrame columns after reindex:\", feature_df.columns.tolist())\n",
    "    \n",
    "    # Get success probability\n",
    "    success_prob = model.predict_proba(feature_df)[0, 1]\n",
    "    if debug:\n",
    "        print(f\"[Debug] Objective with params {params}: success_prob = {success_prob:.4f}\")\n",
    "    return -success_prob\n",
    "\n",
    "\n",
    "\n",
    "def perform_optimization(wrapper_objective, search_space, n_calls, debug,\n",
    "                         delta_threshold=0.005, n_best=3):  # adjusted values\n",
    "    \"\"\"\n",
    "    Run gp_minimize with the wrapped objective and search space.\n",
    "    Adds early stopping via a callback if improvement is below delta_threshold\n",
    "    for n_best iterations.\n",
    "    \"\"\"\n",
    "    stopper = DeltaYStopper(delta=delta_threshold, n_best=n_best)\n",
    "    callbacks = [stopper]\n",
    "\n",
    "    result = gp_minimize(func=wrapper_objective, \n",
    "                         dimensions=search_space, \n",
    "                         n_calls=n_calls, \n",
    "                         random_state=42,\n",
    "                         callback=callbacks)\n",
    "\n",
    "    if debug:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(range(1, len(result.func_vals) + 1), [-val for val in result.func_vals], marker='o')\n",
    "        plt.title(\"Bayesian Optimization Progress (Transformed Domain)\")\n",
    "        plt.xlabel(\"Iteration\")\n",
    "        plt.ylabel(\"Success Probability\")\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "def bayesian_optimization_main(config: AppConfig, delta_threshold: float, n_best: float, n_calls: float, df: pd.DataFrame, debug=False):\n",
    "    # Your implementation here\n",
    "    if debug:\n",
    "        print(f\"Delta Threshold: {delta_threshold}, Type: {type(delta_threshold)}\")\n",
    "        print(f\"n_best: {n_best}, Type: {type(n_best)}\")\n",
    "        print(f\"n_calls: {n_calls}, Type: {type(n_calls)}\")\n",
    "    # ----------------------------\n",
    "    # Step 1: Extract Configuration Values\n",
    "    # ----------------------------\n",
    "    data_dir = Path(config.paths.data_dir).resolve()\n",
    "    features_file = data_dir / config.paths.features_metadata_file\n",
    "    model_save_dir = Path(config.paths.model_save_base_dir).resolve()\n",
    "    transformers_dir = Path(config.paths.transformers_save_base_dir).resolve()\n",
    "    \n",
    "    # ----------------------------\n",
    "    # Step 2: Load Optimization Columns via Feature Assets\n",
    "    # ----------------------------\n",
    "    try:\n",
    "        with open(features_file, 'rb') as f:\n",
    "            selected_features = pickle.load(f)\n",
    "        # Allow for either list or DataFrame format.\n",
    "        if isinstance(selected_features, list):\n",
    "            optimization_columns = selected_features\n",
    "        else:\n",
    "            optimization_columns = selected_features.columns.tolist()\n",
    "        log_debug(f\"[Debug] Loaded optimization columns: {optimization_columns}\", debug)\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] Failed to load selected features: {e}\")\n",
    "        raise\n",
    "\n",
    "    # Remove y_variable(s) from optimization columns if present.\n",
    "    y_variables = config.features.y_variable  # List of target variables\n",
    "    log_debug(f\"[Debug] Target variables to remove: {y_variables}\", debug)\n",
    "    for y_var in y_variables:\n",
    "        if y_var in optimization_columns:\n",
    "            optimization_columns.remove(y_var)\n",
    "            log_debug(f\"[Debug] Removed '{y_var}' from optimization columns.\", debug)\n",
    "        else:\n",
    "            log_debug(f\"[Debug] '{y_var}' not found in optimization columns.\", debug)\n",
    "    \n",
    "    log_debug(f\"[Debug] Final optimization columns: {optimization_columns}\", debug)\n",
    "    \n",
    "    # ----------------------------\n",
    "    # Step 3: Initialize DataPreprocessor Using Column Assets\n",
    "    # ----------------------------\n",
    "    paths = config.paths\n",
    "    feature_paths = {\n",
    "        'features': Path('../../data/preprocessor/features_info/final_ml_df_selected_features_columns.pkl'),\n",
    "        'ordinal_categoricals': Path('../../data/preprocessor/features_info/ordinal_categoricals.pkl'),\n",
    "        'nominal_categoricals': Path('../../data/preprocessor/features_info/nominal_categoricals.pkl'),\n",
    "        'numericals': Path('../../data/preprocessor/features_info/numericals.pkl'),\n",
    "        'y_variable': Path('../../data/preprocessor/features_info/y_variable.pkl')\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        feature_lists = manage_features(mode='load', paths=feature_paths)\n",
    "        y_variable_list = feature_lists.get('y_variable', [])\n",
    "        ordinal_categoricals = feature_lists.get('ordinal_categoricals', [])\n",
    "        nominal_categoricals = feature_lists.get('nominal_categoricals', [])\n",
    "        numericals = feature_lists.get('numericals', [])\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] Failed to load feature lists: {e}\")\n",
    "        raise\n",
    "\n",
    "    preprocessor = DataPreprocessor(\n",
    "        model_type=\"Tree Based Classifier\",\n",
    "        y_variable=y_variable_list,\n",
    "        ordinal_categoricals=ordinal_categoricals,\n",
    "        nominal_categoricals=nominal_categoricals,\n",
    "        numericals=numericals,\n",
    "        mode='predict',  # The same mode as in predict pipelines\n",
    "        options={},\n",
    "        debug=False,\n",
    "        normalize_debug=False,\n",
    "        normalize_graphs_output=False,\n",
    "        graphs_output_dir=Path(config.paths.plots_output_dir).resolve(),\n",
    "        transformers_dir=transformers_dir\n",
    "    )\n",
    "\n",
    "    # ----------------------------\n",
    "    # Step 4: Preprocess the Data (Transformed Domain)\n",
    "    # ----------------------------\n",
    "    X_preprocessed, X_inversed = get_preprocessing_results(preprocessor, df, debug)\n",
    "    # Compute optimization ranges using X_preprocessed (transformed space)\n",
    "    opt_ranges = compute_optimization_ranges(X_preprocessed, optimization_columns, debug)\n",
    "    \n",
    "    # ----------------------------\n",
    "    # Step 5: Load the Trained Model\n",
    "    # ----------------------------\n",
    "    # For example, load the best model based on tuning info; here we assume you know the best model name.\n",
    "    trained_model = load_model('CatBoost', model_save_dir)  # Replace with your logic if available\n",
    "\n",
    "    # ----------------------------\n",
    "    # Step 6: Define the Search Space and Objective Function\n",
    "    # ----------------------------\n",
    "    search_space = define_search_space(opt_ranges, optimization_columns, debug)\n",
    "    wrapper_objective = lambda params: objective(params, optimization_columns, X_preprocessed, trained_model, debug)\n",
    "\n",
    "    # ----------------------------\n",
    "    # Step 7: Perform Bayesian Optimization with Early Stopping\n",
    "    # ----------------------------\n",
    "    res = perform_optimization(wrapper_objective, search_space, n_calls=n_calls, debug=debug,\n",
    "                               delta_threshold=delta_threshold, n_best=n_best)\n",
    "    params_df = pd.DataFrame(res.x_iters, columns=optimization_columns)\n",
    "    params_df['success_prob'] = [-val for val in res.func_vals]\n",
    "\n",
    "    # ----------------------------\n",
    "    # Step 8: Compare Baseline vs. Optimized (in Transformed Domain)\n",
    "    # ----------------------------\n",
    "    # Use the preprocessed (transformed) mean for the baseline.\n",
    "    baseline_feature_vector = X_preprocessed.mean(axis=0)\n",
    "    print(\"[Debug] Baseline feature vector (transformed):\", baseline_feature_vector)\n",
    "    baseline_df = pd.DataFrame([baseline_feature_vector])\n",
    "    # Use this:\n",
    "    expected_feature_order = get_model_feature_order(trained_model)\n",
    "    baseline_df = baseline_df.reindex(columns=expected_feature_order)\n",
    "    print(\"[Debug] Baseline DataFrame columns:\", baseline_df.columns.tolist())\n",
    "    baseline_success = trained_model.predict_proba(baseline_df)[0, 1]\n",
    "\n",
    "    print(f\"[Debug] Baseline success probability: {baseline_success:.4f}\")\n",
    "\n",
    "    # ----- Step 8: Comparison using Real Numbers -----\n",
    "\n",
    "    # (A) Compute the baseline (real) values using X_inversed.\n",
    "    baseline_real = {}\n",
    "    for col in optimization_columns:\n",
    "        baseline_real[col] = X_inversed[col].mean()\n",
    "    baseline_real_series = pd.Series(baseline_real)\n",
    "\n",
    "    # (B) Compute candidate (optimized) parameters in the real domain.\n",
    "    # We already have candidate values (res.x) in the transformed domain.\n",
    "    real_ranges = compute_real_ranges(X_inversed, optimization_columns, debug)\n",
    "    candidate_real = {}\n",
    "    for col, cand_val in zip(optimization_columns, res.x):\n",
    "        trans_range = opt_ranges[col]  # from transformed X_preprocessed\n",
    "        candidate_real[col] = map_transformed_to_real(cand_val, trans_range, real_ranges[col])\n",
    "    candidate_real_series = pd.Series(candidate_real)\n",
    "\n",
    "    # (C) Build the parameter comparison table\n",
    "    min_values_real = [real_ranges[col][0] for col in optimization_columns]\n",
    "    max_values_real = [real_ranges[col][1] for col in optimization_columns]\n",
    "\n",
    "    comparison_real = pd.DataFrame({\n",
    "        \"Parameter\": optimization_columns,\n",
    "        \"Baseline (Real)\": baseline_real_series.values,\n",
    "        \"Optimized (Candidate, Real)\": candidate_real_series.values,\n",
    "        \"Difference\": candidate_real_series.values - baseline_real_series.values,\n",
    "        \"Min (Real)\": min_values_real,\n",
    "        \"Max (Real)\": max_values_real\n",
    "    })\n",
    "\n",
    "    # (D) Compute success rates from the model.\n",
    "    baseline_success = trained_model.predict_proba(pd.DataFrame([X_preprocessed.mean(axis=0)]))[0, 1]\n",
    "    candidate_success = -res.fun  # recall objective returns negative success probability\n",
    "\n",
    "    # Instead of appending a separate row, add new columns for the success rates.\n",
    "    comparison_real[\"Success Rate (Baseline)\"] = baseline_success\n",
    "    comparison_real[\"Success Rate (Candidate)\"] = candidate_success\n",
    "    comparison_real[\"Success Rate Diff\"] = candidate_success - baseline_success\n",
    "\n",
    "    print(\"Comparison of Baseline vs. Optimized Parameters (Real Domain):\")\n",
    "    print(comparison_real)\n",
    "    \n",
    "    # save the results to data\\predictions\\bayesian_optimization_results\n",
    "    output_dir = Path(config.paths.predictions_output_dir).resolve() / 'bayesian_optimization_results'\n",
    "    comparison_real.to_csv(output_dir / 'bayesian_optimization_results.csv', index=False)\n",
    "\n",
    "    return comparison_real\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config_path = Path('../../data/model/preprocessor_config/preprocessor_config.yaml')\n",
    "    try:\n",
    "        config: AppConfig = load_config(config_path)\n",
    "        print(f\"Config loaded from {config_path}\")\n",
    "    except Exception as e:\n",
    "        print(\"Failed to load config:\", e)\n",
    "        exit(1)\n",
    "    \n",
    "    data_dir = Path(config.paths.data_dir).resolve()\n",
    "    df_path = data_dir / config.paths.raw_data\n",
    "    df = pd.read_csv(df_path)\n",
    "    \n",
    "    results = bayesian_optimization_main(config, \n",
    "                                         delta_threshold=0.001, # the minimum improvement (change in the objective value) that must be observed for the optimizer to consider a new candidate as “better.”\n",
    "                                         n_best=5, #the number of successive iterations that are compared to decide if the improvement is below the delta_threshold.\n",
    "                                         n_calls=50, #maximum number of function evaluations (iterations) that the optimization algorithm will perform.\n",
    "                                         df=df, \n",
    "                                         debug=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science_ft_bio_predictions",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
