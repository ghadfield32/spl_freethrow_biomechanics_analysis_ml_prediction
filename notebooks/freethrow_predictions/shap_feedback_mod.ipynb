{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 Overview of the Codebase\n",
    "\n",
    "Your codebase is structured into several modules, each handling different aspects of SHAP value computation, visualization, feedback generation, and data handling. Here's a brief summary:\n",
    "\n",
    "    shap_calculator.py: Computes SHAP values for the dataset and individual trials.\n",
    "    shap_visualizer.py: Generates SHAP plots (summary, dependence, and force plots).\n",
    "    feedback_generator.py: Generates global and individual feedback based on SHAP values.\n",
    "    shap_data_handler.py: Handles saving and loading SHAP values.\n",
    "    predict_with_shap_usage.py: Orchestrates the entire pipeline: preprocessing, prediction, SHAP computation, visualization, and feedback generation.\n",
    "\n",
    "1.2 Understanding the Flow\n",
    "\n",
    "    Data Preprocessing:\n",
    "        The DataPreprocessor filters and transforms input data.\n",
    "        The preprocessed data (X_preprocessed) and the inversed data (X_inversed) are prepared for prediction and SHAP analysis.\n",
    "\n",
    "    Model Prediction:\n",
    "        The best model (e.g., CatBoost) is loaded and used to make predictions and compute prediction probabilities.\n",
    "\n",
    "    SHAP Value Computation:\n",
    "        ShapCalculator computes SHAP values for the preprocessed data.\n",
    "        These SHAP values are used for visualization and feedback generation.\n",
    "\n",
    "    Visualization:\n",
    "        ShapVisualizer creates summary, dependence, and force plots based on SHAP values.\n",
    "\n",
    "    Feedback Generation:\n",
    "        FeedbackGenerator creates both global recommendations and trial-specific feedback.\n",
    "\n",
    "    Final Output:\n",
    "        The final dataset with SHAP annotations and feedback is saved for review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ml/shap/shap_utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ml/shap/shap_utils.py\n",
    "import pandas as pd\n",
    "import logging\n",
    "import logging.config\n",
    "import numpy as np\n",
    "import ast\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Optional\n",
    "from ml.config.config_models import AppConfig  # Adjust the import based on your project structure\n",
    "from ml.config.config_loader import load_config  # Assuming you have a config_loader module\n",
    "\n",
    "def load_dataset(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load dataset from the specified path.\n",
    "    \n",
    "    :param path: Path to the CSV file.\n",
    "    :return: Loaded DataFrame.\n",
    "    \"\"\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Dataset not found at {path}\")\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "\n",
    "def setup_logging(config: AppConfig, log_file_path: Path) -> logging.Logger:\n",
    "    \"\"\"\n",
    "    Set up logging based on the configuration.\n",
    "    \n",
    "    :param config: Application configuration.\n",
    "    :param log_file_path: Path to the log file.\n",
    "    :return: Configured logger.\n",
    "    \"\"\"\n",
    "    log_level = config.logging.level.upper()\n",
    "    logging_config = {\n",
    "        'version': 1,\n",
    "        'disable_existing_loggers': False,\n",
    "        'formatters': {\n",
    "            'standard': {\n",
    "                'format': '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "            },\n",
    "        },\n",
    "        'handlers': {\n",
    "            'console': {\n",
    "                'class': 'logging.StreamHandler',\n",
    "                'level': log_level,\n",
    "                'formatter': 'standard',\n",
    "                'stream': 'ext://sys.stdout',\n",
    "            },\n",
    "            'file': {\n",
    "                'class': 'logging.FileHandler',\n",
    "                'level': log_level,\n",
    "                'formatter': 'standard',\n",
    "                'filename': str(log_file_path),\n",
    "            },\n",
    "        },\n",
    "        'loggers': {\n",
    "            '': {  # root logger\n",
    "                'handlers': ['console', 'file'],\n",
    "                'level': log_level,\n",
    "                'propagate': True\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "    logging.config.dictConfig(logging_config)\n",
    "    return logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def load_configuration(config_path: Path) -> AppConfig:\n",
    "    \"\"\"\n",
    "    Load the application configuration from a YAML file.\n",
    "\n",
    "    :param config_path: Path to the configuration YAML file.\n",
    "    :return: AppConfig object containing configuration parameters.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        config: AppConfig = load_config(config_path)\n",
    "        logging.getLogger(__name__).info(f\"Configuration loaded successfully from {config_path}.\")\n",
    "        return config\n",
    "    except Exception as e:\n",
    "        logging.getLogger(__name__).error(f\"Failed to load configuration from {config_path}: {e}\")\n",
    "        raise\n",
    "\n",
    "def initialize_logger(config: AppConfig, log_file: Path) -> logging.Logger:\n",
    "    \"\"\"\n",
    "    Initialize and return a logger based on the configuration.\n",
    "\n",
    "    :param config: AppConfig object containing configuration parameters.\n",
    "    :param log_file: Path to the log file.\n",
    "    :return: Configured logger instance.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger = setup_logging(config, log_file)\n",
    "        logger.info(\"Logger initialized successfully.\")\n",
    "        return logger\n",
    "    except Exception as e:\n",
    "        logging.basicConfig(level=logging.ERROR)\n",
    "        logging.getLogger(__name__).error(f\"Failed to initialize logger: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Test code to verify the functions in shap_utils.py\n",
    "    print(\"Testing shap_utils.py module...\")\n",
    "\n",
    "    # Step 1: Load Configuration\n",
    "    config_path = Path('../../data/model/preprocessor_config/preprocessor_config.yaml')\n",
    "    try:\n",
    "        config: AppConfig = load_config(config_path)\n",
    "        print(f\"✅ Configuration loaded successfully from {config_path}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to load configuration: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "    # Step 2: Set Up Logging\n",
    "    log_file = Path(config.paths.log_file).resolve()\n",
    "    try:\n",
    "        logger = setup_logging(config, log_file)\n",
    "        logger.info(\"Logging has been set up successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to set up logging: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "    # Step 3: Load Dataset\n",
    "    raw_data_path = Path(config.paths.data_dir).resolve() / config.paths.raw_data\n",
    "    try:\n",
    "        df = load_dataset(raw_data_path)\n",
    "        print(f\"✅ Dataset loaded successfully from {raw_data_path}.\")\n",
    "        print(f\"📊 Dataset Columns: {df.columns.tolist()}\")\n",
    "        logger.info(f\"Dataset loaded with shape: {df.shape}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Failed to load dataset: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ml/shap/shap_calculator.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ml/shap/shap_calculator.py\n",
    "import logging\n",
    "import shap\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Any, Dict, List, Tuple, Optional\n",
    "from pathlib import Path\n",
    "\n",
    "from ml.config.config_models import AppConfig\n",
    "from ml.config.config_loader import load_config\n",
    "from ml.shap.shap_utils import load_dataset, setup_logging, load_configuration, initialize_logger\n",
    "\n",
    "class ShapCalculator:\n",
    "    def __init__(self, model, model_type: Optional[str] = None, logger: logging.Logger = None):\n",
    "        \"\"\"        \n",
    "        Initialize the ShapCalculator with a model and an optional logger.\n",
    "        Detects the appropriate SHAP explainer based on model type.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.logger = logger or logging.getLogger(__name__)\n",
    "        self.explainer = None\n",
    "        self.model_type = model_type or self._infer_model_type()\n",
    "        self.logger.debug(f\"ShapCalculator initialized with model type: {self.model_type}\")\n",
    "    \n",
    "    def _infer_model_type(self) -> str:\n",
    "        import xgboost as xgb\n",
    "        if isinstance(self.model, xgb.XGBModel):\n",
    "            self.logger.debug(\"XGBoost model detected via XGBModel interface.\")\n",
    "            return \"xgboost\"\n",
    "        if isinstance(self.model, xgb.Booster):\n",
    "            self.logger.debug(\"XGBoost Booster instance detected.\")\n",
    "            return \"xgboost\"\n",
    "        if hasattr(self.model, \"get_booster\"):\n",
    "            self.logger.debug(\"XGBoost model detected via get_booster().\")\n",
    "            return \"xgboost\"\n",
    "        elif hasattr(self.model, 'tree_structure__'):\n",
    "            return 'tree'\n",
    "        elif hasattr(self.model, 'coef_'):\n",
    "            return 'linear'\n",
    "        elif hasattr(self.model, 'layers_'):\n",
    "            return 'deep'\n",
    "        else:\n",
    "            self.logger.warning(\"Unable to infer model type. Defaulting to 'tree'.\")\n",
    "            return 'tree'\n",
    "\n",
    "    def compute_shap_values(self, X: pd.DataFrame, debug: bool = False) -> Tuple[shap.Explainer, np.ndarray]:\n",
    "        self.logger.info(\"Initializing SHAP explainer...\")\n",
    "        try:\n",
    "            # Ensure all features are numeric.\n",
    "            non_numeric_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "            if non_numeric_features:\n",
    "                self.logger.error(f\"Non-numeric features detected in input data: {non_numeric_features}\")\n",
    "                raise ValueError(f\"Preprocessed data contains non-numeric features: {non_numeric_features}\")\n",
    "            else:\n",
    "                self.logger.debug(\"All features are numeric.\")\n",
    "\n",
    "            # For XGBoost, align feature names if needed.\n",
    "            if self.model_type == 'xgboost':\n",
    "                booster = self.model.get_booster()\n",
    "                expected_features = booster.feature_names\n",
    "                if expected_features is None:\n",
    "                    self.logger.warning(\"No feature names found in the booster; skipping feature alignment.\")\n",
    "                else:\n",
    "                    missing = set(expected_features) - set(X.columns)\n",
    "                    if missing:\n",
    "                        self.logger.error(f\"Missing features in SHAP input: {missing}\")\n",
    "                        raise ValueError(\"Feature mismatch between model and input data\")\n",
    "                    self.logger.debug(f\"Reordering features to match expected order: {expected_features}\")\n",
    "                    X = X[expected_features]\n",
    "\n",
    "            # Choose explainer.\n",
    "            if self.model_type in ['tree', 'xgboost']:\n",
    "                model_to_use = self.model if self.model_type == 'tree' else self.model.get_booster()\n",
    "                self.logger.debug(\"Using SHAP TreeExplainer.\")\n",
    "                self.explainer = shap.TreeExplainer(model_to_use, feature_perturbation=\"tree_path_dependent\")\n",
    "            elif self.model_type == 'linear':\n",
    "                self.explainer = shap.LinearExplainer(self.model, X, feature_dependence=\"independent\")\n",
    "            elif self.model_type == 'deep':\n",
    "                self.explainer = shap.DeepExplainer(self.model, X)\n",
    "            else:\n",
    "                self.logger.warning(f\"Unrecognized model type '{self.model_type}'. Defaulting to TreeExplainer.\")\n",
    "                self.explainer = shap.TreeExplainer(self.model, feature_perturbation=\"tree_path_dependent\")\n",
    "\n",
    "            if debug:\n",
    "                self.logger.debug(f\"SHAP Explainer initialized: {type(self.explainer)}\")\n",
    "                self.logger.debug(f\"Explainer details: {self.explainer}\")\n",
    "\n",
    "            # Compute SHAP values.\n",
    "            shap_values = self.explainer.shap_values(X)\n",
    "            self.logger.debug(f\"Type of shap_values before conversion: {type(shap_values)}\")\n",
    "            if hasattr(shap_values, \"values\"):\n",
    "                self.logger.debug(\"Converting shap_values to numpy array using .values attribute.\")\n",
    "                shap_values = shap_values.values\n",
    "\n",
    "            if debug:\n",
    "                self.logger.debug(f\"SHAP values computed: {type(shap_values)}\")\n",
    "                self.logger.debug(f\"Shape of SHAP values: {np.shape(shap_values)}\")\n",
    "\n",
    "            # If the SHAP values array is 3D, slice to 2D.\n",
    "            if isinstance(shap_values, np.ndarray) and shap_values.ndim == 3:\n",
    "                if shap_values.shape[2] == 2:\n",
    "                    self.logger.warning(f\"SHAP values are 3D with shape {shap_values.shape}; selecting slice index 1.\")\n",
    "                    shap_values = shap_values[:, :, 1]\n",
    "                else:\n",
    "                    self.logger.error(f\"Unexpected SHAP values shape: {shap_values.shape}\")\n",
    "                    raise ValueError(\"Unexpected SHAP values shape.\")\n",
    "\n",
    "            # If the SHAP values come as a list (e.g., multiclass), select the appropriate element.\n",
    "            if isinstance(shap_values, list):\n",
    "                if len(shap_values) > 1:\n",
    "                    self.logger.debug(\"Multiclass detected; selecting positive class (index 1).\")\n",
    "                    shap_values = shap_values[1]\n",
    "                else:\n",
    "                    shap_values = shap_values[0]\n",
    "\n",
    "            if shap_values.ndim == 1:\n",
    "                shap_values = np.atleast_2d(shap_values)\n",
    "                self.logger.debug(\"Converted 1D SHAP output to 2D row vector.\")\n",
    "\n",
    "            if hasattr(self.model, 'classes_'):\n",
    "                n_classes = len(self.model.classes_)\n",
    "            elif hasattr(self.model, 'n_classes_'):\n",
    "                n_classes = self.model.n_classes_\n",
    "            else:\n",
    "                n_classes = 1\n",
    "            self.logger.debug(f\"Number of classes in the model: {n_classes}\")\n",
    "\n",
    "            shap_values_class = self._process_shap_values(shap_values, n_classes, debug)\n",
    "            return self.explainer, shap_values_class\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to compute SHAP values: {e}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "    def _process_shap_values(self, shap_values, n_classes, debug: bool):\n",
    "        if isinstance(shap_values, list):\n",
    "            if n_classes > 1:\n",
    "                shap_values_class = shap_values[1]\n",
    "                self.logger.debug(f\"Extracted SHAP values for class 1: Shape {shap_values_class.shape}\")\n",
    "            else:\n",
    "                shap_values_class = shap_values[0]\n",
    "                self.logger.debug(f\"Extracted SHAP values for single-class: Shape {shap_values_class.shape}\")\n",
    "        elif isinstance(shap_values, np.ndarray):\n",
    "            shap_values_class = shap_values\n",
    "            self.logger.debug(f\"SHAP values array shape: {shap_values_class.shape}\")\n",
    "        else:\n",
    "            self.logger.error(f\"Unexpected SHAP values type: {type(shap_values)}\")\n",
    "            raise ValueError(\"Unexpected SHAP values type.\")\n",
    "        return shap_values_class\n",
    "\n",
    "    def compute_individual_shap_values(self, X_transformed: pd.DataFrame, trial_index: int, debug: bool = False) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute SHAP values for a single trial.\n",
    "        For models like XGBoost, the output may be a list; if so, select the positive class slice and squeeze the trial dimension.\n",
    "        Returns a 1D numpy array of SHAP values for the trial.\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"Computing SHAP values for trial at index {trial_index}...\")\n",
    "        try:\n",
    "            trial = X_transformed.iloc[[trial_index]]  # Keep as DataFrame for SHAP explainer\n",
    "            shap_values_trial = self.explainer.shap_values(trial)\n",
    "            self.logger.debug(f\"Raw SHAP values for trial (type: {type(shap_values_trial)}): {shap_values_trial}\")\n",
    "            \n",
    "            # If the result is a list (e.g., for multiclass/binary classifiers)\n",
    "            if isinstance(shap_values_trial, list):\n",
    "                if len(shap_values_trial) > 1:\n",
    "                    self.logger.debug(\"Multiclass detected in individual trial; selecting positive class (index 1).\")\n",
    "                    shap_values_trial = shap_values_trial[1]\n",
    "                else:\n",
    "                    shap_values_trial = shap_values_trial[0]\n",
    "            \n",
    "            # At this point, shap_values_trial should have shape (1, n_features)\n",
    "            # Squeeze the trial dimension so that we have a 1D array of length n_features.\n",
    "            shap_values_trial = np.squeeze(shap_values_trial, axis=0)\n",
    "            self.logger.debug(f\"Processed SHAP values for trial (after squeezing): {shap_values_trial} with shape {np.shape(shap_values_trial)}\")\n",
    "            \n",
    "            # NEW: Log the dtype so we know it is numeric\n",
    "            self.logger.debug(f\"Data type of individual SHAP values: {shap_values_trial.dtype}\")\n",
    "            return shap_values_trial\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to compute SHAP values for trial {trial_index}: {e}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "\n",
    "    def extract_force_plot_values(self, shap_values: np.ndarray, trial_id: Any, X_original: pd.DataFrame) -> Dict[str, Any]:\n",
    "        self.logger.info(f\"Extracting SHAP values for trial ID '{trial_id}'...\")\n",
    "        try:\n",
    "            if trial_id not in X_original.index:\n",
    "                self.logger.warning(f\"Trial ID '{trial_id}' not found in X_original index.\")\n",
    "                return {}\n",
    "            pos = X_original.index.get_loc(trial_id)\n",
    "            self.logger.debug(f\"Trial ID '{trial_id}' is at position {pos}.\")\n",
    "            shap_values_trial = shap_values[pos]\n",
    "            feature_contributions = dict(zip(X_original.columns, shap_values_trial))\n",
    "            self.logger.debug(f\"Feature contributions for trial '{trial_id}': {feature_contributions}\")\n",
    "            return feature_contributions\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error extracting SHAP values for trial '{trial_id}': {e}\")\n",
    "            raise\n",
    "\n",
    "    def get_shap_row(self, shap_values: np.ndarray, df: pd.DataFrame, trial_id: Any) -> Optional[np.ndarray]:\n",
    "        self.logger.info(f\"Retrieving SHAP values for trial ID '{trial_id}'...\")\n",
    "        try:\n",
    "            if trial_id not in df.index:\n",
    "                self.logger.warning(f\"Trial ID '{trial_id}' not found in DataFrame index.\")\n",
    "                return None\n",
    "            pos = df.index.get_loc(trial_id)\n",
    "            shap_row = shap_values[pos]\n",
    "            self.logger.debug(f\"SHAP values for trial ID '{trial_id}' at position {pos}: {shap_row}\")\n",
    "            return shap_row\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error retrieving SHAP row for trial ID '{trial_id}': {e}\")\n",
    "            raise\n",
    "\n",
    "# The main execution block remains largely the same but now benefits from the enhanced ShapCalculator.\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Test code to verify the ShapCalculator class\n",
    "    print(\"Testing ShapCalculator module...\")\n",
    "    from ml.train_utils.train_utils import load_model\n",
    "    from datapreprocessor import DataPreprocessor\n",
    "    from ml.predict.predict import predict_and_attach_predict_probs\n",
    "    from ml.feature_selection.feature_importance_calculator import manage_features\n",
    "\n",
    "    # Import utility functions\n",
    "    # from ml.shap.shap_utils import (\n",
    "    #     load_dataset,\n",
    "    #     setup_logging, load_configuration, initialize_logger\n",
    "    # )\n",
    "\n",
    "\n",
    "    # **Load Configuration and Initialize Logger**\n",
    "    config_path = Path('../../data/model/preprocessor_config/preprocessor_config.yaml')\n",
    "    try:\n",
    "        config = load_configuration(config_path)\n",
    "        print(f\"✅ Configuration loaded successfully from {config_path}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to load configuration: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "    log_file = Path(config.paths.log_file).resolve()\n",
    "    try:\n",
    "        logger = initialize_logger(config, log_file)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to set up logging: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "    # **Load Dataset**\n",
    "    raw_data_path = Path(config.paths.data_dir).resolve() / config.paths.raw_data\n",
    "    try:\n",
    "        df = load_dataset(raw_data_path)\n",
    "        print(f\"✅ Dataset loaded successfully from {raw_data_path}.\")\n",
    "        print(f\"📊 Dataset Columns: {df.columns.tolist()}\")\n",
    "        logger.info(f\"Dataset loaded with shape: {df.shape}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Failed to load dataset: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "    # **Load Model**\n",
    "    try:\n",
    "        model = load_model('XGBoost', Path(config.paths.model_save_base_dir).resolve())\n",
    "        print(\"✅ Model loaded successfully.\")\n",
    "        logger.info(\"Model loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Failed to load model: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "    # **Initialize ShapCalculator with Dynamic Model Type**\n",
    "    shap_calculator = ShapCalculator(model=model, logger=logger)\n",
    "    print(\"✅ ShapCalculator initialized successfully.\")\n",
    "    logger.info(\"ShapCalculator initialized successfully.\")\n",
    "\n",
    "    # **Load Feature Lists (via manage_features)**\n",
    "    base_dir = Path(\"../../data\") / \"preprocessor\" / \"features_info\"\n",
    "    features_file = (Path(config.paths.data_dir) / config.paths.features_metadata_file).resolve()\n",
    "    ordinal_file = Path(f'{base_dir}/ordinal_categoricals.pkl')\n",
    "    nominal_file = Path(f'{base_dir}/nominal_categoricals.pkl')\n",
    "    numericals_file = Path(f'{base_dir}/numericals.pkl')\n",
    "    y_variable_file = Path(f'{base_dir}/y_variable.pkl')\n",
    "    model_save_dir_override = Path(config.paths.model_save_base_dir)\n",
    "    transformers_dir_override = Path(config.paths.transformers_save_base_dir)\n",
    "\n",
    "    feature_paths = {\n",
    "        'features': features_file,\n",
    "        'ordinal_categoricals': ordinal_file,\n",
    "        'nominal_categoricals': nominal_file,\n",
    "        'numericals': numericals_file,\n",
    "        'y_variable': y_variable_file\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        feature_lists = manage_features(mode='load', paths=feature_paths)\n",
    "        y_variable_list = feature_lists.get('y_variable', [])\n",
    "        ordinal_categoricals = feature_lists.get('ordinal_categoricals', [])\n",
    "        nominal_categoricals = feature_lists.get('nominal_categoricals', [])\n",
    "        numericals = feature_lists.get('numericals', [])\n",
    "        if logger:\n",
    "            logger.debug(f\"Loaded Feature Lists: y_variable={y_variable_list}, ordinal_categoricals={ordinal_categoricals}, nominal_categoricals={nominal_categoricals}, numericals={numericals}\")\n",
    "    except Exception as e:\n",
    "        if logger:\n",
    "            logger.warning(f\"Feature lists could not be loaded: {e}\")\n",
    "        y_variable_list, ordinal_categoricals, nominal_categoricals, numericals = [], [], [], []\n",
    "\n",
    "    # **Initialize DataPreprocessor**\n",
    "    preprocessor = DataPreprocessor(\n",
    "        model_type=\"Tree Based Classifier\",\n",
    "        y_variable=y_variable_list,\n",
    "        ordinal_categoricals=ordinal_categoricals,\n",
    "        nominal_categoricals=nominal_categoricals,\n",
    "        numericals=numericals,\n",
    "        mode='predict',\n",
    "        options={},\n",
    "        debug=True,\n",
    "        normalize_debug=False,\n",
    "        normalize_graphs_output=False,\n",
    "        graphs_output_dir=Path(config.paths.plots_output_dir).resolve(),\n",
    "        transformers_dir=transformers_dir_override\n",
    "    )\n",
    "\n",
    "    # **Preprocess Data**\n",
    "    try:\n",
    "        X_preprocessed, recommendations, X_inversed = preprocessor.final_preprocessing(df)\n",
    "        logger.info(\"Preprocessing completed successfully in predict mode.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Preprocessing failed: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "    # **Validate Preprocessed Data**\n",
    "    try:\n",
    "        non_numeric_features = X_preprocessed.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "        if non_numeric_features:\n",
    "            logger.error(f\"Non-numeric features detected in preprocessed data: {non_numeric_features}\")\n",
    "            raise ValueError(f\"Preprocessed data contains non-numeric features: {non_numeric_features}\")\n",
    "        else:\n",
    "            logger.debug(\"All features in X_preprocessed are numeric.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Validation failed: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "    # **Compute SHAP Values**\n",
    "    try:\n",
    "        explainer, shap_values = shap_calculator.compute_shap_values(X_preprocessed, debug=True)\n",
    "        print(\"✅ SHAP values computed successfully.\")\n",
    "        logger.info(f\"SHAP values computed with shape: {shap_values.shape}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Failed to compute SHAP values: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "    # **Compute Individual SHAP Values for a Specific Trial**\n",
    "    try:\n",
    "        trial_id = X_inversed.index[0]  # Example: first trial\n",
    "        trial_index = X_preprocessed.index.get_loc(trial_id)\n",
    "        shap_values_trial = shap_calculator.compute_individual_shap_values(X_preprocessed, trial_index, debug=True)\n",
    "        print(f\"✅ SHAP values for trial '{trial_id}' computed successfully.\")\n",
    "        logger.info(f\"SHAP values for trial '{trial_id}' computed.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Failed to compute SHAP values for trial '{trial_id}': {e}\")\n",
    "\n",
    "    # **Extract Force Plot Values for a Specific Trial**\n",
    "    try:\n",
    "        feature_contributions = shap_calculator.extract_force_plot_values(shap_values, trial_id, df)\n",
    "        print(f\"✅ Feature contributions for trial '{trial_id}' extracted successfully:\")\n",
    "        for feature, contribution in feature_contributions.items():\n",
    "            print(f\"  - {feature}: {contribution}\")\n",
    "        logger.info(f\"Feature contributions for trial '{trial_id}' extracted.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Failed to extract feature contributions for trial '{trial_id}': {e}\")\n",
    "\n",
    "    # **Retrieve SHAP Row for a Specific Trial**\n",
    "    try:\n",
    "        shap_row = shap_calculator.get_shap_row(shap_values, df, trial_id)\n",
    "        if shap_row is not None:\n",
    "            print(f\"✅ Retrieved SHAP row for trial '{trial_id}': {shap_row}\")\n",
    "            logger.info(f\"SHAP row for trial '{trial_id}' retrieved.\")\n",
    "        else:\n",
    "            print(f\"⚠️ SHAP row for trial '{trial_id}' not found.\")\n",
    "            logger.warning(f\"SHAP row for trial '{trial_id}' not found.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Failed to retrieve SHAP row for trial '{trial_id}': {e}\")\n",
    "\n",
    "    print(\"✅ All tests in shap_calculator.py passed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ml/shap/shap_visualizer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ml/shap/shap_visualizer.py\n",
    "\n",
    "import logging\n",
    "import shap\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from typing import Optional, List\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from ml.shap.shap_calculator import ShapCalculator\n",
    "from ml.shap.shap_utils import load_dataset, setup_logging, load_configuration, initialize_logger\n",
    "\n",
    "class ShapVisualizer:\n",
    "    def __init__(self, logger: logging.Logger = None):\n",
    "        \"\"\"\n",
    "        Initialize the ShapVisualizer with an optional logger.\n",
    "        \"\"\"\n",
    "        self.logger = logger or logging.getLogger(__name__)\n",
    "    \n",
    "    def plot_summary(self, shap_values, X_original: pd.DataFrame, save_path: Path, debug: bool = False):\n",
    "        \"\"\"\n",
    "        Generate and save a SHAP summary plot.\n",
    "\n",
    "        :param shap_values: SHAP values array.\n",
    "        :param X_original: Original feature DataFrame.\n",
    "        :param save_path: Path to save the plot.\n",
    "        :param debug: Enable detailed debug logs.\n",
    "        \"\"\"\n",
    "        self.logger.info(\"Generating SHAP summary plot...\")\n",
    "        self._log_shap_values(shap_values, X_original, debug)\n",
    "        \n",
    "        try:\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            shap.summary_plot(shap_values, X_original, show=False)\n",
    "            plt.tight_layout()\n",
    "            save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            plt.savefig(save_path, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            self.logger.debug(f\"SHAP summary plot saved to {save_path}\")\n",
    "            self.logger.info(\"SHAP summary plot generated successfully.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to generate SHAP summary plot: {e}\")\n",
    "            raise\n",
    "\n",
    "    def plot_dependence(self, shap_values, feature: str, X_original: pd.DataFrame, save_path: Path, interaction_index: Optional[str] = None, debug: bool = False):\n",
    "        self.logger.info(f\"Generating SHAP dependence plot for feature '{feature}'...\")\n",
    "        try:\n",
    "            # Instead of slicing the SHAP values to a 1D vector,\n",
    "            # verify that the full shap_values array has the expected number of columns.\n",
    "            if not (isinstance(shap_values, np.ndarray) and shap_values.ndim == 2):\n",
    "                msg = f\"'shap_values' must be a 2D array, but got shape {np.shape(shap_values)}\"\n",
    "                self.logger.error(msg)\n",
    "                raise ValueError(msg)\n",
    "            \n",
    "            if shap_values.shape[1] != len(X_original.columns):\n",
    "                msg = f\"'shap_values' has {shap_values.shape[1]} columns but 'features' has {len(X_original.columns)} columns!\"\n",
    "                self.logger.error(msg)\n",
    "                raise ValueError(msg)\n",
    "            \n",
    "            # Log the shapes for debugging.\n",
    "            self.logger.debug(f\"Full shap_values shape: {shap_values.shape}\")\n",
    "            self.logger.debug(f\"X_original shape: {X_original.shape}\")\n",
    "            \n",
    "            # Call the dependence plot using the full 2D array and feature name.\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            shap.dependence_plot(feature, shap_values, X_original, interaction_index=interaction_index, show=False)\n",
    "            plt.tight_layout()\n",
    "            save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            plt.savefig(save_path, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            self.logger.debug(f\"SHAP dependence plot for '{feature}' saved to {save_path}\")\n",
    "            self.logger.info(f\"SHAP dependence plot for feature '{feature}' generated successfully.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to generate SHAP dependence plot for feature '{feature}': {e}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "\n",
    "    def plot_force(self, shap_explainer, shap_values, X_original: pd.DataFrame, trial_id: str, save_path: Path, debug: bool = False):\n",
    "        self.logger.info(f\"Generating SHAP force plot for trial {trial_id}...\")\n",
    "        try:\n",
    "            trial_index = X_original.index.get_loc(trial_id) if trial_id in X_original.index else None\n",
    "            if trial_index is None:\n",
    "                self.logger.error(f\"Trial ID '{trial_id}' not found in X_original index.\")\n",
    "                raise ValueError(f\"Trial ID '{trial_id}' not found.\")\n",
    "            \n",
    "            # Log and inspect the expected value from the explainer.\n",
    "            base_val = shap_explainer.expected_value\n",
    "            self.logger.debug(f\"explainer.expected_value type: {type(base_val)}\")\n",
    "            if isinstance(base_val, (list, np.ndarray)):\n",
    "                self.logger.debug(f\"explainer.expected_value shape: {np.shape(base_val)}\")\n",
    "                base_val = base_val[0]  # For multi-output, select the first element.\n",
    "            self.logger.debug(f\"Selected base value for force plot: {base_val}\")\n",
    "            \n",
    "            # Extract and squeeze SHAP values for the trial.\n",
    "            trial_shap = shap_values[trial_index]\n",
    "            self.logger.debug(f\"Trial SHAP values shape before squeezing: {np.shape(trial_shap)}\")\n",
    "            if trial_shap.ndim > 1:\n",
    "                trial_shap = np.squeeze(trial_shap)\n",
    "                self.logger.debug(f\"Trial SHAP values shape after squeezing: {np.shape(trial_shap)}\")\n",
    "            \n",
    "            # Call the new force plot API.\n",
    "            shap_plot = shap.force_plot(\n",
    "                base_val,\n",
    "                trial_shap,\n",
    "                X_original.iloc[trial_index],\n",
    "                matplotlib=False\n",
    "            )\n",
    "            shap.save_html(str(save_path), shap_plot)\n",
    "            self.logger.debug(f\"SHAP force plot saved to {save_path}\")\n",
    "            self.logger.info(f\"SHAP force plot for trial {trial_id} generated successfully.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to generate SHAP force plot for trial '{trial_id}': {e}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "    def plot_interaction(self, shap_values, feature: str, X_original: pd.DataFrame, save_path: Path, debug: bool = False):\n",
    "        \"\"\"\n",
    "        Generate and save a SHAP interaction plot for a specific feature.\n",
    "\n",
    "        :param shap_values: SHAP values array.\n",
    "        :param feature: Feature name for interaction plot.\n",
    "        :param X_original: Original feature DataFrame.\n",
    "        :param save_path: Path to save the plot.\n",
    "        :param debug: Enable detailed debug logs.\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"Generating SHAP interaction plot for feature '{feature}'...\")\n",
    "        try:\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            shap.dependence_plot(feature, shap_values, X_original, interaction_index=feature, show=False)\n",
    "            plt.tight_layout()\n",
    "            save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            plt.savefig(save_path, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            self.logger.debug(f\"SHAP interaction plot for '{feature}' saved to {save_path}\")\n",
    "            self.logger.info(f\"SHAP interaction plot for feature '{feature}' generated successfully.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to generate SHAP interaction plot for feature '{feature}': {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate_all_plots(self, shap_values, explainer, X_original: pd.DataFrame, save_dir: Path, debug: bool = False):\n",
    "        \"\"\"\n",
    "        Generate and save all relevant SHAP plots.\n",
    "\n",
    "        :param shap_values: SHAP values array.\n",
    "        :param explainer: SHAP explainer object.\n",
    "        :param X_original: Original feature DataFrame.\n",
    "        :param save_dir: Directory to save all plots.\n",
    "        :param debug: Enable detailed debug logs.\n",
    "        \"\"\"\n",
    "        self.logger.info(\"Generating all SHAP plots...\")\n",
    "        try:\n",
    "            # Ensure save directory exists\n",
    "            save_dir.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            # Summary Plot\n",
    "            summary_plot_path = save_dir / \"shap_summary.png\"\n",
    "            self.plot_summary(shap_values, X_original, summary_plot_path, debug=debug)\n",
    "\n",
    "            # Dependence and Interaction Plots for Top Features\n",
    "            top_features = self.get_top_features(shap_values, X_original, top_n=5)\n",
    "            for feature in top_features:\n",
    "                dependence_plot_path = save_dir / f\"shap_dependence_{feature}.png\"\n",
    "                self.plot_dependence(shap_values, feature, X_original, dependence_plot_path, interaction_index=None, debug=debug)\n",
    "\n",
    "                # Interaction Plot (Optional)\n",
    "                interaction_plot_path = save_dir / f\"shap_interaction_{feature}.png\"\n",
    "                self.plot_interaction(shap_values, feature, X_original, interaction_plot_path, debug=debug)\n",
    "\n",
    "            # Force Plot for a Specific Trial (e.g., first trial)\n",
    "            trial_id = X_original.index[0]\n",
    "            force_plot_path = save_dir / f\"shap_force_plot_{trial_id}.html\"\n",
    "            self.plot_force(explainer, shap_values, X_original, trial_id, force_plot_path, debug=debug)\n",
    "\n",
    "            self.logger.info(\"All SHAP plots generated and saved successfully.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to generate all SHAP plots: {e}\")\n",
    "            raise\n",
    "\n",
    "    def get_top_features(self, shap_values, X_original: pd.DataFrame, top_n: int = 5) -> List[str]:\n",
    "        \"\"\"\n",
    "        Identify top N features based on mean absolute SHAP values.\n",
    "\n",
    "        :param shap_values: SHAP values array.\n",
    "        :param X_original: Original feature DataFrame.\n",
    "        :param top_n: Number of top features to identify.\n",
    "        :return: List of top feature names.\n",
    "        \"\"\"\n",
    "        if isinstance(shap_values, list):\n",
    "            # Multiclass: average over classes\n",
    "            shap_values_avg = np.mean(np.abs(shap_values), axis=0)\n",
    "        else:\n",
    "            shap_values_avg = np.mean(np.abs(shap_values), axis=0)\n",
    "        \n",
    "        feature_importance = pd.Series(shap_values_avg, index=X_original.columns)\n",
    "        top_features = feature_importance.sort_values(ascending=False).head(top_n).index.tolist()\n",
    "        self.logger.debug(f\"Top {top_n} features based on SHAP values: {top_features}\")\n",
    "        return top_features\n",
    "\n",
    "    def _log_shap_values(self, shap_values, X_original: pd.DataFrame, debug: bool):\n",
    "        \"\"\"\n",
    "        Log details about SHAP values and feature alignment.\n",
    "\n",
    "        :param shap_values: SHAP values array.\n",
    "        :param X_original: Original feature DataFrame.\n",
    "        :param debug: Enable detailed debug logs.\n",
    "        \"\"\"\n",
    "        if debug:\n",
    "            self.logger.debug(f\"Type of shap_values: {type(shap_values)}\")\n",
    "            self.logger.debug(f\"Shape of shap_values: {np.shape(shap_values)}\")\n",
    "            if isinstance(shap_values, list):\n",
    "                self.logger.debug(f\"Number of class SHAP arrays: {len(shap_values)}\")\n",
    "                if len(shap_values) > 0 and hasattr(shap_values[0], 'shape'):\n",
    "                    self.logger.debug(f\"Shape of first class SHAP array: {shap_values[0].shape}\")\n",
    "            elif isinstance(shap_values, np.ndarray):\n",
    "                self.logger.debug(f\"Shape of shap_values: {shap_values.shape}\")\n",
    "            if hasattr(shap_values, 'values'):\n",
    "                self.logger.debug(f\"Sample SHAP values:\\n{shap_values.values[:2]}\")\n",
    "            if hasattr(shap_values, 'feature_names'):\n",
    "                self.logger.debug(f\"SHAP feature names: {shap_values.feature_names}\")\n",
    "\n",
    "            self.logger.debug(f\"Type of X_original: {type(X_original)}\")\n",
    "            self.logger.debug(f\"Shape of X_original: {X_original.shape}\")\n",
    "            self.logger.debug(f\"Columns in X_original: {X_original.columns.tolist()}\")\n",
    "\n",
    "            # Verify column alignment\n",
    "            shap_feature_names = self._get_shap_feature_names(shap_values, X_original)\n",
    "            if list(shap_feature_names) != list(X_original.columns):\n",
    "                self.logger.error(\"Column mismatch between SHAP values and X_original.\")\n",
    "                self.logger.error(f\"SHAP feature names ({len(shap_feature_names)}): {shap_feature_names}\")\n",
    "                self.logger.error(f\"X_original columns ({len(X_original.columns)}): {X_original.columns.tolist()}\")\n",
    "                raise ValueError(\"Column mismatch between SHAP values and X_original.\")\n",
    "            else:\n",
    "                self.logger.debug(\"Column alignment verified between SHAP values and X_original.\")\n",
    "\n",
    "    def _get_shap_feature_names(self, shap_values, X_original: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Extract feature names from shap_values.\n",
    "\n",
    "        :param shap_values: SHAP values array.\n",
    "        :param X_original: Original feature DataFrame.\n",
    "        :return: List of feature names.\n",
    "        \"\"\"\n",
    "        if isinstance(shap_values, shap.Explainer):\n",
    "            return shap_values.feature_names\n",
    "        elif hasattr(shap_values, 'feature_names'):\n",
    "            return shap_values.feature_names\n",
    "        else:\n",
    "            return X_original.columns.tolist()  # Fallback\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Test code to verify the ShapVisualizer class\n",
    "    print(\"Testing SHAPvisualizer module...\")\n",
    "\n",
    "    from ml.train_utils.train_utils import load_model\n",
    "    from datapreprocessor import DataPreprocessor\n",
    "    from ml.predict.predict import predict_and_attach_predict_probs\n",
    "    from ml.feature_selection.feature_importance_calculator import manage_features\n",
    "\n",
    "    # from ml.shap.shap_utils import (\n",
    "    #     load_dataset,\n",
    "    #     setup_logging, load_configuration, initialize_logger\n",
    "    # )\n",
    "\n",
    "\n",
    "    # **Load Configuration and Initialize Logger**\n",
    "    config_path = Path('../../data/model/preprocessor_config/preprocessor_config.yaml')\n",
    "    try:\n",
    "        config = load_configuration(config_path)\n",
    "        print(f\"✅ Configuration loaded successfully from {config_path}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to load configuration: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "    log_file = Path(config.paths.log_file).resolve()\n",
    "    try:\n",
    "        logger = initialize_logger(config, log_file)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to set up logging: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "    # **Load Dataset**\n",
    "    raw_data_path = Path(config.paths.data_dir).resolve() / config.paths.raw_data\n",
    "    try:\n",
    "        df = load_dataset(raw_data_path)\n",
    "        print(f\"✅ Dataset loaded successfully from {raw_data_path}.\")\n",
    "        print(f\"📊 Dataset Columns: {df.columns.tolist()}\")\n",
    "        logger.info(f\"Dataset loaded with shape: {df.shape}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Failed to load dataset: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "    # **Load Model**\n",
    "    try:\n",
    "        model = load_model('CatBoost', Path(config.paths.model_save_base_dir).resolve())\n",
    "        print(\"✅ Model loaded successfully.\")\n",
    "        logger.info(\"Model loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Failed to load model: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "    # **Initialize ShapCalculator**\n",
    "    shap_calculator = ShapCalculator(model=model, logger=logger)\n",
    "    print(\"✅ ShapCalculator initialized successfully.\")\n",
    "    logger.info(\"ShapCalculator initialized successfully.\")\n",
    "\n",
    "    # **Load Feature Lists (via manage_features)**\n",
    "    base_dir = Path(\"../../data\") / \"preprocessor\" / \"features_info\"\n",
    "    features_file = (Path(config.paths.data_dir) / config.paths.features_metadata_file).resolve()\n",
    "    ordinal_file = Path(f'{base_dir}/ordinal_categoricals.pkl')\n",
    "    nominal_file = Path(f'{base_dir}/nominal_categoricals.pkl')\n",
    "    numericals_file = Path(f'{base_dir}/numericals.pkl')\n",
    "    y_variable_file = Path(f'{base_dir}/y_variable.pkl')\n",
    "    model_save_dir_override = Path(config.paths.model_save_base_dir)\n",
    "    transformers_dir_override = Path(config.paths.transformers_save_base_dir)\n",
    "    best_model_name = \"CatBoost\"\n",
    "    save_dir = Path(config.paths.predictions_output_dir).resolve() / \"shap_results\"\n",
    "    model_path = model_save_dir_override / best_model_name.replace(' ', '_') / \"trained_model.pkl\"\n",
    "    results = {}\n",
    "    feature_paths = {\n",
    "        'features': features_file,\n",
    "        'ordinal_categoricals': ordinal_file,\n",
    "        'nominal_categoricals': nominal_file,\n",
    "        'numericals': numericals_file,\n",
    "        'y_variable': y_variable_file\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        feature_lists = manage_features(mode='load', paths=feature_paths)\n",
    "        y_variable_list = feature_lists.get('y_variable', [])\n",
    "        ordinal_categoricals = feature_lists.get('ordinal_categoricals', [])\n",
    "        nominal_categoricals = feature_lists.get('nominal_categoricals', [])\n",
    "        numericals = feature_lists.get('numericals', [])\n",
    "        if logger:\n",
    "            logger.debug(f\"Loaded Feature Lists: y_variable={y_variable_list}, ordinal_categoricals={ordinal_categoricals}, nominal_categoricals={nominal_categoricals}, numericals={numericals}\")\n",
    "    except Exception as e:\n",
    "        if logger:\n",
    "            logger.warning(f\"Feature lists could not be loaded: {e}\")\n",
    "        y_variable_list, ordinal_categoricals, nominal_categoricals, numericals = [], [], [], []\n",
    "\n",
    "    # **Initialize DataPreprocessor**\n",
    "    preprocessor = DataPreprocessor(\n",
    "        model_type=\"Tree Based Classifier\",\n",
    "        y_variable=y_variable_list,\n",
    "        ordinal_categoricals=ordinal_categoricals,\n",
    "        nominal_categoricals=nominal_categoricals,\n",
    "        numericals=numericals,\n",
    "        mode='predict',\n",
    "        options={},\n",
    "        debug=True,\n",
    "        normalize_debug=False,\n",
    "        normalize_graphs_output=False,\n",
    "        graphs_output_dir=Path(config.paths.plots_output_dir).resolve(),\n",
    "        transformers_dir=transformers_dir_override\n",
    "    )\n",
    "\n",
    "    # **Preprocess Data**\n",
    "    try:\n",
    "        X_preprocessed, recommendations, X_inversed = preprocessor.final_preprocessing(df)\n",
    "        if logger:\n",
    "            logger.info(\"Preprocessing completed successfully in predict mode.\")\n",
    "    except Exception as e:\n",
    "        if logger:\n",
    "            logger.error(f\"Preprocessing failed: {e}\")\n",
    "        raise\n",
    "\n",
    "    # **Validate Preprocessed Data**\n",
    "    try:\n",
    "        non_numeric_features = X_preprocessed.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "        if non_numeric_features:\n",
    "            logger.error(f\"Non-numeric features detected in preprocessed data: {non_numeric_features}\")\n",
    "            raise ValueError(f\"Preprocessed data contains non-numeric features: {non_numeric_features}\")\n",
    "        else:\n",
    "            logger.debug(\"All features in X_preprocessed are numeric.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Validation failed: {e}\")\n",
    "        raise\n",
    "\n",
    "    duplicates = X_inversed.index.duplicated()\n",
    "    if duplicates.any():\n",
    "        print(\"Duplicate trial IDs found:\", X_inversed.index[duplicates].tolist())\n",
    "    else:\n",
    "        print(\"Trial IDs are unique.\")\n",
    "\n",
    "    try:\n",
    "        model = load_model(best_model_name, model_save_dir_override)\n",
    "        if logger:\n",
    "            logger.info(f\"Trained model loaded from '{model_path}'.\")\n",
    "    except Exception as e:\n",
    "        if logger:\n",
    "            logger.error(f\"Failed to load the best model '{best_model_name}': {e}\")\n",
    "        raise\n",
    "\n",
    "    try:\n",
    "        predictions, prediction_probs, X_inversed = predict_and_attach_predict_probs(model, X_preprocessed, X_inversed)\n",
    "        results['predictions'] = predictions\n",
    "        results['prediction_probs'] = prediction_probs\n",
    "        if logger:\n",
    "            logger.info(\"Predictions generated and attached to the dataset.\")\n",
    "    except Exception as e:\n",
    "        if logger:\n",
    "            logger.error(f\"Prediction failed: {e}\")\n",
    "        raise\n",
    "    shap_visualizer = ShapVisualizer(logger=logger)\n",
    "    generate_summary_plot = True\n",
    "    if generate_summary_plot:\n",
    "        shap_summary_path = save_dir / \"shap_summary.png\"\n",
    "        try:\n",
    "            shap_visualizer.plot_summary(shap_values, X_preprocessed, shap_summary_path, debug=config.logging.debug)\n",
    "            results['shap_summary_plot'] = str(shap_summary_path)\n",
    "            logger.info(f\"SHAP summary plot saved at {shap_summary_path}.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to generate SHAP summary plot: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ml/shap/feedback_generator.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ml/shap/feedback_generator.py\n",
    "\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Any, Dict, Optional\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Optional, List\n",
    "\n",
    "from ml.shap.shap_calculator import ShapCalculator\n",
    "from ml.shap.shap_utils import load_dataset, setup_logging, load_configuration, initialize_logger\n",
    "\n",
    "class FeedbackGenerator:\n",
    "    def __init__(self, logger: logging.Logger = None):\n",
    "        \"\"\"\n",
    "        Initialize the FeedbackGenerator with an optional logger.\n",
    "        \"\"\"\n",
    "        self.logger = logger or logging.getLogger(__name__)\n",
    "\n",
    "    def generate_global_recommendations(\n",
    "        self,\n",
    "        shap_values: np.ndarray,\n",
    "        X_original: pd.DataFrame,\n",
    "        top_n: int = 5,\n",
    "        use_mad: bool = False,\n",
    "        debug: bool = False\n",
    "    ) -> Dict[str, Dict[str, Any]]:\n",
    "        self.logger.info(\"Generating feature importance based on SHAP values...\")\n",
    "        self.logger.debug(f\"Received SHAP values with shape: {np.shape(shap_values)}\")\n",
    "        \n",
    "        # Handle 3D array by selecting one slice if necessary.\n",
    "        if isinstance(shap_values, np.ndarray) and shap_values.ndim == 3:\n",
    "            self.logger.warning(f\"SHAP values are 3D with shape {shap_values.shape}; selecting one slice for global recommendations.\")\n",
    "            if shap_values.shape[2] == 2:\n",
    "                shap_values = shap_values[:, :, 1]\n",
    "                self.logger.debug(\"Selected the positive class slice (index 1) for SHAP values.\")\n",
    "            else:\n",
    "                error_msg = f\"Unexpected 3D shape for SHAP values: {shap_values.shape}. Cannot determine which slice to use.\"\n",
    "                self.logger.error(error_msg)\n",
    "                raise ValueError(error_msg)\n",
    "\n",
    "        try:\n",
    "            shap_df = pd.DataFrame(shap_values, columns=X_original.columns)\n",
    "            feature_importance = pd.DataFrame({\n",
    "                'feature': X_original.columns,\n",
    "                'importance': np.abs(shap_df).mean(axis=0),\n",
    "                'mean_shap': shap_df.mean(axis=0)\n",
    "            }).sort_values(by='importance', ascending=False)\n",
    "            if debug:\n",
    "                self.logger.debug(f\"Feature importance (top {top_n}):\\n{feature_importance.head(top_n)}\")\n",
    "            top_features = feature_importance.head(top_n)['feature'].tolist()\n",
    "            recommendations = {}\n",
    "            for feature in top_features:\n",
    "                feature_values = X_original[feature]\n",
    "                range_str = self._compute_feature_range(feature_values, use_mad, debug)\n",
    "                mean_shap = feature_importance.loc[feature_importance['feature'] == feature, 'mean_shap'].values[0]\n",
    "                direction = 'increase' if mean_shap > 0 else 'decrease'\n",
    "                recommendations[feature] = {\n",
    "                    'range': range_str,\n",
    "                    'importance': round(feature_importance.loc[feature_importance['feature'] == feature, 'importance'].values[0], 4),\n",
    "                    'direction': direction\n",
    "                }\n",
    "                if debug:\n",
    "                    self.logger.debug(\n",
    "                        f\"Recommendation for {feature}: Range={range_str}, \"\n",
    "                        f\"Importance={feature_importance.loc[feature_importance['feature'] == feature, 'importance'].values[0]}, \"\n",
    "                        f\"Direction={direction}\"\n",
    "                    )\n",
    "            if debug:\n",
    "                self.logger.debug(f\"Final Recommendations with Importance and Direction: {recommendations}\")\n",
    "            return recommendations\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to generate global recommendations: {e}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "\n",
    "    def _compute_feature_range(self, feature_values: pd.Series, use_mad: bool, debug: bool) -> str:\n",
    "        if use_mad:\n",
    "            median = feature_values.median()\n",
    "            mad = feature_values.mad()\n",
    "            lower_bound = median - 1.5 * mad\n",
    "            upper_bound = median + 1.5 * mad\n",
    "            range_str = f\"{lower_bound:.1f}–{upper_bound:.1f}\"\n",
    "            if debug:\n",
    "                self.logger.debug(f\"Computed MAD-based range for feature: {range_str}\")\n",
    "        else:\n",
    "            lower_bound = feature_values.quantile(0.25)\n",
    "            upper_bound = feature_values.quantile(0.75)\n",
    "            range_str = f\"{lower_bound:.1f}–{upper_bound:.1f}\"\n",
    "            if debug:\n",
    "                self.logger.debug(f\"Computed IQR-based range for feature: {range_str}\")\n",
    "        return range_str\n",
    "\n",
    "    def _normalize_feature_name(self, name: str) -> str:\n",
    "        # Ensure the name is a string.\n",
    "        return str(name).replace(\"num__\", \"\").replace(\"cat__\", \"\")\n",
    "\n",
    "    def generate_individual_feedback(\n",
    "        self,\n",
    "        trial: pd.Series,\n",
    "        shap_values_trial: np.ndarray,\n",
    "        percentile: float = 10.0,\n",
    "        expected_features: Optional[List[str]] = None\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Generate individual feedback for a trial.\n",
    "        This version normalizes feature names from both the expected list and trial index.\n",
    "        In addition to numeric outputs, a detailed textual explanation is provided.\n",
    "        \"\"\"\n",
    "        if expected_features is None:\n",
    "            expected_features = trial.index.tolist()\n",
    "        \n",
    "        # Normalize expected features and trial keys.\n",
    "        norm_expected = {self._normalize_feature_name(f) for f in expected_features}\n",
    "        norm_trial = {self._normalize_feature_name(f) for f in trial.index.tolist()}\n",
    "        \n",
    "        self.logger.debug(f\"Normalized expected features: {norm_expected}\")\n",
    "        self.logger.debug(f\"Normalized trial features: {norm_trial}\")\n",
    "        \n",
    "        # NEW: Log the raw trial keys for extra visibility\n",
    "        self.logger.debug(f\"Original trial keys: {list(trial.index)}\")\n",
    "        \n",
    "        missing = norm_expected - norm_trial\n",
    "        if missing:\n",
    "            self.logger.error(f\"Critical feature mismatch after normalization: {missing}\")\n",
    "            raise ValueError(f\"Feature alignment failed: {missing}\")\n",
    "\n",
    "        # Check if shap_values_trial is 2D; if so, select one column.\n",
    "        shape_trial = np.shape(shap_values_trial)\n",
    "        self.logger.debug(f\"SHAP values for trial before processing: shape = {shape_trial}\")\n",
    "        if len(shape_trial) == 2 and shape_trial[1] == 2:\n",
    "            self.logger.debug(\"Detected 2D SHAP values for individual trial; selecting positive class (index 1).\")\n",
    "            shap_values_trial = shap_values_trial[:, 1]\n",
    "            self.logger.debug(f\"New shape of SHAP values for trial: {np.shape(shap_values_trial)}\")\n",
    "\n",
    "        # If the SHAP output is a scalar, wrap it in a list.\n",
    "        if np.isscalar(shap_values_trial):\n",
    "            shap_values_trial = [shap_values_trial]\n",
    "            self.logger.debug(\"SHAP output was a scalar; wrapped into a list.\")\n",
    "\n",
    "        num_expected = len(expected_features)\n",
    "        num_shap = len(shap_values_trial)\n",
    "        if num_expected != num_shap:\n",
    "            self.logger.warning(f\"Length mismatch: expected features length = {num_expected} but SHAP values length = {num_shap}. Iterating up to the minimum length.\")\n",
    "            min_len = min(num_expected, num_shap)\n",
    "        else:\n",
    "            min_len = num_expected\n",
    "\n",
    "        feedback = {}\n",
    "        for i in range(min_len):\n",
    "            orig_feature = expected_features[i]\n",
    "            shap_val = shap_values_trial[i]\n",
    "            norm_feature = self._normalize_feature_name(orig_feature)\n",
    "            \n",
    "            # Try to match the normalized key; if not found, try with and without the 'num__' prefix\n",
    "            matching_keys = [k for k in trial.index if self._normalize_feature_name(k) == norm_feature]\n",
    "            if not matching_keys:\n",
    "                alt_key = \"num__\" + norm_feature\n",
    "                if alt_key in trial.index:\n",
    "                    matching_keys = [alt_key]\n",
    "                else:\n",
    "                    self.logger.debug(f\"No matching key found in trial for normalized feature '{norm_feature}'. Skipping.\")\n",
    "                    continue\n",
    "            \n",
    "            trial_key = matching_keys[0]\n",
    "            trial_value = trial.get(trial_key, None)\n",
    "            \n",
    "            self.logger.debug(f\"Processing feature '{orig_feature}' (normalized: '{norm_feature}'): SHAP value = {shap_val}, Trial value = {trial_value}\")\n",
    "            \n",
    "            # NEW: Updated type checks to accept numpy numeric types as well\n",
    "            if not isinstance(shap_val, (int, float, np.number)) or pd.isna(shap_val):\n",
    "                self.logger.debug(f\"Invalid SHAP value type {type(shap_val)} for '{orig_feature}' ({shap_val}). Skipping.\")\n",
    "                continue\n",
    "            if not isinstance(trial_value, (int, float, np.number)) or pd.isna(trial_value):\n",
    "                self.logger.debug(f\"Invalid trial value type {type(trial_value)} for '{orig_feature}' ({trial_value}). Skipping.\")\n",
    "                continue\n",
    "            \n",
    "            suggestion = \"increase\" if shap_val > 0 else \"decrease\"\n",
    "            adjustment_factor = 0.1\n",
    "            unit_change_value = adjustment_factor * abs(trial_value)\n",
    "            goal_value = trial_value + unit_change_value if suggestion == \"increase\" else trial_value - unit_change_value\n",
    "            unit = \"units\"\n",
    "            tolerance = self.compute_tolerance(shap_val, percentile)\n",
    "            min_value = goal_value - tolerance\n",
    "            max_value = goal_value + tolerance\n",
    "\n",
    "            feedback_text = (\n",
    "                f\"For feature '{orig_feature}': The SHAP value of {shap_val:.2f} suggests to {suggestion} the value. \"\n",
    "                f\"Current value is {trial_value:.2f}. A 10% adjustment equals {unit_change_value:.2f} {unit}, \"\n",
    "                f\"which would set a target of {goal_value:.2f}. The acceptable range is [{min_value:.2f}, {max_value:.2f}], \"\n",
    "                f\"classifying this metric as '{self.classify_metric(trial_value, min_value, max_value)}'.\"\n",
    "            )\n",
    "\n",
    "            feedback[f\"shap_{norm_feature}_unit_change\"] = round(unit_change_value, 2)\n",
    "            feedback[f\"shap_{norm_feature}_unit\"] = unit\n",
    "            feedback[f\"shap_{norm_feature}_direction\"] = suggestion\n",
    "            feedback[f\"shap_{norm_feature}_importance\"] = round(abs(shap_val), 4)\n",
    "            feedback[f\"shap_{norm_feature}_goal\"] = round(goal_value, 3)\n",
    "            feedback[f\"shap_{norm_feature}_min\"] = round(min_value, 3)\n",
    "            feedback[f\"shap_{norm_feature}_max\"] = round(max_value, 3)\n",
    "            feedback[f\"shap_{norm_feature}_classification\"] = self.classify_metric(trial_value, min_value, max_value)\n",
    "            feedback[f\"shap_{norm_feature}_feedback_text\"] = feedback_text\n",
    "            \n",
    "            self.logger.debug(\n",
    "                f\"For feature '{norm_feature}': suggestion={suggestion}, goal={goal_value:.3f}, \"\n",
    "                f\"min={min_value:.3f}, max={max_value:.3f}, classification={self.classify_metric(trial_value, min_value, max_value)}, \"\n",
    "                f\"trial value={trial_value}, unit_change={unit_change_value:.3f} {unit}. \"\n",
    "                f\"Feedback text: {feedback_text}\"\n",
    "            )\n",
    "        \n",
    "        if not feedback:\n",
    "            self.logger.warning(\"No individual feedback was generated; returning raw SHAP values for each feature.\")\n",
    "            feedback = {}\n",
    "            for f in expected_features:\n",
    "                norm_f = self._normalize_feature_name(f)\n",
    "                value = trial.get(norm_f)\n",
    "                if value is None:\n",
    "                    value = trial.get(\"num__\" + norm_f)\n",
    "                feedback[norm_f] = f\"Raw SHAP value: {value}\"\n",
    "        \n",
    "        return feedback\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def compute_tolerance(self, shap_val: float, percentile: float) -> float:\n",
    "        return percentile / 100.0 * abs(shap_val)\n",
    "\n",
    "    def classify_metric(self, value: float, min_val: float, max_val: float) -> str:\n",
    "        if value < min_val:\n",
    "            return \"Early\"\n",
    "        elif value > max_val:\n",
    "            return \"Late\"\n",
    "        else:\n",
    "            return \"Good\"\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Test code to verify the FeedbackGenerator class\n",
    "    print(\"Testing FeedbackGenerator module...\")\n",
    "\n",
    "    from ml.train_utils.train_utils import load_model\n",
    "    from datapreprocessor import DataPreprocessor\n",
    "    from ml.predict.predict import predict_and_attach_predict_probs\n",
    "    from ml.feature_selection.feature_importance_calculator import manage_features\n",
    "\n",
    "    # from ml.shap.shap_utils import (\n",
    "    #     load_dataset,\n",
    "    #     setup_logging, load_configuration, initialize_logger\n",
    "    # )\n",
    "\n",
    "\n",
    "    # **Load Configuration and Initialize Logger**\n",
    "    config_path = Path('../../data/model/preprocessor_config/preprocessor_config.yaml')\n",
    "    model = 'Random Forest' #XGBoost, CatBoost, Random Forest\n",
    "    try:\n",
    "        config = load_configuration(config_path)\n",
    "        print(f\"✅ Configuration loaded successfully from {config_path}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to load configuration: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "    log_file = Path(config.paths.log_file).resolve()\n",
    "    try:\n",
    "        logger = initialize_logger(config, log_file)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to set up logging: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "    # **Load Dataset**\n",
    "    raw_data_path = Path(config.paths.data_dir).resolve() / config.paths.raw_data\n",
    "    try:\n",
    "        df = load_dataset(raw_data_path)\n",
    "        print(f\"✅ Dataset loaded successfully from {raw_data_path}.\")\n",
    "        print(f\"📊 Dataset Columns: {df.columns.tolist()}\")\n",
    "        logger.info(f\"Dataset loaded with shape: {df.shape}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Failed to load dataset: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "    # **Load Model**\n",
    "    try:\n",
    "        model = load_model(model, Path(config.paths.model_save_base_dir).resolve())\n",
    "        print(\"✅ Model loaded successfully.\")\n",
    "        logger.info(\"Model loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Failed to load model: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "    # **Initialize ShapCalculator**\n",
    "    shap_calculator = ShapCalculator(model=model, logger=logger)\n",
    "    print(\"✅ ShapCalculator initialized successfully.\")\n",
    "    logger.info(\"ShapCalculator initialized successfully.\")\n",
    "\n",
    "    # **Load Feature Lists (via manage_features)**\n",
    "    base_dir = Path(\"../../data\") / \"preprocessor\" / \"features_info\"\n",
    "    features_file = (Path(config.paths.data_dir) / config.paths.features_metadata_file).resolve()\n",
    "    ordinal_file = Path(f'{base_dir}/ordinal_categoricals.pkl')\n",
    "    nominal_file = Path(f'{base_dir}/nominal_categoricals.pkl')\n",
    "    numericals_file = Path(f'{base_dir}/numericals.pkl')\n",
    "    y_variable_file = Path(f'{base_dir}/y_variable.pkl')\n",
    "    model_save_dir_override = Path(config.paths.model_save_base_dir)\n",
    "    transformers_dir_override = Path(config.paths.transformers_save_base_dir)\n",
    "\n",
    "    feature_paths = {\n",
    "        'features': features_file,\n",
    "        'ordinal_categoricals': ordinal_file,\n",
    "        'nominal_categoricals': nominal_file,\n",
    "        'numericals': numericals_file,\n",
    "        'y_variable': y_variable_file\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        feature_lists = manage_features(mode='load', paths=feature_paths)\n",
    "        y_variable_list = feature_lists.get('y_variable', [])\n",
    "        ordinal_categoricals = feature_lists.get('ordinal_categoricals', [])\n",
    "        nominal_categoricals = feature_lists.get('nominal_categoricals', [])\n",
    "        numericals = feature_lists.get('numericals', [])\n",
    "        if logger:\n",
    "            logger.debug(f\"Loaded Feature Lists: y_variable={y_variable_list}, ordinal_categoricals={ordinal_categoricals}, nominal_categoricals={nominal_categoricals}, numericals={numericals}\")\n",
    "    except Exception as e:\n",
    "        if logger:\n",
    "            logger.warning(f\"Feature lists could not be loaded: {e}\")\n",
    "        y_variable_list, ordinal_categoricals, nominal_categoricals, numericals = [], [], [], []\n",
    "\n",
    "    # **Initialize DataPreprocessor**\n",
    "    preprocessor = DataPreprocessor(\n",
    "        model_type=\"Tree Based Classifier\",\n",
    "        y_variable=y_variable_list,\n",
    "        ordinal_categoricals=ordinal_categoricals,\n",
    "        nominal_categoricals=nominal_categoricals,\n",
    "        numericals=numericals,\n",
    "        mode='predict',\n",
    "        options={},\n",
    "        debug=True,\n",
    "        normalize_debug=False,\n",
    "        normalize_graphs_output=False,\n",
    "        graphs_output_dir=Path(config.paths.plots_output_dir).resolve(),\n",
    "        transformers_dir=transformers_dir_override\n",
    "    )\n",
    "\n",
    "    # **Preprocess Data**\n",
    "    try:\n",
    "        X_preprocessed, recommendations, X_inversed = preprocessor.final_preprocessing(df)\n",
    "        logger.info(\"Preprocessing completed successfully in predict mode.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Preprocessing failed: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "    # **Validate Preprocessed Data**\n",
    "    try:\n",
    "        non_numeric_features = X_preprocessed.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "        if non_numeric_features:\n",
    "            logger.error(f\"Non-numeric features detected in preprocessed data: {non_numeric_features}\")\n",
    "            raise ValueError(f\"Preprocessed data contains non-numeric features: {non_numeric_features}\")\n",
    "        else:\n",
    "            logger.debug(\"All features in X_preprocessed are numeric.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Validation failed: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "    # **Compute SHAP Values**\n",
    "    try:\n",
    "        explainer, shap_values = shap_calculator.compute_shap_values(X_preprocessed, debug=True)\n",
    "        print(\"✅ SHAP values computed successfully.\")\n",
    "        logger.info(f\"SHAP values computed with shape: {shap_values.shape}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Failed to compute SHAP values: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "    # **Initialize FeedbackGenerator**\n",
    "    feedback_generator = FeedbackGenerator(logger=logger)\n",
    "    print(\"✅ FeedbackGenerator initialized successfully.\")\n",
    "    logger.info(\"FeedbackGenerator initialized successfully.\")\n",
    "\n",
    "    # **Generate Global Recommendations**\n",
    "    try:\n",
    "        recommendations = feedback_generator.generate_global_recommendations(\n",
    "            shap_values=shap_values,\n",
    "            X_original=X_preprocessed,\n",
    "            top_n=5,\n",
    "            use_mad=False,\n",
    "            debug=True\n",
    "        )\n",
    "        print(\"✅ Global recommendations generated successfully:\")\n",
    "        for feature, rec in recommendations.items():\n",
    "            print(f\"  - {feature}: {rec}\")\n",
    "        logger.info(\"Global recommendations generated successfully.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Failed to generate global recommendations: {e}\")\n",
    "\n",
    "    # **Generate Individual Feedback for a Specific Trial**\n",
    "    try:\n",
    "        trial_id = 1  # Intended trial id\n",
    "        trial_index = X_preprocessed.index.get_loc(trial_id)\n",
    "        shap_values_trial = shap_values[trial_index]  # Use the row corresponding to trial_id\n",
    "        expected_features = X_preprocessed.columns.tolist()\n",
    "        print(\"expected_features\", expected_features)\n",
    "        feedback = feedback_generator.generate_individual_feedback(trial=X_inversed.loc[trial_id],\n",
    "                                                                shap_values_trial=shap_values_trial,\n",
    "                                                                percentile=10.0,\n",
    "                                                                expected_features=expected_features)\n",
    "\n",
    "\n",
    "        print(f\"\\n✅ Individual feedback for trial '{trial_id}':\")\n",
    "        for metric, suggestion in feedback.items():\n",
    "            print(f\"  - {metric}: {suggestion}\")\n",
    "        logger.info(f\"Individual feedback for trial '{trial_id}' generated successfully.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Failed to generate individual feedback for trial '{trial_id}': {e}\")\n",
    "\n",
    "    print(\"✅ All tests in feedback_generator.py passed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ml/shap/feedback_generator.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ml/shap/feedback_generator.py\n",
    "\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Any, Dict, Optional\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Optional, List, Tuple\n",
    "\n",
    "from ml.shap.shap_calculator import ShapCalculator\n",
    "from ml.shap.shap_utils import load_dataset, setup_logging, load_configuration, initialize_logger\n",
    "\n",
    "class FeedbackGenerator:\n",
    "    def __init__(self, logger: logging.Logger = None):\n",
    "        \"\"\"\n",
    "        Initialize the FeedbackGenerator with an optional logger.\n",
    "        \"\"\"\n",
    "        self.logger = logger or logging.getLogger(__name__)\n",
    "\n",
    "    def generate_global_recommendations(\n",
    "        self,\n",
    "        shap_values: np.ndarray,\n",
    "        X_original: pd.DataFrame,\n",
    "        top_n: int = 5,\n",
    "        use_mad: bool = False,\n",
    "        debug: bool = False\n",
    "    ) -> Dict[str, Dict[str, Any]]:\n",
    "        self.logger.info(\"Generating feature importance based on SHAP values...\")\n",
    "        self.logger.debug(f\"Received SHAP values with shape: {np.shape(shap_values)}\")\n",
    "        \n",
    "        # Handle 3D array by selecting one slice if necessary.\n",
    "        if isinstance(shap_values, np.ndarray) and shap_values.ndim == 3:\n",
    "            self.logger.warning(f\"SHAP values are 3D with shape {shap_values.shape}; selecting one slice for global recommendations.\")\n",
    "            if shap_values.shape[2] == 2:\n",
    "                shap_values = shap_values[:, :, 1]\n",
    "                self.logger.debug(\"Selected the positive class slice (index 1) for SHAP values.\")\n",
    "            else:\n",
    "                error_msg = f\"Unexpected 3D shape for SHAP values: {shap_values.shape}. Cannot determine which slice to use.\"\n",
    "                self.logger.error(error_msg)\n",
    "                raise ValueError(error_msg)\n",
    "\n",
    "        try:\n",
    "            shap_df = pd.DataFrame(shap_values, columns=X_original.columns)\n",
    "            feature_importance = pd.DataFrame({\n",
    "                'feature': X_original.columns,\n",
    "                'importance': np.abs(shap_df).mean(axis=0),\n",
    "                'mean_shap': shap_df.mean(axis=0)\n",
    "            }).sort_values(by='importance', ascending=False)\n",
    "            if debug:\n",
    "                self.logger.debug(f\"Feature importance (top {top_n}):\\n{feature_importance.head(top_n)}\")\n",
    "            top_features = feature_importance.head(top_n)['feature'].tolist()\n",
    "            recommendations = {}\n",
    "            for feature in top_features:\n",
    "                feature_values = X_original[feature]\n",
    "                range_str = self._compute_feature_range(feature_values, use_mad, debug)\n",
    "                mean_shap = feature_importance.loc[feature_importance['feature'] == feature, 'mean_shap'].values[0]\n",
    "                direction = 'increase' if mean_shap > 0 else 'decrease'\n",
    "                recommendations[feature] = {\n",
    "                    'range': range_str,\n",
    "                    'importance': round(feature_importance.loc[feature_importance['feature'] == feature, 'importance'].values[0], 4),\n",
    "                    'direction': direction\n",
    "                }\n",
    "                if debug:\n",
    "                    self.logger.debug(\n",
    "                        f\"Recommendation for {feature}: Range={range_str}, \"\n",
    "                        f\"Importance={feature_importance.loc[feature_importance['feature'] == feature, 'importance'].values[0]}, \"\n",
    "                        f\"Direction={direction}\"\n",
    "                    )\n",
    "            if debug:\n",
    "                self.logger.debug(f\"Final Recommendations with Importance and Direction: {recommendations}\")\n",
    "            return recommendations\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to generate global recommendations: {e}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "\n",
    "    def _compute_feature_range(self, feature_values: pd.Series, use_mad: bool, debug: bool) -> str:\n",
    "        if use_mad:\n",
    "            median = feature_values.median()\n",
    "            mad = feature_values.mad()\n",
    "            lower_bound = median - 1.5 * mad\n",
    "            upper_bound = median + 1.5 * mad\n",
    "            range_str = f\"{lower_bound:.1f}–{upper_bound:.1f}\"\n",
    "            if debug:\n",
    "                self.logger.debug(f\"Computed MAD-based range for feature: {range_str}\")\n",
    "        else:\n",
    "            lower_bound = feature_values.quantile(0.25)\n",
    "            upper_bound = feature_values.quantile(0.75)\n",
    "            range_str = f\"{lower_bound:.1f}–{upper_bound:.1f}\"\n",
    "            if debug:\n",
    "                self.logger.debug(f\"Computed IQR-based range for feature: {range_str}\")\n",
    "        return range_str\n",
    "\n",
    "    def _normalize_feature_name(self, name: str) -> str:\n",
    "        # Ensure the name is a string.\n",
    "        return str(name).replace(\"num__\", \"\").replace(\"cat__\", \"\")\n",
    "\n",
    "    def generate_individual_feedback(\n",
    "        self,\n",
    "        trial: pd.Series,\n",
    "        shap_values_trial: np.ndarray,\n",
    "        percentile: float = 10.0,\n",
    "        expected_features: Optional[List[str]] = None,\n",
    "        # New optional parameter: the reference dataset from which to compute the metric min and max.\n",
    "        reference_dataset: Optional[pd.DataFrame] = None\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Generate individual feedback for a trial.\n",
    "        This version normalizes feature names from both the expected list and trial index.\n",
    "        In addition to numeric outputs, a detailed textual explanation is provided.\n",
    "\n",
    "        Parameters:\n",
    "            trial (pd.Series): The trial data (e.g. from the inverse transform).\n",
    "            shap_values_trial (np.ndarray): The SHAP values for this trial.\n",
    "            percentile (float): The input percentile used to scale the tolerance.\n",
    "            expected_features (Optional[List[str]]): The list of expected features.\n",
    "            reference_dataset (Optional[pd.DataFrame]): A DataFrame containing the reference data.\n",
    "                If provided, for each feature the tolerance is computed as:\n",
    "                    tolerance = ((dataset_max - dataset_min) * (percentile/100.0)) / 2\n",
    "                Otherwise, the tolerance is computed using the default method.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, Any]: A dictionary with detailed feedback for each feature.\n",
    "        \"\"\"\n",
    "        if expected_features is None:\n",
    "            expected_features = trial.index.tolist()\n",
    "        \n",
    "        # Normalize expected features and trial keys.\n",
    "        norm_expected = {self._normalize_feature_name(f) for f in expected_features}\n",
    "        norm_trial = {self._normalize_feature_name(f) for f in trial.index.tolist()}\n",
    "        \n",
    "        self.logger.debug(f\"Normalized expected features: {norm_expected}\")\n",
    "        self.logger.debug(f\"Normalized trial features: {norm_trial}\")\n",
    "        \n",
    "        # Log the raw trial keys for extra visibility.\n",
    "        self.logger.debug(f\"Original trial keys: {list(trial.index)}\")\n",
    "        \n",
    "        missing = norm_expected - norm_trial\n",
    "        if missing:\n",
    "            self.logger.error(f\"Critical feature mismatch after normalization: {missing}\")\n",
    "            raise ValueError(f\"Feature alignment failed: {missing}\")\n",
    "\n",
    "        # Check if shap_values_trial is 2D; if so, select one column.\n",
    "        shape_trial = np.shape(shap_values_trial)\n",
    "        self.logger.debug(f\"SHAP values for trial before processing: shape = {shape_trial}\")\n",
    "        if len(shape_trial) == 2 and shape_trial[1] == 2:\n",
    "            self.logger.debug(\"Detected 2D SHAP values for individual trial; selecting positive class (index 1).\")\n",
    "            shap_values_trial = shap_values_trial[:, 1]\n",
    "            self.logger.debug(f\"New shape of SHAP values for trial: {np.shape(shap_values_trial)}\")\n",
    "\n",
    "        # If the SHAP output is a scalar, wrap it in a list.\n",
    "        if np.isscalar(shap_values_trial):\n",
    "            shap_values_trial = [shap_values_trial]\n",
    "            self.logger.debug(\"SHAP output was a scalar; wrapped into a list.\")\n",
    "\n",
    "        num_expected = len(expected_features)\n",
    "        num_shap = len(shap_values_trial)\n",
    "        if num_expected != num_shap:\n",
    "            self.logger.warning(f\"Length mismatch: expected features length = {num_expected} but SHAP values length = {num_shap}. Iterating up to the minimum length.\")\n",
    "            min_len = min(num_expected, num_shap)\n",
    "        else:\n",
    "            min_len = num_expected\n",
    "\n",
    "        feedback = {}\n",
    "        for i in range(min_len):\n",
    "            orig_feature = expected_features[i]\n",
    "            shap_val = shap_values_trial[i]\n",
    "            norm_feature = self._normalize_feature_name(orig_feature)\n",
    "            \n",
    "            # Try to match the normalized key; if not found, try with and without the 'num__' prefix.\n",
    "            matching_keys = [k for k in trial.index if self._normalize_feature_name(k) == norm_feature]\n",
    "            if not matching_keys:\n",
    "                alt_key = \"num__\" + norm_feature\n",
    "                if alt_key in trial.index:\n",
    "                    matching_keys = [alt_key]\n",
    "                else:\n",
    "                    self.logger.debug(f\"No matching key found in trial for normalized feature '{norm_feature}'. Skipping.\")\n",
    "                    continue\n",
    "            \n",
    "            trial_key = matching_keys[0]\n",
    "            trial_value = trial.get(trial_key, None)\n",
    "            \n",
    "            self.logger.debug(f\"Processing feature '{orig_feature}' (normalized: '{norm_feature}'): SHAP value = {shap_val}, Trial value = {trial_value}\")\n",
    "            \n",
    "            # Updated type checks to accept numpy numeric types as well.\n",
    "            if not isinstance(shap_val, (int, float, np.number)) or pd.isna(shap_val):\n",
    "                self.logger.debug(f\"Invalid SHAP value type {type(shap_val)} for '{orig_feature}' ({shap_val}). Skipping.\")\n",
    "                continue\n",
    "            if not isinstance(trial_value, (int, float, np.number)) or pd.isna(trial_value):\n",
    "                self.logger.debug(f\"Invalid trial value type {type(trial_value)} for '{orig_feature}' ({trial_value}). Skipping.\")\n",
    "                continue\n",
    "            \n",
    "            suggestion = \"increase\" if shap_val > 0 else \"decrease\"\n",
    "            adjustment_factor = 0.1\n",
    "            unit_change_value = adjustment_factor * abs(trial_value)\n",
    "            goal_value = trial_value + unit_change_value if suggestion == \"increase\" else trial_value - unit_change_value\n",
    "            unit = \"units\"\n",
    "            \n",
    "            # --- New Min/Max Strategy Implementation ---\n",
    "            if reference_dataset is not None:\n",
    "                self.logger.debug(f\"Reference dataset provided for feature '{orig_feature}'. Available columns: {list(reference_dataset.columns)}\")\n",
    "                # Try to locate the corresponding column in the reference dataset by matching normalized names.\n",
    "                ref_col = None\n",
    "                for col in reference_dataset.columns:\n",
    "                    normalized_col = self._normalize_feature_name(col)\n",
    "                    self.logger.debug(f\"Checking reference dataset column '{col}' normalized as '{normalized_col}' against normalized feature '{norm_feature}'\")\n",
    "                    if normalized_col == norm_feature:\n",
    "                        ref_col = col\n",
    "                        break\n",
    "                if ref_col is not None:\n",
    "                    metric_min = reference_dataset[ref_col].min()\n",
    "                    metric_max = reference_dataset[ref_col].max()\n",
    "                    range_span = metric_max - metric_min\n",
    "                    total_adjustment = range_span * (percentile / 100.0)\n",
    "                    tolerance = total_adjustment / 2.0\n",
    "                    self.logger.debug(\n",
    "                        f\"Using dataset-based tolerance for '{orig_feature}': \"\n",
    "                        f\"ref_col='{ref_col}', metric_min={metric_min}, metric_max={metric_max}, \"\n",
    "                        f\"range_span={range_span}, total_adjustment={total_adjustment}, tolerance={tolerance}\"\n",
    "                    )\n",
    "                else:\n",
    "                    self.logger.debug(f\"Reference dataset provided but no matching column found for '{orig_feature}' (normalized as '{norm_feature}').\")\n",
    "                    tolerance = 0\n",
    "                    self.logger.debug(f\"Falling back to default tolerance: {tolerance}\")\n",
    "            else:\n",
    "                tolerance = 0\n",
    "                self.logger.debug(f\"Reference dataset not provided. Using default tolerance for '{orig_feature}': tolerance={tolerance}\")\n",
    "            # Set the acceptable range around the goal.\n",
    "            min_value = goal_value - tolerance\n",
    "            max_value = goal_value + tolerance\n",
    "            # --- End New Strategy ---\n",
    "\n",
    "            # --- End New Strategy ---\n",
    "            \n",
    "            feedback_text = (\n",
    "                f\"For feature '{orig_feature}': The SHAP value of {shap_val:.2f} suggests to {suggestion} the value. \"\n",
    "                f\"Current value is {trial_value:.2f}. A 10% adjustment equals {unit_change_value:.2f} {unit}, \"\n",
    "                f\"which would set a target of {goal_value:.2f}. The acceptable range is [{min_value:.2f}, {max_value:.2f}], \"\n",
    "                f\"classifying this metric as '{self.classify_metric(trial_value, min_value, max_value)}'.\"\n",
    "            )\n",
    "\n",
    "            feedback[f\"shap_{norm_feature}_unit_change\"] = round(unit_change_value, 2)\n",
    "            feedback[f\"shap_{norm_feature}_unit\"] = unit\n",
    "            feedback[f\"shap_{norm_feature}_direction\"] = suggestion\n",
    "            feedback[f\"shap_{norm_feature}_importance\"] = round(abs(shap_val), 4)\n",
    "            feedback[f\"shap_{norm_feature}_goal\"] = round(goal_value, 3)\n",
    "            feedback[f\"shap_{norm_feature}_min\"] = round(min_value, 3)\n",
    "            feedback[f\"shap_{norm_feature}_max\"] = round(max_value, 3)\n",
    "            feedback[f\"shap_{norm_feature}_classification\"] = self.classify_metric(trial_value, min_value, max_value)\n",
    "            feedback[f\"shap_{norm_feature}_feedback_text\"] = feedback_text\n",
    "            \n",
    "            self.logger.debug(\n",
    "                f\"For feature '{norm_feature}': suggestion={suggestion}, goal={goal_value:.3f}, \"\n",
    "                f\"min={min_value:.3f}, max={max_value:.3f}, classification={self.classify_metric(trial_value, min_value, max_value)}, \"\n",
    "                f\"trial value={trial_value}, unit_change={unit_change_value:.3f} {unit}. \"\n",
    "                f\"Feedback text: {feedback_text}\"\n",
    "            )\n",
    "        \n",
    "        if not feedback:\n",
    "            self.logger.warning(\"No individual feedback was generated; returning raw SHAP values for each feature.\")\n",
    "            feedback = {}\n",
    "            for f in expected_features:\n",
    "                norm_f = self._normalize_feature_name(f)\n",
    "                value = trial.get(norm_f)\n",
    "                if value is None:\n",
    "                    value = trial.get(\"num__\" + norm_f)\n",
    "                feedback[norm_f] = f\"Raw SHAP value: {value}\"\n",
    "        \n",
    "        return feedback\n",
    "\n",
    "\n",
    "\n",
    "    def classify_metric(self, value: float, min_val: float, max_val: float) -> str:\n",
    "        if value < min_val:\n",
    "            return \"Early\"\n",
    "        elif value > max_val:\n",
    "            return \"Late\"\n",
    "        else:\n",
    "            return \"Good\"\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Test code to verify the FeedbackGenerator class\n",
    "    print(\"Testing FeedbackGenerator module...\")\n",
    "\n",
    "    from ml.train_utils.train_utils import load_model\n",
    "    from datapreprocessor import DataPreprocessor\n",
    "    from ml.predict.predict import predict_and_attach_predict_probs\n",
    "    from ml.feature_selection.feature_importance_calculator import manage_features\n",
    "\n",
    "    # from ml.shap.shap_utils import (\n",
    "    #     load_dataset,\n",
    "    #     setup_logging, load_configuration, initialize_logger\n",
    "    # )\n",
    "\n",
    "\n",
    "    # **Load Configuration and Initialize Logger**\n",
    "    config_path = Path('../../data/model/preprocessor_config/preprocessor_config.yaml')\n",
    "    model = 'Random Forest' #XGBoost, CatBoost, Random Forest\n",
    "    try: \n",
    "        config = load_configuration(config_path)\n",
    "        print(f\"✅ Configuration loaded successfully from {config_path}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to load configuration: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "    log_file = Path(config.paths.log_file).resolve()\n",
    "    try:\n",
    "        logger = initialize_logger(config, log_file)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to set up logging: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "    # **Load Dataset**\n",
    "    raw_data_path = Path(config.paths.data_dir).resolve() / config.paths.raw_data\n",
    "    try:\n",
    "        df = load_dataset(raw_data_path)\n",
    "        print(f\"✅ Dataset loaded successfully from {raw_data_path}.\")\n",
    "        print(f\"📊 Dataset Columns: {df.columns.tolist()}\")\n",
    "        logger.info(f\"Dataset loaded with shape: {df.shape}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Failed to load dataset: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "    # **Load Model**\n",
    "    try:\n",
    "        model = load_model(model, Path(config.paths.model_save_base_dir).resolve())\n",
    "        print(\"✅ Model loaded successfully.\")\n",
    "        logger.info(\"Model loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Failed to load model: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "    # **Initialize ShapCalculator**\n",
    "    shap_calculator = ShapCalculator(model=model, logger=logger)\n",
    "    print(\"✅ ShapCalculator initialized successfully.\")\n",
    "    logger.info(\"ShapCalculator initialized successfully.\")\n",
    "\n",
    "    # **Load Feature Lists (via manage_features)**\n",
    "    base_dir = Path(\"../../data\") / \"preprocessor\" / \"features_info\"\n",
    "    features_file = (Path(config.paths.data_dir) / config.paths.features_metadata_file).resolve()\n",
    "    ordinal_file = Path(f'{base_dir}/ordinal_categoricals.pkl')\n",
    "    nominal_file = Path(f'{base_dir}/nominal_categoricals.pkl')\n",
    "    numericals_file = Path(f'{base_dir}/numericals.pkl')\n",
    "    y_variable_file = Path(f'{base_dir}/y_variable.pkl')\n",
    "    model_save_dir_override = Path(config.paths.model_save_base_dir)\n",
    "    transformers_dir_override = Path(config.paths.transformers_save_base_dir)\n",
    "\n",
    "    feature_paths = {\n",
    "        'features': features_file,\n",
    "        'ordinal_categoricals': ordinal_file,\n",
    "        'nominal_categoricals': nominal_file,\n",
    "        'numericals': numericals_file,\n",
    "        'y_variable': y_variable_file\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        feature_lists = manage_features(mode='load', paths=feature_paths)\n",
    "        y_variable_list = feature_lists.get('y_variable', [])\n",
    "        ordinal_categoricals = feature_lists.get('ordinal_categoricals', [])\n",
    "        nominal_categoricals = feature_lists.get('nominal_categoricals', [])\n",
    "        numericals = feature_lists.get('numericals', [])\n",
    "        if logger:\n",
    "            logger.debug(f\"Loaded Feature Lists: y_variable={y_variable_list}, ordinal_categoricals={ordinal_categoricals}, nominal_categoricals={nominal_categoricals}, numericals={numericals}\")\n",
    "    except Exception as e:\n",
    "        if logger:\n",
    "            logger.warning(f\"Feature lists could not be loaded: {e}\")\n",
    "        y_variable_list, ordinal_categoricals, nominal_categoricals, numericals = [], [], [], []\n",
    "\n",
    "    # **Initialize DataPreprocessor**\n",
    "    preprocessor = DataPreprocessor(\n",
    "        model_type=\"Tree Based Classifier\",\n",
    "        y_variable=y_variable_list,\n",
    "        ordinal_categoricals=ordinal_categoricals,\n",
    "        nominal_categoricals=nominal_categoricals,\n",
    "        numericals=numericals,\n",
    "        mode='predict',\n",
    "        options={},\n",
    "        debug=True,\n",
    "        normalize_debug=False,\n",
    "        normalize_graphs_output=False,\n",
    "        graphs_output_dir=Path(config.paths.plots_output_dir).resolve(),\n",
    "        transformers_dir=transformers_dir_override\n",
    "    )\n",
    "\n",
    "    # **Preprocess Data**\n",
    "    try:\n",
    "        X_preprocessed, recommendations, X_inversed = preprocessor.final_preprocessing(df)\n",
    "        logger.info(\"Preprocessing completed successfully in predict mode.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Preprocessing failed: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "    # **Validate Preprocessed Data**\n",
    "    try:\n",
    "        non_numeric_features = X_preprocessed.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "        if non_numeric_features:\n",
    "            logger.error(f\"Non-numeric features detected in preprocessed data: {non_numeric_features}\")\n",
    "            raise ValueError(f\"Preprocessed data contains non-numeric features: {non_numeric_features}\")\n",
    "        else:\n",
    "            logger.debug(\"All features in X_preprocessed are numeric.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Validation failed: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "    # **Compute SHAP Values**\n",
    "    try:\n",
    "        explainer, shap_values = shap_calculator.compute_shap_values(X_preprocessed, debug=True)\n",
    "        print(\"✅ SHAP values computed successfully.\")\n",
    "        logger.info(f\"SHAP values computed with shape: {shap_values.shape}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Failed to compute SHAP values: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "    # **Initialize FeedbackGenerator**\n",
    "    feedback_generator = FeedbackGenerator(logger=logger)\n",
    "    print(\"✅ FeedbackGenerator initialized successfully.\")\n",
    "    logger.info(\"FeedbackGenerator initialized successfully.\")\n",
    "\n",
    "    # **Generate Global Recommendations**\n",
    "    try:\n",
    "        recommendations = feedback_generator.generate_global_recommendations(\n",
    "            shap_values=shap_values,\n",
    "            X_original=X_preprocessed,\n",
    "            top_n=5,\n",
    "            use_mad=False,\n",
    "            debug=True\n",
    "        )\n",
    "        print(\"✅ Global recommendations generated successfully:\")\n",
    "        for feature, rec in recommendations.items():\n",
    "            print(f\"  - {feature}: {rec}\")\n",
    "        logger.info(\"Global recommendations generated successfully.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Failed to generate global recommendations: {e}\")\n",
    "\n",
    "    # **Generate Individual Feedback for a Specific Trial**\n",
    "    try:\n",
    "        trial_id = 1  # Intended trial id\n",
    "        trial_index = X_preprocessed.index.get_loc(trial_id)\n",
    "        shap_values_trial = shap_values[trial_index]  # Use the row corresponding to trial_id\n",
    "        expected_features = X_preprocessed.columns.tolist()\n",
    "        print(\"expected_features\", expected_features)\n",
    "        feedback = feedback_generator.generate_individual_feedback(trial=X_inversed.loc[trial_id],\n",
    "                                                                shap_values_trial=shap_values_trial,\n",
    "                                                                percentile=10,\n",
    "                                                                expected_features=expected_features,\n",
    "                                                                reference_dataset=X_inversed)\n",
    "\n",
    "\n",
    "        print(f\"\\n✅ Individual feedback for trial '{trial_id}':\")\n",
    "        for metric, suggestion in feedback.items():\n",
    "            print(f\"  - {metric}: {suggestion}\")\n",
    "        logger.info(f\"Individual feedback for trial '{trial_id}' generated successfully.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Failed to generate individual feedback for trial '{trial_id}': {e}\")\n",
    "\n",
    "    print(\"✅ All tests in feedback_generator.py passed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ml/shap/predict_with_shap_usage_utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ml/shap/predict_with_shap_usage_utils.py\n",
    "import pandas as pd\n",
    "import logging\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Optional,  List\n",
    "from ml.config.config_models import AppConfig\n",
    "from ml.config.config_loader import load_config\n",
    "\n",
    "def compute_original_metric_error(df: pd.DataFrame, percentile: float, logger: Optional[logging.Logger] = None) -> pd.DataFrame:\n",
    "    df_out = df.copy()\n",
    "    logger.info(\"[Step C] Computing original metric error with corrected feature extraction.\")\n",
    "\n",
    "    shap_unit_change_cols = [c for c in df_out.columns if c.startswith(\"shap_\") and c.endswith(\"_unit_change\")]\n",
    "    logger.debug(f\"[Debug] Found shap_ columns with '_unit_change': {shap_unit_change_cols}\")\n",
    "\n",
    "    for col in shap_unit_change_cols:\n",
    "        feature_name = col[len(\"shap_\"): col.rfind(\"_unit_change\")]\n",
    "        goal_col = f\"shap_{feature_name}_goal\"\n",
    "        min_col  = f\"shap_{feature_name}_min\"\n",
    "        max_col  = f\"shap_{feature_name}_max\"\n",
    "        class_col= f\"shap_{feature_name}_classification\"\n",
    "\n",
    "        if goal_col not in df_out.columns:\n",
    "            logger.debug(f\"[Debug] {goal_col} missing => skipping min/max/classification for '{feature_name}'.\")\n",
    "            df_out[min_col] = np.nan\n",
    "            df_out[max_col] = np.nan\n",
    "            df_out[class_col] = \"No data\"\n",
    "            continue\n",
    "\n",
    "        df_out[goal_col] = pd.to_numeric(df_out[goal_col], errors='coerce')\n",
    "        df_out[min_col] = pd.to_numeric(df_out[min_col], errors='coerce')\n",
    "        df_out[max_col] = pd.to_numeric(df_out[max_col], errors='coerce')\n",
    "        df_out[feature_name] = pd.to_numeric(df_out[feature_name], errors='coerce')\n",
    "\n",
    "        if df_out[col].notna().any():\n",
    "            tolerance = np.percentile(df_out[col].dropna(), percentile)\n",
    "        else:\n",
    "            tolerance = 0.0\n",
    "\n",
    "        logger.debug(f\"[Debug] For feature '{feature_name}': tolerance={tolerance:.3f}\")\n",
    "        df_out[min_col] = df_out[goal_col] - tolerance\n",
    "        df_out[max_col] = df_out[goal_col] + tolerance\n",
    "\n",
    "        def classify(row):\n",
    "            try:\n",
    "                current_val = row.get(feature_name, None)\n",
    "                min_val = row.get(min_col, None)\n",
    "                max_val = row.get(max_col, None)\n",
    "            except Exception as e:\n",
    "                logger.debug(f\"Row {row.name}, feature {feature_name} => {row[feature_name]}\")\n",
    "                raise e\n",
    "\n",
    "            try:\n",
    "                val  = float(current_val) if pd.notnull(current_val) else np.nan\n",
    "                vmin = float(min_val) if pd.notnull(min_val) else np.nan\n",
    "                vmax = float(max_val) if pd.notnull(max_val) else np.nan\n",
    "            except (ValueError, TypeError) as e:\n",
    "                logger.debug(f\"[Debug] Classification error for {feature_name}: {e}\")\n",
    "                return \"No data\"\n",
    "\n",
    "            if pd.isnull(val) or pd.isnull(vmin) or pd.isnull(vmax):\n",
    "                return \"No data\"\n",
    "            if val < vmin:\n",
    "                return \"Early\"\n",
    "            elif val > vmax:\n",
    "                return \"Late\"\n",
    "            else:\n",
    "                return \"Good\"\n",
    "\n",
    "        df_out[class_col] = df_out.apply(classify, axis=1)\n",
    "        if not pd.api.types.is_numeric_dtype(df_out[min_col]) or not pd.api.types.is_numeric_dtype(df_out[max_col]):\n",
    "            logger.error(f\"Min or Max columns for feature '{feature_name}' contain non-numeric data after conversion.\")\n",
    "            df_out[class_col] = \"No data\"\n",
    "\n",
    "    return df_out\n",
    "\n",
    "def generate_feedback_and_expand(\n",
    "    X_inversed: pd.DataFrame,\n",
    "    shap_values: np.ndarray,\n",
    "    logger: logging.Logger,\n",
    "    feedback_generator: Any,\n",
    "    metrics_percentile: float = 10.0,\n",
    "    expected_features: Optional[List[str]] = None,\n",
    "    reference_index: Optional[pd.Index] = None\n",
    "):\n",
    "    logger.info(\"[Step A] Generating feedback for each trial with detailed debug information.\")\n",
    "    logger.debug(f\"X_inversed.shape = {X_inversed.shape}, shap_values.shape = {shap_values.shape}\")\n",
    "    if reference_index is not None:\n",
    "        logger.debug(\"Reindexing X_inversed to match the reference index.\")\n",
    "        X_inversed = X_inversed.reindex(reference_index)\n",
    "        logger.debug(f\"New X_inversed index: {X_inversed.index.tolist()}\")\n",
    "\n",
    "    if X_inversed.shape[0] != shap_values.shape[0]:\n",
    "        logger.error(\"Mismatch between number of shap_values and number of trials.\")\n",
    "        raise ValueError(\"Mismatch between shap_values and trials.\")\n",
    "\n",
    "    feedback_list = []\n",
    "    for pos in range(X_inversed.shape[0]):\n",
    "        try:\n",
    "            logger.debug(f\"[feedback-loop] Processing row at position {pos}\")\n",
    "            shap_values_trial = shap_values[pos]\n",
    "            logger.debug(f\"[feedback-loop] Retrieved shap_values_trial for pos={pos}: {shap_values_trial}\")\n",
    "            trial_features = X_inversed.iloc[pos]\n",
    "            logger.debug(f\"[feedback-loop] Trial features (type {type(trial_features)}): {trial_features}\")\n",
    "            feedback = feedback_generator.generate_individual_feedback(\n",
    "                trial=trial_features,\n",
    "                shap_values_trial=shap_values_trial,\n",
    "                percentile=metrics_percentile,\n",
    "                expected_features=expected_features\n",
    "            )\n",
    "            logger.debug(f\"[feedback-loop] Generated feedback for pos={pos}: {feedback}\")\n",
    "            feedback_list.append(feedback)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error generating feedback for row at position {pos}: {e}\")\n",
    "            feedback_list.append({})\n",
    "            continue\n",
    "\n",
    "    feedback_df = pd.DataFrame(feedback_list)\n",
    "    logger.debug(f\"Feedback DataFrame shape: {feedback_df.shape}\")\n",
    "    # Convert columns that hold non-numeric feedback to object type to avoid future dtype warnings.\n",
    "    for col in feedback_df.columns:\n",
    "        if feedback_df[col].dtype.kind in 'if':\n",
    "            feedback_df[col] = feedback_df[col].astype(object)\n",
    "\n",
    "    existing_shap_cols = {col for col in X_inversed.columns if col.startswith(\"shap_\")}\n",
    "    new_shap_cols = {col for col in feedback_df.columns if col.startswith(\"shap_\")}\n",
    "    duplicate_shap_cols = existing_shap_cols.intersection(new_shap_cols)\n",
    "    if duplicate_shap_cols:\n",
    "        logger.warning(f\"Duplicate shap_ columns detected: {duplicate_shap_cols}. Renaming new shap_ columns to avoid conflicts.\")\n",
    "        feedback_df = feedback_df.rename(columns=lambda x: f\"{x}.1\" if x in duplicate_shap_cols else x)\n",
    "\n",
    "    X_inversed = pd.concat([X_inversed.reset_index(drop=True), feedback_df.reset_index(drop=True)], axis=1)\n",
    "    logger.debug(f\"X_inversed shape after merging feedback: {X_inversed.shape}\")\n",
    "\n",
    "    logger.info(\"[Step B] Computing original metric error with corrected feature extraction.\")\n",
    "    X_inversed = compute_original_metric_error(\n",
    "        df=X_inversed,\n",
    "        percentile=metrics_percentile,\n",
    "        logger=logger\n",
    "    )\n",
    "\n",
    "    return X_inversed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available in shap_utils:\n",
      "  - Function: initialize_logger\n",
      "  - Function: load_config\n",
      "  - Function: load_configuration\n",
      "  - Function: load_dataset\n",
      "  - Function: setup_logging\n",
      "[Config Loader] ✅ Successfully loaded configuration from ../../data/model/preprocessor_config/preprocessor_config.yaml\n",
      "Configuration loaded successfully from ../../data/model/preprocessor_config/preprocessor_config.yaml.\n",
      "2025-04-07 17:00:41,178 - ml.shap.shap_utils - INFO - Starting prediction module (unified predict_and_shap).\n",
      "Columns in input data: ['result', 'landing_x', 'landing_y', 'entry_angle', 'L_ANKLE_min_power', 'L_ANKLE_max_power', 'L_ANKLE_avg_power', 'L_ANKLE_std_power', 'R_ANKLE_min_power', 'R_ANKLE_max_power', 'R_ANKLE_avg_power', 'R_ANKLE_std_power', 'L_KNEE_min_power', 'L_KNEE_max_power', 'L_KNEE_avg_power', 'L_KNEE_std_power', 'R_KNEE_min_power', 'R_KNEE_max_power', 'R_KNEE_avg_power', 'R_KNEE_std_power', 'L_HIP_min_power', 'L_HIP_max_power', 'L_HIP_avg_power', 'L_HIP_std_power', 'R_HIP_min_power', 'R_HIP_max_power', 'R_HIP_avg_power', 'R_HIP_std_power', 'L_ELBOW_min_power', 'L_ELBOW_max_power', 'L_ELBOW_avg_power', 'L_ELBOW_std_power', 'R_ELBOW_min_power', 'R_ELBOW_max_power', 'R_ELBOW_avg_power', 'R_ELBOW_std_power', 'L_WRIST_min_power', 'L_WRIST_max_power', 'L_WRIST_avg_power', 'L_WRIST_std_power', 'R_WRIST_min_power', 'R_WRIST_max_power', 'R_WRIST_avg_power', 'R_WRIST_std_power', 'L_1STFINGER_min_power', 'L_1STFINGER_max_power', 'L_1STFINGER_avg_power', 'L_1STFINGER_std_power', 'L_5THFINGER_min_power', 'L_5THFINGER_max_power', 'L_5THFINGER_avg_power', 'L_5THFINGER_std_power', 'R_1STFINGER_min_power', 'R_1STFINGER_max_power', 'R_1STFINGER_avg_power', 'R_1STFINGER_std_power', 'R_5THFINGER_min_power', 'R_5THFINGER_max_power', 'R_5THFINGER_avg_power', 'R_5THFINGER_std_power', 'L_ELBOW_max_angle', 'L_ELBOW_release_angle', 'L_WRIST_max_angle', 'L_WRIST_release_angle', 'L_KNEE_max_angle', 'L_KNEE_release_angle', 'R_ELBOW_max_angle', 'R_ELBOW_release_angle', 'R_WRIST_max_angle', 'R_WRIST_release_angle', 'R_KNEE_max_angle', 'R_KNEE_release_angle', 'release_ball_speed', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z', 'release_ball_direction_x', 'release_ball_direction_y', 'release_ball_direction_z', 'release_ball_x', 'release_ball_y', 'release_ball_z', 'release_frame_time', 'release_angle', 'time_to_peak', 'peak_height_relative', 'player_participant_id', 'player_height_in_meters', 'player_weight__in_kg', 'player_dominant_hand', 'player_estimated_wingspan_cm', 'player_estimated_standing_reach_cm', 'player_estimated_hand_length_cm', 'trial_id', 'shot_id', 'initial_release_angle', 'calculated_release_angle', 'optimal_release_angle', 'angle_difference', 'L_ANKLE_energy_mean', 'R_ANKLE_energy_mean', 'L_KNEE_energy_mean', 'R_KNEE_energy_mean', 'L_HIP_energy_mean', 'R_HIP_energy_mean', 'L_ELBOW_energy_mean', 'R_ELBOW_energy_mean', 'L_WRIST_energy_mean', 'R_WRIST_energy_mean', 'L_1STFINGER_energy_mean', 'R_1STFINGER_energy_mean', 'L_5THFINGER_energy_mean', 'R_5THFINGER_energy_mean', 'L_ANKLE_energy_max', 'R_ANKLE_energy_max', 'L_KNEE_energy_max', 'R_KNEE_energy_max', 'L_HIP_energy_max', 'R_HIP_energy_max', 'L_ELBOW_energy_max', 'R_ELBOW_energy_max', 'L_WRIST_energy_max', 'R_WRIST_energy_max', 'L_1STFINGER_energy_max', 'R_1STFINGER_energy_max', 'L_5THFINGER_energy_max', 'R_5THFINGER_energy_max', 'L_ANKLE_energy_std', 'R_ANKLE_energy_std', 'L_KNEE_energy_std', 'R_KNEE_energy_std', 'L_HIP_energy_std', 'R_HIP_energy_std', 'L_ELBOW_energy_std', 'R_ELBOW_energy_std', 'L_WRIST_energy_std', 'R_WRIST_energy_std', 'L_1STFINGER_energy_std', 'R_1STFINGER_energy_std', 'L_5THFINGER_energy_std', 'R_5THFINGER_energy_std', 'player_height_in_meters_category', 'player_weight__in_kg_category', 'player_estimated_wingspan_cm_category', 'player_estimated_standing_reach_cm_category', 'player_estimated_hand_length_cm_category']\n",
      "2025-04-07 17:00:41,195 - ml.shap.shap_utils - INFO - Prediction input data loaded from /workspaces/spl_freethrow_biomechanics_analysis_ml_prediction/data/preprocessor/../../data/processed/final_ml_dataset.csv.\n",
      "\n",
      "--- Running pipeline for model: XGBoost ---\n",
      "tuning_results_path: /workspaces/spl_freethrow_biomechanics_analysis_ml_prediction/data/model/tuning_results.json\n",
      "2025-04-07 17:00:41,204 - ml.shap.shap_utils - INFO - Overriding best model selection; using model: XGBoost\n",
      "2025-04-07 17:00:41,223 - ml.train_utils.train_utils - INFO - Model loaded from /workspaces/spl_freethrow_biomechanics_analysis_ml_prediction/data/model/XGBoost_model.pkl\n",
      "2025-04-07 17:00:41,224 - ml.shap.shap_utils - INFO - Trained model loaded from '/workspaces/spl_freethrow_biomechanics_analysis_ml_prediction/data/model' using model name 'XGBoost'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:41,236 [INFO] Starting: Final Preprocessing Pipeline in 'predict' mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Features loaded from /workspaces/spl_freethrow_biomechanics_analysis_ml_prediction/data/preprocessor/features_info/final_ml_df_selected_features_columns.pkl\n",
      "✅ Ordinal categoricals loaded from ../../data/preprocessor/features_info/ordinal_categoricals.pkl\n",
      "✅ Nominal categoricals loaded from ../../data/preprocessor/features_info/nominal_categoricals.pkl\n",
      "✅ Numericals loaded from ../../data/preprocessor/features_info/numericals.pkl\n",
      "✅ Y variable loaded from ../../data/preprocessor/features_info/y_variable.pkl\n",
      "2025-04-07 17:00:41,236 - DataPreprocessor - INFO - Starting: Final Preprocessing Pipeline in 'predict' mode.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:41,238 [INFO] Step: filter_columns\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:41,238 - DataPreprocessor - INFO - Step: filter_columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:41,240 [INFO] ✅ Filtered DataFrame to include only specified features. Shape: (125, 14)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:41,240 - DataPreprocessor - INFO - ✅ Filtered DataFrame to include only specified features. Shape: (125, 14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:41,241 [DEBUG] Selected Features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'R_ELBOW_release_angle', 'R_ELBOW_max_angle', 'R_WRIST_release_angle', 'R_WRIST_max_angle', 'R_KNEE_release_angle', 'R_KNEE_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z']\n",
      "2025-04-07 17:00:41,241 [INFO] ✅ Column filtering completed successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:41,241 - DataPreprocessor - INFO - ✅ Column filtering completed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:41,243 [INFO] Step: Preprocess Predict\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:41,243 - DataPreprocessor - INFO - Step: Preprocess Predict\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:41,244 [DEBUG] Initial columns in prediction data: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'R_ELBOW_release_angle', 'R_ELBOW_max_angle', 'R_WRIST_release_angle', 'R_WRIST_max_angle', 'R_KNEE_release_angle', 'R_KNEE_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z']\n",
      "2025-04-07 17:00:41,244 [DEBUG] Initial number of features: 14\n",
      "2025-04-07 17:00:41,244 [INFO] Step: Load Transformers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:41,244 - DataPreprocessor - INFO - Step: Load Transformers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:41,245 [DEBUG] Loading transformers from: /workspaces/spl_freethrow_biomechanics_analysis_ml_prediction/data/preprocessor/transformers/transformers.pkl\n",
      "2025-04-07 17:00:41,252 [DEBUG] Pipeline loaded. Ready to transform new data.\n",
      "2025-04-07 17:00:41,253 [INFO] Transformers loaded successfully from '/workspaces/spl_freethrow_biomechanics_analysis_ml_prediction/data/preprocessor/transformers/transformers.pkl'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:41,253 - DataPreprocessor - INFO - Transformers loaded successfully from '/workspaces/spl_freethrow_biomechanics_analysis_ml_prediction/data/preprocessor/transformers/transformers.pkl'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:41,254 [DEBUG] Transformers loaded successfully.\n",
      "2025-04-07 17:00:41,254 [INFO] Step: filter_columns\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:41,254 - DataPreprocessor - INFO - Step: filter_columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:41,256 [INFO] ✅ Filtered DataFrame to include only specified features. Shape: (125, 14)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:41,256 - DataPreprocessor - INFO - ✅ Filtered DataFrame to include only specified features. Shape: (125, 14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:41,257 [DEBUG] Selected Features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'R_ELBOW_release_angle', 'R_ELBOW_max_angle', 'R_WRIST_release_angle', 'R_WRIST_max_angle', 'R_KNEE_release_angle', 'R_KNEE_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z']\n",
      "2025-04-07 17:00:41,257 [DEBUG] Columns after filtering: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'R_ELBOW_release_angle', 'R_ELBOW_max_angle', 'R_WRIST_release_angle', 'R_WRIST_max_angle', 'R_KNEE_release_angle', 'R_KNEE_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z']\n",
      "2025-04-07 17:00:41,257 [DEBUG] Number of features after filtering: 14\n",
      "2025-04-07 17:00:41,258 [INFO] Step: Handle Missing Values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:41,258 - DataPreprocessor - INFO - Step: Handle Missing Values\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:41,263 [DEBUG] Columns after handling missing values: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'R_ELBOW_release_angle', 'R_ELBOW_max_angle', 'R_WRIST_release_angle', 'R_WRIST_max_angle', 'R_KNEE_release_angle', 'R_KNEE_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z']\n",
      "2025-04-07 17:00:41,264 [DEBUG] Number of features after handling missing values: 14\n",
      "2025-04-07 17:00:41,264 [DEBUG] Expected raw features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'R_ELBOW_release_angle', 'R_ELBOW_max_angle', 'R_WRIST_release_angle', 'R_WRIST_max_angle', 'R_KNEE_release_angle', 'R_KNEE_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z']\n",
      "2025-04-07 17:00:41,265 [DEBUG] Provided features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'R_ELBOW_release_angle', 'R_ELBOW_max_angle', 'R_WRIST_release_angle', 'R_WRIST_max_angle', 'R_KNEE_release_angle', 'R_KNEE_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z']\n",
      "2025-04-07 17:00:41,265 [DEBUG] Reordered columns to match the pipeline's raw feature expectations.\n",
      "2025-04-07 17:00:41,269 [DEBUG] Transformed data shape: (125, 14)\n",
      "2025-04-07 17:00:41,270 [DEBUG] Derived feature names from pipeline: ['num__release_ball_direction_x', 'num__release_ball_direction_z', 'num__release_ball_direction_y', 'num__R_ELBOW_release_angle', 'num__R_ELBOW_max_angle', 'num__R_WRIST_release_angle', 'num__R_WRIST_max_angle', 'num__R_KNEE_release_angle', 'num__R_KNEE_max_angle', 'num__release_ball_speed', 'num__calculated_release_angle', 'num__release_ball_velocity_x', 'num__release_ball_velocity_y', 'num__release_ball_velocity_z']\n",
      "2025-04-07 17:00:41,270 [DEBUG] X_preprocessed_df columns: ['num__release_ball_direction_x', 'num__release_ball_direction_z', 'num__release_ball_direction_y', 'num__R_ELBOW_release_angle', 'num__R_ELBOW_max_angle', 'num__R_WRIST_release_angle', 'num__R_WRIST_max_angle', 'num__R_KNEE_release_angle', 'num__R_KNEE_max_angle', 'num__release_ball_speed', 'num__calculated_release_angle', 'num__release_ball_velocity_x', 'num__release_ball_velocity_y', 'num__release_ball_velocity_z']\n",
      "2025-04-07 17:00:41,273 [DEBUG] Sample of X_preprocessed_df:\n",
      "   num__release_ball_direction_x  num__release_ball_direction_z  \\\n",
      "0                       0.256808                      -0.014333   \n",
      "1                       0.653985                      -0.770471   \n",
      "2                       0.316179                      -0.134154   \n",
      "3                      -0.573663                       0.925278   \n",
      "4                       0.382947                      -0.241799   \n",
      "\n",
      "   num__release_ball_direction_y  num__R_ELBOW_release_angle  \\\n",
      "0                       1.145467                    1.044847   \n",
      "1                      -0.889734                   -1.374648   \n",
      "2                       0.190965                    1.316104   \n",
      "3                       1.149799                    0.887968   \n",
      "4                       0.013044                    1.270071   \n",
      "\n",
      "   num__R_ELBOW_max_angle  num__R_WRIST_release_angle  num__R_WRIST_max_angle  \\\n",
      "0                1.095488                    0.424441               -0.778748   \n",
      "1               -1.250958                    1.913313                0.102943   \n",
      "2                1.471855                   -0.176303                0.199613   \n",
      "3               -0.011954                   -1.316314               -0.640673   \n",
      "4               -0.184445                   -1.602153               -1.219041   \n",
      "\n",
      "   num__R_KNEE_release_angle  num__R_KNEE_max_angle  num__release_ball_speed  \\\n",
      "0                   0.668492               0.797754                -0.199124   \n",
      "1                   1.092750               1.846483                 0.922120   \n",
      "2                   0.991403               1.686793                 0.020369   \n",
      "3                  -0.430889               1.373530                -0.633392   \n",
      "4                   0.743514               1.121268                 0.505381   \n",
      "\n",
      "   num__calculated_release_angle  num__release_ball_velocity_x  \\\n",
      "0                       0.353184                     -0.010464   \n",
      "1                       1.052441                      0.777058   \n",
      "2                      -0.568742                      0.123024   \n",
      "3                       0.513443                     -0.702970   \n",
      "4                      -0.364409                      0.380674   \n",
      "\n",
      "   num__release_ball_velocity_y  num__release_ball_velocity_z  \n",
      "0                      1.217131                     -0.207739  \n",
      "1                     -1.333284                      0.940890  \n",
      "2                      0.174788                      0.029956  \n",
      "3                      1.217131                     -0.525123  \n",
      "4                     -0.115226                      0.590530  \n",
      "2025-04-07 17:00:41,274 [DEBUG] [DEBUG] Original data shape before inverse transform: (125, 14)\n",
      "2025-04-07 17:00:41,277 [DEBUG] [DEBUG] Inversed data shape: (125, 14)\n",
      "2025-04-07 17:00:41,277 [INFO] Step: Generate Preprocessor Recommendations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:41,277 - DataPreprocessor - INFO - Step: Generate Preprocessor Recommendations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:41,279 [INFO] Preprocessing Recommendations generated.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:41,279 - DataPreprocessor - INFO - Preprocessing Recommendations generated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:41,280 [INFO] Step 'Generate Preprocessor Recommendations' completed: Recommendations generated.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:41,280 - DataPreprocessor - INFO - Step 'Generate Preprocessor Recommendations' completed: Recommendations generated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:41,281 [DEBUG] Generated preprocessing recommendations.\n",
      "2025-04-07 17:00:41,282 [INFO] ✅ Preprocessing completed successfully in predict mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:41,282 - DataPreprocessor - INFO - ✅ Preprocessing completed successfully in predict mode.\n",
      "2025-04-07 17:00:41,283 - ml.shap.shap_utils - INFO - Preprocessing completed successfully in predict mode.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:41,291 - PredictAndAttachLogger - INFO - ✅ Predictions made successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:41,291 - PredictAndAttachLogger - INFO - ✅ Predictions made successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:41,303 - PredictAndAttachLogger - INFO - ✅ Prediction probabilities computed successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:41,303 - PredictAndAttachLogger - INFO - ✅ Prediction probabilities computed successfully.\n",
      "2025-04-07 17:00:41,312 - ml.shap.shap_utils - INFO - Predictions generated and attached to the dataset.\n",
      "2025-04-07 17:00:41,313 - ml.shap.shap_utils - INFO - Initializing SHAP explainer...\n",
      "2025-04-07 17:00:41,563 - ml.shap.shap_utils - INFO - SHAP values computed successfully.\n",
      "2025-04-07 17:00:41,564 - ml.shap.shap_utils - INFO - [Step A] Generating feedback for each trial with detailed debug information.\n",
      "2025-04-07 17:00:41,682 - ml.shap.shap_utils - INFO - [Step B] Computing original metric error with corrected feature extraction.\n",
      "2025-04-07 17:00:41,683 - ml.shap.shap_utils - INFO - [Step C] Computing original metric error with corrected feature extraction.\n",
      "2025-04-07 17:00:41,719 - ml.shap.shap_utils - INFO - Feedback generation and metric threshold application completed.\n",
      "2025-04-07 17:00:41,720 - ml.shap.shap_utils - INFO - Generating SHAP summary plot...\n",
      "2025-04-07 17:00:41,983 - ml.shap.shap_utils - INFO - SHAP summary plot generated successfully.\n",
      "2025-04-07 17:00:41,984 - ml.shap.shap_utils - INFO - SHAP summary plot saved at /workspaces/spl_freethrow_biomechanics_analysis_ml_prediction/data/predictions/shap_results/XGBoost/shap_summary.png.\n",
      "2025-04-07 17:00:41,985 - ml.shap.shap_utils - INFO - Generating feature importance based on SHAP values...\n",
      "2025-04-07 17:00:41,996 - ml.shap.shap_utils - INFO - Generating SHAP dependence plot for feature 'num__release_ball_direction_y'...\n",
      "2025-04-07 17:00:42,093 - ml.shap.shap_utils - INFO - SHAP dependence plot for feature 'num__release_ball_direction_y' generated successfully.\n",
      "2025-04-07 17:00:42,094 - ml.shap.shap_utils - INFO - Generating SHAP dependence plot for feature 'num__R_ELBOW_max_angle'...\n",
      "2025-04-07 17:00:42,180 - ml.shap.shap_utils - INFO - SHAP dependence plot for feature 'num__R_ELBOW_max_angle' generated successfully.\n",
      "2025-04-07 17:00:42,181 - ml.shap.shap_utils - INFO - Generating SHAP dependence plot for feature 'num__R_WRIST_release_angle'...\n",
      "2025-04-07 17:00:42,273 - ml.shap.shap_utils - INFO - SHAP dependence plot for feature 'num__R_WRIST_release_angle' generated successfully.\n",
      "2025-04-07 17:00:42,275 - ml.shap.shap_utils - INFO - Generating SHAP dependence plot for feature 'num__calculated_release_angle'...\n",
      "2025-04-07 17:00:42,364 - ml.shap.shap_utils - INFO - SHAP dependence plot for feature 'num__calculated_release_angle' generated successfully.\n",
      "2025-04-07 17:00:42,365 - ml.shap.shap_utils - INFO - Generating SHAP dependence plot for feature 'num__R_KNEE_max_angle'...\n",
      "2025-04-07 17:00:42,455 - ml.shap.shap_utils - INFO - SHAP dependence plot for feature 'num__R_KNEE_max_angle' generated successfully.\n",
      "2025-04-07 17:00:42,456 - ml.shap.shap_utils - INFO - Generating SHAP dependence plot for feature 'num__R_ELBOW_release_angle'...\n",
      "2025-04-07 17:00:42,556 - ml.shap.shap_utils - INFO - SHAP dependence plot for feature 'num__R_ELBOW_release_angle' generated successfully.\n",
      "2025-04-07 17:00:42,558 - ml.shap.shap_utils - INFO - Generating SHAP dependence plot for feature 'num__R_WRIST_max_angle'...\n",
      "2025-04-07 17:00:42,650 - ml.shap.shap_utils - INFO - SHAP dependence plot for feature 'num__R_WRIST_max_angle' generated successfully.\n",
      "2025-04-07 17:00:42,651 - ml.shap.shap_utils - INFO - Generating SHAP dependence plot for feature 'num__release_ball_velocity_z'...\n",
      "2025-04-07 17:00:42,735 - ml.shap.shap_utils - INFO - SHAP dependence plot for feature 'num__release_ball_velocity_z' generated successfully.\n",
      "2025-04-07 17:00:42,736 - ml.shap.shap_utils - INFO - Generating SHAP dependence plot for feature 'num__R_KNEE_release_angle'...\n",
      "2025-04-07 17:00:42,819 - ml.shap.shap_utils - INFO - SHAP dependence plot for feature 'num__R_KNEE_release_angle' generated successfully.\n",
      "2025-04-07 17:00:42,820 - ml.shap.shap_utils - INFO - Generating SHAP dependence plot for feature 'num__release_ball_speed'...\n",
      "2025-04-07 17:00:42,907 - ml.shap.shap_utils - INFO - SHAP dependence plot for feature 'num__release_ball_speed' generated successfully.\n",
      "2025-04-07 17:00:42,908 - ml.shap.shap_utils - INFO - SHAP dependence plots saved at /workspaces/spl_freethrow_biomechanics_analysis_ml_prediction/data/predictions/shap_results/XGBoost/shap_dependence_plots.\n",
      "2025-04-07 17:00:42,911 - ml.shap.shap_utils - INFO - Generating SHAP force plot for trial 0...\n",
      "2025-04-07 17:00:42,918 - ml.shap.shap_utils - INFO - SHAP force plot for trial 0 generated successfully.\n",
      "2025-04-07 17:00:42,919 - ml.shap.shap_utils - INFO - SHAP force plots saved at /workspaces/spl_freethrow_biomechanics_analysis_ml_prediction/data/predictions/shap_results/XGBoost/shap_force_plots.\n",
      "2025-04-07 17:00:42,919 - ml.shap.shap_utils - INFO - Adding columns from df_input to final_df: ['trial_id']\n",
      "2025-04-07 17:00:42,920 - ml.shap.shap_utils - INFO - Columns added successfully: ['trial_id']\n",
      "2025-04-07 17:00:42,921 - ml.shap.shap_utils - INFO - Saving final dataset with SHAP annotations to /workspaces/spl_freethrow_biomechanics_analysis_ml_prediction/data/predictions/shap_results/XGBoost/final_predictions_with_shap.csv.\n",
      "2025-04-07 17:00:42,949 - ml.shap.shap_utils - INFO - Final dataset and global recommendations saved.\n",
      "2025-04-07 17:00:42,950 - ml.shap.shap_utils - INFO - Predict+SHAP pipeline completed successfully.\n",
      "2025-04-07 17:00:42,952 - ml.shap.shap_utils - INFO - Unified predict_and_shap function executed successfully for model XGBoost.\n",
      "\n",
      "Final Predictions with SHAP annotations for model XGBoost (preview):\n",
      "   release_ball_direction_x  release_ball_direction_z  \\\n",
      "0                  0.377012                  0.926203   \n",
      "1                  0.417644                  0.901906   \n",
      "2                  0.383086                  0.922353   \n",
      "3                  0.292054                  0.956396   \n",
      "4                  0.389917                  0.918894   \n",
      "\n",
      "   release_ball_direction_y  R_ELBOW_release_angle  R_ELBOW_max_angle  \\\n",
      "0                  0.002969              72.325830         106.272118   \n",
      "1                 -0.110176              58.430068         101.798798   \n",
      "2                 -0.050096              73.883728         106.989633   \n",
      "3                  0.003209              71.424835         104.160865   \n",
      "4                 -0.059987              73.619348         103.832024   \n",
      "\n",
      "   R_WRIST_release_angle  R_WRIST_max_angle  R_KNEE_release_angle  \\\n",
      "0              28.102765          35.918406             32.646276   \n",
      "1              32.766626          38.693790             33.834026   \n",
      "2              26.220943          38.998089             33.550293   \n",
      "3              22.649881          36.353038             29.568453   \n",
      "4              21.754498          34.532455             32.856306   \n",
      "\n",
      "   R_KNEE_max_angle  release_ball_speed  ...  \\\n",
      "0         63.541007            9.907618  ...   \n",
      "1         65.565635           11.826803  ...   \n",
      "2         65.257346           10.283314  ...   \n",
      "3         64.652573            9.164302  ...   \n",
      "4         64.165568           11.113489  ...   \n",
      "\n",
      "   shap_release_ball_velocity_z_unit_change  \\\n",
      "0                                      0.92   \n",
      "1                                      1.07   \n",
      "2                                      0.95   \n",
      "3                                      0.88   \n",
      "4                                      1.02   \n",
      "\n",
      "   shap_release_ball_velocity_z_unit  shap_release_ball_velocity_z_direction  \\\n",
      "0                              units                                increase   \n",
      "1                              units                                decrease   \n",
      "2                              units                                increase   \n",
      "3                              units                                increase   \n",
      "4                              units                                decrease   \n",
      "\n",
      "   shap_release_ball_velocity_z_importance  shap_release_ball_velocity_z_goal  \\\n",
      "0                                   0.2815                             10.094   \n",
      "1                                   0.2504                              9.600   \n",
      "2                                   0.2194                             10.433   \n",
      "3                                   0.0997                              9.641   \n",
      "4                                   0.1733                              9.191   \n",
      "\n",
      "   shap_release_ball_velocity_z_min  shap_release_ball_velocity_z_max  \\\n",
      "0                             9.370                            10.818   \n",
      "1                             8.876                            10.324   \n",
      "2                             9.709                            11.157   \n",
      "3                             8.917                            10.365   \n",
      "4                             8.467                             9.915   \n",
      "\n",
      "   shap_release_ball_velocity_z_classification  \\\n",
      "0                                        Early   \n",
      "1                                         Late   \n",
      "2                                        Early   \n",
      "3                                        Early   \n",
      "4                                         Late   \n",
      "\n",
      "          shap_release_ball_velocity_z_feedback_text trial_id  \n",
      "0  For feature 'num__release_ball_velocity_z': Th...    T0001  \n",
      "1  For feature 'num__release_ball_velocity_z': Th...    T0002  \n",
      "2  For feature 'num__release_ball_velocity_z': Th...    T0003  \n",
      "3  For feature 'num__release_ball_velocity_z': Th...    T0004  \n",
      "4  For feature 'num__release_ball_velocity_z': Th...    T0005  \n",
      "\n",
      "[5 rows x 144 columns]\n",
      "\n",
      "Feedback for trial at index 0 for model XGBoost:\n",
      "  - shap_release_ball_direction_x_unit_change: 0.04\n",
      "  - shap_release_ball_direction_x_unit: units\n",
      "  - shap_release_ball_direction_x_direction: increase\n",
      "  - shap_release_ball_direction_x_importance: 0.017799999564886\n",
      "  - shap_release_ball_direction_x_goal: 0.415\n",
      "  - shap_release_ball_direction_x_min: 0.3949999999999999\n",
      "  - shap_release_ball_direction_x_max: 0.435\n",
      "  - shap_release_ball_direction_x_classification: Early\n",
      "  - shap_release_ball_direction_x_feedback_text: For feature 'num__release_ball_direction_x': The SHAP value of 0.02 suggests to increase the value. Current value is 0.38. A 10% adjustment equals 0.04 units, which would set a target of 0.41. The acceptable range is [0.41, 0.41], classifying this metric as 'Early'.\n",
      "  - shap_release_ball_direction_z_unit_change: 0.09\n",
      "  - shap_release_ball_direction_z_unit: units\n",
      "  - shap_release_ball_direction_z_direction: increase\n",
      "  - shap_release_ball_direction_z_importance: 0.0566999986767768\n",
      "  - shap_release_ball_direction_z_goal: 1.019\n",
      "  - shap_release_ball_direction_z_min: 0.929\n",
      "  - shap_release_ball_direction_z_max: 1.109\n",
      "  - shap_release_ball_direction_z_classification: Early\n",
      "  - shap_release_ball_direction_z_feedback_text: For feature 'num__release_ball_direction_z': The SHAP value of 0.06 suggests to increase the value. Current value is 0.93. A 10% adjustment equals 0.09 units, which would set a target of 1.02. The acceptable range is [1.02, 1.02], classifying this metric as 'Early'.\n",
      "  - shap_release_ball_direction_y_unit_change: 0.0\n",
      "  - shap_release_ball_direction_y_unit: units\n",
      "  - shap_release_ball_direction_y_direction: decrease\n",
      "  - shap_release_ball_direction_y_importance: 0.541700005531311\n",
      "  - shap_release_ball_direction_y_goal: 0.003\n",
      "  - shap_release_ball_direction_y_min: 0.003\n",
      "  - shap_release_ball_direction_y_max: 0.003\n",
      "  - shap_release_ball_direction_y_classification: Early\n",
      "  - shap_release_ball_direction_y_feedback_text: For feature 'num__release_ball_direction_y': The SHAP value of -0.54 suggests to decrease the value. Current value is 0.00. A 10% adjustment equals 0.00 units, which would set a target of 0.00. The acceptable range is [0.00, 0.00], classifying this metric as 'Late'.\n",
      "  - shap_R_ELBOW_release_angle_unit_change: 7.23\n",
      "  - shap_R_ELBOW_release_angle_unit: units\n",
      "  - shap_R_ELBOW_release_angle_direction: decrease\n",
      "  - shap_R_ELBOW_release_angle_importance: 0.1017000004649162\n",
      "  - shap_R_ELBOW_release_angle_goal: 65.093\n",
      "  - shap_R_ELBOW_release_angle_min: 59.517\n",
      "  - shap_R_ELBOW_release_angle_max: 70.669\n",
      "  - shap_R_ELBOW_release_angle_classification: Late\n",
      "  - shap_R_ELBOW_release_angle_feedback_text: For feature 'num__R_ELBOW_release_angle': The SHAP value of -0.10 suggests to decrease the value. Current value is 72.33. A 10% adjustment equals 7.23 units, which would set a target of 65.09. The acceptable range is [65.09, 65.09], classifying this metric as 'Late'.\n",
      "  - shap_R_ELBOW_max_angle_unit_change: 10.63\n",
      "  - shap_R_ELBOW_max_angle_unit: units\n",
      "  - shap_R_ELBOW_max_angle_direction: decrease\n",
      "  - shap_R_ELBOW_max_angle_importance: 1.079200029373169\n",
      "  - shap_R_ELBOW_max_angle_goal: 95.645\n",
      "  - shap_R_ELBOW_max_angle_min: 85.50099999999999\n",
      "  - shap_R_ELBOW_max_angle_max: 105.789\n",
      "  - shap_R_ELBOW_max_angle_classification: Late\n",
      "  - shap_R_ELBOW_max_angle_feedback_text: For feature 'num__R_ELBOW_max_angle': The SHAP value of -1.08 suggests to decrease the value. Current value is 106.27. A 10% adjustment equals 10.63 units, which would set a target of 95.64. The acceptable range is [95.64, 95.64], classifying this metric as 'Late'.\n",
      "  - shap_R_WRIST_release_angle_unit_change: 2.81\n",
      "  - shap_R_WRIST_release_angle_unit: units\n",
      "  - shap_R_WRIST_release_angle_direction: decrease\n",
      "  - shap_R_WRIST_release_angle_importance: 0.3871999979019165\n",
      "  - shap_R_WRIST_release_angle_goal: 25.292\n",
      "  - shap_R_WRIST_release_angle_min: 23.062\n",
      "  - shap_R_WRIST_release_angle_max: 27.522\n",
      "  - shap_R_WRIST_release_angle_classification: Late\n",
      "  - shap_R_WRIST_release_angle_feedback_text: For feature 'num__R_WRIST_release_angle': The SHAP value of -0.39 suggests to decrease the value. Current value is 28.10. A 10% adjustment equals 2.81 units, which would set a target of 25.29. The acceptable range is [25.29, 25.29], classifying this metric as 'Late'.\n",
      "  - shap_R_WRIST_max_angle_unit_change: 3.59\n",
      "  - shap_R_WRIST_max_angle_unit: units\n",
      "  - shap_R_WRIST_max_angle_direction: increase\n",
      "  - shap_R_WRIST_max_angle_importance: 0.4501000046730041\n",
      "  - shap_R_WRIST_max_angle_goal: 39.51\n",
      "  - shap_R_WRIST_max_angle_min: 36.06\n",
      "  - shap_R_WRIST_max_angle_max: 42.96\n",
      "  - shap_R_WRIST_max_angle_classification: Early\n",
      "  - shap_R_WRIST_max_angle_feedback_text: For feature 'num__R_WRIST_max_angle': The SHAP value of 0.45 suggests to increase the value. Current value is 35.92. A 10% adjustment equals 3.59 units, which would set a target of 39.51. The acceptable range is [39.51, 39.51], classifying this metric as 'Early'.\n",
      "  - shap_R_KNEE_release_angle_unit_change: 3.26\n",
      "  - shap_R_KNEE_release_angle_unit: units\n",
      "  - shap_R_KNEE_release_angle_direction: decrease\n",
      "  - shap_R_KNEE_release_angle_importance: 0.1533000022172927\n",
      "  - shap_R_KNEE_release_angle_goal: 29.382\n",
      "  - shap_R_KNEE_release_angle_min: 26.712000000000003\n",
      "  - shap_R_KNEE_release_angle_max: 32.052\n",
      "  - shap_R_KNEE_release_angle_classification: Late\n",
      "  - shap_R_KNEE_release_angle_feedback_text: For feature 'num__R_KNEE_release_angle': The SHAP value of -0.15 suggests to decrease the value. Current value is 32.65. A 10% adjustment equals 3.26 units, which would set a target of 29.38. The acceptable range is [29.38, 29.38], classifying this metric as 'Late'.\n",
      "  - shap_R_KNEE_max_angle_unit_change: 6.35\n",
      "  - shap_R_KNEE_max_angle_unit: units\n",
      "  - shap_R_KNEE_max_angle_direction: decrease\n",
      "  - shap_R_KNEE_max_angle_importance: 0.3303000032901764\n",
      "  - shap_R_KNEE_max_angle_goal: 57.187\n",
      "  - shap_R_KNEE_max_angle_min: 51.249\n",
      "  - shap_R_KNEE_max_angle_max: 63.125\n",
      "  - shap_R_KNEE_max_angle_classification: Late\n",
      "  - shap_R_KNEE_max_angle_feedback_text: For feature 'num__R_KNEE_max_angle': The SHAP value of -0.33 suggests to decrease the value. Current value is 63.54. A 10% adjustment equals 6.35 units, which would set a target of 57.19. The acceptable range is [57.19, 57.19], classifying this metric as 'Late'.\n",
      "  - shap_release_ball_speed_unit_change: 0.99\n",
      "  - shap_release_ball_speed_unit: units\n",
      "  - shap_release_ball_speed_direction: increase\n",
      "  - shap_release_ball_speed_importance: 0.0296999998390674\n",
      "  - shap_release_ball_speed_goal: 10.898\n",
      "  - shap_release_ball_speed_min: 10.158\n",
      "  - shap_release_ball_speed_max: 11.638\n",
      "  - shap_release_ball_speed_classification: Early\n",
      "  - shap_release_ball_speed_feedback_text: For feature 'num__release_ball_speed': The SHAP value of 0.03 suggests to increase the value. Current value is 9.91. A 10% adjustment equals 0.99 units, which would set a target of 10.90. The acceptable range is [10.90, 10.90], classifying this metric as 'Early'.\n",
      "  - shap_calculated_release_angle_unit_change: 6.3\n",
      "  - shap_calculated_release_angle_unit: units\n",
      "  - shap_calculated_release_angle_direction: decrease\n",
      "  - shap_calculated_release_angle_importance: 0.8623999953269958\n",
      "  - shap_calculated_release_angle_goal: 56.663\n",
      "  - shap_calculated_release_angle_min: 50.769\n",
      "  - shap_calculated_release_angle_max: 62.557\n",
      "  - shap_calculated_release_angle_classification: Late\n",
      "  - shap_calculated_release_angle_feedback_text: For feature 'num__calculated_release_angle': The SHAP value of -0.86 suggests to decrease the value. Current value is 62.96. A 10% adjustment equals 6.30 units, which would set a target of 56.66. The acceptable range is [56.66, 56.66], classifying this metric as 'Late'.\n",
      "  - shap_release_ball_velocity_x_unit_change: 0.37\n",
      "  - shap_release_ball_velocity_x_unit: units\n",
      "  - shap_release_ball_velocity_x_direction: increase\n",
      "  - shap_release_ball_velocity_x_importance: 0.0478000007569789\n",
      "  - shap_release_ball_velocity_x_goal: 4.109\n",
      "  - shap_release_ball_velocity_x_min: 3.989\n",
      "  - shap_release_ball_velocity_x_max: 4.229\n",
      "  - shap_release_ball_velocity_x_classification: Early\n",
      "  - shap_release_ball_velocity_x_feedback_text: For feature 'num__release_ball_velocity_x': The SHAP value of 0.05 suggests to increase the value. Current value is 3.74. A 10% adjustment equals 0.37 units, which would set a target of 4.11. The acceptable range is [4.11, 4.11], classifying this metric as 'Early'.\n",
      "  - shap_release_ball_velocity_y_unit_change: 0.0\n",
      "  - shap_release_ball_velocity_y_unit: units\n",
      "  - shap_release_ball_velocity_y_direction: increase\n",
      "  - shap_release_ball_velocity_y_importance: 0.12219999730587\n",
      "  - shap_release_ball_velocity_y_goal: 0.032\n",
      "  - shap_release_ball_velocity_y_min: 0.022\n",
      "  - shap_release_ball_velocity_y_max: 0.042\n",
      "  - shap_release_ball_velocity_y_classification: Good\n",
      "  - shap_release_ball_velocity_y_feedback_text: For feature 'num__release_ball_velocity_y': The SHAP value of 0.12 suggests to increase the value. Current value is 0.03. A 10% adjustment equals 0.00 units, which would set a target of 0.03. The acceptable range is [0.03, 0.03], classifying this metric as 'Early'.\n",
      "  - shap_release_ball_velocity_z_unit_change: 0.92\n",
      "  - shap_release_ball_velocity_z_unit: units\n",
      "  - shap_release_ball_velocity_z_direction: increase\n",
      "  - shap_release_ball_velocity_z_importance: 0.2815000116825104\n",
      "  - shap_release_ball_velocity_z_goal: 10.094\n",
      "  - shap_release_ball_velocity_z_min: 9.37\n",
      "  - shap_release_ball_velocity_z_max: 10.818\n",
      "  - shap_release_ball_velocity_z_classification: Early\n",
      "  - shap_release_ball_velocity_z_feedback_text: For feature 'num__release_ball_velocity_z': The SHAP value of 0.28 suggests to increase the value. Current value is 9.18. A 10% adjustment equals 0.92 units, which would set a target of 10.09. The acceptable range is [10.09, 10.09], classifying this metric as 'Early'.\n",
      "\n",
      "--- Running pipeline for model: Random Forest ---\n",
      "tuning_results_path: /workspaces/spl_freethrow_biomechanics_analysis_ml_prediction/data/model/tuning_results.json\n",
      "2025-04-07 17:00:42,980 - ml.shap.shap_utils - INFO - Overriding best model selection; using model: Random Forest\n",
      "2025-04-07 17:00:42,990 - ml.train_utils.train_utils - INFO - Model loaded from /workspaces/spl_freethrow_biomechanics_analysis_ml_prediction/data/model/Random Forest_model.pkl\n",
      "2025-04-07 17:00:42,991 - ml.shap.shap_utils - INFO - Trained model loaded from '/workspaces/spl_freethrow_biomechanics_analysis_ml_prediction/data/model' using model name 'Random Forest'.\n",
      "✅ Features loaded from /workspaces/spl_freethrow_biomechanics_analysis_ml_prediction/data/preprocessor/features_info/final_ml_df_selected_features_columns.pkl\n",
      "✅ Ordinal categoricals loaded from ../../data/preprocessor/features_info/ordinal_categoricals.pkl\n",
      "✅ Nominal categoricals loaded from ../../data/preprocessor/features_info/nominal_categoricals.pkl\n",
      "✅ Numericals loaded from ../../data/preprocessor/features_info/numericals.pkl\n",
      "✅ Y variable loaded from ../../data/preprocessor/features_info/y_variable.pkl\n",
      "2025-04-07 17:00:43,001 - ml.shap.shap_utils - WARNING - Unable to infer model type. Defaulting to 'tree'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/data_science_ft_bio_predictions/lib/python3.10/site-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator DecisionTreeClassifier from version 1.5.2 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/data_science_ft_bio_predictions/lib/python3.10/site-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator RandomForestClassifier from version 1.5.2 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "2025-04-07 17:00:43,002 [INFO] Starting: Final Preprocessing Pipeline in 'predict' mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:43,002 - DataPreprocessor - INFO - Starting: Final Preprocessing Pipeline in 'predict' mode.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:43,003 [INFO] Step: filter_columns\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:43,003 - DataPreprocessor - INFO - Step: filter_columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:43,004 [INFO] ✅ Filtered DataFrame to include only specified features. Shape: (125, 14)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:43,004 - DataPreprocessor - INFO - ✅ Filtered DataFrame to include only specified features. Shape: (125, 14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:43,006 [DEBUG] Selected Features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'R_ELBOW_release_angle', 'R_ELBOW_max_angle', 'R_WRIST_release_angle', 'R_WRIST_max_angle', 'R_KNEE_release_angle', 'R_KNEE_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z']\n",
      "2025-04-07 17:00:43,006 [INFO] ✅ Column filtering completed successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:43,006 - DataPreprocessor - INFO - ✅ Column filtering completed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:43,008 [INFO] Step: Preprocess Predict\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:43,008 - DataPreprocessor - INFO - Step: Preprocess Predict\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:43,008 [DEBUG] Initial columns in prediction data: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'R_ELBOW_release_angle', 'R_ELBOW_max_angle', 'R_WRIST_release_angle', 'R_WRIST_max_angle', 'R_KNEE_release_angle', 'R_KNEE_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z']\n",
      "2025-04-07 17:00:43,009 [DEBUG] Initial number of features: 14\n",
      "2025-04-07 17:00:43,009 [INFO] Step: Load Transformers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:43,009 - DataPreprocessor - INFO - Step: Load Transformers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:43,010 [DEBUG] Loading transformers from: /workspaces/spl_freethrow_biomechanics_analysis_ml_prediction/data/preprocessor/transformers/transformers.pkl\n",
      "2025-04-07 17:00:43,016 [DEBUG] Pipeline loaded. Ready to transform new data.\n",
      "2025-04-07 17:00:43,016 [INFO] Transformers loaded successfully from '/workspaces/spl_freethrow_biomechanics_analysis_ml_prediction/data/preprocessor/transformers/transformers.pkl'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:43,016 - DataPreprocessor - INFO - Transformers loaded successfully from '/workspaces/spl_freethrow_biomechanics_analysis_ml_prediction/data/preprocessor/transformers/transformers.pkl'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:43,017 [DEBUG] Transformers loaded successfully.\n",
      "2025-04-07 17:00:43,017 [INFO] Step: filter_columns\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:43,017 - DataPreprocessor - INFO - Step: filter_columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:43,019 [INFO] ✅ Filtered DataFrame to include only specified features. Shape: (125, 14)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:43,019 - DataPreprocessor - INFO - ✅ Filtered DataFrame to include only specified features. Shape: (125, 14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:43,020 [DEBUG] Selected Features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'R_ELBOW_release_angle', 'R_ELBOW_max_angle', 'R_WRIST_release_angle', 'R_WRIST_max_angle', 'R_KNEE_release_angle', 'R_KNEE_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z']\n",
      "2025-04-07 17:00:43,020 [DEBUG] Columns after filtering: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'R_ELBOW_release_angle', 'R_ELBOW_max_angle', 'R_WRIST_release_angle', 'R_WRIST_max_angle', 'R_KNEE_release_angle', 'R_KNEE_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z']\n",
      "2025-04-07 17:00:43,020 [DEBUG] Number of features after filtering: 14\n",
      "2025-04-07 17:00:43,021 [INFO] Step: Handle Missing Values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:43,021 - DataPreprocessor - INFO - Step: Handle Missing Values\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:43,027 [DEBUG] Columns after handling missing values: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'R_ELBOW_release_angle', 'R_ELBOW_max_angle', 'R_WRIST_release_angle', 'R_WRIST_max_angle', 'R_KNEE_release_angle', 'R_KNEE_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z']\n",
      "2025-04-07 17:00:43,028 [DEBUG] Number of features after handling missing values: 14\n",
      "2025-04-07 17:00:43,028 [DEBUG] Expected raw features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'R_ELBOW_release_angle', 'R_ELBOW_max_angle', 'R_WRIST_release_angle', 'R_WRIST_max_angle', 'R_KNEE_release_angle', 'R_KNEE_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z']\n",
      "2025-04-07 17:00:43,028 [DEBUG] Provided features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'R_ELBOW_release_angle', 'R_ELBOW_max_angle', 'R_WRIST_release_angle', 'R_WRIST_max_angle', 'R_KNEE_release_angle', 'R_KNEE_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z']\n",
      "2025-04-07 17:00:43,029 [DEBUG] Reordered columns to match the pipeline's raw feature expectations.\n",
      "2025-04-07 17:00:43,033 [DEBUG] Transformed data shape: (125, 14)\n",
      "2025-04-07 17:00:43,034 [DEBUG] Derived feature names from pipeline: ['num__release_ball_direction_x', 'num__release_ball_direction_z', 'num__release_ball_direction_y', 'num__R_ELBOW_release_angle', 'num__R_ELBOW_max_angle', 'num__R_WRIST_release_angle', 'num__R_WRIST_max_angle', 'num__R_KNEE_release_angle', 'num__R_KNEE_max_angle', 'num__release_ball_speed', 'num__calculated_release_angle', 'num__release_ball_velocity_x', 'num__release_ball_velocity_y', 'num__release_ball_velocity_z']\n",
      "2025-04-07 17:00:43,034 [DEBUG] X_preprocessed_df columns: ['num__release_ball_direction_x', 'num__release_ball_direction_z', 'num__release_ball_direction_y', 'num__R_ELBOW_release_angle', 'num__R_ELBOW_max_angle', 'num__R_WRIST_release_angle', 'num__R_WRIST_max_angle', 'num__R_KNEE_release_angle', 'num__R_KNEE_max_angle', 'num__release_ball_speed', 'num__calculated_release_angle', 'num__release_ball_velocity_x', 'num__release_ball_velocity_y', 'num__release_ball_velocity_z']\n",
      "2025-04-07 17:00:43,037 [DEBUG] Sample of X_preprocessed_df:\n",
      "   num__release_ball_direction_x  num__release_ball_direction_z  \\\n",
      "0                       0.256808                      -0.014333   \n",
      "1                       0.653985                      -0.770471   \n",
      "2                       0.316179                      -0.134154   \n",
      "3                      -0.573663                       0.925278   \n",
      "4                       0.382947                      -0.241799   \n",
      "\n",
      "   num__release_ball_direction_y  num__R_ELBOW_release_angle  \\\n",
      "0                       1.145467                    1.044847   \n",
      "1                      -0.889734                   -1.374648   \n",
      "2                       0.190965                    1.316104   \n",
      "3                       1.149799                    0.887968   \n",
      "4                       0.013044                    1.270071   \n",
      "\n",
      "   num__R_ELBOW_max_angle  num__R_WRIST_release_angle  num__R_WRIST_max_angle  \\\n",
      "0                1.095488                    0.424441               -0.778748   \n",
      "1               -1.250958                    1.913313                0.102943   \n",
      "2                1.471855                   -0.176303                0.199613   \n",
      "3               -0.011954                   -1.316314               -0.640673   \n",
      "4               -0.184445                   -1.602153               -1.219041   \n",
      "\n",
      "   num__R_KNEE_release_angle  num__R_KNEE_max_angle  num__release_ball_speed  \\\n",
      "0                   0.668492               0.797754                -0.199124   \n",
      "1                   1.092750               1.846483                 0.922120   \n",
      "2                   0.991403               1.686793                 0.020369   \n",
      "3                  -0.430889               1.373530                -0.633392   \n",
      "4                   0.743514               1.121268                 0.505381   \n",
      "\n",
      "   num__calculated_release_angle  num__release_ball_velocity_x  \\\n",
      "0                       0.353184                     -0.010464   \n",
      "1                       1.052441                      0.777058   \n",
      "2                      -0.568742                      0.123024   \n",
      "3                       0.513443                     -0.702970   \n",
      "4                      -0.364409                      0.380674   \n",
      "\n",
      "   num__release_ball_velocity_y  num__release_ball_velocity_z  \n",
      "0                      1.217131                     -0.207739  \n",
      "1                     -1.333284                      0.940890  \n",
      "2                      0.174788                      0.029956  \n",
      "3                      1.217131                     -0.525123  \n",
      "4                     -0.115226                      0.590530  \n",
      "2025-04-07 17:00:43,038 [DEBUG] [DEBUG] Original data shape before inverse transform: (125, 14)\n",
      "2025-04-07 17:00:43,041 [DEBUG] [DEBUG] Inversed data shape: (125, 14)\n",
      "2025-04-07 17:00:43,042 [INFO] Step: Generate Preprocessor Recommendations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:43,042 - DataPreprocessor - INFO - Step: Generate Preprocessor Recommendations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:43,044 [INFO] Preprocessing Recommendations generated.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:43,044 - DataPreprocessor - INFO - Preprocessing Recommendations generated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:43,044 [INFO] Step 'Generate Preprocessor Recommendations' completed: Recommendations generated.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:43,044 - DataPreprocessor - INFO - Step 'Generate Preprocessor Recommendations' completed: Recommendations generated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:43,045 [DEBUG] Generated preprocessing recommendations.\n",
      "2025-04-07 17:00:43,046 [INFO] ✅ Preprocessing completed successfully in predict mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:43,046 - DataPreprocessor - INFO - ✅ Preprocessing completed successfully in predict mode.\n",
      "2025-04-07 17:00:43,047 - ml.shap.shap_utils - INFO - Preprocessing completed successfully in predict mode.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:43,048 - PredictAndAttachLogger - ERROR - ❌ Prediction failed: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- num__R_ELBOW_max_angle\n",
      "- num__R_ELBOW_release_angle\n",
      "- num__R_KNEE_max_angle\n",
      "- num__R_KNEE_release_angle\n",
      "- num__R_WRIST_max_angle\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- num__elbow_max_angle\n",
      "- num__elbow_release_angle\n",
      "- num__knee_max_angle\n",
      "- num__knee_release_angle\n",
      "- num__wrist_max_angle\n",
      "- ...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:43,048 - PredictAndAttachLogger - ERROR - ❌ Prediction failed: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- num__R_ELBOW_max_angle\n",
      "- num__R_ELBOW_release_angle\n",
      "- num__R_KNEE_max_angle\n",
      "- num__R_KNEE_release_angle\n",
      "- num__R_WRIST_max_angle\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- num__elbow_max_angle\n",
      "- num__elbow_release_angle\n",
      "- num__knee_max_angle\n",
      "- num__knee_release_angle\n",
      "- num__wrist_max_angle\n",
      "- ...\n",
      "\n",
      "2025-04-07 17:00:43,050 - ml.shap.shap_utils - ERROR - Unified predict_and_shap function failed for model Random Forest: cannot unpack non-iterable NoneType object\n",
      "\n",
      "--- Running pipeline for model: CatBoost ---\n",
      "tuning_results_path: /workspaces/spl_freethrow_biomechanics_analysis_ml_prediction/data/model/tuning_results.json\n",
      "2025-04-07 17:00:43,060 - ml.shap.shap_utils - INFO - Overriding best model selection; using model: CatBoost\n",
      "2025-04-07 17:00:43,066 - ml.train_utils.train_utils - INFO - Model loaded from /workspaces/spl_freethrow_biomechanics_analysis_ml_prediction/data/model/CatBoost_model.pkl\n",
      "2025-04-07 17:00:43,067 - ml.shap.shap_utils - INFO - Trained model loaded from '/workspaces/spl_freethrow_biomechanics_analysis_ml_prediction/data/model' using model name 'CatBoost'.\n",
      "✅ Features loaded from /workspaces/spl_freethrow_biomechanics_analysis_ml_prediction/data/preprocessor/features_info/final_ml_df_selected_features_columns.pkl\n",
      "✅ Ordinal categoricals loaded from ../../data/preprocessor/features_info/ordinal_categoricals.pkl\n",
      "✅ Nominal categoricals loaded from ../../data/preprocessor/features_info/nominal_categoricals.pkl\n",
      "✅ Numericals loaded from ../../data/preprocessor/features_info/numericals.pkl\n",
      "✅ Y variable loaded from ../../data/preprocessor/features_info/y_variable.pkl\n",
      "2025-04-07 17:00:43,079 - ml.shap.shap_utils - WARNING - Unable to infer model type. Defaulting to 'tree'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:43,081 [INFO] Starting: Final Preprocessing Pipeline in 'predict' mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:43,081 - DataPreprocessor - INFO - Starting: Final Preprocessing Pipeline in 'predict' mode.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:43,082 [INFO] Step: filter_columns\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:43,082 - DataPreprocessor - INFO - Step: filter_columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:43,084 [INFO] ✅ Filtered DataFrame to include only specified features. Shape: (125, 14)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:43,084 - DataPreprocessor - INFO - ✅ Filtered DataFrame to include only specified features. Shape: (125, 14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:43,085 [DEBUG] Selected Features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'R_ELBOW_release_angle', 'R_ELBOW_max_angle', 'R_WRIST_release_angle', 'R_WRIST_max_angle', 'R_KNEE_release_angle', 'R_KNEE_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z']\n",
      "2025-04-07 17:00:43,085 [INFO] ✅ Column filtering completed successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:43,085 - DataPreprocessor - INFO - ✅ Column filtering completed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:43,087 [INFO] Step: Preprocess Predict\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:43,087 - DataPreprocessor - INFO - Step: Preprocess Predict\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:43,089 [DEBUG] Initial columns in prediction data: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'R_ELBOW_release_angle', 'R_ELBOW_max_angle', 'R_WRIST_release_angle', 'R_WRIST_max_angle', 'R_KNEE_release_angle', 'R_KNEE_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z']\n",
      "2025-04-07 17:00:43,089 [DEBUG] Initial number of features: 14\n",
      "2025-04-07 17:00:43,089 [INFO] Step: Load Transformers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:43,089 - DataPreprocessor - INFO - Step: Load Transformers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:43,091 [DEBUG] Loading transformers from: /workspaces/spl_freethrow_biomechanics_analysis_ml_prediction/data/preprocessor/transformers/transformers.pkl\n",
      "2025-04-07 17:00:43,097 [DEBUG] Pipeline loaded. Ready to transform new data.\n",
      "2025-04-07 17:00:43,098 [INFO] Transformers loaded successfully from '/workspaces/spl_freethrow_biomechanics_analysis_ml_prediction/data/preprocessor/transformers/transformers.pkl'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:43,098 - DataPreprocessor - INFO - Transformers loaded successfully from '/workspaces/spl_freethrow_biomechanics_analysis_ml_prediction/data/preprocessor/transformers/transformers.pkl'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:43,099 [DEBUG] Transformers loaded successfully.\n",
      "2025-04-07 17:00:43,099 [INFO] Step: filter_columns\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:43,099 - DataPreprocessor - INFO - Step: filter_columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:43,101 [INFO] ✅ Filtered DataFrame to include only specified features. Shape: (125, 14)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:43,101 - DataPreprocessor - INFO - ✅ Filtered DataFrame to include only specified features. Shape: (125, 14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:43,103 [DEBUG] Selected Features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'R_ELBOW_release_angle', 'R_ELBOW_max_angle', 'R_WRIST_release_angle', 'R_WRIST_max_angle', 'R_KNEE_release_angle', 'R_KNEE_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z']\n",
      "2025-04-07 17:00:43,103 [DEBUG] Columns after filtering: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'R_ELBOW_release_angle', 'R_ELBOW_max_angle', 'R_WRIST_release_angle', 'R_WRIST_max_angle', 'R_KNEE_release_angle', 'R_KNEE_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z']\n",
      "2025-04-07 17:00:43,104 [DEBUG] Number of features after filtering: 14\n",
      "2025-04-07 17:00:43,104 [INFO] Step: Handle Missing Values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:43,104 - DataPreprocessor - INFO - Step: Handle Missing Values\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:43,109 [DEBUG] Columns after handling missing values: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'R_ELBOW_release_angle', 'R_ELBOW_max_angle', 'R_WRIST_release_angle', 'R_WRIST_max_angle', 'R_KNEE_release_angle', 'R_KNEE_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z']\n",
      "2025-04-07 17:00:43,110 [DEBUG] Number of features after handling missing values: 14\n",
      "2025-04-07 17:00:43,111 [DEBUG] Expected raw features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'R_ELBOW_release_angle', 'R_ELBOW_max_angle', 'R_WRIST_release_angle', 'R_WRIST_max_angle', 'R_KNEE_release_angle', 'R_KNEE_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z']\n",
      "2025-04-07 17:00:43,111 [DEBUG] Provided features: ['release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y', 'R_ELBOW_release_angle', 'R_ELBOW_max_angle', 'R_WRIST_release_angle', 'R_WRIST_max_angle', 'R_KNEE_release_angle', 'R_KNEE_max_angle', 'release_ball_speed', 'calculated_release_angle', 'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z']\n",
      "2025-04-07 17:00:43,113 [DEBUG] Reordered columns to match the pipeline's raw feature expectations.\n",
      "2025-04-07 17:00:43,117 [DEBUG] Transformed data shape: (125, 14)\n",
      "2025-04-07 17:00:43,118 [DEBUG] Derived feature names from pipeline: ['num__release_ball_direction_x', 'num__release_ball_direction_z', 'num__release_ball_direction_y', 'num__R_ELBOW_release_angle', 'num__R_ELBOW_max_angle', 'num__R_WRIST_release_angle', 'num__R_WRIST_max_angle', 'num__R_KNEE_release_angle', 'num__R_KNEE_max_angle', 'num__release_ball_speed', 'num__calculated_release_angle', 'num__release_ball_velocity_x', 'num__release_ball_velocity_y', 'num__release_ball_velocity_z']\n",
      "2025-04-07 17:00:43,119 [DEBUG] X_preprocessed_df columns: ['num__release_ball_direction_x', 'num__release_ball_direction_z', 'num__release_ball_direction_y', 'num__R_ELBOW_release_angle', 'num__R_ELBOW_max_angle', 'num__R_WRIST_release_angle', 'num__R_WRIST_max_angle', 'num__R_KNEE_release_angle', 'num__R_KNEE_max_angle', 'num__release_ball_speed', 'num__calculated_release_angle', 'num__release_ball_velocity_x', 'num__release_ball_velocity_y', 'num__release_ball_velocity_z']\n",
      "2025-04-07 17:00:43,122 [DEBUG] Sample of X_preprocessed_df:\n",
      "   num__release_ball_direction_x  num__release_ball_direction_z  \\\n",
      "0                       0.256808                      -0.014333   \n",
      "1                       0.653985                      -0.770471   \n",
      "2                       0.316179                      -0.134154   \n",
      "3                      -0.573663                       0.925278   \n",
      "4                       0.382947                      -0.241799   \n",
      "\n",
      "   num__release_ball_direction_y  num__R_ELBOW_release_angle  \\\n",
      "0                       1.145467                    1.044847   \n",
      "1                      -0.889734                   -1.374648   \n",
      "2                       0.190965                    1.316104   \n",
      "3                       1.149799                    0.887968   \n",
      "4                       0.013044                    1.270071   \n",
      "\n",
      "   num__R_ELBOW_max_angle  num__R_WRIST_release_angle  num__R_WRIST_max_angle  \\\n",
      "0                1.095488                    0.424441               -0.778748   \n",
      "1               -1.250958                    1.913313                0.102943   \n",
      "2                1.471855                   -0.176303                0.199613   \n",
      "3               -0.011954                   -1.316314               -0.640673   \n",
      "4               -0.184445                   -1.602153               -1.219041   \n",
      "\n",
      "   num__R_KNEE_release_angle  num__R_KNEE_max_angle  num__release_ball_speed  \\\n",
      "0                   0.668492               0.797754                -0.199124   \n",
      "1                   1.092750               1.846483                 0.922120   \n",
      "2                   0.991403               1.686793                 0.020369   \n",
      "3                  -0.430889               1.373530                -0.633392   \n",
      "4                   0.743514               1.121268                 0.505381   \n",
      "\n",
      "   num__calculated_release_angle  num__release_ball_velocity_x  \\\n",
      "0                       0.353184                     -0.010464   \n",
      "1                       1.052441                      0.777058   \n",
      "2                      -0.568742                      0.123024   \n",
      "3                       0.513443                     -0.702970   \n",
      "4                      -0.364409                      0.380674   \n",
      "\n",
      "   num__release_ball_velocity_y  num__release_ball_velocity_z  \n",
      "0                      1.217131                     -0.207739  \n",
      "1                     -1.333284                      0.940890  \n",
      "2                      0.174788                      0.029956  \n",
      "3                      1.217131                     -0.525123  \n",
      "4                     -0.115226                      0.590530  \n",
      "2025-04-07 17:00:43,123 [DEBUG] [DEBUG] Original data shape before inverse transform: (125, 14)\n",
      "2025-04-07 17:00:43,127 [DEBUG] [DEBUG] Inversed data shape: (125, 14)\n",
      "2025-04-07 17:00:43,128 [INFO] Step: Generate Preprocessor Recommendations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:43,128 - DataPreprocessor - INFO - Step: Generate Preprocessor Recommendations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:43,130 [INFO] Preprocessing Recommendations generated.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:43,130 - DataPreprocessor - INFO - Preprocessing Recommendations generated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:43,131 [INFO] Step 'Generate Preprocessor Recommendations' completed: Recommendations generated.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:43,131 - DataPreprocessor - INFO - Step 'Generate Preprocessor Recommendations' completed: Recommendations generated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:43,132 [DEBUG] Generated preprocessing recommendations.\n",
      "2025-04-07 17:00:43,133 [INFO] ✅ Preprocessing completed successfully in predict mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:43,133 - DataPreprocessor - INFO - ✅ Preprocessing completed successfully in predict mode.\n",
      "2025-04-07 17:00:43,134 - ml.shap.shap_utils - INFO - Preprocessing completed successfully in predict mode.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:43,136 - PredictAndAttachLogger - ERROR - ❌ Prediction failed: catboost/libs/data/model_dataset_compatibility.cpp:81: At position 3 should be feature with name num__elbow_release_angle (found num__R_ELBOW_release_angle).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:00:43,136 - PredictAndAttachLogger - ERROR - ❌ Prediction failed: catboost/libs/data/model_dataset_compatibility.cpp:81: At position 3 should be feature with name num__elbow_release_angle (found num__R_ELBOW_release_angle).\n",
      "2025-04-07 17:00:43,137 - ml.shap.shap_utils - ERROR - Unified predict_and_shap function failed for model CatBoost: cannot unpack non-iterable NoneType object\n",
      "All tests completed.\n",
      "ATTENTION HERE= ../../data/model\n",
      "Current working directory: /workspaces/spl_freethrow_biomechanics_analysis_ml_prediction/notebooks/freethrow_predictions\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %%writefile ml/shap/predict_with_shap_usage.py\n",
    "import pandas as pd\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import joblib\n",
    "import shap \n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import logging.config\n",
    "import ast\n",
    "\n",
    "# Import configuration loader and models\n",
    "from ml.config.config_loader import load_config\n",
    "from ml.config.config_models import AppConfig\n",
    "# Import other necessary modules\n",
    "from ml.train_utils.train_utils import load_model\n",
    "from ml.feature_selection.feature_importance_calculator import manage_features\n",
    "\n",
    "# Assume these are imported in the modules that call them\n",
    "from datapreprocessor import DataPreprocessor\n",
    "from ml.predict.predict import predict_and_attach_predict_probs\n",
    "\n",
    "# Import utility functions from our SHAP modules\n",
    "from ml.shap.predict_with_shap_usage_utils import compute_original_metric_error, generate_feedback_and_expand\n",
    "from ml.shap.shap_utils import load_dataset, setup_logging, load_configuration, initialize_logger\n",
    "\n",
    "# Import SHAP helper classes\n",
    "from ml.shap.shap_calculator import ShapCalculator\n",
    "from ml.shap.shap_visualizer import ShapVisualizer\n",
    "from ml.shap.feedback_generator import FeedbackGenerator\n",
    "# Add at the top of predict_with_shap_usage.py\n",
    "import inspect\n",
    "from ml.shap import shap_utils\n",
    "\n",
    "print(\"Available in shap_utils:\")\n",
    "for name, obj in inspect.getmembers(shap_utils):\n",
    "    if inspect.isfunction(obj):\n",
    "        print(f\"  - Function: {name}\")\n",
    "\n",
    "\n",
    "def convert_np_types(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: convert_np_types(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_np_types(i) for i in obj]\n",
    "    elif isinstance(obj, np.generic):\n",
    "        return obj.item()\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "def predict_and_shap(\n",
    "      config: AppConfig,\n",
    "      df_input: pd.DataFrame,\n",
    "      save_dir: Path,\n",
    "      columns_to_add: Optional[List[str]] = None,\n",
    "      generate_summary_plot: bool = True,\n",
    "      generate_dependence_plots: bool = False,\n",
    "      generate_force_plots: bool = False,\n",
    "      force_plot_indices: Optional[List[int]] = None,\n",
    "      top_n_features: int = 10,\n",
    "      use_mad: bool = False,\n",
    "      logger: Optional[logging.Logger] = None,\n",
    "      # Optional overrides for feature file paths:\n",
    "      features_file: Optional[Path] = None,\n",
    "      ordinal_file: Optional[Path] = None,\n",
    "      nominal_file: Optional[Path] = None,\n",
    "      numericals_file: Optional[Path] = None,\n",
    "      y_variable_file: Optional[Path] = None,\n",
    "      model_save_dir_override: Optional[Path] = None,\n",
    "      transformers_dir_override: Optional[Path] = None,\n",
    "      metrics_percentile: float = 10,\n",
    "      override_model_name: Optional[str] = None  # <<-- NEW parameter to override the best model name\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Perform prediction and SHAP analysis on the input DataFrame.\n",
    "    (Docstring unchanged for brevity.)\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    # Use dot‑notation to access configuration values.\n",
    "    data_dir = Path(config.paths.data_dir).resolve()\n",
    "    model_save_dir = (Path(config.paths.model_save_base_dir).resolve() \n",
    "                      if model_save_dir_override is None \n",
    "                      else model_save_dir_override.resolve())\n",
    "    transformers_dir = (Path(config.paths.transformers_save_base_dir).resolve() \n",
    "                        if transformers_dir_override is None \n",
    "                        else transformers_dir_override.resolve())\n",
    "    # Use configuration for feature paths.\n",
    "    features_file = Path(config.paths.features_metadata_file) if features_file is None else features_file\n",
    "    ordinal_file = Path(config.paths.ordinal_categoricals_file) if ordinal_file is None else ordinal_file\n",
    "    nominal_file = Path(config.paths.nominal_categoricals_file) if nominal_file is None else nominal_file\n",
    "    numericals_file = Path(config.paths.numericals_file) if numericals_file is None else numericals_file\n",
    "    y_variable_file = Path(config.paths.y_variable_file) if y_variable_file is None else y_variable_file\n",
    "\n",
    "    # Load tuning results and select the best model.\n",
    "    tuning_results_path = model_save_dir / \"tuning_results.json\"\n",
    "    print(f\"tuning_results_path: {tuning_results_path}\")\n",
    "    if not tuning_results_path.exists():\n",
    "        raise FileNotFoundError(f\"Tuning results not found at '{tuning_results_path}'.\")\n",
    "    with open(tuning_results_path, 'r') as f:\n",
    "        tuning_results = json.load(f)\n",
    "    best_model_info = tuning_results.get(\"Best Model\")\n",
    "    if not best_model_info:\n",
    "        raise ValueError(\"Best model information not found in tuning results.\")\n",
    "    best_model_name = best_model_info.get(\"model_name\")\n",
    "    if not best_model_name:\n",
    "        raise ValueError(\"Best model name not found in tuning results.\")\n",
    "    # <<-- If an override is provided, use that model name instead.\n",
    "    if override_model_name:\n",
    "        best_model_name = override_model_name\n",
    "        if logger:\n",
    "            logger.info(f\"Overriding best model selection; using model: {best_model_name}\")\n",
    "    else:\n",
    "        if logger:\n",
    "            logger.info(f\"Best model identified from tuning results: {best_model_name}\")\n",
    "\n",
    "    # Load the best model from the consistent save directory.\n",
    "    try:\n",
    "        model = load_model(best_model_name, model_save_dir)\n",
    "        if logger:\n",
    "            logger.info(f\"Trained model loaded from '{model_save_dir}' using model name '{best_model_name}'.\")\n",
    "    except Exception as e:\n",
    "        if logger:\n",
    "            logger.error(f\"Failed to load the best model '{best_model_name}': {e}\")\n",
    "        raise\n",
    "\n",
    "    # Load feature lists via manage_features.\n",
    "    feature_paths = {\n",
    "        'features': features_file,\n",
    "        'ordinal_categoricals': ordinal_file,\n",
    "        'nominal_categoricals': nominal_file,\n",
    "        'numericals': numericals_file,\n",
    "        'y_variable': y_variable_file\n",
    "    }\n",
    "    try:\n",
    "        feature_lists = manage_features(mode='load', paths=feature_paths)\n",
    "        y_variable_list = feature_lists.get('y_variable', [])\n",
    "        ordinal_categoricals = feature_lists.get('ordinal_categoricals', [])\n",
    "        nominal_categoricals = feature_lists.get('nominal_categoricals', [])\n",
    "        numericals = feature_lists.get('numericals', [])\n",
    "        if logger:\n",
    "            logger.debug(f\"Loaded Feature Lists: y_variable={y_variable_list}, ordinal_categoricals={ordinal_categoricals}, nominal_categoricals={nominal_categoricals}, numericals={numericals}\")\n",
    "    except Exception as e:\n",
    "        if logger:\n",
    "            logger.warning(f\"Feature lists could not be loaded: {e}\")\n",
    "        y_variable_list, ordinal_categoricals, nominal_categoricals, numericals = [], [], [], []\n",
    "\n",
    "    # Initialize the SHAP helper classes.\n",
    "    shap_calculator = ShapCalculator(model=model, logger=logger)\n",
    "    feedback_generator = FeedbackGenerator(logger=logger)\n",
    "    shap_visualizer = ShapVisualizer(logger=logger)\n",
    "\n",
    "    # Initialize DataPreprocessor.\n",
    "    preprocessor = DataPreprocessor(\n",
    "        model_type=\"Tree Based Classifier\",\n",
    "        y_variable=y_variable_list,\n",
    "        ordinal_categoricals=ordinal_categoricals,\n",
    "        nominal_categoricals=nominal_categoricals,\n",
    "        numericals=numericals,\n",
    "        mode='predict',\n",
    "        options={},\n",
    "        debug=True,\n",
    "        normalize_debug=False,\n",
    "        normalize_graphs_output=False,\n",
    "        graphs_output_dir=Path(config.paths.plots_output_dir).resolve(),\n",
    "        transformers_dir=transformers_dir\n",
    "    )\n",
    "    \n",
    "    # Preprocess the input DataFrame.\n",
    "    X_preprocessed, recommendations, X_inversed = preprocessor.final_preprocessing(df_input)\n",
    "    logger.info(\"Preprocessing completed successfully in predict mode.\")\n",
    "\n",
    "    # Reindex inverse-transformed data to match the preprocessed data.\n",
    "    X_inversed = X_inversed.reindex(X_preprocessed.index)\n",
    "    logger.debug(f\"X_preprocessed index: {X_preprocessed.index.tolist()}\")\n",
    "    logger.debug(f\"X_inversed index after reindexing: {X_inversed.index.tolist()}\")\n",
    "\n",
    "    # Compute predictions and attach them.\n",
    "    predictions, prediction_probs, X_inversed = predict_and_attach_predict_probs(model, X_preprocessed, X_inversed)\n",
    "    results['predictions'] = predictions\n",
    "    results['prediction_probs'] = prediction_probs\n",
    "    logger.info(\"Predictions generated and attached to the dataset.\")\n",
    "\n",
    "    # Compute SHAP values using our updated SHAP calculator.\n",
    "    explainer, shap_values = shap_calculator.compute_shap_values(X_preprocessed, debug=config.logging.debug)\n",
    "    results['shap_values'] = shap_values\n",
    "    results['explainer'] = explainer\n",
    "    results['X_preprocessed'] = X_preprocessed\n",
    "    logger.info(\"SHAP values computed successfully.\")\n",
    "    logger.debug(f\"SHAP values shape: {shap_values.shape}\")\n",
    "    \n",
    "    # -----------------------------------------------------------\n",
    "    # IMPORTANT UPDATE:\n",
    "    # Instead of using X_inversed (17 features) for expected_features,\n",
    "    # we use X_preprocessed (14 features) to match the SHAP computation.\n",
    "    # -----------------------------------------------------------\n",
    "    expected_features = X_preprocessed.columns.tolist()\n",
    "    logger.debug(f\"SHAP features count: {len(X_preprocessed.columns)}\")\n",
    "    logger.debug(f\"Expected features count (from X_preprocessed): {len(expected_features)}\")\n",
    "    # Optionally, add an assertion during development:\n",
    "    # assert set(X_preprocessed.columns) == set(expected_features), \"Feature mismatch!\"\n",
    "    \n",
    "    # Generate and expand feedback.\n",
    "    X_inversed = generate_feedback_and_expand(\n",
    "        X_inversed=X_inversed,\n",
    "        shap_values=shap_values,\n",
    "        logger=logger,\n",
    "        feedback_generator=feedback_generator,\n",
    "        metrics_percentile=metrics_percentile,\n",
    "        expected_features=expected_features,  # now using the correct features list\n",
    "        reference_index=X_preprocessed.index\n",
    "    )\n",
    "    results['final_dataset'] = X_inversed\n",
    "    logger.info(\"Feedback generation and metric threshold application completed.\")\n",
    "\n",
    "    # Generate plots if required.\n",
    "    if generate_summary_plot:\n",
    "        shap_summary_path = save_dir / \"shap_summary.png\"\n",
    "        try:\n",
    "            shap_visualizer.plot_summary(shap_values, X_preprocessed, shap_summary_path, debug=config.logging.debug)\n",
    "            results['shap_summary_plot'] = str(shap_summary_path)\n",
    "            logger.info(f\"SHAP summary plot saved at {shap_summary_path}.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to generate SHAP summary plot: {e}\")\n",
    "\n",
    "    if generate_dependence_plots:\n",
    "        try:\n",
    "            recommendations_dict = feedback_generator.generate_global_recommendations(\n",
    "                shap_values=shap_values,\n",
    "                X_original=X_preprocessed,\n",
    "                top_n=top_n_features,\n",
    "                use_mad=use_mad,\n",
    "                debug=config.logging.debug\n",
    "            )\n",
    "            results['recommendations'] = recommendations_dict\n",
    "            shap_dependence_dir = save_dir / \"shap_dependence_plots\"\n",
    "            shap_dependence_dir.mkdir(parents=True, exist_ok=True)\n",
    "            for feature in recommendations_dict.keys():\n",
    "                dep_path = shap_dependence_dir / f\"shap_dependence_{feature}.png\"\n",
    "                shap_visualizer.plot_dependence(shap_values, feature, X_preprocessed, dep_path, interaction_index=None, debug=config.logging.debug)\n",
    "            logger.info(f\"SHAP dependence plots saved at {shap_dependence_dir}.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to generate SHAP dependence plots: {e}\")\n",
    "\n",
    "    if generate_force_plots and force_plot_indices:\n",
    "        try:\n",
    "            force_plots_dir = save_dir / \"shap_force_plots\"\n",
    "            force_plots_dir.mkdir(parents=True, exist_ok=True)\n",
    "            for idx in force_plot_indices:\n",
    "                if idx < 0 or idx >= X_preprocessed.shape[0]:\n",
    "                    logger.warning(f\"Index {idx} is out of bounds. Skipping.\")\n",
    "                    continue\n",
    "                force_path = force_plots_dir / f\"shap_force_plot_{idx}.html\"\n",
    "                shap_visualizer.plot_force(explainer, shap_values, X_preprocessed, idx, force_path, debug=config.logging.debug)\n",
    "            logger.info(f\"SHAP force plots saved at {force_plots_dir}.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to generate SHAP force plots: {e}\")\n",
    "\n",
    "    # Optionally add extra columns from the input DataFrame.\n",
    "    if columns_to_add:\n",
    "        try:\n",
    "            logger.info(f\"Adding columns from df_input to final_df: {columns_to_add}\")\n",
    "            for column in columns_to_add:\n",
    "                if column not in df_input.columns:\n",
    "                    logger.warning(f\"Column '{column}' not found in input DataFrame.\")\n",
    "                    continue\n",
    "                if len(df_input) != len(X_inversed):\n",
    "                    logger.error(\"Length mismatch between df_input and X_inversed.\")\n",
    "                    raise ValueError(\"Length mismatch between df_input and X_inversed.\")\n",
    "                X_inversed[column] = df_input[column].values\n",
    "            logger.info(f\"Columns added successfully: {columns_to_add}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to add columns to final_df: {e}\")\n",
    "            raise\n",
    "\n",
    "    # Save the final dataset.\n",
    "    try:\n",
    "        final_dataset_path = save_dir / \"final_predictions_with_shap.csv\"\n",
    "        logger.info(f\"Saving final dataset with SHAP annotations to {final_dataset_path}.\")\n",
    "        X_inversed.to_csv(final_dataset_path, index=True)\n",
    "        results['final_dataset'] = str(final_dataset_path)\n",
    "        logger.info(\"Final dataset and global recommendations saved.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to save outputs: {e}\")\n",
    "        raise\n",
    "\n",
    "    results = convert_np_types(results)\n",
    "    logger.info(\"Predict+SHAP pipeline completed successfully.\")\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Main testing block for trying multiple models.\n",
    "    config_path = Path('../../data/model/preprocessor_config/preprocessor_config.yaml')\n",
    "    try:\n",
    "        config: AppConfig = load_config(config_path)\n",
    "        print(f\"Configuration loaded successfully from {config_path}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to load configuration: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "    data_dir = Path(config.paths.data_dir).resolve()\n",
    "    raw_data_path = data_dir / config.paths.raw_data\n",
    "    predictions_output_path = Path(config.paths.predictions_output_dir).resolve() / \"shap_results\"\n",
    "\n",
    "    log_dir = Path(config.paths.log_dir).resolve()\n",
    "    log_file = Path(config.paths.log_file).resolve()\n",
    "\n",
    "    try:\n",
    "        logger = setup_logging(config, log_file)\n",
    "        logger.info(\"Starting prediction module (unified predict_and_shap).\")\n",
    "        logger.debug(f\"Paths: {config.paths}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to set up logging: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "    try:\n",
    "        df_predict = load_dataset(raw_data_path)\n",
    "        print(\"Columns in input data:\", df_predict.columns.tolist())\n",
    "        logger.info(f\"Prediction input data loaded from {raw_data_path}.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load input data: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "    # List of model names to test.\n",
    "    test_model_names = [\"XGBoost\", \"Random Forest\", \"CatBoost\"]\n",
    "\n",
    "    for model_name in test_model_names:\n",
    "        print(f\"\\n--- Running pipeline for model: {model_name} ---\")\n",
    "        # Create a separate output subdirectory for each model.\n",
    "        model_output_dir = predictions_output_path / model_name.replace(\" \", \"_\")\n",
    "        model_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        try:\n",
    "            results = predict_and_shap(\n",
    "                config=config,\n",
    "                df_input=df_predict,\n",
    "                save_dir=model_output_dir,\n",
    "                columns_to_add=['trial_id'],\n",
    "                generate_summary_plot=True,\n",
    "                generate_dependence_plots=True,\n",
    "                generate_force_plots=True,\n",
    "                force_plot_indices=[0],\n",
    "                top_n_features=10,\n",
    "                use_mad=False,\n",
    "                logger=logger,\n",
    "                features_file=(Path(config.paths.data_dir) / config.paths.features_metadata_file).resolve(),\n",
    "                ordinal_file=Path(f'{Path(\"../../data\") / \"preprocessor\" / \"features_info\"}/ordinal_categoricals.pkl'),\n",
    "                nominal_file=Path(f'{Path(\"../../data\") / \"preprocessor\" / \"features_info\"}/nominal_categoricals.pkl'),\n",
    "                numericals_file=Path(f'{Path(\"../../data\") / \"preprocessor\" / \"features_info\"}/numericals.pkl'),\n",
    "                y_variable_file=Path(f'{Path(\"../../data\") / \"preprocessor\" / \"features_info\"}/y_variable.pkl'),\n",
    "                model_save_dir_override=Path(config.paths.model_save_base_dir),\n",
    "                transformers_dir_override=Path(config.paths.transformers_save_base_dir),\n",
    "                metrics_percentile=10,\n",
    "                override_model_name=model_name  # <<-- override to use the current model in the loop, comment out if not needed\n",
    "            )\n",
    "            logger.info(f\"Unified predict_and_shap function executed successfully for model {model_name}.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Unified predict_and_shap function failed for model {model_name}: {e}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            print(f\"\\nFinal Predictions with SHAP annotations for model {model_name} (preview):\")\n",
    "            final_df = pd.read_csv(results['final_dataset'], index_col=0)\n",
    "            print(final_df.head())\n",
    "            logger.debug(f\"Final DataFrame columns for {model_name}: {final_df.columns.tolist()}\")\n",
    "            trial_index = 0\n",
    "            if trial_index < final_df.shape[0]:\n",
    "                shap_columns = [col for col in final_df.columns if col.startswith('shap_')]\n",
    "                logger.debug(f\"'shap_' columns for feedback in {model_name}: {shap_columns}\")\n",
    "                if not shap_columns:\n",
    "                    logger.error(\"No 'shap_' columns found in the final DataFrame.\")\n",
    "                    print(\"No feedback columns found in the final DataFrame.\")\n",
    "                else:\n",
    "                    print(f\"\\nFeedback for trial at index {trial_index} for model {model_name}:\")\n",
    "                    feedback = final_df.iloc[trial_index][shap_columns].to_dict()\n",
    "                    for metric, suggestion in feedback.items():\n",
    "                        print(f\"  - {metric}: {suggestion}\")\n",
    "            else:\n",
    "                print(f\"No feedback found for trial at index {trial_index} in model {model_name}.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to display outputs for model {model_name}: {e}\")\n",
    "\n",
    "    print(\"All tests completed.\")\n",
    "    print(\"ATTENTION HERE=\", config.paths.model_save_base_dir)\n",
    "    print(\"Current working directory:\", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science_ft_bio_predictions",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
