{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final full idea put together\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "joints to work with:\n",
    "        # Define connections between joints\n",
    "        connections = [\n",
    "            (\"R_EYE\", \"L_EYE\"), (\"R_EYE\", \"NOSE\"), (\"L_EYE\", \"NOSE\"),\n",
    "            (\"R_EYE\", \"R_EAR\"), (\"L_EYE\", \"L_EAR\"), (\"R_SHOULDER\", \"L_SHOULDER\"),\n",
    "            (\"R_SHOULDER\", \"R_ELBOW\"), (\"L_SHOULDER\", \"L_ELBOW\"), (\"R_ELBOW\", \"R_WRIST\"),\n",
    "            (\"L_ELBOW\", \"L_WRIST\"), (\"R_SHOULDER\", \"R_HIP\"), (\"L_SHOULDER\", \"L_HIP\"),\n",
    "            (\"R_HIP\", \"L_HIP\"), (\"R_HIP\", \"R_KNEE\"), (\"L_HIP\", \"L_KNEE\"),\n",
    "            (\"R_KNEE\", \"R_ANKLE\"), (\"L_KNEE\", \"L_ANKLE\"), (\"R_WRIST\", \"R_1STFINGER\"),\n",
    "            (\"R_WRIST\", \"R_5THFINGER\"), (\"L_WRIST\", \"L_1STFINGER\"), (\"L_WRIST\", \"L_5THFINGER\"),\n",
    "            (\"R_ANKLE\", \"R_1STTOE\"), (\"R_ANKLE\", \"R_5THTOE\"), (\"L_ANKLE\", \"L_1STTOE\"),\n",
    "            (\"L_ANKLE\", \"L_5THTOE\"), (\"R_ANKLE\", \"R_CALC\"), (\"L_ANKLE\", \"L_CALC\"),\n",
    "            (\"R_1STTOE\", \"R_5THTOE\"), (\"L_1STTOE\", \"L_5THTOE\"), (\"R_1STTOE\", \"R_CALC\"),\n",
    "            (\"L_1STTOE\", \"L_CALC\"), (\"R_5THTOE\", \"R_CALC\"), (\"L_5THTOE\", \"L_CALC\"),\n",
    "            (\"R_1STFINGER\", \"R_5THFINGER\"), (\"L_1STFINGER\", \"L_5THFINGER\")\n",
    "        ]\n",
    "\n",
    "ADDITIONS:\n",
    "#, '1sttoe', '5thtoe' < ADD WHEN WE ADD TO DATA LOAD AND PREPARE IN MODULE %%writefile ml/feature_engineering/energy_exhaustion_metrics.py\n",
    "\n",
    "look into why \n",
    "INFO: Angle column 'L_KNEE_angle' not found; skipping ROM metrics for L KNEE.\n",
    "INFO: Angle column 'R_KNEE_angle' not found; skipping ROM metrics for R KNEE.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a step‐by‐step walkthrough of how you can modify your existing pipeline to update the predictor variables, integrate additional metrics (asymmetry for all joints, power ratios, and ROM-based features), and then evaluate their importance relative to the target outcomes (exhaustion and injury risk). I’ve included plenty of breathing space between steps so you can follow along.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 1: Review and Update Predictor Variables\n",
    "\n",
    "**What We’re Changing:**\n",
    "\n",
    "1. **Asymmetry Metrics:**  \n",
    "   - **Before:** You had asymmetry for a few joints (e.g., elbows, knees).  \n",
    "   - **Now:** Include asymmetry for wrists, shoulders, hips, ankles, feet—and, if data permits, compare left versus right sides of the feet (for landing and usage comparisons).  \n",
    "   - **How:** For each paired joint (e.g., left/right shoulder), compute:\n",
    "     ```python\n",
    "     data['shoulder_asymmetry'] = np.abs(data['L_SHOULDER_energy'] - data['R_SHOULDER_energy'])\n",
    "     ```\n",
    "     Repeat for wrists, hips, ankles, and feet. For feet, if you have sub-foot measurements (e.g., left forefoot vs. right forefoot), compute those asymmetries as well.\n",
    "\n",
    "2. **Power Ratios for All Joints:**  \n",
    "   - **Before:** You had an ankle power ratio.  \n",
    "   - **Now:** Compute power ratios for every joint pair.  \n",
    "   - **How:** For example, for the shoulder:\n",
    "     ```python\n",
    "     data['shoulder_power_ratio'] = data['L_SHOULDER_power'] / (data['R_SHOULDER_power'] + 1e-6)\n",
    "     ```\n",
    "     Do similar computations for wrists, hips, knees, ankles, and feet.\n",
    "\n",
    "3. **Range-of-Motion (ROM) Metrics:**  \n",
    "   - **Before:** Your pipeline computed ROM for some joints and flagged extremes using deviation scores and binary flags.  \n",
    "   - **Now:** For every joint you’re interested in (e.g., knee, shoulder, hip, etc.), compute:\n",
    "     - The ROM:  \n",
    "       ```python\n",
    "       data['knee_ROM'] = data.groupby('trial_id')['knee_angle'].transform(lambda x: x.max() - x.min())\n",
    "       ```\n",
    "     - The deviation score relative to normative thresholds (e.g., using CDC normative values):  \n",
    "       ```python\n",
    "       normal_min_knee, normal_max_knee = 120, 135  # example values\n",
    "       data['knee_ROM_deviation'] = np.maximum(0, normal_min_knee - data['knee_ROM']) + np.maximum(0, data['knee_ROM'] - normal_max_knee)\n",
    "       ```\n",
    "     - The binary flag indicating an extreme value:  \n",
    "       ```python\n",
    "       data['knee_ROM_extreme'] = ((data['knee_ROM'] < normal_min_knee) | (data['knee_ROM'] > normal_max_knee)).astype(int)\n",
    "       ```\n",
    "     - **Tip:** Repeat similar calculations for the shoulders, hips, ankles, and (if applicable) wrists.  \n",
    "     \n",
    "4. **Removing Non-Contributing Features:**  \n",
    "   - You mentioned that the wrist angle at release is not useful for this model.  \n",
    "   - **Action:** Remove or exclude this feature from your predictors:\n",
    "     ```python\n",
    "     predictors = [col for col in predictors if col != 'wrist_angle_release']\n",
    "     ```\n",
    "\n",
    "Take a breath—now we have updated the individual metrics.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 2: Integrate Additional Metrics into the Feature Engineering Pipeline\n",
    "\n",
    "### 2.1. Expand Asymmetry Calculations\n",
    "\n",
    "For each joint pair, calculate asymmetry. For instance:\n",
    "- **Shoulders, Hips, Ankles, Wrists, and Feet:**\n",
    "  ```python\n",
    "  for joint in ['shoulder', 'hip', 'ankle', 'wrist']:\n",
    "      data[f'{joint}_asymmetry'] = np.abs(data[f'L_{joint.upper()}_energy'] - data[f'R_{joint.upper()}_energy'])\n",
    "  # If foot sub-measurements exist:\n",
    "  data['foot_asymmetry'] = np.abs(data['left_foot_energy'] - data['right_foot_energy'])\n",
    "  ```\n",
    "  \n",
    "### 2.2. Compute Power Ratios Across All Joints\n",
    "\n",
    "Similarly, loop over each joint to calculate a power ratio:\n",
    "  ```python\n",
    "  for joint in ['shoulder', 'hip', 'knee', 'ankle', 'wrist']:\n",
    "      data[f'{joint}_power_ratio'] = data[f'L_{joint.upper()}_power'] / (data[f'R_{joint.upper()}_power'] + 1e-6)\n",
    "  # For feet, if you have sub-joint or left/right foot power:\n",
    "  data['foot_power_ratio'] = data['left_foot_power'] / (data['right_foot_power'] + 1e-6)\n",
    "  ```\n",
    "\n",
    "### 2.3. Incorporate Expanded ROM Metrics\n",
    "\n",
    "For each joint (e.g., knee, shoulder, hip, ankle), compute ROM, deviation scores, and binary flags. Example for the knee was given above; repeat for each joint:\n",
    "  ```python\n",
    "  # Example for the shoulder (adjust normative thresholds accordingly)\n",
    "  normal_min_shoulder, normal_max_shoulder = 0, 150  # example values for abduction\n",
    "  data['shoulder_ROM'] = data.groupby('trial_id')['shoulder_angle'].transform(lambda x: x.max() - x.min())\n",
    "  data['shoulder_ROM_deviation'] = np.maximum(0, normal_min_shoulder - data['shoulder_ROM']) + np.maximum(0, data['shoulder_ROM'] - normal_max_shoulder)\n",
    "  data['shoulder_ROM_extreme'] = ((data['shoulder_ROM'] < normal_min_shoulder) | (data['shoulder_ROM'] > normal_max_shoulder)).astype(int)\n",
    "  ```\n",
    "\n",
    "### 2.4. Remove the Wrist Angle at Release\n",
    "\n",
    "Before proceeding to the model training phase, ensure you drop or ignore the `wrist_angle_release` feature:\n",
    "  ```python\n",
    "  if 'wrist_angle_release' in data.columns:\n",
    "      data.drop(columns=['wrist_angle_release'], inplace=True)\n",
    "  ```\n",
    "\n",
    "Let’s take another breath—now we have a fully enriched set of predictor variables.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 3: Define Predictor (X) and Target (y) Variables for Each Model\n",
    "\n",
    "### 3.1. Exhaustion Prediction\n",
    "\n",
    "- **Target Variable (y):**  \n",
    "  - `by_trial_exhaustion_score` (continuous measure)\n",
    "  logic:\n",
    "  trial time is every .33 seconds\n",
    "    data['exhaustion_rate'] = data['by_trial_exhaustion_score'].diff() / data['by_trial_time'].diff()\n",
    "\n",
    "- **Predictor Variables (X) should now include:**  \n",
    "  - **Aggregated energy and power metrics:** joint_power, joint_energy  \n",
    "  - **Asymmetry metrics:** elbow_asymmetry, knee_asymmetry, shoulder_asymmetry, hip_asymmetry, ankle_asymmetry, wrist_asymmetry, foot_asymmetry  \n",
    "  - **Power ratios:** shoulder_power_ratio, hip_power_ratio, knee_power_ratio, ankle_power_ratio, wrist_power_ratio, foot_power_ratio  \n",
    "  - **Lagged and rolling features:** exhaustion_lag1, power_avg_5, rolling_energy_std  \n",
    "  - **Simulated physiological proxies:** simulated_HR, energy_acceleration  \n",
    "  - **Anthropometrics:** player_height_in_meters, player_weight_in_kg  \n",
    "  - **ROM-based features:** For each joint, include ROM, deviation scores (e.g., knee_ROM_deviation), and binary extreme flags (e.g., knee_ROM_extreme)\n",
    "\n",
    "### 3.2. Injury Risk Prediction\n",
    "\n",
    "- **Target Variable (y):**  \n",
    "  - `injury_risk` (binary: 0 for low risk, 1 for high risk)\n",
    "  logic:\n",
    "    data['rolling_exhaustion'] = data['by_trial_exhaustion_score'].rolling(window=rolling_window).sum()\n",
    "    data['injury_risk'] = np.where(data['rolling_exhaustion'] > quantile_75, 1, 0)\n",
    "\n",
    "- **Predictor Variables (X) should include:**  \n",
    "  - Many of the same metrics as exhaustion prediction but with an added emphasis on movement asymmetries and ROM abnormalities:\n",
    "    - joint_power, joint_energy  \n",
    "    - All asymmetry metrics  \n",
    "    - All power ratios  \n",
    "    - Lagged features like exhaustion_lag1  \n",
    "    - Rolling averages (power_avg_5) and simulated_HR  \n",
    "    - Anthropometrics  \n",
    "    - **ROM Metrics:** All computed ROM values, deviation scores, and binary flags for each joint\n",
    "\n",
    "### 3.2. Injury Risk Prediction\n",
    "\n",
    "- **Target Variable (y):**  \n",
    "  - knee, shoulder, ankle, and wrist `injury_risk` (binary: 0 for low risk, 1 for high risk)\n",
    "  logic:\n",
    "    data['{joint}_exhaustion_rate'] = data['{joint}_by_trial_exhaustion_score'].diff() / data['by_trial_time'].diff()\n",
    "    data['{joint}_rolling_exhaustion'] = data['{joint}_by_trial_exhaustion_score'].rolling(window=rolling_window).sum()\n",
    "    data['{joint}_injury_risk'] = np.where(data['{joint}_rolling_exhaustion'] > quantile_75, 1, 0)\n",
    "\n",
    "- **Predictor Variables (X) should include:**  \n",
    "  - Many of the same metrics as exhaustion prediction but with an added emphasis on movement asymmetries and ROM abnormalities:\n",
    "    - joint_power, joint_energy  \n",
    "    - All asymmetry metrics  \n",
    "    - All power ratios  \n",
    "    - Lagged features like exhaustion_lag1  \n",
    "    - Rolling averages (power_avg_5) and simulated_HR  \n",
    "    - Anthropometrics  \n",
    "    - **ROM Metrics:** All computed ROM values, deviation scores, and binary flags for each joint\n",
    "Take a breath—you now have a robust and enriched feature set for both exhaustion and injury risk predictions.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 4: Train Models and Evaluate Feature Importance\n",
    "\n",
    "### 4.1. Model Building\n",
    "\n",
    "- **Exhaustion Prediction Model (Regression):**  \n",
    "  Build an LSTM or any other regression model using the enriched feature set.\n",
    "- **Injury Risk Prediction Model (Classification):**  \n",
    "  Similarly, train a classification model using your combined features.\n",
    "\n",
    "### 4.2. Evaluating Feature Importance\n",
    "\n",
    "Once the models are trained, determine the relative importance of each feature using methods such as:\n",
    "\n",
    "1. **Permutation Importance:**  \n",
    "   Shuffle each predictor and measure the drop in performance.\n",
    "   ```python\n",
    "   from sklearn.inspection import permutation_importance\n",
    "   results = permutation_importance(trained_model, X_test, y_test, scoring='neg_mean_absolute_error')\n",
    "   importance_df = pd.DataFrame({'feature': X_test.columns, 'importance': results.importances_mean})\n",
    "   ```\n",
    "\n",
    "2. **Tree-based Methods:**  \n",
    "   If you are using a random forest or gradient boosting method, use built-in feature importance metrics.\n",
    "   ```python\n",
    "   importance_df = pd.DataFrame({\n",
    "       'feature': X_test.columns,\n",
    "       'importance': trained_model.feature_importances_\n",
    "   })\n",
    "   ```\n",
    "\n",
    "3. **SHAP Analysis:**  \n",
    "   SHAP values can help you understand the impact of each feature on individual predictions.\n",
    "   ```python\n",
    "   import shap\n",
    "   explainer = shap.TreeExplainer(trained_model)\n",
    "   shap_values = explainer.shap_values(X_test)\n",
    "   shap.summary_plot(shap_values, X_test)\n",
    "   ```\n",
    "\n",
    "4. **Correlation Analysis:**  \n",
    "   Compute Pearson or Spearman correlation coefficients between each predictor and the target variable for a first-pass indication.\n",
    "\n",
    "After evaluating, you can identify which features—be it asymmetry metrics, power ratios, or ROM-based measures—are most predictive of exhaustion or injury risk.\n",
    "\n",
    "Take one final breath—your integrated pipeline now not only includes all the desired metrics but also has a clear process to assess which features are most influential in predicting the target outcomes.\n",
    "\n",
    "---\n",
    "\n",
    "## Final Recap\n",
    "\n",
    "- **Updated Features:**  \n",
    "  We added asymmetry calculations for all major joints (wrists, shoulders, hips, ankles, feet), computed power ratios for each joint pair, and expanded our ROM analysis to include deviation scores and binary flags.\n",
    "  \n",
    "- **Feature Removal:**  \n",
    "  The wrist angle at release was removed as it was not helpful for the prediction task.\n",
    "  \n",
    "- **Integration:**  \n",
    "  All these features were added to both the exhaustion and injury risk predictor sets.\n",
    "  \n",
    "- **Evaluation:**  \n",
    "  We then laid out steps to train models and perform feature importance analyses (using permutation importance, tree-based methods, and SHAP).\n",
    "\n",
    "By following these steps, you ensure that your workout simulation model incorporates comprehensive biomechanical metrics, enhancing the predictive power for both fatigue (exhaustion) and joint injury risk.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the CDC resource (see citeturn1search28 and the archived CDC page at https://archive.cdc.gov/www_cdc_gov/ncbddd/jointrom/index.html), here is an example of normative thresholds for joint range of motion in adults. (Note that these numbers are approximate—actual values may vary slightly by population and measurement method.)\n",
    "\n",
    "- **Shoulder:**  \n",
    "  - Flexion: Approximately 0° to 180°  \n",
    "  - Abduction: Approximately 0° to 150°\n",
    "\n",
    "- **Elbow:**  \n",
    "  - Flexion: Approximately 0° to 150°  \n",
    "  - Extension: 0° (full extension)\n",
    "\n",
    "- **Wrist:**  \n",
    "  - Flexion: Approximately 0° to 80°  \n",
    "  - Extension: Approximately 0° to 70°\n",
    "\n",
    "- **Hip:**  \n",
    "  - Flexion: Approximately 0° to 120°  \n",
    "  - Extension: Approximately 0° to 30°\n",
    "\n",
    "- **Knee:**  \n",
    "  - Flexion: Approximately 0° to 135°  \n",
    "  - Extension: 0° (full extension)\n",
    "\n",
    "- **Ankle:**  \n",
    "  - Dorsiflexion: Approximately 0° to 20°  \n",
    "  - Plantarflexion: Approximately 0° to 50°\n",
    "\n",
    "### How These Thresholds Would Work in the Pipeline\n",
    "\n",
    "1. **Data Extraction:**  \n",
    "   After loading your granular data, you’ll extract joint angle data for each joint over each free throw. For example, if you have a column named “knee_angle,” you can calculate its maximum and minimum value during each trial.\n",
    "\n",
    "2. **ROM Calculation:**  \n",
    "   For each joint, compute the range of motion as:  \n",
    "   \\[\n",
    "   \\text{ROM} = \\max(\\text{joint\\_angle}) - \\min(\\text{joint\\_angle})\n",
    "   \\]\n",
    "   This gives you the actual ROM for that trial.\n",
    "\n",
    "3. **Comparison with Normative Thresholds:**  \n",
    "   Using the thresholds above, you can then determine if the measured ROM is “extreme” (i.e., either below the lower bound or above the upper bound). For example, for the knee:\n",
    "   - **Normal:** 0° to 135° of flexion  \n",
    "   - **Abnormal/Extreme:** Less than 120° might be considered too stiff, while more than 135° (or above a clinically accepted maximum) could suggest hypermobility.\n",
    "   \n",
    "   You could flag these cases with:\n",
    "   ```python\n",
    "   normal_min_knee, normal_max_knee = 120, 135  # example thresholds\n",
    "   data['knee_ROM'] = data.groupby('trial_id')['knee_angle'].transform(lambda x: x.max() - x.min())\n",
    "   data['knee_ROM_extreme'] = ((data['knee_ROM'] < normal_min_knee) | (data['knee_ROM'] > normal_max_knee)).astype(int)\n",
    "   ```\n",
    "   Similar computations can be done for the shoulder, wrist, hip, and ankle.\n",
    "\n",
    "4. **Using as Predictors:**  \n",
    "   - For **injury risk prediction**, add these ROM metrics (or binary flags/deviation scores) to your feature set. The idea is that joints operating outside of their normative ranges may be at increased risk for injury.\n",
    "   - For **exhaustion prediction**, while ROM might be a secondary predictor compared to energy and power, abnormal or highly variable ROM might still indicate inefficient or compensatory movement patterns that could correlate with fatigue.\n",
    "\n",
    "5. **Integration with Other Metrics:**  \n",
    "   Once these new ROM features are computed, they can be included alongside energy, power, and fatigue measures in both your exploratory analysis and predictive modeling steps. For instance, abnormal ROM in the shoulder (e.g., a flexion range below 170° or above 180°) might combine with high simulated heart rate or high energy acceleration to signal increased injury risk.\n",
    "\n",
    "### Final Thoughts\n",
    "\n",
    "Incorporating these threshold values from the CDC gives you objective criteria to flag when a joint’s range of motion is not within a healthy or expected range. When integrated into your pipeline, these metrics help provide a fuller picture of an athlete’s biomechanics—combining the magnitude of force (energy and power) with the quality of movement (ROM and its extremes). This approach can improve injury risk predictions and provide actionable insights for corrective interventions.\n",
    "\n",
    "Sources:  \n",
    "citeturn1search28 (Encyclopedia.com on Range of Motion) and the archived CDC page at https://archive.cdc.gov/www_cdc_gov/ncbddd/jointrom/index.html.\n",
    "\n",
    "Below is an overview of the best ways to incorporate extreme range‐of‐motion (ROM) metrics into your workflow, along with a discussion of how these metrics can serve as an “injury index” by flagging movements that push joints toward their limits—potentially indicating over-stress in compromised positions.\n",
    "\n",
    "---\n",
    "\n",
    "### Incorporating Extreme ROM Metrics into the Workflow\n",
    "\n",
    "#### 1. **Data Extraction & Calculation**\n",
    "\n",
    "- **Extract Joint Angle Data:**  \n",
    "  From your granular (frame-by-frame) dataset, extract the joint angle time series for each joint of interest (e.g., knee_angle, shoulder_angle, etc.) for each free throw or trial.\n",
    "\n",
    "- **Compute the ROM for Each Joint:**  \n",
    "  For each trial, calculate the range of motion as:  \n",
    "  \\[\n",
    "  \\text{ROM} = \\max(\\text{joint\\_angle}) - \\min(\\text{joint\\_angle})\n",
    "  \\]\n",
    "  For example, if the knee angle varies between 30° and 140° in one trial, the ROM for that trial is 110°.\n",
    "\n",
    "#### 2. **Comparison with Normative Thresholds**\n",
    "\n",
    "Based on the CDC resource, you have normative thresholds for each joint. For instance:\n",
    "  \n",
    "- **Shoulder:**  \n",
    "  - Flexion: 0° to 180°  \n",
    "  - Abduction: 0° to 150°\n",
    "  \n",
    "- **Elbow:**  \n",
    "  - Flexion: 0° to 150°  \n",
    "  - Extension: 0° (full extension)\n",
    "  \n",
    "- **Wrist:**  \n",
    "  - Flexion: 0° to 80°  \n",
    "  - Extension: 0° to 70°\n",
    "  \n",
    "- **Hip:**  \n",
    "  - Flexion: 0° to 120°  \n",
    "  - Extension: 0° to 30°\n",
    "  \n",
    "- **Knee:**  \n",
    "  - Flexion: 0° to 135°  \n",
    "  - Extension: 0°\n",
    "  \n",
    "- **Ankle:**  \n",
    "  - Dorsiflexion: 0° to 20°  \n",
    "  - Plantarflexion: 0° to 50°\n",
    "\n",
    "For each joint, decide on “extreme” thresholds that may indicate over-stress. For example, for the knee, you might define:\n",
    "- **Normal Knee Flexion:** 0° to 135°  \n",
    "- **Abnormally Restricted (Stiff) Knee:** ROM less than 120°  \n",
    "- **Abnormally Excessive (Hypermobile) Knee:** ROM greater than 135° (or above an accepted clinical maximum)\n",
    "\n",
    "#### 3. **Creating ROM-Based Features**\n",
    "\n",
    "- **Deviation Metrics:**  \n",
    "  For each joint, calculate a deviation measure that quantifies how far the measured ROM deviates from the normative (or “optimal”) range. For example:\n",
    "  ```python\n",
    "  # For the knee, using illustrative thresholds\n",
    "  normal_min_knee, normal_max_knee = 120, 135\n",
    "  data['knee_ROM'] = data.groupby('trial_id')['knee_angle'].transform(lambda x: x.max() - x.min())\n",
    "  data['knee_ROM_deviation'] = np.maximum(0, normal_min_knee - data['knee_ROM']) + np.maximum(0, data['knee_ROM'] - normal_max_knee)\n",
    "  ```\n",
    "  This continuous deviation score provides a “meter” of how far the joint is operating outside the normal range.\n",
    "\n",
    "- **Binary Flags:**  \n",
    "  Alternatively, create binary flags that indicate whether a joint’s ROM is extreme:\n",
    "  ```python\n",
    "  data['knee_ROM_extreme'] = ((data['knee_ROM'] < normal_min_knee) | (data['knee_ROM'] > normal_max_knee)).astype(int)\n",
    "  ```\n",
    "  These flags can be used directly in your injury risk prediction models.\n",
    "\n",
    "#### 4. **Integrating into the Injury Index**\n",
    "\n",
    "- **Weighted Injury Index:**  \n",
    "  Combine the deviation scores (or binary flags) from multiple joints into an aggregated “injury index.” For instance, you might take a weighted sum of the deviation scores for key joints (e.g., knee, shoulder, hip) based on their importance in your analysis:\n",
    "  ```python\n",
    "  data['injury_index'] = (w1 * data['knee_ROM_deviation'] + \n",
    "                          w2 * data['shoulder_ROM_deviation'] + \n",
    "                          w3 * data['hip_ROM_deviation'])\n",
    "  ```\n",
    "  where *w1*, *w2*, and *w3* are weights determined either by domain expertise or through feature importance analysis.\n",
    "\n",
    "- **Interpretation:**  \n",
    "  The injury index provides a quantitative measure of how “compromised” a joint (or combination of joints) is by operating at extreme ranges. A higher index suggests that the athlete’s joints are spending more time (or exhibiting more variability) near their extremes, which could be a sign of over-stress and a higher risk for injury.\n",
    "\n",
    "#### 5. **Integration with Other Metrics and Modeling**\n",
    "\n",
    "- **As Predictors:**  \n",
    "  Add these ROM-derived metrics (both continuous deviations and binary flags) to your predictor set (X) in the injury risk model. They can be used alongside existing metrics like joint_energy, energy_acceleration, and simulated_HR.\n",
    "  \n",
    "- **Model Analysis:**  \n",
    "  Run feature importance or SHAP analysis on the injury risk model to see how strongly these ROM features contribute to predicting injury. If they are significant, it confirms that operating in extreme ROM is an important risk factor.\n",
    "\n",
    "- **Dynamic Feedback:**  \n",
    "  In a real-time or near-real-time monitoring system, these ROM metrics could provide immediate feedback. For example, if an athlete’s knee ROM suddenly deviates from the norm, the system could alert coaches or trigger a warning to adjust the training load.\n",
    "\n",
    "---\n",
    "\n",
    "### Final Thoughts\n",
    "\n",
    "By incorporating extreme range-of-motion metrics, your workflow gains a new dimension that quantifies not only the volume of joint movement (energy and power) but also its quality. This allows you to:\n",
    "- **Detect Abnormal Movement Patterns:** Identify when joints are operating at the edge of their healthy range.\n",
    "- **Quantify Over-Stress:** Create an injury index that reflects cumulative stress from abnormal ROM.\n",
    "- **Enhance Predictive Models:** Integrate these metrics into both exhaustion and injury risk prediction, potentially increasing model accuracy.\n",
    "- **Support Preventive Interventions:** Use the injury index as a real-time tool to flag potentially compromised movements, guiding corrective actions or modifications in training.\n",
    "\n",
    "In summary, incorporating extreme ROM metrics (using the thresholds from the CDC resource) into your pipeline could indeed provide a valuable “meter” that measures the degree of flexion towards the extremes. This, in turn, serves as an injury index that captures over-stress in compromised positions—enhancing your ability to predict and ultimately prevent injuries.\n",
    "\n",
    "\n",
    "Below is a discussion of how you might add abnormal joint ROM patterns and additional metrics—ultimately building an “injury index”—into your workflow. This discussion expands on the concept using the normative thresholds from the CDC resource and explains step by step how these extra metrics can be computed, integrated, and used to flag over-stress or compromised positions that might increase injury risk.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Concept Overview\n",
    "\n",
    "Our goal is to enhance the existing biomechanical pipeline by adding range-of-motion (ROM) metrics that capture not only whether a joint is moving but whether it is operating within a healthy, normative range. Using the CDC thresholds as guidelines (for example, shoulder flexion is normally 0°–180°, knee flexion is normally 0°–135°, etc.), we can flag instances where a joint’s ROM is either too restricted or too excessive. These abnormal values may indicate:\n",
    "  \n",
    "- **Stiffness or limited mobility:** Suggesting potential overuse, injury, or compensatory movement patterns.\n",
    "- **Hypermobility:** Which might also lead to instability and increase the risk of injury.\n",
    "\n",
    "By quantifying how far a joint’s ROM deviates from the normative range, we can create additional features that serve as predictors. When combined with energy, power, and fatigue metrics, these ROM-derived metrics could help form an “injury index” that measures the cumulative stress on an athlete’s joints.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Calculating Abnormal ROM Metrics\n",
    "\n",
    "**Step A. Data Extraction & ROM Calculation**\n",
    "\n",
    "- **Extract Joint Angle Data:**  \n",
    "  For each trial (or free throw), extract the time series for each joint’s angle. For instance, for the knee, use the column `knee_angle`.\n",
    "\n",
    "- **Compute ROM:**  \n",
    "  Calculate the ROM per trial as:\n",
    "  \\[\n",
    "  \\text{ROM} = \\max(\\text{joint\\_angle}) - \\min(\\text{joint\\_angle})\n",
    "  \\]\n",
    "  For example, if during a trial, `knee_angle` varies between 30° and 140°, the ROM is 110°.\n",
    "\n",
    "**Step B. Compare with Normative Thresholds**\n",
    "\n",
    "Using the CDC resource, we have normative ranges—for instance:\n",
    "  \n",
    "- **Knee:**  \n",
    "  - Normal flexion: 0° to 135°\n",
    "  - Abnormal/Extreme: A ROM below 120° might be too restricted (stiff), and above 135° could suggest hypermobility.\n",
    "\n",
    "You would then:\n",
    "  \n",
    "- **Compute Deviation Metrics:**  \n",
    "  Determine how far the measured ROM is from the “ideal” range. For the knee:\n",
    "  ```python\n",
    "  normal_min_knee, normal_max_knee = 120, 135  # Example thresholds\n",
    "  data['knee_ROM'] = data.groupby('trial_id')['knee_angle'].transform(lambda x: x.max() - x.min())\n",
    "  data['knee_ROM_deviation'] = np.maximum(0, normal_min_knee - data['knee_ROM']) + np.maximum(0, data['knee_ROM'] - normal_max_knee)\n",
    "  ```\n",
    "  This continuous deviation score indicates how far the knee is from its optimal ROM.\n",
    "\n",
    "- **Create Binary Flags:**  \n",
    "  Alternatively, or additionally, create a flag to indicate if the ROM is extreme:\n",
    "  ```python\n",
    "  data['knee_ROM_extreme'] = ((data['knee_ROM'] < normal_min_knee) | (data['knee_ROM'] > normal_max_knee)).astype(int)\n",
    "  ```\n",
    "\n",
    "Similar calculations would be applied to other joints (shoulder, elbow, wrist, hip, ankle) using their respective normative ranges.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Building an Injury Index\n",
    "\n",
    "Once you have the abnormal ROM metrics for key joints, you can combine these to form an overall “injury index.” This index quantifies how much an athlete’s joints are operating outside of their healthy ranges:\n",
    "\n",
    "- **Weighted Sum or Composite Score:**  \n",
    "  For example, if you have deviation scores for the knee, shoulder, and hip, you might compute:\n",
    "  ```python\n",
    "  data['injury_index'] = (w1 * data['knee_ROM_deviation'] + \n",
    "                          w2 * data['shoulder_ROM_deviation'] + \n",
    "                          w3 * data['hip_ROM_deviation'])\n",
    "  ```\n",
    "  Here, *w1*, *w2*, and *w3* are weights you can set based on domain expertise or derived from feature importance analysis. Alternatively, you could sum the binary flags to get a count of joints with abnormal ROM.\n",
    "\n",
    "- **Interpreting the Index:**  \n",
    "  A higher injury index means that, across multiple joints, the athlete is operating further outside normative ranges. This could signal that the body is under stress or that certain joints are compensating—both of which are risk factors for injury.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Integrating into the Workflow\n",
    "\n",
    "#### **A. During Data Preparation and Feature Engineering:**\n",
    "\n",
    "- **Calculate ROM Metrics:**  \n",
    "  Add a routine in your joint feature preparation step that calculates the ROM for each joint, compares it to normative thresholds, and computes both deviation scores and binary flags.\n",
    "\n",
    "- **Include in Feature Set:**  \n",
    "  Integrate these ROM metrics alongside your existing features (joint_energy, joint_power, energy_acceleration, simulated_HR, etc.). This way, when you perform lagged and rolling calculations, the ROM data is also available for analysis.\n",
    "\n",
    "#### **B. In Predictive Modeling:**\n",
    "\n",
    "- **Injury Risk Prediction Model:**  \n",
    "  Add the abnormal ROM metrics (e.g., knee_ROM_deviation, knee_ROM_extreme, etc.) as additional predictors (X). The hypothesis is that if an athlete’s ROM is frequently outside the normative range, this could indicate increased injury risk.\n",
    "\n",
    "- **Exhaustion Prediction Model:**  \n",
    "  While ROM might be less directly linked to exhaustion, abnormal or highly variable ROM might also indicate inefficiencies in movement that contribute to fatigue. Thus, these metrics can serve as secondary predictors.\n",
    "\n",
    "- **Analysis & Interpretation:**  \n",
    "  Use feature importance analyses (e.g., SHAP values) to assess how these new ROM features contribute to predicting injury risk. If they emerge as significant predictors, they can guide further intervention strategies.\n",
    "\n",
    "#### **C. Real-Time Monitoring and Feedback:**\n",
    "\n",
    "- **Dynamic Injury Index:**  \n",
    "  In a real-time monitoring system, compute the injury index on the fly during a free throw or training session. If the index exceeds a critical threshold, the system can alert coaches or trainers to adjust the athlete’s workload or technique.\n",
    "\n",
    "- **Longitudinal Monitoring:**  \n",
    "  Track changes in the injury index over time. If an athlete’s index begins to increase, it may indicate deteriorating joint function or increased stress—prompting early intervention.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Final Thoughts on Implementation\n",
    "\n",
    "By incorporating these abnormal ROM metrics into your workflow, you create an additional “meter” that quantifies how far an athlete’s joint movements deviate from healthy norms. This injury index, derived from a combination of deviation scores or binary flags from key joints, serves several purposes:\n",
    "\n",
    "- **Enhanced Prediction:**  \n",
    "  It improves injury risk models by adding information on movement quality and joint health.\n",
    "\n",
    "- **Preventive Insight:**  \n",
    "  It allows for early detection of abnormal movement patterns, enabling timely corrective interventions.\n",
    "\n",
    "- **Holistic View:**  \n",
    "  Combined with energy, power, and fatigue metrics, it provides a comprehensive picture of an athlete’s biomechanical function.\n",
    "\n",
    "Integrating these features into your existing pipeline means that your data preparation, feature engineering, and modeling processes will now capture not only the intensity of movement but also its quality—ultimately leading to better predictions and more actionable insights for injury prevention.\n",
    "\n",
    "This approach, based on CDC normative thresholds, transforms raw joint angle data into meaningful metrics that inform both clinical assessments and real-time coaching interventions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Data Loading and Merging\n",
    "\n",
    "    Load the Raw Data:\n",
    "        CSV Input:\n",
    "        The main CSV file contains free throw data captured every 0.33 seconds across 125 trials. Each row represents a frame, providing highly granular time series information.\n",
    "        JSON Input:\n",
    "        Participant information (e.g., anthropometrics) is loaded from a JSON file.\n",
    "    Merge the Data Sources:\n",
    "        A participant ID is added as a new column in the CSV data.\n",
    "        The CSV and JSON data are merged on the participant ID.\n",
    "    Debugging and Logging:\n",
    "        After the merge, the pipeline logs the new dataset shape and prints sample column values to ensure the merge was successful.\n",
    "\n",
    "2. Preparing Joint Features\n",
    "\n",
    "    Renaming and Cleaning:\n",
    "        Rename columns for clarity (for example, height_in_meters is renamed to player_height_in_meters and weight__in_kg becomes player_weight__in_kg).\n",
    "    Aggregating Joint Data:\n",
    "        Identify Columns:\n",
    "        All columns related to joint energy (e.g., columns with “_energy”) and ongoing power (e.g., columns with “_power”) are identified.\n",
    "        Create Aggregated Features:\n",
    "            joint_energy:\n",
    "            The sum of all individual joint energy columns.\n",
    "            joint_power:\n",
    "            The sum of all individual joint power columns.\n",
    "    New Feature Creation:\n",
    "        Energy Acceleration:\n",
    "        Compute the change in joint_energy divided by the change in continuous_frame_time (i.e., the derivative) to capture instantaneous energy acceleration.\n",
    "        Implementation Example:\n",
    "\n",
    "energy_acceleration = diff(joint_energy) / diff(continuous_frame_time)\n",
    "\n",
    "Ankle Power Ratio:\n",
    "Calculate the ratio between left and right ankle ongoing power (with a small constant in the denominator for numerical stability).\n",
    "Implementation Example:\n",
    "\n",
    "        ankle_power_ratio = L_ANKLE_power / (R_ANKLE_power + 1e-6)\n",
    "\n",
    "    Ordering and Sorting:\n",
    "        Data is sorted by participant_id and continuous_frame_time to maintain the correct temporal order.\n",
    "    Asymmetry Calculation:\n",
    "        Compute asymmetry features such as elbow_asymmetry and knee_asymmetry by taking the absolute difference between the left and right joint energy measurements.\n",
    "    Exhaustion and Simulated Heart Rate:\n",
    "        exhaustion_rate:\n",
    "        Computed as the change in the exhaustion score over time.\n",
    "        simulated_HR:\n",
    "        A synthetic heart rate generated based on the exhaustion score and joint energy values.\n",
    "\n",
    "3. Feature Engineering\n",
    "\n",
    "    Lagged and Rolling Features:\n",
    "        Lagged Features:\n",
    "        Create a one-step lag for the exhaustion score to capture temporal dependencies.\n",
    "        Rolling Averages and Standard Deviations:\n",
    "        Calculate rolling statistics for features such as joint_power and now also joint_energy (using a fixed window, e.g., 5 frames).\n",
    "        Optional New Feature – Rolling Energy Standard Deviation:\n",
    "        Compute the rolling standard deviation of joint_energy to capture its short-term variability.\n",
    "    Temporal Calculations:\n",
    "        Time Since Start:\n",
    "        Compute the elapsed time since the beginning of the session using the continuous_frame_time column.\n",
    "        Exponential Moving Average:\n",
    "        Smooth the exhaustion score using an exponential moving average.\n",
    "    Injury Risk Simulation:\n",
    "        A binary injury risk label is derived by comparing the rolling sum of the exhaustion score (calculated over a longer window, e.g., 20 frames) against the 75th percentile.\n",
    "    Handling Missing Values:\n",
    "        Any rows with missing data (which may result from shifting or rolling operations) are dropped.\n",
    "\n",
    "4. Simulated Player Metrics\n",
    "\n",
    "    Additional Simulated Features:\n",
    "        Alternative Simulated Heart Rate (simulated_HR_fake):\n",
    "        Uses a different formula based on exhaustion and normalized joint energy.\n",
    "        Fatigue Index and Rate:\n",
    "        Compute a synthetic fatigue index and its frame-to-frame rate of change.\n",
    "        HR Variability:\n",
    "        Calculate the rolling standard deviation of the simulated heart rate.\n",
    "    Purpose:\n",
    "    These additional metrics help provide extra insight into player fatigue and can be cross-validated with the primary metrics.\n",
    "\n",
    "5. Analysis Functions\n",
    "\n",
    "The pipeline includes multiple analysis functions to explore different aspects of the free throw and movement data:\n",
    "\n",
    "    Joint-Specific Analysis:\n",
    "        Energy Distributions:\n",
    "        Boxplots are generated to view the energy distribution for each joint.\n",
    "        Injury Risk:\n",
    "        Joint-specific injury risk is analyzed by comparing joint energies across groups with low and high injury risk.\n",
    "        Cumulative Energy Patterns:\n",
    "        Energy accumulation over time is visualized using cumulative sum plots.\n",
    "    Movement Pattern Analysis:\n",
    "        Angular Velocity Histograms:\n",
    "        Density plots (with KDE) are created for any angular velocity features.\n",
    "        Asymmetry Relationships:\n",
    "        Pairplots are generated to display relationships among asymmetry metrics alongside injury risk.\n",
    "    Temporal Analysis:\n",
    "        Lagged Correlations:\n",
    "        Lagged correlations between joint_energy and the exhaustion score are computed to examine temporal dependencies.\n",
    "        Autocorrelation Plots:\n",
    "        Autocorrelation of joint_energy is visualized to understand repeating patterns over time.\n",
    "    Multivariate Analysis:\n",
    "        3D Visualizations:\n",
    "        Separate 3D scatter plots are created for left- and right-sided joint energies, with colors indicating injury risk.\n",
    "        Clustering:\n",
    "        KMeans clustering is applied on subsets of features to identify distinct movement patterns.\n",
    "    Statistical Testing:\n",
    "        Mann–Whitney U Tests:\n",
    "        For each joint energy column, a nonparametric test compares groups with low and high injury risk.\n",
    "    Fatigue–Injury Interaction Analysis:\n",
    "        Fatigue-Injury Matrix:\n",
    "        A heatmap visualizes the relationship between binned exhaustion scores and joint energy.\n",
    "        SHAP Interaction Analysis:\n",
    "        A Random Forest model is used to study interaction effects (for example, combining player height and joint energy) using SHAP values.\n",
    "\n",
    "6. Preprocessing and LSTM Setup\n",
    "\n",
    "    Temporal Train–Test Split:\n",
    "        The data is split based on time (with earlier frames for training and later frames for testing) to preserve temporal order.\n",
    "    Feature Scaling:\n",
    "        Features are scaled using a standard scaler to enable efficient convergence of the LSTM model.\n",
    "    Sequence Creation:\n",
    "        The data is restructured into sequences (e.g., groups of 5 consecutive frames) to feed into the LSTM model, preserving the free throw motion’s time dependencies.\n",
    "\n",
    "7. Model Building, Training, and Evaluation\n",
    "\n",
    "    LSTM Model Architecture:\n",
    "        Enhanced Architecture:\n",
    "        The model includes two LSTM layers with dropout regularization, followed by a dense hidden layer.\n",
    "        Output Layer Choice:\n",
    "            For regression (predicting the exhaustion score), a linear output is used.\n",
    "            For classification (predicting injury risk), a sigmoid activation is applied.\n",
    "    Training Process:\n",
    "        The model is trained with early stopping based on validation loss to avoid overfitting.\n",
    "        Training is performed using the sequences generated from the scaled data.\n",
    "    Evaluation:\n",
    "        Regression Metrics:\n",
    "        Mean Absolute Error (MAE) and R² are computed.\n",
    "        Classification Metrics:\n",
    "        Accuracy, precision, recall, F1-score, ROC-AUC, and confusion matrix visualizations are provided.\n",
    "    Visualization:\n",
    "        Predictions over time are plotted against the actual values to visualize model performance.\n",
    "        Injury risk probabilities are plotted alongside actual injury events to provide further insight.\n",
    "\n",
    "Future Improvements\n",
    "\n",
    "    Pre-Workout Baseline Evaluation:\n",
    "    The next iteration of the pipeline will include evaluating the player's activity and energy levels from the day before the workout. This baseline metric will be captured and stored for comparison.\n",
    "    Dynamic Monitoring During the Workout:\n",
    "    Once the baseline is established, the same metric will be continuously monitored during the workout. The goal is to compare in-session activity against the baseline, thereby providing a real-time indicator of potential injury and fatigue.\n",
    "    Integration with Real-Time Analytics:\n",
    "    Future versions may incorporate real-time prediction and alerting systems that use the evolving metrics to flag abnormal fatigue or elevated injury risk, enabling preemptive interventions.\n",
    "    Range of Motion Analysis:\n",
    "    As an additional future improvement, incorporate analysis of joint range-of-motion:\n",
    "        Identify Unusual Ranges:\n",
    "        Calculate and monitor the range (maximum–minimum) for each joint’s movement during the free throw.\n",
    "        Use as a Predictor:\n",
    "        Unusual joint ranges (outside of typical values) can be flagged as potential risk factors and used as predictors for injury. This metric could help in understanding the impact on each joint and may offer additional insight when combined with energy and fatigue metrics.\n",
    "\n",
    "\n",
    "8. Predictor and Target Variables (X and y)\n",
    "\n",
    "In our pipeline, we build two separate predictive models: one for forecasting the exhaustion score and one for predicting injury risk. Understanding which variables serve as inputs (predictors) and which are our outcomes (targets) is essential to interpret our modeling strategy and its results.\n",
    "8.1 Exhaustion Prediction\n",
    "\n",
    "Target Variable (y):\n",
    "\n",
    "    by_trial_exhaustion_score\n",
    "        Definition: This continuous numeric variable represents the exhaustion level measured (or computed) for each trial.\n",
    "        Role: It is the outcome we aim to predict using regression. Higher scores typically indicate a greater degree of fatigue.\n",
    "\n",
    "Predictor Variables (X):\n",
    "The following features are used to predict the exhaustion score:\n",
    "\n",
    "    joint_power\n",
    "        Definition: The aggregated sum of all instantaneous joint power measurements.\n",
    "        Rationale: Represents overall power output during the free throw motion.\n",
    "    joint_energy\n",
    "        Definition: The aggregated sum of energy values across joints.\n",
    "        Rationale: Reflects the total energy expenditure during the motion.\n",
    "    elbow_asymmetry\n",
    "        Definition: The absolute difference between the left and right elbow energy values.\n",
    "        Rationale: Imbalances in elbow function may correlate with overall fatigue.\n",
    "    wrist_angle\n",
    "        Definition: The angle of the wrist at ball release.\n",
    "        Rationale: Captures the kinematic aspect of the throw which may be related to fatigue.\n",
    "    exhaustion_lag1\n",
    "        Definition: The one-frame lag of the exhaustion score.\n",
    "        Rationale: Incorporates temporal dependencies, allowing the model to understand recent fatigue trends.\n",
    "    power_avg_5\n",
    "        Definition: The rolling average of joint power computed over a fixed window (e.g., 5 frames).\n",
    "        Rationale: Provides a smoothed measure of power output that reduces noise.\n",
    "    simulated_HR\n",
    "        Definition: A synthetic heart rate computed based on exhaustion and energy values.\n",
    "        Rationale: Acts as a proxy for physiological stress or fatigue.\n",
    "    player_height_in_meters and player_weight__in_kg\n",
    "        Definition: Anthropometric measures of the player.\n",
    "        Rationale: These may moderate the relationship between movement metrics and fatigue.\n",
    "    (Additional features from preprocessing):\n",
    "        Metrics such as energy_acceleration, ankle_power_ratio, and rolling_energy_std are also computed. Although they may not be included in the default feature lists, they are available for further exploration as predictors.\n",
    "            energy_acceleration: Captures the rate of change of energy expenditure.\n",
    "            ankle_power_ratio: Reflects lower-limb power symmetry.\n",
    "            rolling_energy_std: Indicates variability in joint energy over short time spans.\n",
    "\n",
    "\n",
    "Deep Dive Summary for Exhaustion:\n",
    "The model uses these X features to learn how patterns in biomechanical output (power, energy, and asymmetries) and derived physiological proxies (simulated HR, rolling averages, etc.) relate to the player's level of fatigue. By including lagged features and rolling statistics, the model can capture temporal dynamics and smooth short-term fluctuations, ultimately predicting the exhaustion score for each trial.\n",
    "8.2 Injury Risk Prediction\n",
    "\n",
    "Target Variable (y):\n",
    "\n",
    "    injury_risk\n",
    "        Definition: A binary indicator where 0 denotes low injury risk and 1 indicates high injury risk.\n",
    "        Role: It is the outcome for classification. The label is derived by comparing the rolling sum of exhaustion scores against a threshold (e.g., the 75th percentile), and it may later be enriched with additional predictors such as joint range-of-motion.\n",
    "\n",
    "Predictor Variables (X):\n",
    "The features used for injury risk prediction are similar to those for exhaustion but with some emphasis on metrics that capture asymmetries and imbalances:\n",
    "\n",
    "    joint_power\n",
    "        Definition: Overall power output.\n",
    "    joint_energy\n",
    "        Definition: Total energy expenditure.\n",
    "    elbow_asymmetry\n",
    "        Definition: Imbalance between left and right elbow energy.\n",
    "    knee_asymmetry\n",
    "        Definition: Imbalance between left and right knee energy.\n",
    "    wrist_angle\n",
    "        Definition: The wrist angle at ball release.\n",
    "    exhaustion_lag1\n",
    "        Definition: The previous frame’s exhaustion score, highlighting temporal trends.\n",
    "    power_avg_5\n",
    "        Definition: Rolling average of joint power.\n",
    "    simulated_HR\n",
    "        Definition: Synthetic heart rate based on the interplay between exhaustion and energy.\n",
    "    player_height_in_meters and player_weight__in_kg\n",
    "        Definition: Anthropometric information which may affect injury susceptibility.\n",
    "    (Additional features for future work):\n",
    "        Range of Motion Metrics:\n",
    "            Definition: Calculated as the range (maximum – minimum) of each joint’s movement over a trial.\n",
    "            Rationale: Unusual ranges (too high or too low) might indicate improper technique or joint stress, serving as a potential predictor for injury.\n",
    "        Other Derived Metrics:\n",
    "            Metrics such as energy_acceleration, ankle_power_ratio, and rolling_energy_std (as described above) can be incorporated to enhance the injury risk model by capturing sudden changes, asymmetries, or high variability in movement.\n",
    "\n",
    "Deep Dive Summary for Injury Risk:\n",
    "For injury risk prediction, the model leverages biomechanical and physiological metrics that may indicate improper movement patterns or excessive load. By combining aggregate measures (joint energy and power) with asymmetry features and potentially range-of-motion metrics, the model aims to identify when a player's movement deviates from typical patterns—deviations that may predispose them to injury. The inclusion of both static (anthropometrics) and dynamic (temporal and rolling features) data ensures that the prediction captures both inherent risk factors and situational stresses.\n",
    "8.3 Summary: How X and y Variables Work Together\n",
    "\n",
    "    Exhaustion Prediction:\n",
    "        Y Variable: Continuous exhaustion score (by_trial_exhaustion_score).\n",
    "        X Metrics: A combination of aggregated power and energy outputs, asymmetry measures, lagged and rolling statistics, and physiological proxies (simulated HR), along with anthropometrics.\n",
    "    Injury Risk Prediction:\n",
    "        Y Variable: Binary injury risk indicator (injury_risk).\n",
    "        X Metrics: Similar to exhaustion but with an added focus on asymmetry (e.g., knee_asymmetry), plus potential future predictors like joint range-of-motion. These metrics capture both the immediate and cumulative stresses on the joints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Loaded data from ../../data/processed/final_granular_dataset.csv with shape (16047, 214)\n",
      "INFO: Added 'participant_id' column with value 'P0001'\n",
      "INFO: Loaded participant information from ../../data/basketball/freethrow/participant_information.json\n",
      "INFO: Merged participant data. New shape: (16047, 217)\n",
      "INFO: Step [load_data] completed.\n",
      "INFO: Step [calculate_joint_angles] completed.\n",
      "INFO: Renamed participant anthropometrics.\n",
      "INFO: Identified 15 joint energy and 14 joint power columns.\n",
      "INFO: Created aggregated 'joint_energy' and 'joint_power'.\n",
      "INFO: Created 'energy_acceleration' as derivative of joint_energy over time.\n",
      "INFO: Created 'ankle_power_ratio' feature comparing left to right ankle ongoing power.\n",
      "INFO: Created asymmetry feature: hip_asymmetry\n",
      "INFO: Created asymmetry feature: ankle_asymmetry\n",
      "INFO: Created asymmetry feature: wrist_asymmetry\n",
      "INFO: Created asymmetry feature: elbow_asymmetry\n",
      "INFO: Created asymmetry feature: knee_asymmetry\n",
      "INFO: Created asymmetry feature: 1stfinger_asymmetry\n",
      "INFO: Created asymmetry feature: 5thfinger_asymmetry\n",
      "INFO: Created power ratio feature: hip_power_ratio using columns L_HIP_ongoing_power and R_HIP_ongoing_power\n",
      "INFO: Created power ratio feature: ankle_power_ratio using columns L_ANKLE_ongoing_power and R_ANKLE_ongoing_power\n",
      "INFO: Created power ratio feature: wrist_power_ratio using columns L_WRIST_ongoing_power and R_WRIST_ongoing_power\n",
      "INFO: Created power ratio feature: elbow_power_ratio using columns L_ELBOW_ongoing_power and R_ELBOW_ongoing_power\n",
      "INFO: Created power ratio feature: knee_power_ratio using columns L_KNEE_ongoing_power and R_KNEE_ongoing_power\n",
      "INFO: Created power ratio feature: 1stfinger_power_ratio using columns L_1STFINGER_ongoing_power and R_1STFINGER_ongoing_power\n",
      "INFO: Created power ratio feature: 5thfinger_power_ratio using columns L_5THFINGER_ongoing_power and R_5THFINGER_ongoing_power\n",
      "INFO: Computed ROM for L KNEE as L_KNEE_ROM\n",
      "INFO: Computed ROM deviation for L KNEE as L_KNEE_ROM_deviation\n",
      "INFO: Created binary flag for L KNEE ROM extremes: L_KNEE_ROM_extreme\n",
      "INFO: Computed ROM for R KNEE as R_KNEE_ROM\n",
      "INFO: Computed ROM deviation for R KNEE as R_KNEE_ROM_deviation\n",
      "INFO: Created binary flag for R KNEE ROM extremes: R_KNEE_ROM_extreme\n",
      "INFO: Computed ROM for L SHOULDER as L_SHOULDER_ROM\n",
      "INFO: Computed ROM deviation for L SHOULDER as L_SHOULDER_ROM_deviation\n",
      "INFO: Created binary flag for L SHOULDER ROM extremes: L_SHOULDER_ROM_extreme\n",
      "INFO: Computed ROM for R SHOULDER as R_SHOULDER_ROM\n",
      "INFO: Computed ROM deviation for R SHOULDER as R_SHOULDER_ROM_deviation\n",
      "INFO: Created binary flag for R SHOULDER ROM extremes: R_SHOULDER_ROM_extreme\n",
      "INFO: Computed ROM for L HIP as L_HIP_ROM\n",
      "INFO: Computed ROM deviation for L HIP as L_HIP_ROM_deviation\n",
      "INFO: Created binary flag for L HIP ROM extremes: L_HIP_ROM_extreme\n",
      "INFO: Computed ROM for R HIP as R_HIP_ROM\n",
      "INFO: Computed ROM deviation for R HIP as R_HIP_ROM_deviation\n",
      "INFO: Created binary flag for R HIP ROM extremes: R_HIP_ROM_extreme\n",
      "INFO: Computed ROM for L ANKLE as L_ANKLE_ROM\n",
      "INFO: Computed ROM deviation for L ANKLE as L_ANKLE_ROM_deviation\n",
      "INFO: Created binary flag for L ANKLE ROM extremes: L_ANKLE_ROM_extreme\n",
      "INFO: Computed ROM for R ANKLE as R_ANKLE_ROM\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joint energy columns:  ['L_ANKLE_energy', 'R_ANKLE_energy', 'L_KNEE_energy', 'R_KNEE_energy', 'L_HIP_energy', 'R_HIP_energy', 'L_ELBOW_energy', 'R_ELBOW_energy', 'L_WRIST_energy', 'R_WRIST_energy', 'L_1STFINGER_energy', 'R_1STFINGER_energy', 'L_5THFINGER_energy', 'R_5THFINGER_energy', 'total_energy']\n",
      "Joint power columns:  ['L_ANKLE_ongoing_power', 'R_ANKLE_ongoing_power', 'L_KNEE_ongoing_power', 'R_KNEE_ongoing_power', 'L_HIP_ongoing_power', 'R_HIP_ongoing_power', 'L_ELBOW_ongoing_power', 'R_ELBOW_ongoing_power', 'L_WRIST_ongoing_power', 'R_WRIST_ongoing_power', 'L_1STFINGER_ongoing_power', 'L_5THFINGER_ongoing_power', 'R_1STFINGER_ongoing_power', 'R_5THFINGER_ongoing_power']\n",
      "All angle columns:  ['entry_angle', 'elbow_angle', 'wrist_angle', 'knee_angle', 'initial_release_angle', 'calculated_release_angle', 'angle_difference', 'optimal_release_angle', 'L_SHOULDER_angle', 'R_SHOULDER_angle', 'L_HIP_angle', 'R_HIP_angle', 'L_KNEE_angle', 'R_KNEE_angle', 'L_ANKLE_angle', 'R_ANKLE_angle']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Computed ROM deviation for R ANKLE as R_ANKLE_ROM_deviation\n",
      "INFO: Created binary flag for R ANKLE ROM extremes: R_ANKLE_ROM_extreme\n",
      "INFO: Angle column 'L_WRIST_angle' not found; skipping ROM metrics for L WRIST.\n",
      "INFO: Angle column 'R_WRIST_angle' not found; skipping ROM metrics for R WRIST.\n",
      "INFO: Sorted data by 'participant_id' and 'continuous_frame_time'.\n",
      "INFO: Created 'exhaustion_rate' feature.\n",
      "INFO: Created 'simulated_HR' feature.\n",
      "INFO: Step [prepare_joint_features] completed.\n",
      "C:\\Users\\ghadf\\AppData\\Local\\Temp\\ipykernel_6972\\61768157.py:394: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  data[motion_cols] = data[motion_cols].fillna(method='ffill').fillna(0)\n",
      "INFO: Created 'rolling_energy_std' with sample: [0.0, 0.6225242794725895, 0.592331668013902, 0.5469698974921113, 0.5031647476376774, 0.0346469804150002, 0.05501279562072104, 0.06384083445684284, 0.05698938811919827, 0.039917330417864196]\n",
      "INFO: Created 'rolling_energy_std' with window 5.\n",
      "C:\\Users\\ghadf\\AppData\\Local\\Temp\\ipykernel_6972\\61768157.py:443: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{joint_name}_rolling_exhaustion'] = data[score_col].rolling(rolling_window, min_periods=1).sum()\n",
      "C:\\Users\\ghadf\\AppData\\Local\\Temp\\ipykernel_6972\\61768157.py:447: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{joint_name}_injury_risk'] = (rolling_series > safe_expanding_quantile(rolling_series)).astype(int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "print all the columns with by_trial_exhaustion_score:  ['by_trial_exhaustion_score', 'L_ANKLE_energy_by_trial_exhaustion_score', 'R_ANKLE_energy_by_trial_exhaustion_score', 'L_KNEE_energy_by_trial_exhaustion_score', 'R_KNEE_energy_by_trial_exhaustion_score', 'L_HIP_energy_by_trial_exhaustion_score', 'R_HIP_energy_by_trial_exhaustion_score', 'L_ELBOW_energy_by_trial_exhaustion_score', 'R_ELBOW_energy_by_trial_exhaustion_score', 'L_WRIST_energy_by_trial_exhaustion_score', 'R_WRIST_energy_by_trial_exhaustion_score', 'L_1STFINGER_energy_by_trial_exhaustion_score', 'R_1STFINGER_energy_by_trial_exhaustion_score', 'L_5THFINGER_energy_by_trial_exhaustion_score', 'R_5THFINGER_energy_by_trial_exhaustion_score']\n",
      "['trial_id', 'result', 'landing_x', 'landing_y', 'entry_angle', 'frame_time', 'ball_x', 'ball_y', 'ball_z', 'R_EYE_x', 'R_EYE_y', 'R_EYE_z', 'L_EYE_x', 'L_EYE_y', 'L_EYE_z', 'NOSE_x', 'NOSE_y', 'NOSE_z', 'R_EAR_x', 'R_EAR_y', 'R_EAR_z', 'L_EAR_x', 'L_EAR_y', 'L_EAR_z', 'R_SHOULDER_x', 'R_SHOULDER_y', 'R_SHOULDER_z', 'L_SHOULDER_x', 'L_SHOULDER_y', 'L_SHOULDER_z', 'R_ELBOW_x', 'R_ELBOW_y', 'R_ELBOW_z', 'L_ELBOW_x', 'L_ELBOW_y', 'L_ELBOW_z', 'R_WRIST_x', 'R_WRIST_y', 'R_WRIST_z', 'L_WRIST_x', 'L_WRIST_y', 'L_WRIST_z', 'R_HIP_x', 'R_HIP_y', 'R_HIP_z', 'L_HIP_x', 'L_HIP_y', 'L_HIP_z', 'R_KNEE_x', 'R_KNEE_y', 'R_KNEE_z', 'L_KNEE_x', 'L_KNEE_y', 'L_KNEE_z', 'R_ANKLE_x', 'R_ANKLE_y', 'R_ANKLE_z', 'L_ANKLE_x', 'L_ANKLE_y', 'L_ANKLE_z', 'R_1STFINGER_x', 'R_1STFINGER_y', 'R_1STFINGER_z', 'R_5THFINGER_x', 'R_5THFINGER_y', 'R_5THFINGER_z', 'L_1STFINGER_x', 'L_1STFINGER_y', 'L_1STFINGER_z', 'L_5THFINGER_x', 'L_5THFINGER_y', 'L_5THFINGER_z', 'R_1STTOE_x', 'R_1STTOE_y', 'R_1STTOE_z', 'R_5THTOE_x', 'R_5THTOE_y', 'R_5THTOE_z', 'L_1STTOE_x', 'L_1STTOE_y', 'L_1STTOE_z', 'L_5THTOE_x', 'L_5THTOE_y', 'L_5THTOE_z', 'R_CALC_x', 'R_CALC_y', 'R_CALC_z', 'L_CALC_x', 'L_CALC_y', 'L_CALC_z', 'ball_speed', 'ball_velocity_x', 'ball_velocity_y', 'ball_velocity_z', 'overall_ball_velocity', 'ball_direction_x', 'ball_direction_y', 'ball_direction_z', 'computed_ball_velocity_x', 'computed_ball_velocity_y', 'computed_ball_velocity_z', 'dist_ball_R_1STFINGER', 'dist_ball_R_5THFINGER', 'dist_ball_L_1STFINGER', 'dist_ball_L_5THFINGER', 'ball_in_hands', 'shooting_motion', 'avg_shoulder_height', 'release_point_filter', 'dt', 'dx', 'dy', 'dz', 'L_ANKLE_ongoing_power', 'R_ANKLE_ongoing_power', 'L_KNEE_ongoing_power', 'R_KNEE_ongoing_power', 'L_HIP_ongoing_power', 'R_HIP_ongoing_power', 'L_ELBOW_ongoing_power', 'R_ELBOW_ongoing_power', 'L_WRIST_ongoing_power', 'R_WRIST_ongoing_power', 'L_1STFINGER_ongoing_power', 'L_5THFINGER_ongoing_power', 'R_1STFINGER_ongoing_power', 'R_5THFINGER_ongoing_power', 'elbow_angle', 'wrist_angle', 'knee_angle', 'player_height_in_meters', 'player_height_ft', 'initial_release_angle', 'calculated_release_angle', 'angle_difference', 'distance_to_basket', 'optimal_release_angle', 'by_trial_time', 'continuous_frame_time', 'L_ANKLE_energy', 'R_ANKLE_energy', 'L_KNEE_energy', 'R_KNEE_energy', 'L_HIP_energy', 'R_HIP_energy', 'L_ELBOW_energy', 'R_ELBOW_energy', 'L_WRIST_energy', 'R_WRIST_energy', 'L_1STFINGER_energy', 'R_1STFINGER_energy', 'L_5THFINGER_energy', 'R_5THFINGER_energy', 'total_energy', 'by_trial_energy', 'by_trial_exhaustion_score', 'overall_cumulative_energy', 'overall_exhaustion_score', 'L_ANKLE_energy_by_trial', 'L_ANKLE_energy_by_trial_exhaustion_score', 'L_ANKLE_energy_overall_cumulative', 'L_ANKLE_energy_overall_exhaustion_score', 'R_ANKLE_energy_by_trial', 'R_ANKLE_energy_by_trial_exhaustion_score', 'R_ANKLE_energy_overall_cumulative', 'R_ANKLE_energy_overall_exhaustion_score', 'L_KNEE_energy_by_trial', 'L_KNEE_energy_by_trial_exhaustion_score', 'L_KNEE_energy_overall_cumulative', 'L_KNEE_energy_overall_exhaustion_score', 'R_KNEE_energy_by_trial', 'R_KNEE_energy_by_trial_exhaustion_score', 'R_KNEE_energy_overall_cumulative', 'R_KNEE_energy_overall_exhaustion_score', 'L_HIP_energy_by_trial', 'L_HIP_energy_by_trial_exhaustion_score', 'L_HIP_energy_overall_cumulative', 'L_HIP_energy_overall_exhaustion_score', 'R_HIP_energy_by_trial', 'R_HIP_energy_by_trial_exhaustion_score', 'R_HIP_energy_overall_cumulative', 'R_HIP_energy_overall_exhaustion_score', 'L_ELBOW_energy_by_trial', 'L_ELBOW_energy_by_trial_exhaustion_score', 'L_ELBOW_energy_overall_cumulative', 'L_ELBOW_energy_overall_exhaustion_score', 'R_ELBOW_energy_by_trial', 'R_ELBOW_energy_by_trial_exhaustion_score', 'R_ELBOW_energy_overall_cumulative', 'R_ELBOW_energy_overall_exhaustion_score', 'L_WRIST_energy_by_trial', 'L_WRIST_energy_by_trial_exhaustion_score', 'L_WRIST_energy_overall_cumulative', 'L_WRIST_energy_overall_exhaustion_score', 'R_WRIST_energy_by_trial', 'R_WRIST_energy_by_trial_exhaustion_score', 'R_WRIST_energy_overall_cumulative', 'R_WRIST_energy_overall_exhaustion_score', 'L_1STFINGER_energy_by_trial', 'L_1STFINGER_energy_by_trial_exhaustion_score', 'L_1STFINGER_energy_overall_cumulative', 'L_1STFINGER_energy_overall_exhaustion_score', 'R_1STFINGER_energy_by_trial', 'R_1STFINGER_energy_by_trial_exhaustion_score', 'R_1STFINGER_energy_overall_cumulative', 'R_1STFINGER_energy_overall_exhaustion_score', 'L_5THFINGER_energy_by_trial', 'L_5THFINGER_energy_by_trial_exhaustion_score', 'L_5THFINGER_energy_overall_cumulative', 'L_5THFINGER_energy_overall_exhaustion_score', 'R_5THFINGER_energy_by_trial', 'R_5THFINGER_energy_by_trial_exhaustion_score', 'R_5THFINGER_energy_overall_cumulative', 'R_5THFINGER_energy_overall_exhaustion_score', 'participant_id', 'L_SHOULDER_angle', 'R_SHOULDER_angle', 'L_HIP_angle', 'R_HIP_angle', 'L_KNEE_angle', 'R_KNEE_angle', 'L_ANKLE_angle', 'R_ANKLE_angle', 'player_weight__in_kg', 'joint_energy', 'joint_power', 'energy_acceleration', 'ankle_power_ratio', 'hip_asymmetry', 'ankle_asymmetry', 'wrist_asymmetry', 'elbow_asymmetry', 'knee_asymmetry', '1stfinger_asymmetry', '5thfinger_asymmetry', 'hip_power_ratio', 'wrist_power_ratio', 'elbow_power_ratio', 'knee_power_ratio', '1stfinger_power_ratio', '5thfinger_power_ratio', 'L_KNEE_ROM', 'L_KNEE_ROM_deviation', 'L_KNEE_ROM_extreme', 'R_KNEE_ROM', 'R_KNEE_ROM_deviation', 'R_KNEE_ROM_extreme', 'L_SHOULDER_ROM', 'L_SHOULDER_ROM_deviation', 'L_SHOULDER_ROM_extreme', 'R_SHOULDER_ROM', 'R_SHOULDER_ROM_deviation', 'R_SHOULDER_ROM_extreme', 'L_HIP_ROM', 'L_HIP_ROM_deviation', 'L_HIP_ROM_extreme', 'R_HIP_ROM', 'R_HIP_ROM_deviation', 'R_HIP_ROM_extreme', 'L_ANKLE_ROM', 'L_ANKLE_ROM_deviation', 'L_ANKLE_ROM_extreme', 'R_ANKLE_ROM', 'R_ANKLE_ROM_deviation', 'R_ANKLE_ROM_extreme', 'exhaustion_rate', 'simulated_HR', 'time_since_start', 'power_avg_5', 'rolling_power_std', 'rolling_hr_mean', 'rolling_energy_std', 'exhaustion_lag1', 'ema_exhaustion', 'rolling_exhaustion', 'injury_risk', 'L_ANKLE_exhaustion_rate', 'L_ANKLE_rolling_exhaustion', 'L_ANKLE_injury_risk', 'R_ANKLE_exhaustion_rate', 'R_ANKLE_rolling_exhaustion', 'R_ANKLE_injury_risk', 'L_WRIST_exhaustion_rate', 'L_WRIST_rolling_exhaustion', 'L_WRIST_injury_risk', 'R_WRIST_exhaustion_rate', 'R_WRIST_rolling_exhaustion', 'R_WRIST_injury_risk', 'L_ELBOW_exhaustion_rate', 'L_ELBOW_rolling_exhaustion', 'L_ELBOW_injury_risk', 'R_ELBOW_exhaustion_rate', 'R_ELBOW_rolling_exhaustion', 'R_ELBOW_injury_risk', 'L_KNEE_exhaustion_rate', 'L_KNEE_rolling_exhaustion', 'L_KNEE_injury_risk', 'R_KNEE_exhaustion_rate', 'R_KNEE_rolling_exhaustion', 'R_KNEE_injury_risk', 'L_HIP_exhaustion_rate', 'L_HIP_rolling_exhaustion', 'L_HIP_injury_risk', 'R_HIP_exhaustion_rate', 'R_HIP_rolling_exhaustion', 'R_HIP_injury_risk']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ghadf\\AppData\\Local\\Temp\\ipykernel_6972\\61768157.py:442: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{joint_name}_exhaustion_rate'] = data[score_col].diff() / dt\n",
      "C:\\Users\\ghadf\\AppData\\Local\\Temp\\ipykernel_6972\\61768157.py:443: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{joint_name}_rolling_exhaustion'] = data[score_col].rolling(rolling_window, min_periods=1).sum()\n",
      "C:\\Users\\ghadf\\AppData\\Local\\Temp\\ipykernel_6972\\61768157.py:447: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{joint_name}_injury_risk'] = (rolling_series > safe_expanding_quantile(rolling_series)).astype(int)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import sys\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error, r2_score, accuracy_score,\n",
    "    precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    ")\n",
    "import shap\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# HELPER FUNCTION FOR DEBUG OUTPUTS\n",
    "###############################################################################\n",
    "def _print_debug_info(step_name, df, new_columns=None, debug=False):\n",
    "    \"\"\"\n",
    "    Prints debug information about a DataFrame after a processing step.\n",
    "    \n",
    "    When debug=True, prints:\n",
    "      - The step name.\n",
    "      - The DataFrame shape.\n",
    "      - If new_columns is provided (a list of column names), prints for each:\n",
    "          • Data type and a sample of unique values (up to 5).\n",
    "    When debug=False, prints a single-line message indicating step completion.\n",
    "    \"\"\"\n",
    "    if debug:\n",
    "        logging.info(f\"Step [{step_name}]: DataFrame shape = {df.shape}\")\n",
    "        if new_columns:\n",
    "            logging.info(f\"New columns added: {new_columns}\")\n",
    "            for col in new_columns:\n",
    "                sample = df[col].dropna().unique()[:5]\n",
    "                logging.info(f\" - {col}: dtype={df[col].dtype}, sample values={sample}\")\n",
    "    else:\n",
    "        logging.info(f\"Step [{step_name}] completed.\")\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# FUNCTION DEFINITIONS\n",
    "###############################################################################\n",
    "def load_data(csv_path, json_path, participant_id='P0001', debug=False):\n",
    "    \"\"\"\n",
    "    Loads the main dataset and participant information, then merges them.\n",
    "    \n",
    "    Parameters:\n",
    "      - csv_path (str): Path to the main CSV file.\n",
    "      - json_path (str): Path to the participant information JSON file.\n",
    "      - participant_id (str): Participant identifier.\n",
    "      - debug (bool): If True, prints detailed debug info.\n",
    "    \n",
    "    Returns:\n",
    "      - data (pd.DataFrame): Merged DataFrame.\n",
    "    \"\"\"\n",
    "    # Load main dataset\n",
    "    try:\n",
    "        data = pd.read_csv(csv_path)\n",
    "        logging.info(f\"Loaded data from {csv_path} with shape {data.shape}\")\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"File not found: {csv_path}\")\n",
    "        sys.exit(1)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading {csv_path}: {e}\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    data['participant_id'] = participant_id\n",
    "    logging.info(f\"Added 'participant_id' column with value '{participant_id}'\")\n",
    "    \n",
    "    # Load participant info\n",
    "    try:\n",
    "        with open(json_path, 'r') as file:\n",
    "            participant_info = json.load(file)\n",
    "        participant_df = pd.DataFrame([participant_info])\n",
    "        logging.info(f\"Loaded participant information from {json_path}\")\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"File not found: {json_path}\")\n",
    "        sys.exit(1)\n",
    "    except json.JSONDecodeError:\n",
    "        logging.error(f\"Invalid JSON format in {json_path}\")\n",
    "        sys.exit(1)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading {json_path}: {e}\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    data = pd.merge(data, participant_df, on='participant_id', how='left')\n",
    "    logging.info(f\"Merged participant data. New shape: {data.shape}\")\n",
    "    _print_debug_info(\"load_data\", data, debug=debug)\n",
    "    return data\n",
    "\n",
    "def calculate_joint_angles(df, connections, debug=False):\n",
    "    \"\"\"\n",
    "    Calculates joint angles from coordinate data using vector mathematics.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame containing joint coordinates.\n",
    "        connections (list): Joint connections defining biomechanical segments.\n",
    "        debug (bool): Enable debug logging.\n",
    "        \n",
    "    Returns:\n",
    "        df (pd.DataFrame): Updated DataFrame with new angle columns.\n",
    "    \"\"\"\n",
    "    angle_columns = []\n",
    "    \n",
    "    # Define angle calculation points for key joints\n",
    "    # Note: The new \"KNEE\" definition uses hip, knee, and ankle as the points.\n",
    "    angle_definitions = {\n",
    "        'SHOULDER': {\n",
    "            'left': ['L_HIP', 'L_SHOULDER', 'L_ELBOW'],\n",
    "            'right': ['R_HIP', 'R_SHOULDER', 'R_ELBOW']\n",
    "        },\n",
    "        'HIP': {\n",
    "            'left': ['L_SHOULDER', 'L_HIP', 'L_KNEE'],\n",
    "            'right': ['R_SHOULDER', 'R_HIP', 'R_KNEE']\n",
    "        },\n",
    "        'KNEE': {\n",
    "            'left': ['L_HIP', 'L_KNEE', 'L_ANKLE'],\n",
    "            'right': ['R_HIP', 'R_KNEE', 'R_ANKLE']\n",
    "        },\n",
    "        'ANKLE': {\n",
    "            'left': ['L_KNEE', 'L_ANKLE', 'L_5THTOE'],\n",
    "            'right': ['R_KNEE', 'R_ANKLE', 'R_5THTOE']\n",
    "        }\n",
    "    }\n",
    "\n",
    "    for joint, sides in angle_definitions.items():\n",
    "        for side in ['left', 'right']:\n",
    "            points = sides[side]\n",
    "            prefix = 'L' if side == 'left' else 'R'\n",
    "            \n",
    "            # Build list of required coordinate columns for this calculation\n",
    "            required_cols = []\n",
    "            for point in points:\n",
    "                required_cols += [f'{point}_x', f'{point}_y', f'{point}_z']\n",
    "                \n",
    "            if all(col in df.columns for col in required_cols):\n",
    "                # Calculate the vectors needed for the angle\n",
    "                vec1 = df[[f'{points[0]}_x', f'{points[0]}_y', f'{points[0]}_z']].values - \\\n",
    "                       df[[f'{points[1]}_x', f'{points[1]}_y', f'{points[1]}_z']].values\n",
    "                vec2 = df[[f'{points[2]}_x', f'{points[2]}_y', f'{points[2]}_z']].values - \\\n",
    "                       df[[f'{points[1]}_x', f'{points[1]}_y', f'{points[1]}_z']].values\n",
    "\n",
    "                # Compute the dot product and the norms of the vectors\n",
    "                dot_product = np.sum(vec1 * vec2, axis=1)\n",
    "                norm_product = np.linalg.norm(vec1, axis=1) * np.linalg.norm(vec2, axis=1)\n",
    "                \n",
    "                # Compute the angle (in degrees) and add a small epsilon to avoid division by zero\n",
    "                angles = np.degrees(np.arccos(dot_product / (norm_product + 1e-8)))\n",
    "                \n",
    "                col_name = f'{prefix}_{joint}_angle'\n",
    "                df[col_name] = angles\n",
    "                angle_columns.append(col_name)\n",
    "                \n",
    "                if debug:\n",
    "                    logging.info(f\"Calculated {col_name} with mean: {angles.mean():.2f}°\")\n",
    "            else:\n",
    "                logging.warning(f\"Missing coordinates for {prefix}_{joint} angle calculation\")\n",
    "\n",
    "    _print_debug_info(\"calculate_joint_angles\", df, new_columns=angle_columns, debug=debug)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def prepare_joint_features(data, debug=False):\n",
    "    \"\"\"\n",
    "    Aggregates joint-level energy and power, creates additional biomechanical features,\n",
    "    and adds new features:\n",
    "      - energy_acceleration: instantaneous rate of change of joint_energy.\n",
    "      - ankle_power_ratio: ratio of left to right ankle ongoing power.\n",
    "      - Additional asymmetry metrics for shoulders, hips, ankles, wrists, and feet.\n",
    "      - Power ratios for all joint pairs.\n",
    "      - Side-Specific Range-of-Motion (ROM) metrics (ROM, deviation, and binary extreme flag).\n",
    "      - Removal of the wrist_angle_release column if present.\n",
    "    \n",
    "    Parameters:\n",
    "      - data (pd.DataFrame): Input DataFrame.\n",
    "      - debug (bool): If True, prints detailed debug outputs.\n",
    "    \n",
    "    Returns:\n",
    "      - data (pd.DataFrame): Updated DataFrame with new features.\n",
    "    \"\"\"\n",
    "    step = \"prepare_joint_features\"\n",
    "    new_cols = []\n",
    "    connections = [\n",
    "        (\"R_EYE\", \"L_EYE\"), (\"R_EYE\", \"NOSE\"), (\"L_EYE\", \"NOSE\"),\n",
    "        (\"R_EYE\", \"R_EAR\"), (\"L_EYE\", \"L_EAR\"), (\"R_SHOULDER\", \"L_SHOULDER\"),\n",
    "        (\"R_SHOULDER\", \"R_ELBOW\"), (\"L_SHOULDER\", \"L_ELBOW\"), (\"R_ELBOW\", \"R_WRIST\"),\n",
    "        (\"L_ELBOW\", \"L_WRIST\"), (\"R_SHOULDER\", \"R_HIP\"), (\"L_SHOULDER\", \"L_HIP\"),\n",
    "        (\"R_HIP\", \"L_HIP\"), (\"R_HIP\", \"R_KNEE\"), (\"L_HIP\", \"L_KNEE\"),\n",
    "        (\"R_KNEE\", \"R_ANKLE\"), (\"L_KNEE\", \"L_ANKLE\"), (\"R_WRIST\", \"R_1STFINGER\"),\n",
    "        (\"R_WRIST\", \"R_5THFINGER\"), (\"L_WRIST\", \"L_1STFINGER\"), (\"L_WRIST\", \"L_5THFINGER\"),\n",
    "        (\"R_ANKLE\", \"R_1STTOE\"), (\"R_ANKLE\", \"R_5THTOE\"), (\"L_ANKLE\", \"L_1STTOE\"),\n",
    "        (\"L_ANKLE\", \"L_5THTOE\"), (\"R_ANKLE\", \"R_CALC\"), (\"L_ANKLE\", \"L_CALC\"),\n",
    "        (\"R_1STTOE\", \"R_5THTOE\"), (\"L_1STTOE\", \"L_5THTOE\"), (\"R_1STTOE\", \"R_CALC\"),\n",
    "        (\"L_1STTOE\", \"L_CALC\"), (\"R_5THTOE\", \"R_CALC\"), (\"L_5THTOE\", \"L_CALC\"),\n",
    "        (\"R_1STFINGER\", \"R_5THFINGER\"), (\"L_1STFINGER\", \"L_5THFINGER\")\n",
    "    ]\n",
    "    # Compute joint angles first.\n",
    "    data = calculate_joint_angles(data, connections, debug=debug)\n",
    "    \n",
    "    # Rename participant anthropometrics if available.\n",
    "    if 'height_in_meters' in data.columns and 'weight__in_kg' in data.columns:\n",
    "        data['player_height_in_meters'] = data['height_in_meters']\n",
    "        data['player_weight__in_kg'] = data['weight__in_kg']\n",
    "        data.drop(['height_in_meters', 'weight__in_kg'], axis=1, inplace=True, errors='ignore')\n",
    "        new_cols.extend(['player_height_in_meters', 'player_weight__in_kg'])\n",
    "        logging.info(\"Renamed participant anthropometrics.\")\n",
    "    else:\n",
    "        logging.warning(\"Participant anthropometric columns not found during renaming.\")\n",
    "\n",
    "    # Identify joint energy and power columns.\n",
    "    joint_energy_columns = [col for col in data.columns if '_energy' in col and not ('by_trial' in col or 'overall' in col)]\n",
    "    print(\"Joint energy columns: \", joint_energy_columns)\n",
    "    joint_power_columns = [col for col in data.columns if '_ongoing_power' in col]\n",
    "    print(\"Joint power columns: \", joint_power_columns)\n",
    "    print(\"All angle columns: \", [col for col in data.columns if 'angle' in col])\n",
    "    logging.info(f\"Identified {len(joint_energy_columns)} joint energy and {len(joint_power_columns)} joint power columns.\")\n",
    "    if not joint_energy_columns:\n",
    "        logging.error(\"No joint energy columns found. Check naming conventions.\")\n",
    "        sys.exit(1)\n",
    "    if not joint_power_columns:\n",
    "        logging.error(\"No joint power columns found. Check naming conventions.\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # Create aggregated columns.\n",
    "    data['joint_energy'] = data[joint_energy_columns].sum(axis=1)\n",
    "    data['joint_power'] = data[joint_power_columns].sum(axis=1)\n",
    "    new_cols.extend(['joint_energy', 'joint_power'])\n",
    "    logging.info(\"Created aggregated 'joint_energy' and 'joint_power'.\")\n",
    "\n",
    "    # --- NEW FEATURE: Energy Acceleration ---\n",
    "    if 'continuous_frame_time' in data.columns:\n",
    "        time_diff = data['continuous_frame_time'].diff().replace(0, 1e-6)  # Avoid division by zero\n",
    "        data['energy_acceleration'] = data['joint_energy'].diff() / time_diff\n",
    "        data['energy_acceleration'] = data['energy_acceleration'].replace([np.inf, -np.inf], np.nan)\n",
    "        new_cols.append('energy_acceleration')\n",
    "        logging.info(\"Created 'energy_acceleration' as derivative of joint_energy over time.\")\n",
    "    else:\n",
    "        logging.error(\"Missing 'continuous_frame_time' for energy_acceleration calculation.\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # --- NEW FEATURE: Ankle Power Ratio ---\n",
    "    # For power, look for the '_ongoing_power' suffix.\n",
    "    if 'L_ANKLE_ongoing_power' in data.columns and 'R_ANKLE_ongoing_power' in data.columns:\n",
    "        data['ankle_power_ratio'] = data['L_ANKLE_ongoing_power'] / (data['R_ANKLE_ongoing_power'] + 1e-6)\n",
    "        new_cols.append('ankle_power_ratio')\n",
    "        logging.info(\"Created 'ankle_power_ratio' feature comparing left to right ankle ongoing power.\")\n",
    "    else:\n",
    "        logging.warning(\"Ankle ongoing power columns not found; 'ankle_power_ratio' not created.\")\n",
    "\n",
    "    # --- NEW FEATURES: Additional Asymmetry Metrics ---\n",
    "    additional_asymmetry_joints = ['hip', 'ankle', 'wrist', 'elbow', 'knee', '1stfinger', '5thfinger'] #, '1sttoe', '5thtoe' < ADD WHEN WE ADD TO DATA LOAD AND PREPARE IN MODULE %%writefile ml/feature_engineering/energy_exhaustion_metrics.py\n",
    "    for joint in additional_asymmetry_joints:\n",
    "        # Use joint.upper() for energy columns.\n",
    "        left_col = f\"L_{joint.upper()}_energy\"\n",
    "        right_col = f\"R_{joint.upper()}_energy\"\n",
    "        if left_col in data.columns and right_col in data.columns:\n",
    "            col_name = f\"{joint}_asymmetry\"\n",
    "            data[col_name] = np.abs(data[left_col] - data[right_col])\n",
    "            new_cols.append(col_name)\n",
    "            logging.info(f\"Created asymmetry feature: {col_name}\")\n",
    "        else:\n",
    "            logging.warning(f\"Columns {left_col} and/or {right_col} not found; skipping {joint}_asymmetry.\")\n",
    "\n",
    "    # --- NEW FEATURES: Power Ratios for All Joints ---\n",
    "    joints_for_power_ratio = additional_asymmetry_joints.copy()\n",
    "    if 'knee' not in joints_for_power_ratio:\n",
    "        joints_for_power_ratio.append('knee')\n",
    "    for joint in joints_for_power_ratio:\n",
    "        if joint == 'foot':\n",
    "            left_col = 'left_foot_power'\n",
    "            right_col = 'right_foot_power'\n",
    "        else:\n",
    "            # Construct expected column names with the suffix '_ongoing_power'\n",
    "            left_col = f\"L_{joint.upper()}_ongoing_power\"\n",
    "            right_col = f\"R_{joint.upper()}_ongoing_power\"\n",
    "        # Debug: log the expected column names.\n",
    "        logging.debug(f\"Expecting power columns: {left_col} and {right_col}\")\n",
    "        if left_col in data.columns and right_col in data.columns:\n",
    "            ratio_col = f\"{joint}_power_ratio\"\n",
    "            data[ratio_col] = data[left_col] / (data[right_col] + 1e-6)\n",
    "            new_cols.append(ratio_col)\n",
    "            logging.info(f\"Created power ratio feature: {ratio_col} using columns {left_col} and {right_col}\")\n",
    "        else:\n",
    "            logging.warning(f\"Columns {left_col} and/or {right_col} not found; skipping {joint}_power_ratio.\")\n",
    "\n",
    "\n",
    "\n",
    "    # --- NEW FEATURES: Side-Specific Range-of-Motion (ROM) Metrics ---\n",
    "    # For angles, the dataset uses joint, e.g., \"L_shoulder_angle\".\n",
    "    rom_joints = {\n",
    "        'KNEE': {'min': 120, 'max': 135},\n",
    "        'SHOULDER': {'min': 0,  'max': 150},\n",
    "        'HIP': {'min': 0,  'max': 120},\n",
    "        'ANKLE': {'min': 0,  'max': 20},\n",
    "        'WRIST': {'min': 0,  'max': 80}\n",
    "    }\n",
    "    for joint, thresholds in rom_joints.items():\n",
    "        for side in ['L', 'R']:\n",
    "            angle_col = f\"{side}_{joint}_angle\"\n",
    "            if angle_col in data.columns:\n",
    "                rom_col = f\"{side}_{joint}_ROM\"\n",
    "                data[rom_col] = data.groupby('trial_id')[angle_col].transform(lambda x: x.max() - x.min())\n",
    "                new_cols.append(rom_col)\n",
    "                logging.info(f\"Computed ROM for {side} {joint} as {rom_col}\")\n",
    "\n",
    "                deviation_col = f\"{side}_{joint}_ROM_deviation\"\n",
    "                normal_min = thresholds['min']\n",
    "                normal_max = thresholds['max']\n",
    "                data[deviation_col] = np.maximum(0, normal_min - data[rom_col]) + np.maximum(0, data[rom_col] - normal_max)\n",
    "                new_cols.append(deviation_col)\n",
    "                logging.info(f\"Computed ROM deviation for {side} {joint} as {deviation_col}\")\n",
    "\n",
    "                extreme_col = f\"{side}_{joint}_ROM_extreme\"\n",
    "                data[extreme_col] = ((data[rom_col] < normal_min) | (data[rom_col] > normal_max)).astype(int)\n",
    "                new_cols.append(extreme_col)\n",
    "                logging.info(f\"Created binary flag for {side} {joint} ROM extremes: {extreme_col}\")\n",
    "            else:\n",
    "                logging.info(f\"Angle column '{angle_col}' not found; skipping ROM metrics for {side} {joint}.\")\n",
    "\n",
    "    # --- Removal of Non-Contributing Features ---\n",
    "    if 'wrist_angle_release' in data.columns:\n",
    "        data.drop(columns=['wrist_angle_release'], inplace=True)\n",
    "        logging.info(\"Dropped 'wrist_angle_release' column as it is not helpful for the model.\")\n",
    "    \n",
    "    # --- Sort Data ---\n",
    "    if 'continuous_frame_time' in data.columns and 'participant_id' in data.columns:\n",
    "        data.sort_values(by=['participant_id', 'continuous_frame_time'], inplace=True)\n",
    "        data.reset_index(drop=True, inplace=True)\n",
    "        logging.info(\"Sorted data by 'participant_id' and 'continuous_frame_time'.\")\n",
    "    else:\n",
    "        logging.error(\"Missing required columns for sorting ('participant_id', 'continuous_frame_time').\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # --- Create Exhaustion Rate ---\n",
    "    if 'by_trial_exhaustion_score' in data.columns and 'by_trial_time' in data.columns:\n",
    "        data['exhaustion_rate'] = data['by_trial_exhaustion_score'].diff() / data['by_trial_time'].diff()\n",
    "        print(\"print all the columns with by_trial_exhaustion_score: \", [col for col in data.columns if 'by_trial_exhaustion_score' in col])\n",
    "        new_cols.append('exhaustion_rate')\n",
    "        logging.info(\"Created 'exhaustion_rate' feature.\")\n",
    "    else:\n",
    "        logging.error(\"Missing columns for 'exhaustion_rate' calculation.\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # --- Create Simulated Heart Rate ---\n",
    "    if 'by_trial_exhaustion_score' in data.columns and 'joint_energy' in data.columns:\n",
    "        data['simulated_HR'] = 60 + (data['by_trial_exhaustion_score'] * 1.5) + (data['joint_energy'] * 0.3)\n",
    "        new_cols.append('simulated_HR')\n",
    "        logging.info(\"Created 'simulated_HR' feature.\")\n",
    "    else:\n",
    "        logging.error(\"Missing columns for 'simulated_HR' calculation.\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    _print_debug_info(step, data, new_columns=new_cols, debug=debug)\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "def feature_engineering(data, window_size=5, debug=False):\n",
    "    \"\"\"Optimized feature engineering with vectorized operations.\"\"\"\n",
    "    step = \"feature_engineering\"\n",
    "    new_cols = []\n",
    "    rolling_window = 20\n",
    "    required_columns = {\n",
    "        'base': ['by_trial_exhaustion_score', 'joint_power', \n",
    "                'simulated_HR', 'continuous_frame_time'],\n",
    "        'joints': ['by_trial_time']\n",
    "    }\n",
    "    \n",
    "    # Validate columns upfront\n",
    "    missing = [col for col in required_columns['base'] if col not in data.columns]\n",
    "    if missing:\n",
    "        logging.error(f\"Missing required columns: {missing}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Vectorized temporal features\n",
    "    data['time_since_start'] = data['continuous_frame_time'] - data['continuous_frame_time'].min()\n",
    "    new_cols.append('time_since_start')\n",
    "    \n",
    "    # For ball-related columns, fill with 0 when not in play\n",
    "    ball_cols = ['ball_speed', 'ball_velocity_x', 'ball_velocity_y', 'ball_velocity_z']\n",
    "    data[ball_cols] = data[ball_cols].fillna(0)\n",
    "\n",
    "    # For motion columns (dx, dy, dz), forward-fill missing values\n",
    "    motion_cols = ['dx', 'dy', 'dz']\n",
    "    data[motion_cols] = data[motion_cols].fillna(method='ffill').fillna(0)\n",
    "\n",
    "    # For rolling features, use min_periods=1 to avoid NaNs in early rows\n",
    "    roll_config = {\n",
    "        'power_avg_5': ('joint_power', 'mean'),\n",
    "        'rolling_power_std': ('joint_power', 'std'),\n",
    "        'rolling_hr_mean': ('simulated_HR', 'mean')\n",
    "    }\n",
    "    for new_col, (base_col, func) in roll_config.items():\n",
    "        data[new_col] = getattr(data[base_col].rolling(window_size, min_periods=1), func)()  # <-- Add min_periods\n",
    "\n",
    "    # Optimized expanding quantile calculation\n",
    "    def safe_expanding_quantile(s):\n",
    "        return s.expanding().quantile(0.75).shift().fillna(0)\n",
    "    # --- OPTIONAL NEW FEATURE: Rolling Energy Standard Deviation ---\n",
    "    if 'joint_energy' in data.columns:\n",
    "        data['rolling_energy_std'] = data['joint_energy'].rolling(window=window_size, min_periods=1).std(ddof=0)\n",
    "        logging.info(f\"Created 'rolling_energy_std' with sample: {data['rolling_energy_std'].head(10).tolist()}\")\n",
    "        logging.info(f\"Created 'rolling_energy_std' with window {window_size}.\")\n",
    "    else:\n",
    "        logging.warning(\"Column 'joint_energy' missing for 'rolling_energy_std'.\")\n",
    "    new_cols.append('rolling_energy_std')\n",
    "    \n",
    "    # Vectorized exhaustion features\n",
    "    data['exhaustion_lag1'] = data['by_trial_exhaustion_score'].shift(1)\n",
    "    data['ema_exhaustion'] = data['by_trial_exhaustion_score'].ewm(span=10, adjust=False).mean()\n",
    "    data['rolling_exhaustion'] = data['by_trial_exhaustion_score'].rolling(rolling_window, min_periods=1).sum()\n",
    "    \n",
    "    # Vectorized injury risk calculation\n",
    "    data['injury_risk'] = (data['rolling_exhaustion'] > safe_expanding_quantile(data['rolling_exhaustion'])).astype(int)\n",
    "    new_cols += ['exhaustion_lag1', 'ema_exhaustion', 'rolling_exhaustion', 'injury_risk']\n",
    "\n",
    "    # Joint features using vectorized operations\n",
    "    joints = ['ANKLE', 'WRIST', 'ELBOW', 'KNEE', 'HIP']\n",
    "    sides = ['L', 'R']\n",
    "    \n",
    "    # Precompute time diffs once for all joints\n",
    "    dt = data['by_trial_time'].diff().replace(0, np.nan)\n",
    "    \n",
    "    for joint in joints:\n",
    "        for side in sides:\n",
    "            joint_name = f\"{side}_{joint}\"\n",
    "            score_col = f'{joint_name}_energy_by_trial_exhaustion_score'\n",
    "            \n",
    "            if score_col not in data.columns:\n",
    "                continue\n",
    "                \n",
    "            # Vectorized joint features\n",
    "            data[f'{joint_name}_exhaustion_rate'] = data[score_col].diff() / dt\n",
    "            data[f'{joint_name}_rolling_exhaustion'] = data[score_col].rolling(rolling_window, min_periods=1).sum()\n",
    "            \n",
    "            # Vectorized quantile comparison\n",
    "            rolling_series = data[f'{joint_name}_rolling_exhaustion']\n",
    "            data[f'{joint_name}_injury_risk'] = (rolling_series > safe_expanding_quantile(rolling_series)).astype(int)\n",
    "            \n",
    "            new_cols.extend([\n",
    "                f'{joint_name}_exhaustion_rate',\n",
    "                f'{joint_name}_rolling_exhaustion',\n",
    "                f'{joint_name}_injury_risk'\n",
    "            ])\n",
    "\n",
    "    # Selective NA dropping for lag features only\n",
    "    data.dropna(subset=['exhaustion_lag1'], inplace=True)\n",
    "    \n",
    "    if debug:\n",
    "        _print_debug_info(step, data, new_columns=new_cols, debug=debug)\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def add_simulated_player_metrics(df, window=5, debug=False):\n",
    "    \"\"\"\n",
    "    Adds simulated player metrics to mimic heart rate and fatigue.\n",
    "    \n",
    "    New Metrics:\n",
    "      - simulated_HR_fake: Alternative simulated heart rate.\n",
    "      - fatigue_index_fake: Combined fatigue index.\n",
    "      - fatigue_rate_fake: Frame-by-frame rate of change of fatigue_index_fake.\n",
    "      - HR_variability_fake: Rolling standard deviation of simulated_HR_fake.\n",
    "    \n",
    "    Parameters:\n",
    "      - df (pd.DataFrame): DataFrame with required columns (e.g., by_trial_exhaustion_score, joint_energy, overall_exhaustion_score, dt).\n",
    "      - window (int): Rolling window size for HR variability.\n",
    "      - debug (bool): If True, prints detailed debug outputs.\n",
    "    \n",
    "    Returns:\n",
    "      - df (pd.DataFrame): DataFrame with new simulated metrics.\n",
    "    \"\"\"\n",
    "    step = \"add_simulated_player_metrics\"\n",
    "    new_cols = []\n",
    "    \n",
    "    # Use maximum joint_energy for scaling\n",
    "    max_joint_energy = df['joint_energy'].max() if 'joint_energy' in df.columns else 1\n",
    "    df['simulated_HR_fake'] = 60 + (df['by_trial_exhaustion_score'] * 2.0) + ((df['joint_energy'] / max_joint_energy) * 20)\n",
    "    new_cols.append('simulated_HR_fake')\n",
    "    \n",
    "    df['fatigue_index_fake'] = df['overall_exhaustion_score'] + ((df['simulated_HR_fake'] - 60) / 100)\n",
    "    new_cols.append('fatigue_index_fake')\n",
    "    \n",
    "    df['fatigue_rate_fake'] = df['fatigue_index_fake'].diff() / df['dt']\n",
    "    df['fatigue_rate_fake'] = df['fatigue_rate_fake'].fillna(0)\n",
    "    new_cols.append('fatigue_rate_fake')\n",
    "    \n",
    "    df['HR_variability_fake'] = df['simulated_HR_fake'].rolling(window=window, min_periods=1).std()\n",
    "    new_cols.append('HR_variability_fake')\n",
    "    \n",
    "    _print_debug_info(step, df, new_columns=new_cols, debug=debug)\n",
    "    return df\n",
    "\n",
    "\n",
    "def joint_specific_analysis(data, joint_energy_columns, debug=False):\n",
    "    \"\"\"\n",
    "    Performs joint-specific analysis including:\n",
    "      - Energy distribution per joint.\n",
    "      - Injury risk analysis for each joint.\n",
    "      - Cumulative energy accumulation patterns.\n",
    "    \n",
    "    Parameters:\n",
    "      - data (pd.DataFrame): Input DataFrame.\n",
    "      - joint_energy_columns (list): List of joint energy column names.\n",
    "      - debug (bool): If True, prints debug information.\n",
    "    \"\"\"\n",
    "    step = \"joint_specific_analysis\"\n",
    "    # Energy distribution across joints\n",
    "    joint_energy_melted = data[joint_energy_columns].melt(var_name='Joint', value_name='Energy')\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    order = joint_energy_melted.groupby('Joint')['Energy'].median().sort_values().index\n",
    "    sns.boxplot(x='Joint', y='Energy', data=joint_energy_melted, order=order)\n",
    "    plt.title('Joint Energy Distributions (Sorted by Median Energy)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "    if debug:\n",
    "        logging.info(\"Displayed boxplot for joint energy distributions.\")\n",
    "    else:\n",
    "        logging.info(\"Energy distribution plot displayed.\")\n",
    "    \n",
    "    # Injury risk analysis: only run if 'injury_risk' exists.\n",
    "    if 'injury_risk' not in data.columns:\n",
    "        logging.warning(\"Column 'injury_risk' not found; skipping injury risk analysis in joint_specific_analysis.\")\n",
    "    else:\n",
    "        num_plots = len(joint_energy_columns)\n",
    "        ncols = 4\n",
    "        nrows = int(np.ceil(num_plots / ncols))\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        for i, joint in enumerate(joint_energy_columns, 1):\n",
    "            plt.subplot(nrows, ncols, i)\n",
    "            sns.boxplot(x='injury_risk', y=joint, data=data)\n",
    "            plt.title(f'{joint.split(\"_\")[0].title()} Energy')\n",
    "            plt.tight_layout()\n",
    "        plt.suptitle('Joint Energy Distributions by Injury Risk', y=1.02)\n",
    "        plt.show()\n",
    "        if debug:\n",
    "            logging.info(\"Displayed injury risk analysis plots for joint energy.\")\n",
    "        else:\n",
    "            logging.info(\"Injury risk analysis plots displayed.\")\n",
    "    \n",
    "    # Cumulative energy accumulation patterns\n",
    "    joint_cumulative = data.groupby('participant_id')[joint_energy_columns].cumsum()\n",
    "    joint_cumulative['time'] = data['continuous_frame_time']\n",
    "    joint_cumulative_melted = joint_cumulative.melt(id_vars='time', var_name='Joint', value_name='Cumulative Energy')\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    sns.lineplot(x='time', y='Cumulative Energy', hue='Joint', \n",
    "                 data=joint_cumulative_melted, estimator='median', errorbar=None)\n",
    "    plt.title('Cumulative Joint Energy Over Time (Median Across Participants)')\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('Cumulative Energy')\n",
    "    plt.show()\n",
    "    if debug:\n",
    "        logging.info(\"Displayed cumulative joint energy plot.\")\n",
    "    else:\n",
    "        logging.info(\"Cumulative energy plot displayed.\")\n",
    "    \n",
    "    _print_debug_info(step, data, debug=debug)\n",
    "\n",
    "\n",
    "def movement_pattern_analysis(data, debug=False):\n",
    "    \"\"\"\n",
    "    Performs movement pattern analysis:\n",
    "      - Angular velocity histograms with KDE.\n",
    "      - Asymmetry analysis via pairplot.\n",
    "    \n",
    "    Parameters:\n",
    "      - data (pd.DataFrame): Input DataFrame.\n",
    "      - debug (bool): If True, prints debug information.\n",
    "    \"\"\"\n",
    "    step = \"movement_pattern_analysis\"\n",
    "    # Angular velocity analysis\n",
    "    angular_columns = [col for col in data.columns if '_angular_velocity' in col]\n",
    "    if angular_columns:\n",
    "        fig, axes = plt.subplots(nrows=4, ncols=4, figsize=(20, 15))\n",
    "        axes = axes.flatten()\n",
    "        for ax, col in zip(axes, angular_columns):\n",
    "            sns.histplot(data[col], ax=ax, kde=True)\n",
    "            ax.set_title(f'{col.split(\"_\")[0].title()} Angular Velocity')\n",
    "        for j in range(len(angular_columns), len(axes)):\n",
    "            fig.delaxes(axes[j])\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        logging.info(\"Displayed angular velocity histograms.\")\n",
    "    else:\n",
    "        logging.info(\"No angular velocity columns found.\")\n",
    "    \n",
    "    # Asymmetry analysis\n",
    "    asymmetry_metrics = [col for col in data.columns if 'asymmetry' in col]\n",
    "    if 'injury_risk' in data.columns and asymmetry_metrics:\n",
    "        sns.pairplot(data[asymmetry_metrics + ['injury_risk']], hue='injury_risk', corner=True)\n",
    "        plt.suptitle('Joint Asymmetry Relationships with Injury Risk', y=1.02)\n",
    "        plt.show()\n",
    "        logging.info(\"Displayed asymmetry pairplot.\")\n",
    "    else:\n",
    "        logging.info(\"Required columns for asymmetry analysis not found.\")\n",
    "    \n",
    "    _print_debug_info(step, data, debug=debug)\n",
    "\n",
    "\n",
    "def temporal_analysis_enhancements(data, debug=False):\n",
    "    \"\"\"\n",
    "    Performs temporal analysis enhancements:\n",
    "      - Computes lagged correlations between joint energy and exhaustion score.\n",
    "      - Plots autocorrelation of joint energy.\n",
    "    \n",
    "    Parameters:\n",
    "      - data (pd.DataFrame): Input DataFrame.\n",
    "      - debug (bool): If True, prints debug information.\n",
    "    \"\"\"\n",
    "    step = \"temporal_analysis_enhancements\"\n",
    "    max_lag = 10\n",
    "    lagged_corrs = []\n",
    "    for lag in range(1, max_lag + 1):\n",
    "        corr_val = data['joint_energy'].corr(data['by_trial_exhaustion_score'].shift(lag))\n",
    "        lagged_corrs.append(corr_val)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, max_lag + 1), lagged_corrs, marker='o')\n",
    "    plt.title('Lagged Correlation Between Joint Energy and Exhaustion Score')\n",
    "    plt.xlabel('Time Lag (periods)')\n",
    "    plt.ylabel('Correlation Coefficient')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    from statsmodels.graphics.tsaplots import plot_acf\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plot_acf(data['joint_energy'].dropna(), lags=50, alpha=0.05)\n",
    "    plt.title('Joint Energy Autocorrelation')\n",
    "    plt.xlabel('Lags')\n",
    "    plt.ylabel('Autocorrelation')\n",
    "    plt.show()\n",
    "    \n",
    "    _print_debug_info(step, data, debug=debug)\n",
    "\n",
    "\n",
    "def multivariate_analysis(data, joint_energy_columns, debug=False):\n",
    "    \"\"\"\n",
    "    Performs multivariate analysis separately for left- and right-sided joints:\n",
    "      - 3D visualization of joint energy interactions for each side.\n",
    "      - KMeans clustering on selected features for each side.\n",
    "    \n",
    "    Parameters:\n",
    "      - data (pd.DataFrame): Input DataFrame.\n",
    "      - joint_energy_columns (list): List of all joint energy columns.\n",
    "      - debug (bool): If True, prints debug information.\n",
    "    \"\"\"\n",
    "    step = \"multivariate_analysis\"\n",
    "\n",
    "    # --- 3D Visualization: Left Side ---\n",
    "    required_left = ['L_ELBOW_energy', 'L_KNEE_energy', 'L_ANKLE_energy', 'injury_risk']\n",
    "    if all(col in data.columns for col in required_left):\n",
    "        from mpl_toolkits.mplot3d import Axes3D\n",
    "        fig = plt.figure(figsize=(12, 8))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        scatter = ax.scatter(data['L_ELBOW_energy'], \n",
    "                             data['L_KNEE_energy'], \n",
    "                             data['L_ANKLE_energy'], \n",
    "                             c=data['injury_risk'],\n",
    "                             cmap='viridis',\n",
    "                             alpha=0.6)\n",
    "        ax.set_xlabel('L Elbow Energy')\n",
    "        ax.set_ylabel('L Knee Energy')\n",
    "        ax.set_zlabel('L Ankle Energy')\n",
    "        plt.title('3D Joint Energy Space (Left Side) with Injury Risk Coloring')\n",
    "        plt.colorbar(scatter, label='Injury Risk')\n",
    "        plt.show()\n",
    "    else:\n",
    "        logging.info(\"Required left-side columns for 3D analysis not found; skipping left side 3D plot.\")\n",
    "\n",
    "    # --- 3D Visualization: Right Side ---\n",
    "    required_right = ['R_ELBOW_energy', 'R_KNEE_energy', 'R_ANKLE_energy', 'injury_risk']\n",
    "    if all(col in data.columns for col in required_right):\n",
    "        from mpl_toolkits.mplot3d import Axes3D\n",
    "        fig = plt.figure(figsize=(12, 8))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        scatter = ax.scatter(data['R_ELBOW_energy'], \n",
    "                             data['R_KNEE_energy'], \n",
    "                             data['R_ANKLE_energy'], \n",
    "                             c=data['injury_risk'],\n",
    "                             cmap='viridis',\n",
    "                             alpha=0.6)\n",
    "        ax.set_xlabel('R Elbow Energy')\n",
    "        ax.set_ylabel('R Knee Energy')\n",
    "        ax.set_zlabel('R Ankle Energy')\n",
    "        plt.title('3D Joint Energy Space (Right Side) with Injury Risk Coloring')\n",
    "        plt.colorbar(scatter, label='Injury Risk')\n",
    "        plt.show()\n",
    "    else:\n",
    "        logging.info(\"Required right-side columns for 3D analysis not found; skipping right side 3D plot.\")\n",
    "\n",
    "    # --- Clustering Analysis: Left Side ---\n",
    "    left_features = ['L_ELBOW_energy', 'L_KNEE_energy', 'L_ANKLE_energy']\n",
    "    # Optionally include asymmetry features if desired (they compare L vs R)\n",
    "    left_features = [feat for feat in left_features if feat in data.columns]\n",
    "    if left_features:\n",
    "        from sklearn.cluster import KMeans\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        X_left = data[left_features].dropna()\n",
    "        X_left_scaled = StandardScaler().fit_transform(X_left)\n",
    "        kmeans_left = KMeans(n_clusters=3, random_state=42).fit(X_left_scaled)\n",
    "        data.loc[X_left.index, 'left_movement_cluster'] = kmeans_left.labels_\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.scatterplot(x='L_ELBOW_energy', y='L_KNEE_energy', hue='left_movement_cluster', \n",
    "                        data=data, palette='viridis', alpha=0.6)\n",
    "        plt.title('Left Side Movement Clusters in Elbow-Knee Energy Space')\n",
    "        plt.xlabel('L Elbow Energy')\n",
    "        plt.ylabel('L Knee Energy')\n",
    "        plt.show()\n",
    "    else:\n",
    "        logging.info(\"Not enough left-side features available for clustering analysis.\")\n",
    "\n",
    "    # --- Clustering Analysis: Right Side ---\n",
    "    right_features = ['R_ELBOW_energy', 'R_KNEE_energy', 'R_ANKLE_energy']\n",
    "    right_features = [feat for feat in right_features if feat in data.columns]\n",
    "    if right_features:\n",
    "        from sklearn.cluster import KMeans\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        X_right = data[right_features].dropna()\n",
    "        X_right_scaled = StandardScaler().fit_transform(X_right)\n",
    "        kmeans_right = KMeans(n_clusters=3, random_state=42).fit(X_right_scaled)\n",
    "        data.loc[X_right.index, 'right_movement_cluster'] = kmeans_right.labels_\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.scatterplot(x='R_ELBOW_energy', y='R_KNEE_energy', hue='right_movement_cluster', \n",
    "                        data=data, palette='viridis', alpha=0.6)\n",
    "        plt.title('Right Side Movement Clusters in Elbow-Knee Energy Space')\n",
    "        plt.xlabel('R Elbow Energy')\n",
    "        plt.ylabel('R Knee Energy')\n",
    "        plt.show()\n",
    "    else:\n",
    "        logging.info(\"Not enough right-side features available for clustering analysis.\")\n",
    "\n",
    "    _print_debug_info(step, data, debug=debug)\n",
    "\n",
    "\n",
    "\n",
    "def statistical_testing(data, joint_energy_columns, debug=False):\n",
    "    \"\"\"\n",
    "    Performs Mann-Whitney U tests on each joint energy metric between low and high injury risk groups.\n",
    "    \n",
    "    Parameters:\n",
    "      - data (pd.DataFrame): Input DataFrame.\n",
    "      - joint_energy_columns (list): List of joint energy column names.\n",
    "      - debug (bool): If True, prints detailed test outputs.\n",
    "    \n",
    "    Returns:\n",
    "      - results_df (pd.DataFrame): Summary table of test statistics.\n",
    "    \"\"\"\n",
    "    from scipy.stats import mannwhitneyu\n",
    "    step = \"statistical_testing\"\n",
    "    results = []\n",
    "    for joint in joint_energy_columns:\n",
    "        if joint in data.columns and 'injury_risk' in data.columns:\n",
    "            low_risk = data[data['injury_risk'] == 0][joint]\n",
    "            high_risk = data[data['injury_risk'] == 1][joint]\n",
    "            stat, p = mannwhitneyu(low_risk, high_risk, alternative='two-sided')\n",
    "            effect_size = stat / (len(low_risk) * len(high_risk)) if (len(low_risk) * len(high_risk)) > 0 else np.nan\n",
    "            results.append({\n",
    "                'Joint': joint.split('_')[0],\n",
    "                'U Statistic': stat,\n",
    "                'p-value': p,\n",
    "                'Effect Size': effect_size\n",
    "            })\n",
    "    results_df = pd.DataFrame(results).sort_values('p-value')\n",
    "    logging.info(\"Mann-Whitney U Test Results:\")\n",
    "    logging.info(results_df)\n",
    "    _print_debug_info(step, data, debug=debug)\n",
    "    return results_df\n",
    "\n",
    "\n",
    "def fatigue_injury_interaction_analysis(data, features_exhaustion, target_exhaustion, debug=False):\n",
    "    \"\"\"\n",
    "    Analyzes fatigue-injury interactions by constructing a fatigue-injury matrix and evaluating interaction effects.\n",
    "    \n",
    "    Parameters:\n",
    "      - data (pd.DataFrame): Input DataFrame.\n",
    "      - features_exhaustion (list): Feature list for exhaustion prediction.\n",
    "      - target_exhaustion (str): Exhaustion target variable.\n",
    "      - debug (bool): If True, prints detailed debug outputs.\n",
    "    \"\"\"\n",
    "    step = \"fatigue_injury_interaction_analysis\"\n",
    "    if 'by_trial_exhaustion_score' in data.columns and 'joint_energy' in data.columns:\n",
    "        fatigue_bins = pd.qcut(data['by_trial_exhaustion_score'], 4, labels=['Q1', 'Q2', 'Q3', 'Q4'])\n",
    "        joint_energy_bins = pd.qcut(data['joint_energy'], 4, labels=['Q1', 'Q2', 'Q3', 'Q4'])\n",
    "        fatigue_injury_matrix = pd.crosstab(fatigue_bins, joint_energy_bins, normalize='index')\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(fatigue_injury_matrix, annot=True, cmap='viridis')\n",
    "        plt.title('Fatigue-Injury Matrix (Normalized by Fatigue Quartiles)')\n",
    "        plt.xlabel('Joint Energy Quartiles')\n",
    "        plt.ylabel('Fatigue Score Quartiles')\n",
    "        plt.show()\n",
    "    else:\n",
    "        logging.info(\"Required columns for fatigue-injury matrix not found; skipping this analysis.\")\n",
    "    \n",
    "    if 'player_height_in_meters' in data.columns and 'joint_energy' in data.columns:\n",
    "        X_interaction = data[features_exhaustion].copy()\n",
    "        if 'player_height_in_meters' in X_interaction.columns and 'joint_energy' in X_interaction.columns:\n",
    "            X_interaction['height_energy_interaction'] = X_interaction['player_height_in_meters'] * X_interaction['joint_energy']\n",
    "            X_interaction = X_interaction.fillna(method='ffill').fillna(method='bfill')\n",
    "            from sklearn.ensemble import RandomForestRegressor\n",
    "            model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "            model.fit(X_interaction, data[target_exhaustion])\n",
    "            explainer_interaction = shap.TreeExplainer(model)\n",
    "            shap_interaction = explainer_interaction.shap_values(X_interaction)\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            shap.summary_plot(shap_interaction, X_interaction, max_display=15)\n",
    "            plt.title('SHAP Values with Interaction Effects')\n",
    "            plt.show()\n",
    "        else:\n",
    "            logging.info(\"Required columns for interaction effect missing; skipping SHAP analysis.\")\n",
    "    else:\n",
    "        logging.info(\"Required columns for interaction effect not found; skipping SHAP analysis.\")\n",
    "    \n",
    "    _print_debug_info(step, data, debug=debug)\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# MAIN SCRIPT\n",
    "###############################################################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the main pipeline with debug output enabled.\n",
    "\n",
    "    debug=False\n",
    " \n",
    "    # \"\"\"\n",
    "    # Main processing pipeline:\n",
    "    #   1. Loads and merges data.\n",
    "    #   2. Prepares joint features.\n",
    "    #   3. Performs feature engineering.\n",
    "    #   4. Adds simulated player metrics.\n",
    "    #   5. Executes various analyses (joint-specific, movement pattern, temporal, multivariate, statistical, and fatigue-injury interaction).\n",
    "    \n",
    "    # Parameters:\n",
    "    #   - debug (bool): Controls verbose debug output.\n",
    "    #   - csv_path (str): Path to input CSV file.\n",
    "    #   - json_path (str): Path to participant info JSON.\n",
    "    # \"\"\"\n",
    "    csv_path=\"../../data/processed/final_granular_dataset.csv\"\n",
    "    json_path=\"../../data/basketball/freethrow/participant_information.json\"\n",
    "    data = load_data(csv_path, json_path, debug=debug)\n",
    "    data = prepare_joint_features(data, debug=debug)\n",
    "    data = feature_engineering(data, debug=debug)\n",
    "    print(data.columns.tolist())\n",
    "    # data = add_simulated_player_metrics(data, window=5, debug=debug)\n",
    "    \n",
    "    # # For demonstration, define features/targets (you can adjust these as needed)\n",
    "    # features_exhaustion = [\n",
    "    #     'joint_power', \n",
    "    #     'joint_energy', \n",
    "    #     'elbow_asymmetry',  \n",
    "    #     'wrist_angle', \n",
    "    #     'exhaustion_lag1', \n",
    "    #     'power_avg_5',\n",
    "    #     'simulated_HR',\n",
    "    #     'player_height_in_meters',\n",
    "    #     'player_weight__in_kg'\n",
    "    # ]\n",
    "    # target_exhaustion = 'by_trial_exhaustion_score'\n",
    "    # features_injury = [\n",
    "    #     'joint_power', \n",
    "    #     'joint_energy', \n",
    "    #     'elbow_asymmetry',  \n",
    "    #     'knee_asymmetry', \n",
    "    #     'wrist_angle', \n",
    "    #     'exhaustion_lag1', \n",
    "    #     'power_avg_5',\n",
    "    #     'simulated_HR',\n",
    "    #     'player_height_in_meters',\n",
    "    #     'player_weight__in_kg'\n",
    "    # ]\n",
    "    # target_injury = 'injury_risk'\n",
    "    \n",
    "    # # Identify joint energy columns (excluding the aggregated 'joint_energy')\n",
    "    # joint_energy_columns = [\n",
    "    #     col for col in data.columns\n",
    "    #     if '_energy' in col and not ('by_trial' in col or 'overall' in col) and col != 'joint_energy'\n",
    "    # ]\n",
    "    # logging.info(f\"Joint Energy Columns after excluding 'joint_energy' ({len(joint_energy_columns)}): {joint_energy_columns}\")\n",
    "    \n",
    "    # # Execute analysis functions\n",
    "    # joint_specific_analysis(data, joint_energy_columns, debug=debug)\n",
    "    # movement_pattern_analysis(data, debug=debug)\n",
    "    # temporal_analysis_enhancements(data, debug=debug)\n",
    "    # multivariate_analysis(data, joint_energy_columns, debug=debug)\n",
    "    # statistical_testing(data, joint_energy_columns, debug=debug)\n",
    "    # fatigue_injury_interaction_analysis(data, features_exhaustion, target_exhaustion, debug=debug)\n",
    "    \n",
    "    # logging.info(\"Processing pipeline completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Loaded data from ../../data/processed/final_granular_dataset.csv with shape (16047, 214)\n",
      "INFO: Added 'participant_id' column with value 'P0001'\n",
      "INFO: Loaded participant information from ../../data/basketball/freethrow/participant_information.json\n",
      "INFO: Merged participant data. New shape: (16047, 217)\n",
      "INFO: Step [load_data]: DataFrame shape = (16047, 217)\n",
      "INFO: Calculated L_SHOULDER_angle with mean: 37.07°\n",
      "INFO: Calculated R_SHOULDER_angle with mean: 41.72°\n",
      "INFO: Calculated L_HIP_angle with mean: 157.79°\n",
      "INFO: Calculated R_HIP_angle with mean: 155.49°\n",
      "INFO: Calculated L_KNEE_angle with mean: 157.21°\n",
      "INFO: Calculated R_KNEE_angle with mean: 152.18°\n",
      "INFO: Calculated L_ANKLE_angle with mean: 113.75°\n",
      "INFO: Calculated R_ANKLE_angle with mean: 114.63°\n",
      "INFO: Step [calculate_joint_angles]: DataFrame shape = (16047, 225)\n",
      "INFO: New columns added: ['L_SHOULDER_angle', 'R_SHOULDER_angle', 'L_HIP_angle', 'R_HIP_angle', 'L_KNEE_angle', 'R_KNEE_angle', 'L_ANKLE_angle', 'R_ANKLE_angle']\n",
      "INFO:  - L_SHOULDER_angle: dtype=float64, sample values=[21.60539187 22.06901465 22.36025929 22.40661102 22.35603223]\n",
      "INFO:  - R_SHOULDER_angle: dtype=float64, sample values=[19.7439164  19.99288757 20.17556749 20.32850063 20.41384808]\n",
      "INFO:  - L_HIP_angle: dtype=float64, sample values=[166.66216865 167.54930512 168.38412077 169.13603097 169.77674873]\n",
      "INFO:  - R_HIP_angle: dtype=float64, sample values=[172.36581725 169.3001433  166.60177841 164.42775916 163.15032681]\n",
      "INFO:  - L_KNEE_angle: dtype=float64, sample values=[159.60787091 160.63620962 161.7064911  162.70735335 163.68010105]\n",
      "INFO:  - R_KNEE_angle: dtype=float64, sample values=[144.02241138 138.83820789 135.23462182 133.60754574 134.17926352]\n",
      "INFO:  - L_ANKLE_angle: dtype=float64, sample values=[116.19078287 117.19419756 118.10095304 118.75673954 119.58404627]\n",
      "INFO:  - R_ANKLE_angle: dtype=float64, sample values=[116.77875188 115.72547317 114.66825566 113.3630418  112.54700118]\n",
      "INFO: Renamed participant anthropometrics.\n",
      "INFO: Identified 15 joint energy and 14 joint power columns.\n",
      "INFO: Created aggregated 'joint_energy' and 'joint_power'.\n",
      "INFO: Created 'energy_acceleration' as derivative of joint_energy over time.\n",
      "INFO: Created 'ankle_power_ratio' feature comparing left to right ankle ongoing power.\n",
      "INFO: Created asymmetry feature: hip_asymmetry\n",
      "INFO: Created asymmetry feature: ankle_asymmetry\n",
      "INFO: Created asymmetry feature: wrist_asymmetry\n",
      "INFO: Created asymmetry feature: elbow_asymmetry\n",
      "INFO: Created asymmetry feature: knee_asymmetry\n",
      "INFO: Created asymmetry feature: 1stfinger_asymmetry\n",
      "INFO: Created asymmetry feature: 5thfinger_asymmetry\n",
      "INFO: Created power ratio feature: hip_power_ratio using columns L_HIP_ongoing_power and R_HIP_ongoing_power\n",
      "INFO: Created power ratio feature: ankle_power_ratio using columns L_ANKLE_ongoing_power and R_ANKLE_ongoing_power\n",
      "INFO: Created power ratio feature: wrist_power_ratio using columns L_WRIST_ongoing_power and R_WRIST_ongoing_power\n",
      "INFO: Created power ratio feature: elbow_power_ratio using columns L_ELBOW_ongoing_power and R_ELBOW_ongoing_power\n",
      "INFO: Created power ratio feature: knee_power_ratio using columns L_KNEE_ongoing_power and R_KNEE_ongoing_power\n",
      "INFO: Created power ratio feature: 1stfinger_power_ratio using columns L_1STFINGER_ongoing_power and R_1STFINGER_ongoing_power\n",
      "INFO: Created power ratio feature: 5thfinger_power_ratio using columns L_5THFINGER_ongoing_power and R_5THFINGER_ongoing_power\n",
      "INFO: Computed ROM for L KNEE as L_KNEE_ROM\n",
      "INFO: Computed ROM deviation for L KNEE as L_KNEE_ROM_deviation\n",
      "INFO: Created binary flag for L KNEE ROM extremes: L_KNEE_ROM_extreme\n",
      "INFO: Computed ROM for R KNEE as R_KNEE_ROM\n",
      "INFO: Computed ROM deviation for R KNEE as R_KNEE_ROM_deviation\n",
      "INFO: Created binary flag for R KNEE ROM extremes: R_KNEE_ROM_extreme\n",
      "INFO: Computed ROM for L SHOULDER as L_SHOULDER_ROM\n",
      "INFO: Computed ROM deviation for L SHOULDER as L_SHOULDER_ROM_deviation\n",
      "INFO: Created binary flag for L SHOULDER ROM extremes: L_SHOULDER_ROM_extreme\n",
      "INFO: Computed ROM for R SHOULDER as R_SHOULDER_ROM\n",
      "INFO: Computed ROM deviation for R SHOULDER as R_SHOULDER_ROM_deviation\n",
      "INFO: Created binary flag for R SHOULDER ROM extremes: R_SHOULDER_ROM_extreme\n",
      "INFO: Computed ROM for L HIP as L_HIP_ROM\n",
      "INFO: Computed ROM deviation for L HIP as L_HIP_ROM_deviation\n",
      "INFO: Created binary flag for L HIP ROM extremes: L_HIP_ROM_extreme\n",
      "INFO: Computed ROM for R HIP as R_HIP_ROM\n",
      "INFO: Computed ROM deviation for R HIP as R_HIP_ROM_deviation\n",
      "INFO: Created binary flag for R HIP ROM extremes: R_HIP_ROM_extreme\n",
      "INFO: Computed ROM for L ANKLE as L_ANKLE_ROM\n",
      "INFO: Computed ROM deviation for L ANKLE as L_ANKLE_ROM_deviation\n",
      "INFO: Created binary flag for L ANKLE ROM extremes: L_ANKLE_ROM_extreme\n",
      "INFO: Computed ROM for R ANKLE as R_ANKLE_ROM\n",
      "INFO: Computed ROM deviation for R ANKLE as R_ANKLE_ROM_deviation\n",
      "INFO: Created binary flag for R ANKLE ROM extremes: R_ANKLE_ROM_extreme\n",
      "INFO: Angle column 'L_WRIST_angle' not found; skipping ROM metrics for L WRIST.\n",
      "INFO: Angle column 'R_WRIST_angle' not found; skipping ROM metrics for R WRIST.\n",
      "INFO: Sorted data by 'participant_id' and 'continuous_frame_time'.\n",
      "INFO: Created 'exhaustion_rate' feature.\n",
      "INFO: Created 'simulated_HR' feature.\n",
      "INFO: Step [prepare_joint_features]: DataFrame shape = (16047, 267)\n",
      "INFO: New columns added: ['player_height_in_meters', 'player_weight__in_kg', 'joint_energy', 'joint_power', 'energy_acceleration', 'ankle_power_ratio', 'hip_asymmetry', 'ankle_asymmetry', 'wrist_asymmetry', 'elbow_asymmetry', 'knee_asymmetry', '1stfinger_asymmetry', '5thfinger_asymmetry', 'hip_power_ratio', 'ankle_power_ratio', 'wrist_power_ratio', 'elbow_power_ratio', 'knee_power_ratio', '1stfinger_power_ratio', '5thfinger_power_ratio', 'L_KNEE_ROM', 'L_KNEE_ROM_deviation', 'L_KNEE_ROM_extreme', 'R_KNEE_ROM', 'R_KNEE_ROM_deviation', 'R_KNEE_ROM_extreme', 'L_SHOULDER_ROM', 'L_SHOULDER_ROM_deviation', 'L_SHOULDER_ROM_extreme', 'R_SHOULDER_ROM', 'R_SHOULDER_ROM_deviation', 'R_SHOULDER_ROM_extreme', 'L_HIP_ROM', 'L_HIP_ROM_deviation', 'L_HIP_ROM_extreme', 'R_HIP_ROM', 'R_HIP_ROM_deviation', 'R_HIP_ROM_extreme', 'L_ANKLE_ROM', 'L_ANKLE_ROM_deviation', 'L_ANKLE_ROM_extreme', 'R_ANKLE_ROM', 'R_ANKLE_ROM_deviation', 'R_ANKLE_ROM_extreme', 'exhaustion_rate', 'simulated_HR']\n",
      "INFO:  - player_height_in_meters: dtype=float64, sample values=[1.91]\n",
      "INFO:  - player_weight__in_kg: dtype=float64, sample values=[90.7]\n",
      "INFO:  - joint_energy: dtype=float64, sample values=[0.         1.24504856 1.26769572 1.27596309 1.24122093]\n",
      "INFO:  - joint_power: dtype=float64, sample values=[ 0.         18.86437211 19.2075109  18.76416304 18.80637774]\n",
      "INFO:  - energy_acceleration: dtype=float64, sample values=[ 0.03772874  0.00068628  0.00024316 -0.00105279 -0.00193555]\n",
      "INFO:  - ankle_power_ratio: dtype=float64, sample values=[0.         0.04922415 0.03166691 0.04290572 0.03725055]\n",
      "INFO:  - hip_asymmetry: dtype=float64, sample values=[0.         0.01136788 0.01341974 0.01384676 0.01642692]\n",
      "INFO:  - ankle_asymmetry: dtype=float64, sample values=[0.         0.09462489 0.11441502 0.12814345 0.14156018]\n",
      "INFO:  - wrist_asymmetry: dtype=float64, sample values=[0.         0.00470701 0.00259394 0.00382874 0.00900069]\n",
      "INFO:  - elbow_asymmetry: dtype=float64, sample values=[0.         0.0239616  0.01633813 0.01263925 0.0120038 ]\n",
      "INFO:  - knee_asymmetry: dtype=float64, sample values=[0.         0.12061726 0.11370624 0.0958499  0.07531242]\n",
      "INFO:  - 1stfinger_asymmetry: dtype=float64, sample values=[0.         0.00462894 0.00358597 0.00835639 0.01393676]\n",
      "INFO:  - 5thfinger_asymmetry: dtype=float64, sample values=[0.         0.00906876 0.00535429 0.00715568 0.01116826]\n",
      "INFO:  - hip_power_ratio: dtype=float64, sample values=[0.         0.74516814 0.69886924 0.68432046 0.64373336]\n",
      "INFO:  - ankle_power_ratio: dtype=float64, sample values=[0.         0.04922415 0.03166691 0.04290572 0.03725055]\n",
      "INFO:  - wrist_power_ratio: dtype=float64, sample values=[0.         0.89402977 0.94064628 0.91239215 0.79490791]\n",
      "INFO:  - elbow_power_ratio: dtype=float64, sample values=[0.         0.352389   0.56734365 0.68912505 0.72023539]\n",
      "INFO:  - knee_power_ratio: dtype=float64, sample values=[0.         0.03660208 0.01500412 0.03344499 0.06341159]\n",
      "INFO:  - 1stfinger_power_ratio: dtype=float64, sample values=[0.         0.8973365  0.92009387 0.82159711 0.70123266]\n",
      "INFO:  - 5thfinger_power_ratio: dtype=float64, sample values=[0.         0.81829684 0.8863865  0.84370078 0.74034267]\n",
      "INFO:  - L_KNEE_ROM: dtype=float64, sample values=[52.00092712 53.68236675 54.04526663 53.31592037 49.12814203]\n",
      "INFO:  - L_KNEE_ROM_deviation: dtype=float64, sample values=[67.99907288 66.31763325 65.95473337 66.68407963 70.87185797]\n",
      "INFO:  - L_KNEE_ROM_extreme: dtype=int32, sample values=[1]\n",
      "INFO:  - R_KNEE_ROM: dtype=float64, sample values=[52.46491718 57.16166718 55.28397109 60.32702297 59.65659628]\n",
      "INFO:  - R_KNEE_ROM_deviation: dtype=float64, sample values=[67.53508282 62.83833282 64.71602891 59.67297703 60.34340372]\n",
      "INFO:  - R_KNEE_ROM_extreme: dtype=int32, sample values=[1]\n",
      "INFO:  - L_SHOULDER_ROM: dtype=float64, sample values=[106.59807383 109.54498215 108.02057817 111.7412054  108.67548157]\n",
      "INFO:  - L_SHOULDER_ROM_deviation: dtype=float64, sample values=[0.]\n",
      "INFO:  - L_SHOULDER_ROM_extreme: dtype=int32, sample values=[0]\n",
      "INFO:  - R_SHOULDER_ROM: dtype=float64, sample values=[127.36630171 132.42520607 131.55366227 126.88154535 124.04488661]\n",
      "INFO:  - R_SHOULDER_ROM_deviation: dtype=float64, sample values=[0.]\n",
      "INFO:  - R_SHOULDER_ROM_extreme: dtype=int32, sample values=[0]\n",
      "INFO:  - L_HIP_ROM: dtype=float64, sample values=[41.87334176 42.69399027 44.29157825 43.33340404 39.10888539]\n",
      "INFO:  - L_HIP_ROM_deviation: dtype=float64, sample values=[0.]\n",
      "INFO:  - L_HIP_ROM_extreme: dtype=int32, sample values=[0]\n",
      "INFO:  - R_HIP_ROM: dtype=float64, sample values=[51.00676746 53.8922429  56.17856317 53.15755447 50.23341005]\n",
      "INFO:  - R_HIP_ROM_deviation: dtype=float64, sample values=[0.]\n",
      "INFO:  - R_HIP_ROM_extreme: dtype=int32, sample values=[0]\n",
      "INFO:  - L_ANKLE_ROM: dtype=float64, sample values=[32.32505371 32.99123382 38.821896   35.45367686 32.1162011 ]\n",
      "INFO:  - L_ANKLE_ROM_deviation: dtype=float64, sample values=[12.32505371 12.99123382 18.821896   15.45367686 12.1162011 ]\n",
      "INFO:  - L_ANKLE_ROM_extreme: dtype=int32, sample values=[1]\n",
      "INFO:  - R_ANKLE_ROM: dtype=float64, sample values=[39.81071151 45.54250042 45.70572465 43.1338381  39.83072086]\n",
      "INFO:  - R_ANKLE_ROM_deviation: dtype=float64, sample values=[19.81071151 25.54250042 25.70572465 23.1338381  19.83072086]\n",
      "INFO:  - R_ANKLE_ROM_extreme: dtype=int32, sample values=[1]\n",
      "INFO:  - exhaustion_rate: dtype=float64, sample values=[0.00025383 0.00025845 0.00025248 0.00025305 0.00024003]\n",
      "INFO:  - simulated_HR: dtype=float64, sample values=[60.         60.38607909 60.4056663  60.421023   60.42312625]\n",
      "C:\\Users\\ghadf\\AppData\\Local\\Temp\\ipykernel_31768\\61768157.py:394: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  data[motion_cols] = data[motion_cols].fillna(method='ffill').fillna(0)\n",
      "INFO: Created 'rolling_energy_std' with sample: [0.0, 0.6225242794725895, 0.592331668013902, 0.5469698974921113, 0.5031647476376774, 0.0346469804150002, 0.05501279562072104, 0.06384083445684284, 0.05698938811919827, 0.039917330417864196]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joint energy columns:  ['L_ANKLE_energy', 'R_ANKLE_energy', 'L_KNEE_energy', 'R_KNEE_energy', 'L_HIP_energy', 'R_HIP_energy', 'L_ELBOW_energy', 'R_ELBOW_energy', 'L_WRIST_energy', 'R_WRIST_energy', 'L_1STFINGER_energy', 'R_1STFINGER_energy', 'L_5THFINGER_energy', 'R_5THFINGER_energy', 'total_energy']\n",
      "Joint power columns:  ['L_ANKLE_ongoing_power', 'R_ANKLE_ongoing_power', 'L_KNEE_ongoing_power', 'R_KNEE_ongoing_power', 'L_HIP_ongoing_power', 'R_HIP_ongoing_power', 'L_ELBOW_ongoing_power', 'R_ELBOW_ongoing_power', 'L_WRIST_ongoing_power', 'R_WRIST_ongoing_power', 'L_1STFINGER_ongoing_power', 'L_5THFINGER_ongoing_power', 'R_1STFINGER_ongoing_power', 'R_5THFINGER_ongoing_power']\n",
      "All angle columns:  ['entry_angle', 'elbow_angle', 'wrist_angle', 'knee_angle', 'initial_release_angle', 'calculated_release_angle', 'angle_difference', 'optimal_release_angle', 'L_SHOULDER_angle', 'R_SHOULDER_angle', 'L_HIP_angle', 'R_HIP_angle', 'L_KNEE_angle', 'R_KNEE_angle', 'L_ANKLE_angle', 'R_ANKLE_angle']\n",
      "print all the columns with by_trial_exhaustion_score:  ['by_trial_exhaustion_score', 'L_ANKLE_energy_by_trial_exhaustion_score', 'R_ANKLE_energy_by_trial_exhaustion_score', 'L_KNEE_energy_by_trial_exhaustion_score', 'R_KNEE_energy_by_trial_exhaustion_score', 'L_HIP_energy_by_trial_exhaustion_score', 'R_HIP_energy_by_trial_exhaustion_score', 'L_ELBOW_energy_by_trial_exhaustion_score', 'R_ELBOW_energy_by_trial_exhaustion_score', 'L_WRIST_energy_by_trial_exhaustion_score', 'R_WRIST_energy_by_trial_exhaustion_score', 'L_1STFINGER_energy_by_trial_exhaustion_score', 'R_1STFINGER_energy_by_trial_exhaustion_score', 'L_5THFINGER_energy_by_trial_exhaustion_score', 'R_5THFINGER_energy_by_trial_exhaustion_score']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created 'rolling_energy_std' with window 5.\n",
      "C:\\Users\\ghadf\\AppData\\Local\\Temp\\ipykernel_31768\\61768157.py:443: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{joint_name}_rolling_exhaustion'] = data[score_col].rolling(rolling_window, min_periods=1).sum()\n",
      "C:\\Users\\ghadf\\AppData\\Local\\Temp\\ipykernel_31768\\61768157.py:447: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{joint_name}_injury_risk'] = (rolling_series > safe_expanding_quantile(rolling_series)).astype(int)\n",
      "C:\\Users\\ghadf\\AppData\\Local\\Temp\\ipykernel_31768\\61768157.py:442: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{joint_name}_exhaustion_rate'] = data[score_col].diff() / dt\n",
      "C:\\Users\\ghadf\\AppData\\Local\\Temp\\ipykernel_31768\\61768157.py:443: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{joint_name}_rolling_exhaustion'] = data[score_col].rolling(rolling_window, min_periods=1).sum()\n",
      "C:\\Users\\ghadf\\AppData\\Local\\Temp\\ipykernel_31768\\61768157.py:447: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{joint_name}_injury_risk'] = (rolling_series > safe_expanding_quantile(rolling_series)).astype(int)\n",
      "INFO: Step [feature_engineering]: DataFrame shape = (16046, 306)\n",
      "INFO: New columns added: ['time_since_start', 'rolling_energy_std', 'exhaustion_lag1', 'ema_exhaustion', 'rolling_exhaustion', 'injury_risk', 'L_ANKLE_exhaustion_rate', 'L_ANKLE_rolling_exhaustion', 'L_ANKLE_injury_risk', 'R_ANKLE_exhaustion_rate', 'R_ANKLE_rolling_exhaustion', 'R_ANKLE_injury_risk', 'L_WRIST_exhaustion_rate', 'L_WRIST_rolling_exhaustion', 'L_WRIST_injury_risk', 'R_WRIST_exhaustion_rate', 'R_WRIST_rolling_exhaustion', 'R_WRIST_injury_risk', 'L_ELBOW_exhaustion_rate', 'L_ELBOW_rolling_exhaustion', 'L_ELBOW_injury_risk', 'R_ELBOW_exhaustion_rate', 'R_ELBOW_rolling_exhaustion', 'R_ELBOW_injury_risk', 'L_KNEE_exhaustion_rate', 'L_KNEE_rolling_exhaustion', 'L_KNEE_injury_risk', 'R_KNEE_exhaustion_rate', 'R_KNEE_rolling_exhaustion', 'R_KNEE_injury_risk', 'L_HIP_exhaustion_rate', 'L_HIP_rolling_exhaustion', 'L_HIP_injury_risk', 'R_HIP_exhaustion_rate', 'R_HIP_rolling_exhaustion', 'R_HIP_injury_risk']\n",
      "INFO:  - time_since_start: dtype=int64, sample values=[ 33  66 100 133 166]\n",
      "INFO:  - rolling_energy_std: dtype=float64, sample values=[0.62252428 0.59233167 0.5469699  0.50316475 0.03464698]\n",
      "INFO:  - exhaustion_lag1: dtype=float64, sample values=[0.         0.00837635 0.01690506 0.02548939 0.03383998]\n",
      "INFO:  - ema_exhaustion: dtype=float64, sample values=[0.00152297 0.00431971 0.00816875 0.01283624 0.01809526]\n",
      "INFO:  - rolling_exhaustion: dtype=float64, sample values=[0.00837635 0.0252814  0.05077079 0.08461077 0.12637162]\n",
      "INFO:  - injury_risk: dtype=int32, sample values=[1 0]\n",
      "INFO:  - L_ANKLE_exhaustion_rate: dtype=float64, sample values=[1.23696376e-04 9.44746676e-05 1.40780774e-04 1.38296753e-04\n",
      " 7.98456673e-05]\n",
      "INFO:  - L_ANKLE_rolling_exhaustion: dtype=float64, sample values=[0.00408198 0.01128162 0.02326782 0.0398178  0.05900269]\n",
      "INFO:  - L_ANKLE_injury_risk: dtype=int32, sample values=[1 0]\n",
      "INFO:  - R_ANKLE_exhaustion_rate: dtype=float64, sample values=[0.00142576 0.00169269 0.00186164 0.00210643 0.00218641]\n",
      "INFO:  - R_ANKLE_rolling_exhaustion: dtype=float64, sample values=[0.04704999 0.14995866 0.316163   0.5518794  0.85974725]\n",
      "INFO:  - R_ANKLE_injury_risk: dtype=int32, sample values=[1 0]\n",
      "INFO:  - L_WRIST_exhaustion_rate: dtype=float64, sample values=[0.00017214 0.0001782  0.00016777 0.00015122 0.00012138]\n",
      "INFO:  - L_WRIST_rolling_exhaustion: dtype=float64, sample values=[0.00568072 0.01724215 0.03450767 0.05676355 0.08302483]\n",
      "INFO:  - L_WRIST_injury_risk: dtype=int32, sample values=[1 0]\n",
      "INFO:  - R_WRIST_exhaustion_rate: dtype=float64, sample values=[0.00016915 0.00016642 0.00016153 0.00016712 0.00016516]\n",
      "INFO:  - R_WRIST_rolling_exhaustion: dtype=float64, sample values=[0.00558186 0.01665573 0.03322162 0.05530248 0.08283351]\n",
      "INFO:  - R_WRIST_injury_risk: dtype=int32, sample values=[1 0]\n",
      "INFO:  - L_ELBOW_exhaustion_rate: dtype=float64, sample values=[7.85882352e-05 1.29133648e-04 1.63909048e-04 1.86266503e-04\n",
      " 1.94752627e-04]\n",
      "INFO:  - L_ELBOW_rolling_exhaustion: dtype=float64, sample values=[0.00259341 0.00944823 0.02187596 0.04045049 0.06545185]\n",
      "INFO:  - L_ELBOW_injury_risk: dtype=int32, sample values=[1 0]\n",
      "INFO:  - R_ELBOW_exhaustion_rate: dtype=float64, sample values=[0.00018342 0.0001872  0.00019562 0.00021271 0.0002313 ]\n",
      "INFO:  - R_ELBOW_rolling_exhaustion: dtype=float64, sample values=[0.00605294 0.01828356 0.03716539 0.06306649 0.09660057]\n",
      "INFO:  - R_ELBOW_injury_risk: dtype=int32, sample values=[1 0]\n",
      "INFO:  - L_KNEE_exhaustion_rate: dtype=float64, sample values=[5.75663372e-05 2.17580303e-05 4.04380603e-05 6.40539068e-05\n",
      " 7.43178217e-05]\n",
      "INFO:  - L_KNEE_rolling_exhaustion: dtype=float64, sample values=[0.00189969 0.00451739 0.00850999 0.01461637 0.02317523]\n",
      "INFO:  - L_KNEE_injury_risk: dtype=int32, sample values=[1 0]\n",
      "INFO:  - R_KNEE_exhaustion_rate: dtype=float64, sample values=[0.00140049 0.0012913  0.00107665 0.00089948 0.00066695]\n",
      "INFO:  - R_KNEE_rolling_exhaustion: dtype=float64, sample values=[0.04621615 0.13504507 0.26048023 0.41559839 0.59272603]\n",
      "INFO:  - R_KNEE_injury_risk: dtype=int32, sample values=[1 0]\n",
      "INFO:  - L_HIP_exhaustion_rate: dtype=float64, sample values=[0.00039011 0.0003655  0.0003419  0.00034833 0.00035827]\n",
      "INFO:  - L_HIP_rolling_exhaustion: dtype=float64, sample values=[0.01287351 0.03780854 0.07436817 0.12242266 0.18230005]\n",
      "INFO:  - L_HIP_injury_risk: dtype=int32, sample values=[1 0]\n",
      "INFO:  - R_HIP_exhaustion_rate: dtype=float64, sample values=[0.00049068 0.00049019 0.00046828 0.00050717 0.00051545]\n",
      "INFO:  - R_HIP_rolling_exhaustion: dtype=float64, sample values=[0.01619242 0.04856097 0.09685115 0.16187792 0.24391457]\n",
      "INFO:  - R_HIP_injury_risk: dtype=int32, sample values=[1 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['trial_id', 'result', 'landing_x', 'landing_y', 'entry_angle', 'frame_time', 'ball_x', 'ball_y', 'ball_z', 'R_EYE_x', 'R_EYE_y', 'R_EYE_z', 'L_EYE_x', 'L_EYE_y', 'L_EYE_z', 'NOSE_x', 'NOSE_y', 'NOSE_z', 'R_EAR_x', 'R_EAR_y', 'R_EAR_z', 'L_EAR_x', 'L_EAR_y', 'L_EAR_z', 'R_SHOULDER_x', 'R_SHOULDER_y', 'R_SHOULDER_z', 'L_SHOULDER_x', 'L_SHOULDER_y', 'L_SHOULDER_z', 'R_ELBOW_x', 'R_ELBOW_y', 'R_ELBOW_z', 'L_ELBOW_x', 'L_ELBOW_y', 'L_ELBOW_z', 'R_WRIST_x', 'R_WRIST_y', 'R_WRIST_z', 'L_WRIST_x', 'L_WRIST_y', 'L_WRIST_z', 'R_HIP_x', 'R_HIP_y', 'R_HIP_z', 'L_HIP_x', 'L_HIP_y', 'L_HIP_z', 'R_KNEE_x', 'R_KNEE_y', 'R_KNEE_z', 'L_KNEE_x', 'L_KNEE_y', 'L_KNEE_z', 'R_ANKLE_x', 'R_ANKLE_y', 'R_ANKLE_z', 'L_ANKLE_x', 'L_ANKLE_y', 'L_ANKLE_z', 'R_1STFINGER_x', 'R_1STFINGER_y', 'R_1STFINGER_z', 'R_5THFINGER_x', 'R_5THFINGER_y', 'R_5THFINGER_z', 'L_1STFINGER_x', 'L_1STFINGER_y', 'L_1STFINGER_z', 'L_5THFINGER_x', 'L_5THFINGER_y', 'L_5THFINGER_z', 'R_1STTOE_x', 'R_1STTOE_y', 'R_1STTOE_z', 'R_5THTOE_x', 'R_5THTOE_y', 'R_5THTOE_z', 'L_1STTOE_x', 'L_1STTOE_y', 'L_1STTOE_z', 'L_5THTOE_x', 'L_5THTOE_y', 'L_5THTOE_z', 'R_CALC_x', 'R_CALC_y', 'R_CALC_z', 'L_CALC_x', 'L_CALC_y', 'L_CALC_z', 'ball_speed', 'ball_velocity_x', 'ball_velocity_y', 'ball_velocity_z', 'overall_ball_velocity', 'ball_direction_x', 'ball_direction_y', 'ball_direction_z', 'computed_ball_velocity_x', 'computed_ball_velocity_y', 'computed_ball_velocity_z', 'dist_ball_R_1STFINGER', 'dist_ball_R_5THFINGER', 'dist_ball_L_1STFINGER', 'dist_ball_L_5THFINGER', 'ball_in_hands', 'shooting_motion', 'avg_shoulder_height', 'release_point_filter', 'dt', 'dx', 'dy', 'dz', 'L_ANKLE_ongoing_power', 'R_ANKLE_ongoing_power', 'L_KNEE_ongoing_power', 'R_KNEE_ongoing_power', 'L_HIP_ongoing_power', 'R_HIP_ongoing_power', 'L_ELBOW_ongoing_power', 'R_ELBOW_ongoing_power', 'L_WRIST_ongoing_power', 'R_WRIST_ongoing_power', 'L_1STFINGER_ongoing_power', 'L_5THFINGER_ongoing_power', 'R_1STFINGER_ongoing_power', 'R_5THFINGER_ongoing_power', 'elbow_angle', 'wrist_angle', 'knee_angle', 'player_height_in_meters', 'player_height_ft', 'initial_release_angle', 'calculated_release_angle', 'angle_difference', 'distance_to_basket', 'optimal_release_angle', 'by_trial_time', 'continuous_frame_time', 'L_ANKLE_energy', 'R_ANKLE_energy', 'L_KNEE_energy', 'R_KNEE_energy', 'L_HIP_energy', 'R_HIP_energy', 'L_ELBOW_energy', 'R_ELBOW_energy', 'L_WRIST_energy', 'R_WRIST_energy', 'L_1STFINGER_energy', 'R_1STFINGER_energy', 'L_5THFINGER_energy', 'R_5THFINGER_energy', 'total_energy', 'by_trial_energy', 'by_trial_exhaustion_score', 'overall_cumulative_energy', 'overall_exhaustion_score', 'L_ANKLE_energy_by_trial', 'L_ANKLE_energy_by_trial_exhaustion_score', 'L_ANKLE_energy_overall_cumulative', 'L_ANKLE_energy_overall_exhaustion_score', 'R_ANKLE_energy_by_trial', 'R_ANKLE_energy_by_trial_exhaustion_score', 'R_ANKLE_energy_overall_cumulative', 'R_ANKLE_energy_overall_exhaustion_score', 'L_KNEE_energy_by_trial', 'L_KNEE_energy_by_trial_exhaustion_score', 'L_KNEE_energy_overall_cumulative', 'L_KNEE_energy_overall_exhaustion_score', 'R_KNEE_energy_by_trial', 'R_KNEE_energy_by_trial_exhaustion_score', 'R_KNEE_energy_overall_cumulative', 'R_KNEE_energy_overall_exhaustion_score', 'L_HIP_energy_by_trial', 'L_HIP_energy_by_trial_exhaustion_score', 'L_HIP_energy_overall_cumulative', 'L_HIP_energy_overall_exhaustion_score', 'R_HIP_energy_by_trial', 'R_HIP_energy_by_trial_exhaustion_score', 'R_HIP_energy_overall_cumulative', 'R_HIP_energy_overall_exhaustion_score', 'L_ELBOW_energy_by_trial', 'L_ELBOW_energy_by_trial_exhaustion_score', 'L_ELBOW_energy_overall_cumulative', 'L_ELBOW_energy_overall_exhaustion_score', 'R_ELBOW_energy_by_trial', 'R_ELBOW_energy_by_trial_exhaustion_score', 'R_ELBOW_energy_overall_cumulative', 'R_ELBOW_energy_overall_exhaustion_score', 'L_WRIST_energy_by_trial', 'L_WRIST_energy_by_trial_exhaustion_score', 'L_WRIST_energy_overall_cumulative', 'L_WRIST_energy_overall_exhaustion_score', 'R_WRIST_energy_by_trial', 'R_WRIST_energy_by_trial_exhaustion_score', 'R_WRIST_energy_overall_cumulative', 'R_WRIST_energy_overall_exhaustion_score', 'L_1STFINGER_energy_by_trial', 'L_1STFINGER_energy_by_trial_exhaustion_score', 'L_1STFINGER_energy_overall_cumulative', 'L_1STFINGER_energy_overall_exhaustion_score', 'R_1STFINGER_energy_by_trial', 'R_1STFINGER_energy_by_trial_exhaustion_score', 'R_1STFINGER_energy_overall_cumulative', 'R_1STFINGER_energy_overall_exhaustion_score', 'L_5THFINGER_energy_by_trial', 'L_5THFINGER_energy_by_trial_exhaustion_score', 'L_5THFINGER_energy_overall_cumulative', 'L_5THFINGER_energy_overall_exhaustion_score', 'R_5THFINGER_energy_by_trial', 'R_5THFINGER_energy_by_trial_exhaustion_score', 'R_5THFINGER_energy_overall_cumulative', 'R_5THFINGER_energy_overall_exhaustion_score', 'participant_id', 'L_SHOULDER_angle', 'R_SHOULDER_angle', 'L_HIP_angle', 'R_HIP_angle', 'L_KNEE_angle', 'R_KNEE_angle', 'L_ANKLE_angle', 'R_ANKLE_angle', 'player_weight__in_kg', 'joint_energy', 'joint_power', 'energy_acceleration', 'ankle_power_ratio', 'hip_asymmetry', 'ankle_asymmetry', 'wrist_asymmetry', 'elbow_asymmetry', 'knee_asymmetry', '1stfinger_asymmetry', '5thfinger_asymmetry', 'hip_power_ratio', 'wrist_power_ratio', 'elbow_power_ratio', 'knee_power_ratio', '1stfinger_power_ratio', '5thfinger_power_ratio', 'L_KNEE_ROM', 'L_KNEE_ROM_deviation', 'L_KNEE_ROM_extreme', 'R_KNEE_ROM', 'R_KNEE_ROM_deviation', 'R_KNEE_ROM_extreme', 'L_SHOULDER_ROM', 'L_SHOULDER_ROM_deviation', 'L_SHOULDER_ROM_extreme', 'R_SHOULDER_ROM', 'R_SHOULDER_ROM_deviation', 'R_SHOULDER_ROM_extreme', 'L_HIP_ROM', 'L_HIP_ROM_deviation', 'L_HIP_ROM_extreme', 'R_HIP_ROM', 'R_HIP_ROM_deviation', 'R_HIP_ROM_extreme', 'L_ANKLE_ROM', 'L_ANKLE_ROM_deviation', 'L_ANKLE_ROM_extreme', 'R_ANKLE_ROM', 'R_ANKLE_ROM_deviation', 'R_ANKLE_ROM_extreme', 'exhaustion_rate', 'simulated_HR', 'time_since_start', 'power_avg_5', 'rolling_power_std', 'rolling_hr_mean', 'rolling_energy_std', 'exhaustion_lag1', 'ema_exhaustion', 'rolling_exhaustion', 'injury_risk', 'L_ANKLE_exhaustion_rate', 'L_ANKLE_rolling_exhaustion', 'L_ANKLE_injury_risk', 'R_ANKLE_exhaustion_rate', 'R_ANKLE_rolling_exhaustion', 'R_ANKLE_injury_risk', 'L_WRIST_exhaustion_rate', 'L_WRIST_rolling_exhaustion', 'L_WRIST_injury_risk', 'R_WRIST_exhaustion_rate', 'R_WRIST_rolling_exhaustion', 'R_WRIST_injury_risk', 'L_ELBOW_exhaustion_rate', 'L_ELBOW_rolling_exhaustion', 'L_ELBOW_injury_risk', 'R_ELBOW_exhaustion_rate', 'R_ELBOW_rolling_exhaustion', 'R_ELBOW_injury_risk', 'L_KNEE_exhaustion_rate', 'L_KNEE_rolling_exhaustion', 'L_KNEE_injury_risk', 'R_KNEE_exhaustion_rate', 'R_KNEE_rolling_exhaustion', 'R_KNEE_injury_risk', 'L_HIP_exhaustion_rate', 'L_HIP_rolling_exhaustion', 'L_HIP_injury_risk', 'R_HIP_exhaustion_rate', 'R_HIP_rolling_exhaustion', 'R_HIP_injury_risk']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: All required features and target variables are present.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import logging\n",
    "import sys\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "import shap\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "\n",
    "def save_top_features(results, output_dir=\"feature_lists\", n_top=10):\n",
    "    \"\"\"\n",
    "    Saves top features for each target to pickle files.\n",
    "    \n",
    "    Parameters:\n",
    "      - results (dict): Results dictionary from feature importance analysis.\n",
    "                        Keys are target names and values are tuples (combined DataFrame, model).\n",
    "      - output_dir (str): Directory to save feature lists.\n",
    "      - n_top (int): Number of top features to consider.\n",
    "      \n",
    "    The function selects the top n features based on consensus ranking.\n",
    "    If any of the selected top features have 0 importance in either Permutation Importance or SHAP,\n",
    "    those features are filtered out. The final list of features may contain fewer than n_top features.\n",
    "    \"\"\"\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    for target, (combined, _) in results.items():\n",
    "        if 'Consensus_Rank' not in combined.columns:\n",
    "            combined['Consensus_Rank'] = (\n",
    "                combined['Perm_Importance'].rank(ascending=False) +\n",
    "                combined['RFE_Rank'] +\n",
    "                combined['SHAP_Importance'].rank(ascending=False)\n",
    "            )\n",
    "        valid_features = combined[\n",
    "            (combined['Perm_Importance'] > 0) | (combined['SHAP_Importance'] > 0)\n",
    "        ]\n",
    "        if valid_features.empty:\n",
    "            logging.warning(f\"No features with positive importance for {target}\")\n",
    "            continue\n",
    "        top_n = valid_features.nsmallest(n_top, 'Consensus_Rank')\n",
    "        filtered_top_features = top_n[\n",
    "            (top_n['Perm_Importance'] > 0) & (top_n['SHAP_Importance'] > 0)\n",
    "        ]\n",
    "        top_features = filtered_top_features['Feature'].tolist()\n",
    "        filename = Path(output_dir) / f\"{target}_model_feature_list.pkl\"\n",
    "        pd.to_pickle(top_features, filename)\n",
    "        logging.info(f\"Saved top features for {target} to {filename}: {top_features}\")\n",
    "\n",
    "def perform_feature_importance_analysis(data, features, target, n_features_to_select=5, debug=False):\n",
    "    \"\"\"\n",
    "    Performs feature importance analysis using Permutation Importance, RFE, and SHAP.\n",
    "    \n",
    "    Steps:\n",
    "      1. Prepare the data (handle missing values and split into training/testing sets).\n",
    "      2. Train a RandomForestRegressor (with fewer trees for improved efficiency).\n",
    "      3. Compute Permutation Importance (with fewer repeats).\n",
    "      4. Perform Recursive Feature Elimination (RFE) on the training data.\n",
    "      5. Merge results into one combined DataFrame.\n",
    "      6. Compute SHAP values using a subsample of the test set for faster approximation.\n",
    "      7. Optionally, produce debug plots if debug=True.\n",
    "    \n",
    "    Returns:\n",
    "      - combined (pd.DataFrame): DataFrame containing permutation importance, RFE rankings, and SHAP importance.\n",
    "      - rf (RandomForestRegressor): The fitted model.\n",
    "    \"\"\"\n",
    "    X = data[features].fillna(method='ffill').fillna(method='bfill')\n",
    "    y = data[target].fillna(method='ffill').fillna(method='bfill')\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "    rf = RandomForestRegressor(n_estimators=50, random_state=42)  # Reduced from 100\n",
    "    rf.fit(X_train, y_train)\n",
    "    perm_result = permutation_importance(rf, X_test, y_test, n_repeats=10, random_state=42)  # Reduced from 30\n",
    "    perm_df = pd.DataFrame({\n",
    "        'Feature': features,\n",
    "        'Perm_Importance': perm_result.importances_mean\n",
    "    })\n",
    "    rfe_selector = RFE(estimator=rf, n_features_to_select=n_features_to_select, step=1)\n",
    "    rfe_selector.fit(X_train, y_train)\n",
    "    rfe_df = pd.DataFrame({\n",
    "        'Feature': features,\n",
    "        'RFE_Rank': rfe_selector.ranking_,\n",
    "        'RFE_Support': rfe_selector.support_\n",
    "    })\n",
    "    combined = perm_df.merge(rfe_df, on='Feature')\n",
    "    explainer = shap.TreeExplainer(rf)\n",
    "    sample_size = min(100, X_test.shape[0])\n",
    "    X_test_sampled = X_test.sample(sample_size, random_state=42)\n",
    "    shap_values = explainer.shap_values(X_test_sampled)\n",
    "    shap_abs = np.abs(shap_values).mean(axis=0)\n",
    "    shap_df = pd.DataFrame({\n",
    "        'Feature': features,\n",
    "        'SHAP_Importance': shap_abs\n",
    "    })\n",
    "    combined = combined.merge(shap_df, on='Feature')\n",
    "    if debug:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(x='Perm_Importance', y='Feature', data=combined.nlargest(20, 'Perm_Importance'))\n",
    "        plt.title('Top Permutation Importances')\n",
    "        plt.xlabel('Mean Permutation Importance')\n",
    "        plt.ylabel('Feature')\n",
    "        plt.show()\n",
    "        shap.summary_plot(shap_values, X_test_sampled, plot_type=\"bar\", max_display=20)\n",
    "    return combined, rf\n",
    "\n",
    "def analyze_and_display_top_features(results, n_top=5):\n",
    "    \"\"\"\n",
    "    For each target in the results dictionary, extracts and displays the top features based on:\n",
    "      - Permutation Importance\n",
    "      - RFE (only the features selected by RFE support)\n",
    "      - SHAP Importance\n",
    "      - A consensus ranking calculated from the three metrics.\n",
    "    \n",
    "    Parameters:\n",
    "      - results (dict): A dictionary where keys are target names and values are tuples:\n",
    "                          (combined DataFrame, fitted model).\n",
    "      - n_top (int): Number of top features to display for each method.\n",
    "    \n",
    "    This function prints the top features for each target.\n",
    "    \"\"\"\n",
    "    for target, (combined, _) in results.items():\n",
    "        print(f\"\\n=== Feature Analysis for Target: {target.upper()} ===\")\n",
    "        perm_top = combined.nlargest(n_top, 'Perm_Importance')['Feature'].tolist()\n",
    "        rfe_top = combined[combined['RFE_Support']]['Feature'].tolist()\n",
    "        shap_top = combined.nlargest(n_top, 'SHAP_Importance')['Feature'].tolist()\n",
    "        combined['Consensus_Rank'] = (\n",
    "            combined['Perm_Importance'].rank(ascending=False) +\n",
    "            combined['RFE_Rank'] +\n",
    "            combined['SHAP_Importance'].rank(ascending=False)\n",
    "        )\n",
    "        consensus_top = combined.nsmallest(n_top, 'Consensus_Rank')['Feature'].tolist()\n",
    "        print(f\"Permutation Top {n_top}: {perm_top}\")\n",
    "        print(f\"RFE Selected Features: {rfe_top}\")\n",
    "        print(f\"SHAP Top {n_top}: {shap_top}\")\n",
    "        print(f\"Consensus Top {n_top}: {consensus_top}\")\n",
    "\n",
    "def check_for_invalid_values(df):\n",
    "    \"\"\"Check DataFrame for inf/na values and extreme magnitudes\"\"\"\n",
    "    numeric_df = df.select_dtypes(include=[np.number])\n",
    "    inf_mask = numeric_df.isin([np.inf, -np.inf])\n",
    "    if inf_mask.any().any():\n",
    "        logging.error(f\"Found infinite values in columns: {numeric_df.columns[inf_mask.any()].tolist()}\")\n",
    "    na_mask = numeric_df.isna()\n",
    "    if na_mask.any().any():\n",
    "        logging.error(f\"Found NA values in columns: {numeric_df.columns[na_mask.any()].tolist()}\")\n",
    "    extreme_mask = (numeric_df.abs() > 1e30).any(axis=1)\n",
    "    if extreme_mask.any():\n",
    "        logging.error(f\"Found extreme values (>1e30) in rows: {numeric_df.index[extreme_mask].tolist()}\")\n",
    "    return inf_mask.sum().sum() + na_mask.sum().sum() + extreme_mask.sum()\n",
    "\n",
    "def analyze_joint_injury_features(results, joint, n_top=10):\n",
    "    \"\"\"\n",
    "    Aggregates feature importance metrics for a specific joint injury model.\n",
    "    \n",
    "    This function looks for keys in the results dictionary that correspond\n",
    "    to the given joint (e.g. keys containing '_ANKLE_injury_risk') and then\n",
    "    aggregates the metrics across the different sides (e.g. L and R). The\n",
    "    aggregation is done by averaging the numeric importance metrics and taking\n",
    "    the logical OR for the RFE support.\n",
    "    \n",
    "    After aggregation, a consensus ranking is computed and the top features\n",
    "    (filtered to remove any features with zero in either Permutation or SHAP importance)\n",
    "    are returned.\n",
    "    \n",
    "    Parameters:\n",
    "      - results (dict): Dictionary of results from feature importance analysis.\n",
    "                        Keys are target names and values are tuples (combined DataFrame, model).\n",
    "      - joint (str): The joint name (e.g. \"ANKLE\") for which to aggregate the models.\n",
    "      - n_top (int): Number of top features to select.\n",
    "      \n",
    "    Returns:\n",
    "      - top_features (list): List of aggregated top features for the given joint.\n",
    "      - agg_df (pd.DataFrame): The aggregated dataframe of feature importance metrics.\n",
    "    \"\"\"\n",
    "    joint_keys = [key for key in results if f\"_{joint}_injury_risk\" in key]\n",
    "    if not joint_keys:\n",
    "        logging.warning(f\"No injury models found for joint: {joint}\")\n",
    "        return [], None\n",
    "    df_list = []\n",
    "    for key in joint_keys:\n",
    "        combined_df, _ = results[key]\n",
    "        df_list.append(combined_df.copy())\n",
    "    concat_df = pd.concat(df_list, axis=0)\n",
    "    agg_df = concat_df.groupby(\"Feature\", as_index=False).agg({\n",
    "        'Perm_Importance': 'mean',\n",
    "        'SHAP_Importance': 'mean',\n",
    "        'RFE_Rank': 'mean',\n",
    "        'RFE_Support': 'max'\n",
    "    })\n",
    "    agg_df['Consensus_Rank'] = (\n",
    "        agg_df['Perm_Importance'].rank(ascending=False) +\n",
    "        agg_df['RFE_Rank'].rank(ascending=True) +\n",
    "        agg_df['SHAP_Importance'].rank(ascending=False)\n",
    "    )\n",
    "    agg_df = agg_df.sort_values(\"Consensus_Rank\")\n",
    "    top_n = agg_df.nsmallest(n_top, \"Consensus_Rank\")\n",
    "    filtered_top = top_n[\n",
    "        (top_n['Perm_Importance'] > 0) & (top_n['SHAP_Importance'] > 0)\n",
    "    ]\n",
    "    top_features = filtered_top['Feature'].tolist()\n",
    "    logging.info(f\"Aggregated top features for joint {joint}: {top_features}\")\n",
    "    return top_features, agg_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    debug = True\n",
    "    csv_path = \"../../data/processed/final_granular_dataset.csv\"\n",
    "    json_path = \"../../data/basketball/freethrow/participant_information.json\"\n",
    "    output_dir = \"../../data/Deep_Learning_Final\"  # Directory for saving feature lists\n",
    "    \n",
    "    # Load and process the data (assumed to be defined externally)\n",
    "    data = load_data(csv_path, json_path, debug=debug)\n",
    "    data = prepare_joint_features(data, debug=debug)\n",
    "    data = feature_engineering(data, debug=debug)\n",
    "    print(data.columns.tolist())\n",
    "    \n",
    "    # Check for invalid values before model fitting\n",
    "    invalid_count = check_for_invalid_values(data)\n",
    "    if invalid_count > 0:\n",
    "        logging.error(\"Invalid values detected in feature matrix\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # Define your complete features list (as before)\n",
    "    features = [\n",
    "        'joint_energy', 'joint_power', 'energy_acceleration',\n",
    "        'elbow_asymmetry', 'hip_asymmetry', 'ankle_asymmetry', 'wrist_asymmetry', 'knee_asymmetry', '1stfinger_asymmetry', '5thfinger_asymmetry',\n",
    "        'elbow_power_ratio', 'hip_power_ratio', 'ankle_power_ratio', 'wrist_power_ratio', 'knee_power_ratio', '1stfinger_power_ratio', '5thfinger_power_ratio',\n",
    "        'L_KNEE_ROM', 'L_KNEE_ROM_deviation', 'L_KNEE_ROM_extreme',\n",
    "        'R_KNEE_ROM', 'R_KNEE_ROM_deviation', 'R_KNEE_ROM_extreme',\n",
    "        'L_SHOULDER_ROM', 'L_SHOULDER_ROM_deviation', 'L_SHOULDER_ROM_extreme',\n",
    "        'R_SHOULDER_ROM', 'R_SHOULDER_ROM_deviation', 'R_SHOULDER_ROM_extreme',\n",
    "        'L_HIP_ROM', 'L_HIP_ROM_deviation', 'L_HIP_ROM_extreme',\n",
    "        'R_HIP_ROM', 'R_HIP_ROM_deviation', 'R_HIP_ROM_extreme',\n",
    "        'L_ANKLE_ROM', 'L_ANKLE_ROM_deviation', 'L_ANKLE_ROM_extreme',\n",
    "        'R_ANKLE_ROM', 'R_ANKLE_ROM_deviation', 'R_ANKLE_ROM_extreme',\n",
    "        'exhaustion_lag1', 'power_avg_5', 'rolling_power_std', 'rolling_hr_mean',\n",
    "        'time_since_start', 'ema_exhaustion', 'rolling_exhaustion', 'rolling_energy_std',\n",
    "        'simulated_HR',\n",
    "        'player_height_in_meters', 'player_weight__in_kg'\n",
    "    ]\n",
    "    \n",
    "    # Define target variables.\n",
    "    targets = ['by_trial_exhaustion_score', 'injury_risk']\n",
    "    joints = ['ANKLE', 'WRIST', 'ELBOW', 'KNEE', 'HIP']\n",
    "    for joint in joints:\n",
    "        for side in ['L', 'R']:\n",
    "            targets.append(f\"{side}_{joint}_injury_risk\")\n",
    "    \n",
    "    # Validate that all required features and targets exist in the data.\n",
    "    missing_features = [feat for feat in features if feat not in data.columns]\n",
    "    missing_targets = [t for t in targets if t not in data.columns]\n",
    "    if missing_features:\n",
    "        logging.error(f\"Missing features: {missing_features}\")\n",
    "        sys.exit(1)\n",
    "    if missing_targets:\n",
    "        logging.error(f\"Missing target variables: {missing_targets}\")\n",
    "        sys.exit(1)\n",
    "    else:\n",
    "        logging.info(\"All required features and target variables are present.\")\n",
    "    \n",
    "    # Use parallel processing to perform feature importance analysis for each target concurrently.\n",
    "    results_list = Parallel(n_jobs=-1)(\n",
    "        delayed(perform_feature_importance_analysis)(data, features, target, debug=debug)\n",
    "        for target in targets\n",
    "    )\n",
    "    results = {target: res for target, res in zip(targets, results_list)}\n",
    "    \n",
    "    # Display the top features for each target\n",
    "    analyze_and_display_top_features(results, n_top=10)\n",
    "    \n",
    "    # Save the top features for each target to pickle files\n",
    "    save_top_features(results, output_dir=output_dir, n_top=10)\n",
    "    \n",
    "    # Aggregate and display joint injury models per joint\n",
    "    for joint in joints:\n",
    "        top_features, agg_df = analyze_joint_injury_features(results, joint, n_top=10)\n",
    "        if top_features:\n",
    "            logging.info(f\"Aggregated top features for joint {joint}: {top_features}\")\n",
    "            filename = Path(output_dir) / f\"{joint}_aggregated_feature_importance.pkl\"\n",
    "            pd.to_pickle(agg_df, filename)\n",
    "            logging.info(f\"Saved aggregated feature importance for joint {joint} to {filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and Split (ensure that the preprocessing is correctly occuring to the right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Loaded data from ../../data/processed/final_granular_dataset.csv with shape (16047, 214)\n",
      "INFO: Added 'participant_id' column with value 'P0001'\n",
      "INFO: Loaded participant information from ../../data/basketball/freethrow/participant_information.json\n",
      "INFO: Merged participant data. New shape: (16047, 217)\n",
      "INFO: Step [load_data] completed.\n",
      "INFO: Step [calculate_joint_angles] completed.\n",
      "INFO: Renamed participant anthropometrics.\n",
      "INFO: Identified 15 joint energy and 14 joint power columns.\n",
      "INFO: Created aggregated 'joint_energy' and 'joint_power'.\n",
      "INFO: Created 'energy_acceleration' as derivative of joint_energy over time.\n",
      "INFO: Created 'ankle_power_ratio' feature comparing left to right ankle ongoing power.\n",
      "INFO: Created asymmetry feature: hip_asymmetry\n",
      "INFO: Created asymmetry feature: ankle_asymmetry\n",
      "INFO: Created asymmetry feature: wrist_asymmetry\n",
      "INFO: Created asymmetry feature: elbow_asymmetry\n",
      "INFO: Created asymmetry feature: knee_asymmetry\n",
      "INFO: Created asymmetry feature: 1stfinger_asymmetry\n",
      "INFO: Created asymmetry feature: 5thfinger_asymmetry\n",
      "INFO: Created power ratio feature: hip_power_ratio using columns L_HIP_ongoing_power and R_HIP_ongoing_power\n",
      "INFO: Created power ratio feature: ankle_power_ratio using columns L_ANKLE_ongoing_power and R_ANKLE_ongoing_power\n",
      "INFO: Created power ratio feature: wrist_power_ratio using columns L_WRIST_ongoing_power and R_WRIST_ongoing_power\n",
      "INFO: Created power ratio feature: elbow_power_ratio using columns L_ELBOW_ongoing_power and R_ELBOW_ongoing_power\n",
      "INFO: Created power ratio feature: knee_power_ratio using columns L_KNEE_ongoing_power and R_KNEE_ongoing_power\n",
      "INFO: Created power ratio feature: 1stfinger_power_ratio using columns L_1STFINGER_ongoing_power and R_1STFINGER_ongoing_power\n",
      "INFO: Created power ratio feature: 5thfinger_power_ratio using columns L_5THFINGER_ongoing_power and R_5THFINGER_ongoing_power\n",
      "INFO: Computed ROM for L KNEE as L_KNEE_ROM\n",
      "INFO: Computed ROM deviation for L KNEE as L_KNEE_ROM_deviation\n",
      "INFO: Created binary flag for L KNEE ROM extremes: L_KNEE_ROM_extreme\n",
      "INFO: Computed ROM for R KNEE as R_KNEE_ROM\n",
      "INFO: Computed ROM deviation for R KNEE as R_KNEE_ROM_deviation\n",
      "INFO: Created binary flag for R KNEE ROM extremes: R_KNEE_ROM_extreme\n",
      "INFO: Computed ROM for L SHOULDER as L_SHOULDER_ROM\n",
      "INFO: Computed ROM deviation for L SHOULDER as L_SHOULDER_ROM_deviation\n",
      "INFO: Created binary flag for L SHOULDER ROM extremes: L_SHOULDER_ROM_extreme\n",
      "INFO: Computed ROM for R SHOULDER as R_SHOULDER_ROM\n",
      "INFO: Computed ROM deviation for R SHOULDER as R_SHOULDER_ROM_deviation\n",
      "INFO: Created binary flag for R SHOULDER ROM extremes: R_SHOULDER_ROM_extreme\n",
      "INFO: Computed ROM for L HIP as L_HIP_ROM\n",
      "INFO: Computed ROM deviation for L HIP as L_HIP_ROM_deviation\n",
      "INFO: Created binary flag for L HIP ROM extremes: L_HIP_ROM_extreme\n",
      "INFO: Computed ROM for R HIP as R_HIP_ROM\n",
      "INFO: Computed ROM deviation for R HIP as R_HIP_ROM_deviation\n",
      "INFO: Created binary flag for R HIP ROM extremes: R_HIP_ROM_extreme\n",
      "INFO: Computed ROM for L ANKLE as L_ANKLE_ROM\n",
      "INFO: Computed ROM deviation for L ANKLE as L_ANKLE_ROM_deviation\n",
      "INFO: Created binary flag for L ANKLE ROM extremes: L_ANKLE_ROM_extreme\n",
      "INFO: Computed ROM for R ANKLE as R_ANKLE_ROM\n",
      "INFO: Computed ROM deviation for R ANKLE as R_ANKLE_ROM_deviation\n",
      "INFO: Created binary flag for R ANKLE ROM extremes: R_ANKLE_ROM_extreme\n",
      "INFO: Angle column 'L_WRIST_angle' not found; skipping ROM metrics for L WRIST.\n",
      "INFO: Angle column 'R_WRIST_angle' not found; skipping ROM metrics for R WRIST.\n",
      "INFO: Sorted data by 'participant_id' and 'continuous_frame_time'.\n",
      "INFO: Created 'exhaustion_rate' feature.\n",
      "INFO: Created 'simulated_HR' feature.\n",
      "INFO: Step [prepare_joint_features] completed.\n",
      "C:\\Users\\ghadf\\AppData\\Local\\Temp\\ipykernel_6972\\61768157.py:394: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  data[motion_cols] = data[motion_cols].fillna(method='ffill').fillna(0)\n",
      "INFO: Created 'rolling_energy_std' with sample: [0.0, 0.6225242794725895, 0.592331668013902, 0.5469698974921113, 0.5031647476376774, 0.0346469804150002, 0.05501279562072104, 0.06384083445684284, 0.05698938811919827, 0.039917330417864196]\n",
      "INFO: Created 'rolling_energy_std' with window 5.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joint energy columns:  ['L_ANKLE_energy', 'R_ANKLE_energy', 'L_KNEE_energy', 'R_KNEE_energy', 'L_HIP_energy', 'R_HIP_energy', 'L_ELBOW_energy', 'R_ELBOW_energy', 'L_WRIST_energy', 'R_WRIST_energy', 'L_1STFINGER_energy', 'R_1STFINGER_energy', 'L_5THFINGER_energy', 'R_5THFINGER_energy', 'total_energy']\n",
      "Joint power columns:  ['L_ANKLE_ongoing_power', 'R_ANKLE_ongoing_power', 'L_KNEE_ongoing_power', 'R_KNEE_ongoing_power', 'L_HIP_ongoing_power', 'R_HIP_ongoing_power', 'L_ELBOW_ongoing_power', 'R_ELBOW_ongoing_power', 'L_WRIST_ongoing_power', 'R_WRIST_ongoing_power', 'L_1STFINGER_ongoing_power', 'L_5THFINGER_ongoing_power', 'R_1STFINGER_ongoing_power', 'R_5THFINGER_ongoing_power']\n",
      "All angle columns:  ['entry_angle', 'elbow_angle', 'wrist_angle', 'knee_angle', 'initial_release_angle', 'calculated_release_angle', 'angle_difference', 'optimal_release_angle', 'L_SHOULDER_angle', 'R_SHOULDER_angle', 'L_HIP_angle', 'R_HIP_angle', 'L_KNEE_angle', 'R_KNEE_angle', 'L_ANKLE_angle', 'R_ANKLE_angle']\n",
      "print all the columns with by_trial_exhaustion_score:  ['by_trial_exhaustion_score', 'L_ANKLE_energy_by_trial_exhaustion_score', 'R_ANKLE_energy_by_trial_exhaustion_score', 'L_KNEE_energy_by_trial_exhaustion_score', 'R_KNEE_energy_by_trial_exhaustion_score', 'L_HIP_energy_by_trial_exhaustion_score', 'R_HIP_energy_by_trial_exhaustion_score', 'L_ELBOW_energy_by_trial_exhaustion_score', 'R_ELBOW_energy_by_trial_exhaustion_score', 'L_WRIST_energy_by_trial_exhaustion_score', 'R_WRIST_energy_by_trial_exhaustion_score', 'L_1STFINGER_energy_by_trial_exhaustion_score', 'R_1STFINGER_energy_by_trial_exhaustion_score', 'L_5THFINGER_energy_by_trial_exhaustion_score', 'R_5THFINGER_energy_by_trial_exhaustion_score']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ghadf\\AppData\\Local\\Temp\\ipykernel_6972\\61768157.py:443: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{joint_name}_rolling_exhaustion'] = data[score_col].rolling(rolling_window, min_periods=1).sum()\n",
      "C:\\Users\\ghadf\\AppData\\Local\\Temp\\ipykernel_6972\\61768157.py:447: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{joint_name}_injury_risk'] = (rolling_series > safe_expanding_quantile(rolling_series)).astype(int)\n",
      "C:\\Users\\ghadf\\AppData\\Local\\Temp\\ipykernel_6972\\61768157.py:442: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{joint_name}_exhaustion_rate'] = data[score_col].diff() / dt\n",
      "C:\\Users\\ghadf\\AppData\\Local\\Temp\\ipykernel_6972\\61768157.py:443: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{joint_name}_rolling_exhaustion'] = data[score_col].rolling(rolling_window, min_periods=1).sum()\n",
      "C:\\Users\\ghadf\\AppData\\Local\\Temp\\ipykernel_6972\\61768157.py:447: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{joint_name}_injury_risk'] = (rolling_series > safe_expanding_quantile(rolling_series)).astype(int)\n",
      "INFO: Loaded 10 features for by_trial_exhaustion_score\n",
      "INFO: Loaded 9 features for injury_risk\n",
      "INFO: Performed temporal train-test split with test size = 0.2\n",
      "INFO: Training data shape: (12836, 306), Testing data shape: (3210, 306)\n",
      "INFO: Features have been scaled using StandardScaler.\n",
      "INFO: Created LSTM sequences: (12831, 5, 10), (12831,)\n",
      "INFO: Created LSTM sequences: (3205, 5, 10), (3205,)\n",
      "INFO: Training overall exhaustion model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0446 - val_loss: 0.0061\n",
      "Epoch 2/50\n",
      "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0092 - val_loss: 0.0064\n",
      "Epoch 3/50\n",
      "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0081 - val_loss: 0.0068\n",
      "Epoch 4/50\n",
      "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0074 - val_loss: 0.0060\n",
      "Epoch 5/50\n",
      "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0069 - val_loss: 0.0061\n",
      "Epoch 6/50\n",
      "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0069 - val_loss: 0.0058\n",
      "Epoch 7/50\n",
      "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0061 - val_loss: 0.0066\n",
      "Epoch 8/50\n",
      "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0064 - val_loss: 0.0067\n",
      "Epoch 9/50\n",
      "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0052 - val_loss: 0.0059\n",
      "Epoch 10/50\n",
      "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0056 - val_loss: 0.0072\n",
      "Epoch 11/50\n",
      "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0053 - val_loss: 0.0058\n",
      "Epoch 12/50\n",
      "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0047 - val_loss: 0.0064\n",
      "Epoch 13/50\n",
      "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0048 - val_loss: 0.0060\n",
      "Epoch 14/50\n",
      "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0052 - val_loss: 0.0059\n",
      "Epoch 15/50\n",
      "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0047 - val_loss: 0.0061\n",
      "Epoch 16/50\n",
      "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0043 - val_loss: 0.0062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Features have been scaled using StandardScaler.\n",
      "INFO: Created LSTM sequences: (12831, 5, 9), (12831,)\n",
      "INFO: Created LSTM sequences: (3205, 5, 9), (3205,)\n",
      "INFO: Training overall injury risk model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8998 - loss: 0.2625 - val_accuracy: 0.9785 - val_loss: 0.0624\n",
      "Epoch 2/50\n",
      "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9733 - loss: 0.0925 - val_accuracy: 0.9872 - val_loss: 0.0347\n",
      "Epoch 3/50\n",
      "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9747 - loss: 0.0858 - val_accuracy: 0.9903 - val_loss: 0.0402\n",
      "Epoch 4/50\n",
      "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9753 - loss: 0.0829 - val_accuracy: 0.9778 - val_loss: 0.0614\n",
      "Epoch 5/50\n",
      "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9756 - loss: 0.0846 - val_accuracy: 0.9875 - val_loss: 0.0353\n",
      "Epoch 6/50\n",
      "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9779 - loss: 0.0723 - val_accuracy: 0.9754 - val_loss: 0.0736\n",
      "Epoch 7/50\n",
      "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9803 - loss: 0.0692 - val_accuracy: 0.9747 - val_loss: 0.0788\n",
      "Epoch 8/50\n",
      "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9779 - loss: 0.0713 - val_accuracy: 0.9913 - val_loss: 0.0353\n",
      "Epoch 9/50\n",
      "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9745 - loss: 0.0820 - val_accuracy: 0.9832 - val_loss: 0.0491\n",
      "Epoch 10/50\n",
      "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9776 - loss: 0.0716 - val_accuracy: 0.9885 - val_loss: 0.0404\n",
      "Epoch 11/50\n",
      "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9810 - loss: 0.0699 - val_accuracy: 0.9847 - val_loss: 0.0416\n",
      "Epoch 12/50\n",
      "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9757 - loss: 0.0802 - val_accuracy: 0.9810 - val_loss: 0.0525\n",
      "Epoch 13/50\n",
      "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9784 - loss: 0.0712 - val_accuracy: 0.9891 - val_loss: 0.0388\n",
      "Epoch 14/50\n",
      "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9803 - loss: 0.0691 - val_accuracy: 0.9844 - val_loss: 0.0425\n",
      "Epoch 15/50\n",
      "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9806 - loss: 0.0640 - val_accuracy: 0.9897 - val_loss: 0.0300\n",
      "Epoch 16/50\n",
      "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9794 - loss: 0.0701 - val_accuracy: 0.9885 - val_loss: 0.0363\n",
      "Epoch 17/50\n",
      "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9800 - loss: 0.0683 - val_accuracy: 0.9934 - val_loss: 0.0300\n",
      "Epoch 18/50\n",
      "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9790 - loss: 0.0672 - val_accuracy: 0.9891 - val_loss: 0.0365\n",
      "Epoch 19/50\n",
      "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9798 - loss: 0.0645 - val_accuracy: 0.9841 - val_loss: 0.0414\n",
      "Epoch 20/50\n",
      "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9790 - loss: 0.0714 - val_accuracy: 0.9872 - val_loss: 0.0359\n",
      "Epoch 21/50\n",
      "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9786 - loss: 0.0705 - val_accuracy: 0.9866 - val_loss: 0.0442\n",
      "Epoch 22/50\n",
      "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9778 - loss: 0.0776 - val_accuracy: 0.9844 - val_loss: 0.0430\n",
      "Epoch 23/50\n",
      "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9773 - loss: 0.0749 - val_accuracy: 0.9825 - val_loss: 0.0571\n",
      "Epoch 24/50\n",
      "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9795 - loss: 0.0693 - val_accuracy: 0.9869 - val_loss: 0.0368\n",
      "Epoch 25/50\n",
      "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9811 - loss: 0.0650 - val_accuracy: 0.9856 - val_loss: 0.0406\n",
      "Epoch 26/50\n",
      "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9779 - loss: 0.0772 - val_accuracy: 0.9863 - val_loss: 0.0375\n",
      "Epoch 27/50\n",
      "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9796 - loss: 0.0640 - val_accuracy: 0.9863 - val_loss: 0.0453\n",
      "Epoch 28/50\n",
      "\u001b[1m401/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9811 - loss: 0.0639 - val_accuracy: 0.9906 - val_loss: 0.0359\n",
      "Epoch 29/50\n",
      "\u001b[1m321/401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9801 - loss: 0.0665"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import sys\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error, r2_score, accuracy_score,\n",
    "    precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# ==================== UTILS ====================\n",
    "def load_top_features(target, feature_dir=\"feature_lists\"):\n",
    "    \"\"\"\n",
    "    Loads the saved feature list for a specific target variable.\n",
    "    \n",
    "    Parameters:\n",
    "      - target (str): Target variable name.\n",
    "      - feature_dir (str): Directory containing feature lists.\n",
    "    \n",
    "    Returns:\n",
    "      - List of feature names.\n",
    "    \"\"\"\n",
    "    filename = Path(feature_dir) / f\"{target}_model_feature_list.pkl\"\n",
    "    \n",
    "    try:\n",
    "        features = pd.read_pickle(filename)\n",
    "        logging.info(f\"Loaded {len(features)} features for {target}\")\n",
    "        \n",
    "        # Verify features exist in data (assumes 'data' is already loaded in the global scope)\n",
    "        missing = [f for f in features if f not in data.columns]\n",
    "        if missing:\n",
    "            logging.error(f\"Missing features from {target} list: {missing}\")\n",
    "            sys.exit(1)\n",
    "            \n",
    "        return features\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"No feature list found for {target} at {filename}\")\n",
    "        sys.exit(1)\n",
    "        \n",
    "def temporal_train_test_split(data, test_size=0.2):\n",
    "    \"\"\"Time-based split maintaining temporal order\"\"\"\n",
    "    split_idx = int(len(data) * (1 - test_size))\n",
    "    train_data = data.iloc[:split_idx]\n",
    "    test_data = data.iloc[split_idx:]\n",
    "    logging.info(f\"Performed temporal train-test split with test size = {test_size}\")\n",
    "    logging.info(f\"Training data shape: {train_data.shape}, Testing data shape: {test_data.shape}\")\n",
    "    return train_data, test_data\n",
    "\n",
    "def scale_features(X_train, X_test):\n",
    "    \"\"\"\n",
    "    Scales features using StandardScaler.\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    logging.info(\"Features have been scaled using StandardScaler.\")\n",
    "    return X_train_scaled, X_test_scaled, scaler\n",
    "\n",
    "def create_sequences(X, y, timesteps):\n",
    "    \"\"\"\n",
    "    Creates sequences of data for LSTM input.\n",
    "    \"\"\"\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(timesteps, len(X)):\n",
    "        X_seq.append(X[i-timesteps:i])\n",
    "        y_seq.append(y[i])\n",
    "    X_seq = np.array(X_seq)\n",
    "    y_seq = np.array(y_seq)\n",
    "    logging.info(f\"Created LSTM sequences: {X_seq.shape}, {y_seq.shape}\")\n",
    "    return X_seq, y_seq\n",
    "# ==================== TRAINING FUNCTIONS ====================\n",
    "\n",
    "def train_exhaustion_model(train_data, test_data, features, timesteps):\n",
    "    \"\"\"\n",
    "    Trains the overall exhaustion model.\n",
    "    \n",
    "    Parameters:\n",
    "      - train_data (DataFrame): Training set.\n",
    "      - test_data (DataFrame): Testing set.\n",
    "      - features (list): List of feature column names for exhaustion.\n",
    "      - timesteps (int): Number of past observations to include in each sequence.\n",
    "      \n",
    "    Returns:\n",
    "      - model_exhaustion: Trained Keras model.\n",
    "      - scaler_exhaustion: Fitted scaler for the features.\n",
    "      - X_lstm_exhaustion_val, y_lstm_exhaustion_val: Validation sequences.\n",
    "    \"\"\"\n",
    "    X_train = train_data[features].values\n",
    "    y_train = train_data['by_trial_exhaustion_score'].values\n",
    "    X_test = test_data[features].values\n",
    "    y_test = test_data['by_trial_exhaustion_score'].values\n",
    "    \n",
    "    # Scale features\n",
    "    X_train_scaled, X_test_scaled, scaler_exhaustion = scale_features(X_train, X_test)\n",
    "    # Create sequences for LSTM input\n",
    "    X_lstm, y_lstm = create_sequences(X_train_scaled, y_train, timesteps)\n",
    "    X_lstm_val, y_lstm_val = create_sequences(X_test_scaled, y_test, timesteps)\n",
    "    \n",
    "    # ---- Updated Model Construction using an explicit Input layer ----\n",
    "    from tensorflow.keras import Input\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import LSTM, Dropout, Dense\n",
    "    model_exhaustion = Sequential([\n",
    "        # Use an Input layer to define the shape explicitly\n",
    "        Input(shape=(X_lstm.shape[1], X_lstm.shape[2])),\n",
    "        LSTM(64, return_sequences=False),\n",
    "        Dropout(0.2),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    # ---------------------------------------------------------------------\n",
    "    \n",
    "    model_exhaustion.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "    from tensorflow.keras.callbacks import EarlyStopping\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=5)\n",
    "    \n",
    "    import logging\n",
    "    logging.info(\"Training overall exhaustion model...\")\n",
    "    model_exhaustion.fit(X_lstm, y_lstm, \n",
    "                         epochs=50, batch_size=32, \n",
    "                         validation_data=(X_lstm_val, y_lstm_val), \n",
    "                         callbacks=[early_stop])\n",
    "    \n",
    "    return model_exhaustion, scaler_exhaustion, X_lstm_val, y_lstm_val\n",
    "\n",
    "\n",
    "\n",
    "def train_injury_model(train_data, test_data, features, timesteps):\n",
    "    \"\"\"\n",
    "    Trains the overall injury risk model.\n",
    "    \n",
    "    Parameters:\n",
    "      - train_data (DataFrame): Training set.\n",
    "      - test_data (DataFrame): Testing set.\n",
    "      - features (list): List of feature column names for injury risk.\n",
    "      - timesteps (int): Number of past observations to include in each sequence.\n",
    "      \n",
    "    Returns:\n",
    "      - model_injury: Trained Keras model.\n",
    "      - scaler_injury: Fitted scaler for the features.\n",
    "      - X_lstm_injury_val, y_lstm_injury_val: Validation sequences.\n",
    "    \"\"\"\n",
    "    X_train = train_data[features].values\n",
    "    y_train = train_data['injury_risk'].values\n",
    "    X_test = test_data[features].values\n",
    "    y_test = test_data['injury_risk'].values\n",
    "    \n",
    "    # Scale features\n",
    "    X_train_scaled, X_test_scaled, scaler_injury = scale_features(X_train, X_test)\n",
    "    # Create sequences for LSTM input\n",
    "    X_lstm, y_lstm = create_sequences(X_train_scaled, y_train, timesteps)\n",
    "    X_lstm_val, y_lstm_val = create_sequences(X_test_scaled, y_test, timesteps)\n",
    "    \n",
    "    # ---- Updated Model Construction using an explicit Input layer ----\n",
    "    from tensorflow.keras import Input\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import LSTM, Dropout, Dense\n",
    "    model_injury = Sequential([\n",
    "        Input(shape=(X_lstm.shape[1], X_lstm.shape[2])),\n",
    "        LSTM(64, return_sequences=False),\n",
    "        Dropout(0.2),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    # ---------------------------------------------------------------------\n",
    "    \n",
    "    model_injury.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    import logging\n",
    "    logging.info(\"Training overall injury risk model...\")\n",
    "    model_injury.fit(X_lstm, y_lstm, \n",
    "                     epochs=50, batch_size=32, \n",
    "                     validation_data=(X_lstm_val, y_lstm_val))\n",
    "    \n",
    "    return model_injury, scaler_injury, X_lstm_val, y_lstm_val\n",
    "\n",
    "\n",
    "\n",
    "def train_joint_models(train_data, test_data, joints, timesteps, feature_dir):\n",
    "    \"\"\"\n",
    "    Trains injury risk models for multiple joints.\n",
    "    \n",
    "    Parameters:\n",
    "      - train_data (DataFrame): Training set.\n",
    "      - test_data (DataFrame): Testing set.\n",
    "      - joints (list): List of joint names.\n",
    "      - timesteps (int): Number of past observations to include in each sequence.\n",
    "      - feature_dir (str): Directory containing feature lists.\n",
    "      \n",
    "    Returns:\n",
    "      - joint_models (dict): Dictionary with joint model info.\n",
    "    \"\"\"\n",
    "    import logging\n",
    "    from tensorflow.keras import Input\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import LSTM, Dropout, Dense\n",
    "    joint_models = {}\n",
    "    \n",
    "    for joint in joints:\n",
    "        for side in ['L', 'R']:\n",
    "            target_joint = f\"{side}_{joint}_injury_risk\"\n",
    "            logging.info(f\"Training model for {target_joint}...\")\n",
    "            \n",
    "            joint_features = load_top_features(target_joint, feature_dir=feature_dir)\n",
    "            \n",
    "            X_train_joint = train_data[joint_features].values\n",
    "            y_train_joint = train_data[target_joint].values\n",
    "            X_test_joint = test_data[joint_features].values\n",
    "            y_test_joint = test_data[target_joint].values\n",
    "            \n",
    "            # Scale features for the joint-specific model\n",
    "            X_train_scaled, X_test_scaled, scaler_joint = scale_features(X_train_joint, X_test_joint)\n",
    "            # Create sequences for LSTM input\n",
    "            X_lstm, y_lstm = create_sequences(X_train_scaled, y_train_joint, timesteps)\n",
    "            X_lstm_val, y_lstm_val = create_sequences(X_test_scaled, y_test_joint, timesteps)\n",
    "            \n",
    "            # ---- Updated Model Construction using an explicit Input layer ----\n",
    "            model_joint = Sequential([\n",
    "                Input(shape=(X_lstm.shape[1], X_lstm.shape[2])),\n",
    "                LSTM(64, return_sequences=False),\n",
    "                Dropout(0.2),\n",
    "                Dense(1, activation='sigmoid')\n",
    "            ])\n",
    "            # ---------------------------------------------------------------------\n",
    "            \n",
    "            model_joint.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "            \n",
    "            model_joint.fit(X_lstm, y_lstm, \n",
    "                            epochs=50, batch_size=32, \n",
    "                            validation_data=(X_lstm_val, y_lstm_val))\n",
    "            \n",
    "            joint_models[target_joint] = {\n",
    "                'model': model_joint,\n",
    "                'features': joint_features,\n",
    "                'scaler': scaler_joint\n",
    "            }\n",
    "    \n",
    "    import json\n",
    "    with open(\"loaded_features.json\", \"w\") as f:\n",
    "        json.dump({target: info['features'] for target, info in joint_models.items()}, f, indent=4)\n",
    "    logging.info(\"Saved loaded features list for each joint model to 'loaded_features.json'.\")\n",
    "    \n",
    "    return joint_models\n",
    "\n",
    "\n",
    "# ==================== FORECASTING FUNCTION ====================\n",
    "\n",
    "def forecast_and_plot(model, data_series, scaler, timesteps, future_steps=0, title=\"Forecast\"):\n",
    "    \"\"\"\n",
    "    Generates predictions for the provided univariate data series using the given model,\n",
    "    and plots actual vs. predicted values. Optionally forecasts additional future steps.\n",
    "    \n",
    "    Parameters:\n",
    "      - model: Trained Keras model.\n",
    "      - data_series (ndarray): Univariate numpy array of shape (n, 1) with actual values.\n",
    "      - scaler: Fitted scaler used to transform the target values.\n",
    "      - timesteps (int): Length of the input sequence for the model.\n",
    "      - future_steps (int): Number of future time steps to forecast.\n",
    "      - title (str): Plot title.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    scaled_data = scaler.transform(data_series)\n",
    "    \n",
    "    X = []\n",
    "    for i in range(timesteps, len(scaled_data)):\n",
    "        X.append(scaled_data[i-timesteps:i, 0])\n",
    "    X = np.array(X)\n",
    "    X = X.reshape((X.shape[0], timesteps, 1))\n",
    "    \n",
    "    predictions = model.predict(X)\n",
    "    predictions_inv = scaler.inverse_transform(predictions)\n",
    "    \n",
    "    forecast_predictions_inv = None\n",
    "    if future_steps > 0:\n",
    "        forecast_predictions = []\n",
    "        last_sequence = X[-1].copy()\n",
    "        current_sequence = last_sequence.copy()\n",
    "        for _ in range(future_steps):\n",
    "            next_pred = model.predict(current_sequence.reshape(1, timesteps, 1))\n",
    "            forecast_predictions.append(next_pred[0, 0])\n",
    "            current_sequence = np.append(current_sequence[1:], next_pred, axis=0)\n",
    "        forecast_predictions = np.array(forecast_predictions).reshape(-1, 1)\n",
    "        forecast_predictions_inv = scaler.inverse_transform(forecast_predictions)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(timesteps, len(data_series)), data_series[timesteps:], color='red', label='Actual')\n",
    "    plt.plot(range(timesteps, len(data_series)), predictions_inv, color='blue', label='Predicted')\n",
    "    if forecast_predictions_inv is not None:\n",
    "        future_x = list(range(len(data_series), len(data_series) + future_steps))\n",
    "        plt.plot(future_x, forecast_predictions_inv, color='green', linestyle='--', label='Forecast')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# ==================== MAIN FUNCTION ====================\n",
    "\n",
    "def main():\n",
    "    import logging\n",
    "    logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "    \n",
    "    # Data Loading and Preparation\n",
    "    csv_path = \"../../data/processed/final_granular_dataset.csv\"\n",
    "    json_path = \"../../data/basketball/freethrow/participant_information.json\"\n",
    "    feature_dir = \"../../data/Deep_Learning_Final\"\n",
    "    \n",
    "    data = load_data(csv_path, json_path)\n",
    "    data = prepare_joint_features(data)\n",
    "    data = feature_engineering(data)\n",
    "    \n",
    "    features_exhaustion = load_top_features('by_trial_exhaustion_score', feature_dir=feature_dir)\n",
    "    features_injury = load_top_features('injury_risk', feature_dir=feature_dir)\n",
    "    \n",
    "    train_data, test_data = temporal_train_test_split(data, test_size=0.2)\n",
    "    timesteps = 5\n",
    "    \n",
    "    # Train the overall exhaustion model\n",
    "    model_exhaustion, scaler_exhaustion, X_val_exh, y_val_exh = train_exhaustion_model(\n",
    "        train_data, test_data, features_exhaustion, timesteps)\n",
    "    \n",
    "    # Train the overall injury risk model\n",
    "    model_injury, scaler_injury, X_val_injury, y_val_injury = train_injury_model(\n",
    "        train_data, test_data, features_injury, timesteps)\n",
    "    \n",
    "    # Train joint-specific models\n",
    "    joints = ['ANKLE', 'WRIST', 'ELBOW', 'KNEE', 'HIP']\n",
    "    joint_models = train_joint_models(train_data, test_data, joints, timesteps, feature_dir)\n",
    "    \n",
    "    # Forecasting Example for the Exhaustion Model\n",
    "    import numpy as np\n",
    "    exhaustion_series = test_data['by_trial_exhaustion_score'].values.reshape(-1, 1)\n",
    "    forecast_and_plot(\n",
    "        model=model_exhaustion,\n",
    "        data_series=exhaustion_series,\n",
    "        scaler=scaler_exhaustion,\n",
    "        timesteps=timesteps,\n",
    "        future_steps=10,\n",
    "        title=\"Overall Exhaustion Model Forecast\"\n",
    "    )\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import sys\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ==================== UTILS (unchanged) ====================\n",
    "def load_top_features(target, feature_dir=\"feature_lists\"):\n",
    "    \"\"\"\n",
    "    Loads the saved feature list for a specific target variable.\n",
    "    \n",
    "    Parameters:\n",
    "      - target (str): Target variable name.\n",
    "      - feature_dir (str): Directory containing feature lists.\n",
    "    \n",
    "    Returns:\n",
    "      - List of feature names.\n",
    "    \"\"\"\n",
    "    filename = Path(feature_dir) / f\"{target}_model_feature_list.pkl\"\n",
    "    \n",
    "    try:\n",
    "        features = pd.read_pickle(filename)\n",
    "        logging.info(f\"Loaded {len(features)} features for {target}\")\n",
    "        \n",
    "        # Verify features exist in data (assumes 'data' is already loaded in the global scope)\n",
    "        missing = [f for f in features if f not in data.columns]\n",
    "        if missing:\n",
    "            logging.error(f\"Missing features from {target} list: {missing}\")\n",
    "            sys.exit(1)\n",
    "            \n",
    "        return features\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"No feature list found for {target} at {filename}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "def temporal_train_test_split(data, test_size=0.2):\n",
    "    \"\"\"Time-based split maintaining temporal order\"\"\"\n",
    "    split_idx = int(len(data) * (1 - test_size))\n",
    "    train_data = data.iloc[:split_idx]\n",
    "    test_data = data.iloc[split_idx:]\n",
    "    logging.info(f\"Performed temporal train-test split with test size = {test_size}\")\n",
    "    logging.info(f\"Training data shape: {train_data.shape}, Testing data shape: {test_data.shape}\")\n",
    "    return train_data, test_data\n",
    "\n",
    "def create_sequences(X, y, timesteps):\n",
    "    \"\"\"\n",
    "    Creates sequences of data for LSTM input.\n",
    "    \"\"\"\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(timesteps, len(X)):\n",
    "        X_seq.append(X[i-timesteps:i])\n",
    "        y_seq.append(y[i])\n",
    "    X_seq = np.array(X_seq)\n",
    "    y_seq = np.array(y_seq)\n",
    "    logging.info(f\"Created LSTM sequences: {X_seq.shape}, {y_seq.shape}\")\n",
    "    return X_seq, y_seq\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ghadf\\vscode_projects\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Loaded data from ../../data/processed/final_granular_dataset.csv with shape (16047, 214)\n",
      "INFO: Added 'participant_id' column with value 'P0001'\n",
      "INFO: Loaded participant information from ../../data/basketball/freethrow/participant_information.json\n",
      "INFO: Merged participant data. New shape: (16047, 217)\n",
      "INFO: Step [load_data] completed.\n",
      "INFO: Step [calculate_joint_angles] completed.\n",
      "INFO: Renamed participant anthropometrics.\n",
      "INFO: Identified 15 joint energy and 14 joint power columns.\n",
      "INFO: Created aggregated 'joint_energy' and 'joint_power'.\n",
      "INFO: Created 'energy_acceleration' as derivative of joint_energy over time.\n",
      "INFO: Created 'ankle_power_ratio' feature comparing left to right ankle ongoing power.\n",
      "INFO: Created asymmetry feature: hip_asymmetry\n",
      "INFO: Created asymmetry feature: ankle_asymmetry\n",
      "INFO: Created asymmetry feature: wrist_asymmetry\n",
      "INFO: Created asymmetry feature: elbow_asymmetry\n",
      "INFO: Created asymmetry feature: knee_asymmetry\n",
      "INFO: Created asymmetry feature: 1stfinger_asymmetry\n",
      "INFO: Created asymmetry feature: 5thfinger_asymmetry\n",
      "INFO: Created power ratio feature: hip_power_ratio using columns L_HIP_ongoing_power and R_HIP_ongoing_power\n",
      "INFO: Created power ratio feature: ankle_power_ratio using columns L_ANKLE_ongoing_power and R_ANKLE_ongoing_power\n",
      "INFO: Created power ratio feature: wrist_power_ratio using columns L_WRIST_ongoing_power and R_WRIST_ongoing_power\n",
      "INFO: Created power ratio feature: elbow_power_ratio using columns L_ELBOW_ongoing_power and R_ELBOW_ongoing_power\n",
      "INFO: Created power ratio feature: knee_power_ratio using columns L_KNEE_ongoing_power and R_KNEE_ongoing_power\n",
      "INFO: Created power ratio feature: 1stfinger_power_ratio using columns L_1STFINGER_ongoing_power and R_1STFINGER_ongoing_power\n",
      "INFO: Created power ratio feature: 5thfinger_power_ratio using columns L_5THFINGER_ongoing_power and R_5THFINGER_ongoing_power\n",
      "INFO: Computed ROM for L KNEE as L_KNEE_ROM\n",
      "INFO: Computed ROM deviation for L KNEE as L_KNEE_ROM_deviation\n",
      "INFO: Created binary flag for L KNEE ROM extremes: L_KNEE_ROM_extreme\n",
      "INFO: Computed ROM for R KNEE as R_KNEE_ROM\n",
      "INFO: Computed ROM deviation for R KNEE as R_KNEE_ROM_deviation\n",
      "INFO: Created binary flag for R KNEE ROM extremes: R_KNEE_ROM_extreme\n",
      "INFO: Computed ROM for L SHOULDER as L_SHOULDER_ROM\n",
      "INFO: Computed ROM deviation for L SHOULDER as L_SHOULDER_ROM_deviation\n",
      "INFO: Created binary flag for L SHOULDER ROM extremes: L_SHOULDER_ROM_extreme\n",
      "INFO: Computed ROM for R SHOULDER as R_SHOULDER_ROM\n",
      "INFO: Computed ROM deviation for R SHOULDER as R_SHOULDER_ROM_deviation\n",
      "INFO: Created binary flag for R SHOULDER ROM extremes: R_SHOULDER_ROM_extreme\n",
      "INFO: Computed ROM for L HIP as L_HIP_ROM\n",
      "INFO: Computed ROM deviation for L HIP as L_HIP_ROM_deviation\n",
      "INFO: Created binary flag for L HIP ROM extremes: L_HIP_ROM_extreme\n",
      "INFO: Computed ROM for R HIP as R_HIP_ROM\n",
      "INFO: Computed ROM deviation for R HIP as R_HIP_ROM_deviation\n",
      "INFO: Created binary flag for R HIP ROM extremes: R_HIP_ROM_extreme\n",
      "INFO: Computed ROM for L ANKLE as L_ANKLE_ROM\n",
      "INFO: Computed ROM deviation for L ANKLE as L_ANKLE_ROM_deviation\n",
      "INFO: Created binary flag for L ANKLE ROM extremes: L_ANKLE_ROM_extreme\n",
      "INFO: Computed ROM for R ANKLE as R_ANKLE_ROM\n",
      "INFO: Computed ROM deviation for R ANKLE as R_ANKLE_ROM_deviation\n",
      "INFO: Created binary flag for R ANKLE ROM extremes: R_ANKLE_ROM_extreme\n",
      "INFO: Angle column 'L_WRIST_angle' not found; skipping ROM metrics for L WRIST.\n",
      "INFO: Angle column 'R_WRIST_angle' not found; skipping ROM metrics for R WRIST.\n",
      "INFO: Sorted data by 'participant_id' and 'continuous_frame_time'.\n",
      "INFO: Created 'exhaustion_rate' feature.\n",
      "INFO: Created 'simulated_HR' feature.\n",
      "INFO: Step [prepare_joint_features] completed.\n",
      "C:\\Users\\ghadf\\AppData\\Local\\Temp\\ipykernel_6972\\61768157.py:394: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  data[motion_cols] = data[motion_cols].fillna(method='ffill').fillna(0)\n",
      "INFO: Created 'rolling_energy_std' with sample: [0.0, 0.6225242794725895, 0.592331668013902, 0.5469698974921113, 0.5031647476376774, 0.0346469804150002, 0.05501279562072104, 0.06384083445684284, 0.05698938811919827, 0.039917330417864196]\n",
      "INFO: Created 'rolling_energy_std' with window 5.\n",
      "C:\\Users\\ghadf\\AppData\\Local\\Temp\\ipykernel_6972\\61768157.py:443: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{joint_name}_rolling_exhaustion'] = data[score_col].rolling(rolling_window, min_periods=1).sum()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joint energy columns:  ['L_ANKLE_energy', 'R_ANKLE_energy', 'L_KNEE_energy', 'R_KNEE_energy', 'L_HIP_energy', 'R_HIP_energy', 'L_ELBOW_energy', 'R_ELBOW_energy', 'L_WRIST_energy', 'R_WRIST_energy', 'L_1STFINGER_energy', 'R_1STFINGER_energy', 'L_5THFINGER_energy', 'R_5THFINGER_energy', 'total_energy']\n",
      "Joint power columns:  ['L_ANKLE_ongoing_power', 'R_ANKLE_ongoing_power', 'L_KNEE_ongoing_power', 'R_KNEE_ongoing_power', 'L_HIP_ongoing_power', 'R_HIP_ongoing_power', 'L_ELBOW_ongoing_power', 'R_ELBOW_ongoing_power', 'L_WRIST_ongoing_power', 'R_WRIST_ongoing_power', 'L_1STFINGER_ongoing_power', 'L_5THFINGER_ongoing_power', 'R_1STFINGER_ongoing_power', 'R_5THFINGER_ongoing_power']\n",
      "All angle columns:  ['entry_angle', 'elbow_angle', 'wrist_angle', 'knee_angle', 'initial_release_angle', 'calculated_release_angle', 'angle_difference', 'optimal_release_angle', 'L_SHOULDER_angle', 'R_SHOULDER_angle', 'L_HIP_angle', 'R_HIP_angle', 'L_KNEE_angle', 'R_KNEE_angle', 'L_ANKLE_angle', 'R_ANKLE_angle']\n",
      "print all the columns with by_trial_exhaustion_score:  ['by_trial_exhaustion_score', 'L_ANKLE_energy_by_trial_exhaustion_score', 'R_ANKLE_energy_by_trial_exhaustion_score', 'L_KNEE_energy_by_trial_exhaustion_score', 'R_KNEE_energy_by_trial_exhaustion_score', 'L_HIP_energy_by_trial_exhaustion_score', 'R_HIP_energy_by_trial_exhaustion_score', 'L_ELBOW_energy_by_trial_exhaustion_score', 'R_ELBOW_energy_by_trial_exhaustion_score', 'L_WRIST_energy_by_trial_exhaustion_score', 'R_WRIST_energy_by_trial_exhaustion_score', 'L_1STFINGER_energy_by_trial_exhaustion_score', 'R_1STFINGER_energy_by_trial_exhaustion_score', 'L_5THFINGER_energy_by_trial_exhaustion_score', 'R_5THFINGER_energy_by_trial_exhaustion_score']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ghadf\\AppData\\Local\\Temp\\ipykernel_6972\\61768157.py:447: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{joint_name}_injury_risk'] = (rolling_series > safe_expanding_quantile(rolling_series)).astype(int)\n",
      "C:\\Users\\ghadf\\AppData\\Local\\Temp\\ipykernel_6972\\61768157.py:442: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{joint_name}_exhaustion_rate'] = data[score_col].diff() / dt\n",
      "C:\\Users\\ghadf\\AppData\\Local\\Temp\\ipykernel_6972\\61768157.py:443: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{joint_name}_rolling_exhaustion'] = data[score_col].rolling(rolling_window, min_periods=1).sum()\n",
      "C:\\Users\\ghadf\\AppData\\Local\\Temp\\ipykernel_6972\\61768157.py:447: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{joint_name}_injury_risk'] = (rolling_series > safe_expanding_quantile(rolling_series)).astype(int)\n",
      "INFO: Loaded 9 features for injury_risk\n",
      "INFO: Performed temporal train-test split with test size = 0.2\n",
      "INFO: Training data shape: (12836, 306), Testing data shape: (3210, 306)\n",
      "2025-02-08 08:56:29,521 [INFO] Starting: Final Preprocessing Pipeline in 'train' mode.\n",
      "INFO: Starting: Final Preprocessing Pipeline in 'train' mode.\n",
      "2025-02-08 08:56:29,522 [INFO] Step: filter_columns\n",
      "INFO: Step: filter_columns\n",
      "2025-02-08 08:56:29,525 [INFO] ✅ Filtered DataFrame to include only specified features. Shape: (16046, 13)\n",
      "INFO: ✅ Filtered DataFrame to include only specified features. Shape: (16046, 13)\n",
      "2025-02-08 08:56:29,526 [DEBUG] Selected Features: ['rolling_exhaustion', 'ema_exhaustion', '5thfinger_power_ratio', 'exhaustion_lag1', 'simulated_HR', '1stfinger_asymmetry', 'knee_asymmetry', 'hip_asymmetry', 'knee_power_ratio', 'player_height_in_meters', 'player_weight__in_kg', 'trial_id']\n",
      "DEBUG: Selected Features: ['rolling_exhaustion', 'ema_exhaustion', '5thfinger_power_ratio', 'exhaustion_lag1', 'simulated_HR', '1stfinger_asymmetry', 'knee_asymmetry', 'hip_asymmetry', 'knee_power_ratio', 'player_height_in_meters', 'player_weight__in_kg', 'trial_id']\n",
      "2025-02-08 08:56:29,527 [DEBUG] Retained Target Variable(s): ['injury_risk']\n",
      "DEBUG: Retained Target Variable(s): ['injury_risk']\n",
      "2025-02-08 08:56:29,528 [INFO] ✅ Column filtering completed successfully.\n",
      "INFO: ✅ Column filtering completed successfully.\n",
      "2025-02-08 08:56:29,530 [INFO] Step: Split Dataset into Train and Test\n",
      "INFO: Step: Split Dataset into Train and Test\n",
      "2025-02-08 08:56:29,534 [DEBUG] Sorted X_test and y_test by index for alignment.\n",
      "DEBUG: Sorted X_test and y_test by index for alignment.\n",
      "2025-02-08 08:56:29,536 [DEBUG] Class distribution in y_train:\n",
      "injury_risk\n",
      "0    0.72842\n",
      "1    0.27158\n",
      "Name: proportion, dtype: float64\n",
      "DEBUG: Class distribution in y_train:\n",
      "injury_risk\n",
      "0    0.72842\n",
      "1    0.27158\n",
      "Name: proportion, dtype: float64\n",
      "2025-02-08 08:56:29,536 [DEBUG] Class distribution in y_test:\n",
      "injury_risk\n",
      "0    0.711838\n",
      "1    0.288162\n",
      "Name: proportion, dtype: float64\n",
      "DEBUG: Class distribution in y_test:\n",
      "injury_risk\n",
      "0    0.711838\n",
      "1    0.288162\n",
      "Name: proportion, dtype: float64\n",
      "2025-02-08 08:56:29,538 [DEBUG] X_train and y_train indices are aligned.\n",
      "DEBUG: X_train and y_train indices are aligned.\n",
      "2025-02-08 08:56:29,539 [DEBUG] X_test and y_test indices are aligned.\n",
      "DEBUG: X_test and y_test indices are aligned.\n",
      "2025-02-08 08:56:29,540 [INFO] Step: Handle Missing Values\n",
      "INFO: Step: Handle Missing Values\n",
      "2025-02-08 08:56:29,563 [INFO] Step: Test for Normality\n",
      "INFO: Step: Test for Normality\n",
      "2025-02-08 08:56:29,581 [INFO] Step 'Test for Normality' completed: Normality results computed.\n",
      "INFO: Step 'Test for Normality' completed: Normality results computed.\n",
      "2025-02-08 08:56:29,582 [INFO] Step: Handle Outliers\n",
      "INFO: Step: Handle Outliers\n",
      "2025-02-08 08:56:29,583 [INFO] Applying univariate outlier detection for classification.\n",
      "INFO: Applying univariate outlier detection for classification.\n",
      "2025-02-08 08:56:29,609 [INFO] Step: Generate Preprocessor Recommendations\n",
      "INFO: Step: Generate Preprocessor Recommendations\n",
      "2025-02-08 08:56:29,610 [INFO] Preprocessing Recommendations generated.\n",
      "INFO: Preprocessing Recommendations generated.\n",
      "2025-02-08 08:56:29,611 [INFO] Step 'Generate Preprocessor Recommendations' completed: Recommendations generated.\n",
      "INFO: Step 'Generate Preprocessor Recommendations' completed: Recommendations generated.\n",
      "2025-02-08 08:56:29,612 [DEBUG] Numerical transformer added with imputer 'SimpleImputer' and scaler 'StandardScaler'.\n",
      "DEBUG: Numerical transformer added with imputer 'SimpleImputer' and scaler 'StandardScaler'.\n",
      "2025-02-08 08:56:29,612 [DEBUG] Ordinal transformer added with OrdinalEncoder.\n",
      "DEBUG: Ordinal transformer added with OrdinalEncoder.\n",
      "2025-02-08 08:56:29,613 [DEBUG] Nominal transformer added with OneHotEncoder.\n",
      "DEBUG: Nominal transformer added with OneHotEncoder.\n",
      "2025-02-08 08:56:29,614 [DEBUG] ColumnTransformer constructed with the following transformers:\n",
      "DEBUG: ColumnTransformer constructed with the following transformers:\n",
      "2025-02-08 08:56:29,615 [DEBUG] ('num', Pipeline(steps=[('imputer', SimpleImputer(strategy='median')),\n",
      "                ('scaler', StandardScaler())]), ['rolling_exhaustion', 'ema_exhaustion', '5thfinger_power_ratio', 'exhaustion_lag1', 'simulated_HR', '1stfinger_asymmetry', 'knee_asymmetry', 'hip_asymmetry', 'knee_power_ratio'])\n",
      "DEBUG: ('num', Pipeline(steps=[('imputer', SimpleImputer(strategy='median')),\n",
      "                ('scaler', StandardScaler())]), ['rolling_exhaustion', 'ema_exhaustion', '5thfinger_power_ratio', 'exhaustion_lag1', 'simulated_HR', '1stfinger_asymmetry', 'knee_asymmetry', 'hip_asymmetry', 'knee_power_ratio'])\n",
      "2025-02-08 08:56:29,617 [DEBUG] ('ord', Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')),\n",
      "                ('ordinal_encoder', OrdinalEncoder())]), ['player_height_in_meters', 'player_weight__in_kg'])\n",
      "DEBUG: ('ord', Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')),\n",
      "                ('ordinal_encoder', OrdinalEncoder())]), ['player_height_in_meters', 'player_weight__in_kg'])\n",
      "2025-02-08 08:56:29,619 [DEBUG] ('nominal', Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')),\n",
      "                ('onehot_encoder', OneHotEncoder(handle_unknown='ignore'))]), ['trial_id'])\n",
      "DEBUG: ('nominal', Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')),\n",
      "                ('onehot_encoder', OneHotEncoder(handle_unknown='ignore'))]), ['trial_id'])\n",
      "2025-02-08 08:56:29,640 [INFO] ✅ Preprocessor fitted on training data.\n",
      "INFO: ✅ Preprocessor fitted on training data.\n",
      "2025-02-08 08:56:29,665 [INFO] Step: Implement SMOTE (Train Only)\n",
      "INFO: Step: Implement SMOTE (Train Only)\n",
      "2025-02-08 08:56:29,666 [INFO] Class Distribution before SMOTE: {0: 6237, 1: 1152}\n",
      "INFO: Class Distribution before SMOTE: {0: 6237, 1: 1152}\n",
      "2025-02-08 08:56:29,667 [INFO] Imbalance Ratio (Minority/Majority): 0.1847\n",
      "INFO: Imbalance Ratio (Minority/Majority): 0.1847\n",
      "2025-02-08 08:56:29,668 [INFO] Dataset contains both numerical and categorical features. Using SMOTENC.\n",
      "INFO: Dataset contains both numerical and categorical features. Using SMOTENC.\n",
      "2025-02-08 08:56:29,669 [DEBUG] Categorical feature indices for SMOTENC: [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135]\n",
      "DEBUG: Categorical feature indices for SMOTENC: [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135]\n",
      "2025-02-08 08:56:29,670 [DEBUG] Initialized SMOTENC with categorical features indices: [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135] and n_neighbors=5\n",
      "DEBUG: Initialized SMOTENC with categorical features indices: [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135] and n_neighbors=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in data: ['trial_id', 'result', 'landing_x', 'landing_y', 'entry_angle', 'frame_time', 'ball_x', 'ball_y', 'ball_z', 'R_EYE_x', 'R_EYE_y', 'R_EYE_z', 'L_EYE_x', 'L_EYE_y', 'L_EYE_z', 'NOSE_x', 'NOSE_y', 'NOSE_z', 'R_EAR_x', 'R_EAR_y', 'R_EAR_z', 'L_EAR_x', 'L_EAR_y', 'L_EAR_z', 'R_SHOULDER_x', 'R_SHOULDER_y', 'R_SHOULDER_z', 'L_SHOULDER_x', 'L_SHOULDER_y', 'L_SHOULDER_z', 'R_ELBOW_x', 'R_ELBOW_y', 'R_ELBOW_z', 'L_ELBOW_x', 'L_ELBOW_y', 'L_ELBOW_z', 'R_WRIST_x', 'R_WRIST_y', 'R_WRIST_z', 'L_WRIST_x', 'L_WRIST_y', 'L_WRIST_z', 'R_HIP_x', 'R_HIP_y', 'R_HIP_z', 'L_HIP_x', 'L_HIP_y', 'L_HIP_z', 'R_KNEE_x', 'R_KNEE_y', 'R_KNEE_z', 'L_KNEE_x', 'L_KNEE_y', 'L_KNEE_z', 'R_ANKLE_x', 'R_ANKLE_y', 'R_ANKLE_z', 'L_ANKLE_x', 'L_ANKLE_y', 'L_ANKLE_z', 'R_1STFINGER_x', 'R_1STFINGER_y', 'R_1STFINGER_z', 'R_5THFINGER_x', 'R_5THFINGER_y', 'R_5THFINGER_z', 'L_1STFINGER_x', 'L_1STFINGER_y', 'L_1STFINGER_z', 'L_5THFINGER_x', 'L_5THFINGER_y', 'L_5THFINGER_z', 'R_1STTOE_x', 'R_1STTOE_y', 'R_1STTOE_z', 'R_5THTOE_x', 'R_5THTOE_y', 'R_5THTOE_z', 'L_1STTOE_x', 'L_1STTOE_y', 'L_1STTOE_z', 'L_5THTOE_x', 'L_5THTOE_y', 'L_5THTOE_z', 'R_CALC_x', 'R_CALC_y', 'R_CALC_z', 'L_CALC_x', 'L_CALC_y', 'L_CALC_z', 'ball_speed', 'ball_velocity_x', 'ball_velocity_y', 'ball_velocity_z', 'overall_ball_velocity', 'ball_direction_x', 'ball_direction_y', 'ball_direction_z', 'computed_ball_velocity_x', 'computed_ball_velocity_y', 'computed_ball_velocity_z', 'dist_ball_R_1STFINGER', 'dist_ball_R_5THFINGER', 'dist_ball_L_1STFINGER', 'dist_ball_L_5THFINGER', 'ball_in_hands', 'shooting_motion', 'avg_shoulder_height', 'release_point_filter', 'dt', 'dx', 'dy', 'dz', 'L_ANKLE_ongoing_power', 'R_ANKLE_ongoing_power', 'L_KNEE_ongoing_power', 'R_KNEE_ongoing_power', 'L_HIP_ongoing_power', 'R_HIP_ongoing_power', 'L_ELBOW_ongoing_power', 'R_ELBOW_ongoing_power', 'L_WRIST_ongoing_power', 'R_WRIST_ongoing_power', 'L_1STFINGER_ongoing_power', 'L_5THFINGER_ongoing_power', 'R_1STFINGER_ongoing_power', 'R_5THFINGER_ongoing_power', 'elbow_angle', 'wrist_angle', 'knee_angle', 'player_height_in_meters', 'player_height_ft', 'initial_release_angle', 'calculated_release_angle', 'angle_difference', 'distance_to_basket', 'optimal_release_angle', 'by_trial_time', 'continuous_frame_time', 'L_ANKLE_energy', 'R_ANKLE_energy', 'L_KNEE_energy', 'R_KNEE_energy', 'L_HIP_energy', 'R_HIP_energy', 'L_ELBOW_energy', 'R_ELBOW_energy', 'L_WRIST_energy', 'R_WRIST_energy', 'L_1STFINGER_energy', 'R_1STFINGER_energy', 'L_5THFINGER_energy', 'R_5THFINGER_energy', 'total_energy', 'by_trial_energy', 'by_trial_exhaustion_score', 'overall_cumulative_energy', 'overall_exhaustion_score', 'L_ANKLE_energy_by_trial', 'L_ANKLE_energy_by_trial_exhaustion_score', 'L_ANKLE_energy_overall_cumulative', 'L_ANKLE_energy_overall_exhaustion_score', 'R_ANKLE_energy_by_trial', 'R_ANKLE_energy_by_trial_exhaustion_score', 'R_ANKLE_energy_overall_cumulative', 'R_ANKLE_energy_overall_exhaustion_score', 'L_KNEE_energy_by_trial', 'L_KNEE_energy_by_trial_exhaustion_score', 'L_KNEE_energy_overall_cumulative', 'L_KNEE_energy_overall_exhaustion_score', 'R_KNEE_energy_by_trial', 'R_KNEE_energy_by_trial_exhaustion_score', 'R_KNEE_energy_overall_cumulative', 'R_KNEE_energy_overall_exhaustion_score', 'L_HIP_energy_by_trial', 'L_HIP_energy_by_trial_exhaustion_score', 'L_HIP_energy_overall_cumulative', 'L_HIP_energy_overall_exhaustion_score', 'R_HIP_energy_by_trial', 'R_HIP_energy_by_trial_exhaustion_score', 'R_HIP_energy_overall_cumulative', 'R_HIP_energy_overall_exhaustion_score', 'L_ELBOW_energy_by_trial', 'L_ELBOW_energy_by_trial_exhaustion_score', 'L_ELBOW_energy_overall_cumulative', 'L_ELBOW_energy_overall_exhaustion_score', 'R_ELBOW_energy_by_trial', 'R_ELBOW_energy_by_trial_exhaustion_score', 'R_ELBOW_energy_overall_cumulative', 'R_ELBOW_energy_overall_exhaustion_score', 'L_WRIST_energy_by_trial', 'L_WRIST_energy_by_trial_exhaustion_score', 'L_WRIST_energy_overall_cumulative', 'L_WRIST_energy_overall_exhaustion_score', 'R_WRIST_energy_by_trial', 'R_WRIST_energy_by_trial_exhaustion_score', 'R_WRIST_energy_overall_cumulative', 'R_WRIST_energy_overall_exhaustion_score', 'L_1STFINGER_energy_by_trial', 'L_1STFINGER_energy_by_trial_exhaustion_score', 'L_1STFINGER_energy_overall_cumulative', 'L_1STFINGER_energy_overall_exhaustion_score', 'R_1STFINGER_energy_by_trial', 'R_1STFINGER_energy_by_trial_exhaustion_score', 'R_1STFINGER_energy_overall_cumulative', 'R_1STFINGER_energy_overall_exhaustion_score', 'L_5THFINGER_energy_by_trial', 'L_5THFINGER_energy_by_trial_exhaustion_score', 'L_5THFINGER_energy_overall_cumulative', 'L_5THFINGER_energy_overall_exhaustion_score', 'R_5THFINGER_energy_by_trial', 'R_5THFINGER_energy_by_trial_exhaustion_score', 'R_5THFINGER_energy_overall_cumulative', 'R_5THFINGER_energy_overall_exhaustion_score', 'participant_id', 'L_SHOULDER_angle', 'R_SHOULDER_angle', 'L_HIP_angle', 'R_HIP_angle', 'L_KNEE_angle', 'R_KNEE_angle', 'L_ANKLE_angle', 'R_ANKLE_angle', 'player_weight__in_kg', 'joint_energy', 'joint_power', 'energy_acceleration', 'ankle_power_ratio', 'hip_asymmetry', 'ankle_asymmetry', 'wrist_asymmetry', 'elbow_asymmetry', 'knee_asymmetry', '1stfinger_asymmetry', '5thfinger_asymmetry', 'hip_power_ratio', 'wrist_power_ratio', 'elbow_power_ratio', 'knee_power_ratio', '1stfinger_power_ratio', '5thfinger_power_ratio', 'L_KNEE_ROM', 'L_KNEE_ROM_deviation', 'L_KNEE_ROM_extreme', 'R_KNEE_ROM', 'R_KNEE_ROM_deviation', 'R_KNEE_ROM_extreme', 'L_SHOULDER_ROM', 'L_SHOULDER_ROM_deviation', 'L_SHOULDER_ROM_extreme', 'R_SHOULDER_ROM', 'R_SHOULDER_ROM_deviation', 'R_SHOULDER_ROM_extreme', 'L_HIP_ROM', 'L_HIP_ROM_deviation', 'L_HIP_ROM_extreme', 'R_HIP_ROM', 'R_HIP_ROM_deviation', 'R_HIP_ROM_extreme', 'L_ANKLE_ROM', 'L_ANKLE_ROM_deviation', 'L_ANKLE_ROM_extreme', 'R_ANKLE_ROM', 'R_ANKLE_ROM_deviation', 'R_ANKLE_ROM_extreme', 'exhaustion_rate', 'simulated_HR', 'time_since_start', 'power_avg_5', 'rolling_power_std', 'rolling_hr_mean', 'rolling_energy_std', 'exhaustion_lag1', 'ema_exhaustion', 'rolling_exhaustion', 'injury_risk', 'L_ANKLE_exhaustion_rate', 'L_ANKLE_rolling_exhaustion', 'L_ANKLE_injury_risk', 'R_ANKLE_exhaustion_rate', 'R_ANKLE_rolling_exhaustion', 'R_ANKLE_injury_risk', 'L_WRIST_exhaustion_rate', 'L_WRIST_rolling_exhaustion', 'L_WRIST_injury_risk', 'R_WRIST_exhaustion_rate', 'R_WRIST_rolling_exhaustion', 'R_WRIST_injury_risk', 'L_ELBOW_exhaustion_rate', 'L_ELBOW_rolling_exhaustion', 'L_ELBOW_injury_risk', 'R_ELBOW_exhaustion_rate', 'R_ELBOW_rolling_exhaustion', 'R_ELBOW_injury_risk', 'L_KNEE_exhaustion_rate', 'L_KNEE_rolling_exhaustion', 'L_KNEE_injury_risk', 'R_KNEE_exhaustion_rate', 'R_KNEE_rolling_exhaustion', 'R_KNEE_injury_risk', 'L_HIP_exhaustion_rate', 'L_HIP_rolling_exhaustion', 'L_HIP_injury_risk', 'R_HIP_exhaustion_rate', 'R_HIP_rolling_exhaustion', 'R_HIP_injury_risk']\n",
      "[DEBUG] Features loaded for injury_risk: ['rolling_exhaustion', 'ema_exhaustion', '5thfinger_power_ratio', 'exhaustion_lag1', 'simulated_HR', '1stfinger_asymmetry', 'knee_asymmetry', 'hip_asymmetry', 'knee_power_ratio']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-08 08:56:34,822 [INFO] Applied SMOTENC. Resampled dataset shape: (12474, 136)\n",
      "INFO: Applied SMOTENC. Resampled dataset shape: (12474, 136)\n",
      "2025-02-08 08:56:34,823 [DEBUG] Selected n_neighbors for SMOTE: 5\n",
      "DEBUG: Selected n_neighbors for SMOTE: 5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (12474, 1), indices imply (12474, 136)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 1815\u001b[0m\n\u001b[0;32m   1812\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_injury\u001b[38;5;66;03m#, model_exhaustion, joint_models\u001b[39;00m\n\u001b[0;32m   1814\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 1815\u001b[0m     \u001b[43mrun_training_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[34], line 1807\u001b[0m, in \u001b[0;36mrun_training_pipeline\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1804\u001b[0m data \u001b[38;5;241m=\u001b[39m feature_engineering(data)\n\u001b[0;32m   1805\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumns in data:\u001b[39m\u001b[38;5;124m\"\u001b[39m, data\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[1;32m-> 1807\u001b[0m model_injury \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_overall_injury_risk_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1808\u001b[0m \u001b[38;5;66;03m# model_exhaustion = train_overall_exhaustion_model(data, feature_dir)\u001b[39;00m\n\u001b[0;32m   1809\u001b[0m \u001b[38;5;66;03m# joint_models = train_joint_injury_risk_models(data, feature_dir)\u001b[39;00m\n\u001b[0;32m   1810\u001b[0m \n\u001b[0;32m   1811\u001b[0m \u001b[38;5;66;03m# Optionally, you can save the models to disk here.\u001b[39;00m\n\u001b[0;32m   1812\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model_injury\n",
      "Cell \u001b[1;32mIn[34], line 1642\u001b[0m, in \u001b[0;36mtrain_overall_injury_risk_model\u001b[1;34m(data, feature_dir, timesteps)\u001b[0m\n\u001b[0;32m   1640\u001b[0m \u001b[38;5;66;03m# Split the data\u001b[39;00m\n\u001b[0;32m   1641\u001b[0m train_data, test_data \u001b[38;5;241m=\u001b[39m temporal_train_test_split(data, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m)\n\u001b[1;32m-> 1642\u001b[0m X_train, X_test, y_train, y_test, recommendations, X_test_inverse \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinal_preprocessing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1644\u001b[0m X_train, y_train, _, _ \u001b[38;5;241m=\u001b[39m classifier_preprocessor\u001b[38;5;241m.\u001b[39mfinal_preprocessing(train_data)\n\u001b[0;32m   1645\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX_train shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, X_train\u001b[38;5;241m.\u001b[39mshape)\n",
      "Cell \u001b[1;32mIn[34], line 1559\u001b[0m, in \u001b[0;36mDataPreprocessor.final_preprocessing\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1557\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1558\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget variable \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be provided in train mode.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1559\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1561\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredict\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m   1562\u001b[0m     \u001b[38;5;66;03m# For predict mode, y_variable is not used\u001b[39;00m\n\u001b[0;32m   1563\u001b[0m     X \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mcopy()\n",
      "Cell \u001b[1;32mIn[34], line 1315\u001b[0m, in \u001b[0;36mDataPreprocessor.preprocess_train\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m   1313\u001b[0m \u001b[38;5;66;03m# Step 8: Save Transformers (Including the Pipeline)\u001b[39;00m\n\u001b[0;32m   1314\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_feature_order \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpipeline\u001b[38;5;241m.\u001b[39mget_feature_names_out())\n\u001b[1;32m-> 1315\u001b[0m X_train_final \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_smoted\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinal_feature_order\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1316\u001b[0m X_test_final \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(X_test_preprocessed, columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_feature_order, index\u001b[38;5;241m=\u001b[39mX_test_original\u001b[38;5;241m.\u001b[39mindex) \u001b[38;5;28;01mif\u001b[39;00m X_test_preprocessed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\ghadf\\anaconda3\\envs\\data_science_ft_bio_predictions\\lib\\site-packages\\pandas\\core\\frame.py:867\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    859\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m arrays_to_mgr(\n\u001b[0;32m    860\u001b[0m             arrays,\n\u001b[0;32m    861\u001b[0m             columns,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    864\u001b[0m             typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[0;32m    865\u001b[0m         )\n\u001b[0;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 867\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m \u001b[43mndarray_to_mgr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m            \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    872\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    873\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    876\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m dict_to_mgr(\n\u001b[0;32m    877\u001b[0m         {},\n\u001b[0;32m    878\u001b[0m         index,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    881\u001b[0m         typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[0;32m    882\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ghadf\\anaconda3\\envs\\data_science_ft_bio_predictions\\lib\\site-packages\\pandas\\core\\internals\\construction.py:336\u001b[0m, in \u001b[0;36mndarray_to_mgr\u001b[1;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;66;03m# _prep_ndarraylike ensures that values.ndim == 2 at this point\u001b[39;00m\n\u001b[0;32m    332\u001b[0m index, columns \u001b[38;5;241m=\u001b[39m _get_axes(\n\u001b[0;32m    333\u001b[0m     values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], index\u001b[38;5;241m=\u001b[39mindex, columns\u001b[38;5;241m=\u001b[39mcolumns\n\u001b[0;32m    334\u001b[0m )\n\u001b[1;32m--> 336\u001b[0m \u001b[43m_check_values_indices_shape_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\ghadf\\anaconda3\\envs\\data_science_ft_bio_predictions\\lib\\site-packages\\pandas\\core\\internals\\construction.py:420\u001b[0m, in \u001b[0;36m_check_values_indices_shape_match\u001b[1;34m(values, index, columns)\u001b[0m\n\u001b[0;32m    418\u001b[0m passed \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    419\u001b[0m implied \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlen\u001b[39m(index), \u001b[38;5;28mlen\u001b[39m(columns))\n\u001b[1;32m--> 420\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of passed values is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpassed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, indices imply \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimplied\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Shape of passed values is (12474, 1), indices imply (12474, 136)"
     ]
    }
   ],
   "source": [
    "# train.py\n",
    "import logging\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from joblib import Parallel, delayed\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# from utils import load_top_features, temporal_train_test_split, create_sequences\n",
    "\n",
    "\n",
    "# datapreprocessor.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Optional, Tuple, Any\n",
    "import logging\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.preprocessing import PowerTransformer, OrdinalEncoder, OneHotEncoder, StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, SMOTENC, SMOTEN, BorderlineSMOTE\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import probplot\n",
    "import joblib  # For saving/loading transformers\n",
    "from inspect import signature  # For parameter validation in SMOTE\n",
    "\n",
    "class DataPreprocessor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_type: str,\n",
    "        y_variable: List[str],\n",
    "        ordinal_categoricals: List[str],\n",
    "        nominal_categoricals: List[str],\n",
    "        numericals: List[str],\n",
    "        mode: str,  # 'train', 'predict', 'clustering'\n",
    "        options: Optional[Dict] = None,\n",
    "        debug: bool = False,\n",
    "        normalize_debug: bool = False,\n",
    "        normalize_graphs_output: bool = False,\n",
    "        graphs_output_dir: str = './plots',\n",
    "        transformers_dir: str = './transformers'\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the DataPreprocessor with model type and feature lists.\n",
    "\n",
    "        Args:\n",
    "            model_type (str): Type of the machine learning model (e.g., 'Logistic Regression').\n",
    "            y_variable (List[str]): List of target variable(s).\n",
    "            ordinal_categoricals (List[str]): List of ordinal categorical features.\n",
    "            nominal_categoricals (List[str]): List of nominal categorical features.\n",
    "            numericals (List[str]): List of numerical features.\n",
    "            mode (str): Operational mode ('train', 'predict', 'clustering').\n",
    "            options (Optional[Dict]): User-defined options for preprocessing steps.\n",
    "            debug (bool): General debug flag to control overall verbosity.\n",
    "            normalize_debug (bool): Flag to display normalization plots.\n",
    "            normalize_graphs_output (bool): Flag to save normalization plots.\n",
    "            graphs_output_dir (str): Directory to save plots.\n",
    "            transformers_dir (str): Directory to save/load transformers.\n",
    "        \"\"\"\n",
    "        self.model_type = model_type\n",
    "        self.y_variable = y_variable\n",
    "        self.ordinal_categoricals = ordinal_categoricals\n",
    "        self.nominal_categoricals = nominal_categoricals\n",
    "        self.numericals = numericals\n",
    "        self.mode = mode.lower()\n",
    "        if self.mode not in ['train', 'predict', 'clustering']:\n",
    "            raise ValueError(\"Mode must be one of 'train', 'predict', or 'clustering'.\")\n",
    "        self.options = options or {}\n",
    "        self.debug = debug\n",
    "        self.normalize_debug = normalize_debug\n",
    "        self.normalize_graphs_output = normalize_graphs_output\n",
    "        self.graphs_output_dir = graphs_output_dir\n",
    "        self.transformers_dir = transformers_dir\n",
    "\n",
    "        # Initialize categorical_indices to prevent AttributeError\n",
    "        self.categorical_indices = []\n",
    "\n",
    "        # Define model categories for accurate processing\n",
    "        self.model_category = self.map_model_type_to_category()\n",
    "\n",
    "        if self.model_category == 'unknown':\n",
    "            self.logger = logging.getLogger(self.__class__.__name__)\n",
    "            self.logger.error(f\"Model category for '{self.model_type}' is unknown. Check your configuration.\")\n",
    "            raise ValueError(f\"Model category for '{self.model_type}' is unknown. Check your configuration.\")\n",
    "\n",
    "        # Initialize y_variable based on mode and model category\n",
    "        if self.mode in ['train', 'predict'] and self.model_category in ['classification', 'regression']:\n",
    "            if not self.y_variable:\n",
    "                if self.mode == 'train':\n",
    "                    raise ValueError(\"Target variable 'y_variable' must be specified for supervised models in train mode.\")\n",
    "                # In predict mode, y_variable might not be present\n",
    "        else:\n",
    "            # For 'clustering' mode or unsupervised prediction\n",
    "            self.y_variable = []\n",
    "\n",
    "        # Initialize other variables\n",
    "        self.scaler = None\n",
    "        self.transformer = None\n",
    "        self.ordinal_encoder = None\n",
    "        self.nominal_encoder = None\n",
    "        self.preprocessor = None\n",
    "        self.smote = None\n",
    "        self.feature_reasons = {col: '' for col in self.ordinal_categoricals + self.nominal_categoricals + self.numericals}\n",
    "        self.preprocessing_steps = []\n",
    "        self.normality_results = {}\n",
    "        self.features_to_transform = []\n",
    "        self.nominal_encoded_feature_names = []\n",
    "        self.final_feature_order = []\n",
    "\n",
    "        # Initialize placeholders for clustering-specific transformers\n",
    "        self.cluster_transformers = {}\n",
    "        self.cluster_model = None\n",
    "        self.cluster_labels = None\n",
    "        self.silhouette_score = None\n",
    "\n",
    "        # Define default thresholds for SMOTE recommendations\n",
    "        self.imbalance_threshold = self.options.get('smote_recommendation', {}).get('imbalance_threshold', 0.1)\n",
    "        self.noise_threshold = self.options.get('smote_recommendation', {}).get('noise_threshold', 0.1)\n",
    "        self.overlap_threshold = self.options.get('smote_recommendation', {}).get('overlap_threshold', 0.1)\n",
    "        self.boundary_threshold = self.options.get('smote_recommendation', {}).get('boundary_threshold', 0.1)\n",
    "\n",
    "        self.pipeline = None  # Initialize pipeline\n",
    "\n",
    "        # Initialize logging\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        self.logger.setLevel(logging.DEBUG if self.debug else logging.INFO)\n",
    "        handler = logging.StreamHandler()\n",
    "        formatter = logging.Formatter('%(asctime)s [%(levelname)s] %(message)s')\n",
    "        handler.setFormatter(formatter)\n",
    "        if not self.logger.handlers:\n",
    "            self.logger.addHandler(handler)\n",
    "            \n",
    "        # Initialize feature_reasons with 'all_numericals' for clustering\n",
    "        self.feature_reasons = {col: '' for col in self.ordinal_categoricals + self.nominal_categoricals + self.numericals}\n",
    "        if self.model_category == 'clustering':\n",
    "            self.feature_reasons['all_numericals'] = ''\n",
    "\n",
    "    def get_debug_flag(self, flag_name: str) -> bool:\n",
    "        \"\"\"\n",
    "        Retrieve the value of a specific debug flag from the options.\n",
    "        Args:\n",
    "            flag_name (str): The name of the debug flag.\n",
    "        Returns:\n",
    "            bool: The value of the debug flag.\n",
    "        \"\"\"\n",
    "        return self.options.get(flag_name, False)\n",
    "\n",
    "    def _log(self, message: str, step: str, level: str = 'info'):\n",
    "        \"\"\"\n",
    "        Internal method to log messages based on the step-specific debug flags.\n",
    "        \n",
    "        Args:\n",
    "            message (str): The message to log.\n",
    "            step (str): The preprocessing step name.\n",
    "            level (str): The logging level ('info', 'debug', etc.).\n",
    "        \"\"\"\n",
    "        debug_flag = self.get_debug_flag(f'debug_{step}')\n",
    "        if debug_flag:\n",
    "            if level == 'debug':\n",
    "                self.logger.debug(message)\n",
    "            elif level == 'info':\n",
    "                self.logger.info(message)\n",
    "            elif level == 'warning':\n",
    "                self.logger.warning(message)\n",
    "            elif level == 'error':\n",
    "                self.logger.error(message)\n",
    "\n",
    "    def map_model_type_to_category(self) -> str:\n",
    "        \"\"\"\n",
    "        Map the model_type string to a predefined category based on keywords.\n",
    "\n",
    "        Returns:\n",
    "            str: The model category ('classification', 'regression', 'clustering', etc.).\n",
    "        \"\"\"\n",
    "        classification_keywords = ['classifier', 'classification', 'logistic', 'svm', 'support vector machine', 'knn', 'neural network']\n",
    "        regression_keywords = ['regressor', 'regression', 'linear', 'knn', 'neural network']  # Removed 'svm'\n",
    "        clustering_keywords = ['k-means', 'clustering', 'dbscan', 'kmodes', 'kprototypes']\n",
    "\n",
    "        model_type_lower = self.model_type.lower()\n",
    "\n",
    "        for keyword in classification_keywords:\n",
    "            if keyword in model_type_lower:\n",
    "                return 'classification'\n",
    "\n",
    "        for keyword in regression_keywords:\n",
    "            if keyword in model_type_lower:\n",
    "                return 'regression'\n",
    "\n",
    "        for keyword in clustering_keywords:\n",
    "            if keyword in model_type_lower:\n",
    "                return 'clustering'\n",
    "\n",
    "        return 'unknown'\n",
    "\n",
    "    def filter_columns(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        step_name = \"filter_columns\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "\n",
    "        # Combine all feature lists\n",
    "        desired_features = self.numericals + self.ordinal_categoricals + self.nominal_categoricals\n",
    "\n",
    "        # For 'train' and 'clustering' modes, handle y_variable appropriately\n",
    "        if self.mode == 'train':\n",
    "            # Ensure y_variable is present in the data but not included in features\n",
    "            if not all(col in df.columns for col in self.y_variable):\n",
    "                missing_y = [col for col in self.y_variable if col not in df.columns]\n",
    "                self.logger.error(f\"Target variable(s) {missing_y} not found in the input data.\")\n",
    "                raise ValueError(f\"Target variable(s) {missing_y} not found in the input data.\")\n",
    "            # Exclude y_variable from features\n",
    "            desired_features = [col for col in desired_features if col not in self.y_variable]\n",
    "            # Retain y_variable in the filtered DataFrame\n",
    "            filtered_df = df[desired_features + self.y_variable].copy()\n",
    "        else:\n",
    "            # For 'predict' and 'clustering' modes, exclude y_variable\n",
    "            filtered_df = df[desired_features].copy()\n",
    "\n",
    "        # Identify missing features in the input DataFrame\n",
    "        missing_features = [col for col in desired_features if col not in df.columns]\n",
    "        if missing_features:\n",
    "            self.logger.error(f\"The following required features are missing in the input data: {missing_features}\")\n",
    "            raise ValueError(f\"The following required features are missing in the input data: {missing_features}\")\n",
    "\n",
    "        # Log the filtering action\n",
    "        self.logger.info(f\"✅ Filtered DataFrame to include only specified features. Shape: {filtered_df.shape}\")\n",
    "        self.logger.debug(f\"Selected Features: {desired_features}\")\n",
    "        if self.mode == 'train':\n",
    "            self.logger.debug(f\"Retained Target Variable(s): {self.y_variable}\")\n",
    "\n",
    "        return filtered_df\n",
    "\n",
    "    def split_dataset(\n",
    "        self,\n",
    "        X: pd.DataFrame,\n",
    "        y: Optional[pd.Series] = None\n",
    "    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame], Optional[pd.Series], Optional[pd.Series]]:\n",
    "        \"\"\"\n",
    "        Split the dataset into training and testing sets while retaining original indices.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): Features.\n",
    "            y (Optional[pd.Series]): Target variable.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, Optional[pd.DataFrame], Optional[pd.Series], Optional[pd.Series]]: X_train, X_test, y_train, y_test\n",
    "        \"\"\"\n",
    "        step_name = \"split_dataset\"\n",
    "        self.logger.info(\"Step: Split Dataset into Train and Test\")\n",
    "\n",
    "        # Debugging Statements\n",
    "        self._log(f\"Before Split - X shape: {X.shape}\", step_name, 'debug')\n",
    "        if y is not None:\n",
    "            self._log(f\"Before Split - y shape: {y.shape}\", step_name, 'debug')\n",
    "        else:\n",
    "            self._log(\"Before Split - y is None\", step_name, 'debug')\n",
    "\n",
    "        # Determine splitting based on mode\n",
    "        if self.mode == 'train' and self.model_category in ['classification', 'regression']:\n",
    "            if self.model_category == 'classification':\n",
    "                stratify = y if self.options.get('split_dataset', {}).get('stratify_for_classification', False) else None\n",
    "                test_size = self.options.get('split_dataset', {}).get('test_size', 0.2)\n",
    "                random_state = self.options.get('split_dataset', {}).get('random_state', 42)\n",
    "                X_train, X_test, y_train, y_test = train_test_split(\n",
    "                    X, y, \n",
    "                    test_size=test_size,\n",
    "                    stratify=stratify, \n",
    "                    random_state=random_state\n",
    "                )\n",
    "                self._log(\"Performed stratified split for classification.\", step_name, 'debug')\n",
    "            elif self.model_category == 'regression':\n",
    "                X_train, X_test, y_train, y_test = train_test_split(\n",
    "                    X, y, \n",
    "                    test_size=self.options.get('split_dataset', {}).get('test_size', 0.2),\n",
    "                    random_state=self.options.get('split_dataset', {}).get('random_state', 42)\n",
    "                )\n",
    "                self._log(\"Performed random split for regression.\", step_name, 'debug')\n",
    "        else:\n",
    "            # For 'predict' and 'clustering' modes or other categories\n",
    "            X_train = X.copy()\n",
    "            X_test = None\n",
    "            y_train = y.copy() if y is not None else None\n",
    "            y_test = None\n",
    "            self.logger.info(f\"No splitting performed for mode '{self.mode}' or model category '{self.model_category}'.\")\n",
    "\n",
    "        self.preprocessing_steps.append(\"Split Dataset into Train and Test\")\n",
    "\n",
    "        # Keep Indices Aligned Through Each Step\n",
    "        if X_test is not None and y_test is not None:\n",
    "            # Sort both X_test and y_test by index\n",
    "            X_test = X_test.sort_index()\n",
    "            y_test = y_test.sort_index()\n",
    "            self.logger.debug(\"Sorted X_test and y_test by index for alignment.\")\n",
    "\n",
    "        # Debugging: Log post-split shapes and index alignment\n",
    "        self._log(f\"After Split - X_train shape: {X_train.shape}, X_test shape: {X_test.shape if X_test is not None else 'N/A'}\", step_name, 'debug')\n",
    "        if self.model_category == 'classification' and y_train is not None and y_test is not None:\n",
    "            self.logger.debug(f\"Class distribution in y_train:\\n{y_train.value_counts(normalize=True)}\")\n",
    "            self.logger.debug(f\"Class distribution in y_test:\\n{y_test.value_counts(normalize=True)}\")\n",
    "        elif self.model_category == 'regression' and y_train is not None and y_test is not None:\n",
    "            self.logger.debug(f\"y_train statistics:\\n{y_train.describe()}\")\n",
    "            self.logger.debug(f\"y_test statistics:\\n{y_test.describe()}\")\n",
    "\n",
    "        # Check index alignment\n",
    "        if y_train is not None and X_train.index.equals(y_train.index):\n",
    "            self.logger.debug(\"X_train and y_train indices are aligned.\")\n",
    "        else:\n",
    "            self.logger.warning(\"X_train and y_train indices are misaligned.\")\n",
    "\n",
    "        if X_test is not None and y_test is not None and X_test.index.equals(y_test.index):\n",
    "            self.logger.debug(\"X_test and y_test indices are aligned.\")\n",
    "        elif X_test is not None and y_test is not None:\n",
    "            self.logger.warning(\"X_test and y_test indices are misaligned.\")\n",
    "\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "    def handle_missing_values(self, X_train: pd.DataFrame, X_test: Optional[pd.DataFrame] = None) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n",
    "        \"\"\"\n",
    "        Handle missing values for numerical and categorical features based on user options.\n",
    "        \"\"\"\n",
    "        step_name = \"handle_missing_values\"\n",
    "        self.logger.info(\"Step: Handle Missing Values\")\n",
    "\n",
    "        # Fetch user-defined imputation options or set defaults\n",
    "        impute_options = self.options.get('handle_missing_values', {})\n",
    "        numerical_strategy = impute_options.get('numerical_strategy', {})\n",
    "        categorical_strategy = impute_options.get('categorical_strategy', {})\n",
    "\n",
    "        # Numerical Imputation\n",
    "        numerical_imputer = None\n",
    "        new_columns = []\n",
    "        if self.numericals:\n",
    "            if self.model_category in ['regression', 'classification', 'clustering']:\n",
    "                default_num_strategy = 'median'  # Changed to median as per preprocessor_config.yaml\n",
    "            else:\n",
    "                default_num_strategy = 'median'\n",
    "            num_strategy = numerical_strategy.get('strategy', default_num_strategy)\n",
    "            num_imputer_type = numerical_strategy.get('imputer', 'SimpleImputer')  # Can be 'SimpleImputer', 'KNNImputer', etc.\n",
    "\n",
    "            self._log(f\"Numerical Imputation Strategy: {num_strategy.capitalize()}, Imputer Type: {num_imputer_type}\", step_name, 'debug')\n",
    "\n",
    "            # Initialize numerical imputer based on user option\n",
    "            if num_imputer_type == 'SimpleImputer':\n",
    "                numerical_imputer = SimpleImputer(strategy=num_strategy)\n",
    "            elif num_imputer_type == 'KNNImputer':\n",
    "                knn_neighbors = numerical_strategy.get('knn_neighbors', 5)\n",
    "                numerical_imputer = KNNImputer(n_neighbors=knn_neighbors)\n",
    "            else:\n",
    "                self.logger.error(f\"Numerical imputer type '{num_imputer_type}' is not supported.\")\n",
    "                raise ValueError(f\"Numerical imputer type '{num_imputer_type}' is not supported.\")\n",
    "\n",
    "            # Fit and transform ONLY on X_train\n",
    "            X_train[self.numericals] = numerical_imputer.fit_transform(X_train[self.numericals])\n",
    "            self.numerical_imputer = numerical_imputer  # Assign to self for saving\n",
    "            self.feature_reasons.update({col: self.feature_reasons.get(col, '') + f'Numerical: {num_strategy.capitalize()} Imputation | ' for col in self.numericals})\n",
    "            new_columns.extend(self.numericals)\n",
    "\n",
    "            if X_test is not None:\n",
    "                # Transform ONLY on X_test without fitting\n",
    "                X_test[self.numericals] = numerical_imputer.transform(X_test[self.numericals])\n",
    "\n",
    "        # Categorical Imputation\n",
    "        categorical_imputer = None\n",
    "        all_categoricals = self.ordinal_categoricals + self.nominal_categoricals\n",
    "        if all_categoricals:\n",
    "            default_cat_strategy = 'most_frequent'\n",
    "            cat_strategy = categorical_strategy.get('strategy', default_cat_strategy)\n",
    "            cat_imputer_type = categorical_strategy.get('imputer', 'SimpleImputer')\n",
    "\n",
    "            self._log(f\"Categorical Imputation Strategy: {cat_strategy.capitalize()}, Imputer Type: {cat_imputer_type}\", step_name, 'debug')\n",
    "\n",
    "            # Initialize categorical imputer based on user option\n",
    "            if cat_imputer_type == 'SimpleImputer':\n",
    "                categorical_imputer = SimpleImputer(strategy=cat_strategy)\n",
    "            elif cat_imputer_type == 'ConstantImputer':\n",
    "                fill_value = categorical_strategy.get('fill_value', 'Missing')\n",
    "                categorical_imputer = SimpleImputer(strategy='constant', fill_value=fill_value)\n",
    "            else:\n",
    "                self.logger.error(f\"Categorical imputer type '{cat_imputer_type}' is not supported.\")\n",
    "                raise ValueError(f\"Categorical imputer type '{cat_imputer_type}' is not supported.\")\n",
    "\n",
    "            # Fit and transform ONLY on X_train\n",
    "            X_train[all_categoricals] = categorical_imputer.fit_transform(X_train[all_categoricals])\n",
    "            self.categorical_imputer = categorical_imputer  # Assign to self for saving\n",
    "            self.feature_reasons.update({\n",
    "                col: self.feature_reasons.get(col, '') + (f'Categorical: Constant Imputation (Value={categorical_strategy.get(\"fill_value\", \"Missing\")}) | ' if cat_imputer_type == 'ConstantImputer' else f'Categorical: {cat_strategy.capitalize()} Imputation | ')\n",
    "                for col in all_categoricals\n",
    "            })\n",
    "            new_columns.extend(all_categoricals)\n",
    "\n",
    "            if X_test is not None:\n",
    "                # Transform ONLY on X_test without fitting\n",
    "                X_test[all_categoricals] = categorical_imputer.transform(X_test[all_categoricals])\n",
    "\n",
    "        self.preprocessing_steps.append(\"Handle Missing Values\")\n",
    "\n",
    "        # Debugging: Log post-imputation shapes and missing values\n",
    "        self._log(f\"Completed: Handle Missing Values. Dataset shape after imputation: {X_train.shape}\", step_name, 'debug')\n",
    "        self._log(f\"Missing values after imputation in X_train:\\n{X_train.isnull().sum()}\", step_name, 'debug')\n",
    "        self._log(f\"New columns handled: {new_columns}\", step_name, 'debug')\n",
    "\n",
    "        return X_train, X_test\n",
    "\n",
    "    def handle_outliers(self, X_train: pd.DataFrame, y_train: Optional[pd.Series] = None) -> Tuple[pd.DataFrame, Optional[pd.Series]]:\n",
    "        \"\"\"\n",
    "        Handle outliers based on the model's sensitivity and user options.\n",
    "\n",
    "        Args:\n",
    "            X_train (pd.DataFrame): Training features.\n",
    "            y_train (pd.Series, optional): Training target.\n",
    "\n",
    "        Returns:\n",
    "            tuple: X_train without outliers and corresponding y_train.\n",
    "        \"\"\"\n",
    "        step_name = \"handle_outliers\"\n",
    "        self.logger.info(\"Step: Handle Outliers\")\n",
    "        self._log(\"Starting outlier handling.\", step_name, 'debug')\n",
    "\n",
    "        debug_flag = self.get_debug_flag('debug_handle_outliers')\n",
    "        initial_shape = X_train.shape[0]\n",
    "\n",
    "        # Fetch user-defined outlier handling options or set defaults\n",
    "        outlier_options = self.options.get('handle_outliers', {})\n",
    "        zscore_threshold = outlier_options.get('zscore_threshold', 3)\n",
    "        iqr_multiplier = outlier_options.get('iqr_multiplier', 1.5)\n",
    "        isolation_contamination = outlier_options.get('isolation_contamination', 0.05)\n",
    "\n",
    "        if self.model_category in ['regression', 'classification']:\n",
    "            self.logger.info(f\"Applying univariate outlier detection for {self.model_category}.\")\n",
    "            for col in self.numericals:\n",
    "                # Z-Score Filtering\n",
    "                apply_zscore = outlier_options.get('apply_zscore', True)\n",
    "                if apply_zscore:\n",
    "                    z_scores = np.abs((X_train[col] - X_train[col].mean()) / X_train[col].std())\n",
    "                    mask_z = z_scores < zscore_threshold\n",
    "                    removed_z = (~mask_z).sum()\n",
    "                    X_train = X_train[mask_z]\n",
    "                    if y_train is not None:\n",
    "                        y_train = y_train.loc[X_train.index]\n",
    "                    self.feature_reasons[col] += f'Outliers handled with Z-Score Filtering (threshold={zscore_threshold}) | '\n",
    "                    self._log(f\"Removed {removed_z} outliers from '{col}' using Z-Score Filtering.\", step_name, 'debug')\n",
    "\n",
    "                # IQR Filtering\n",
    "                apply_iqr = outlier_options.get('apply_iqr', True)\n",
    "                if apply_iqr:\n",
    "                    Q1 = X_train[col].quantile(0.25)\n",
    "                    Q3 = X_train[col].quantile(0.75)\n",
    "                    IQR = Q3 - Q1\n",
    "                    lower_bound = Q1 - iqr_multiplier * IQR\n",
    "                    upper_bound = Q3 + iqr_multiplier * IQR\n",
    "                    mask_iqr = (X_train[col] >= lower_bound) & (X_train[col] <= upper_bound)\n",
    "                    removed_iqr = (~mask_iqr).sum()\n",
    "                    X_train = X_train[mask_iqr]\n",
    "                    if y_train is not None:\n",
    "                        y_train = y_train.loc[X_train.index]\n",
    "                    self.feature_reasons[col] += f'Outliers handled with IQR Filtering (multiplier={iqr_multiplier}) | '\n",
    "                    self._log(f\"Removed {removed_iqr} outliers from '{col}' using IQR Filtering.\", step_name, 'debug')\n",
    "\n",
    "        elif self.model_category == 'clustering':\n",
    "            self.logger.info(\"Applying multivariate IsolationForest for clustering.\")\n",
    "            # Apply IsolationForest on all numerical features simultaneously\n",
    "            contamination = isolation_contamination\n",
    "            iso_forest = IsolationForest(contamination=contamination, random_state=42)\n",
    "            preds = iso_forest.fit_predict(X_train[self.numericals])\n",
    "            mask_iso = preds != -1\n",
    "            removed_iso = (preds == -1).sum()\n",
    "            X_train = X_train[mask_iso]\n",
    "            if y_train is not None:\n",
    "                y_train = y_train.loc[X_train.index]  # Should typically be None in clustering\n",
    "            self.feature_reasons['all_numericals'] += f'Outliers handled with Multivariate IsolationForest (contamination={contamination}) | '\n",
    "            self._log(f\"Removed {removed_iso} outliers using Multivariate IsolationForest.\", step_name, 'debug')\n",
    "\n",
    "        else:\n",
    "            self.logger.warning(f\"Model category '{self.model_category}' not recognized for outlier handling.\")\n",
    "\n",
    "        self.preprocessing_steps.append(\"Handle Outliers\")\n",
    "\n",
    "        # Completion Logging\n",
    "        self._log(f\"Completed: Handle Outliers. Initial samples: {initial_shape}, Final samples: {X_train.shape[0]}\", step_name, 'debug')\n",
    "        self._log(f\"Missing values after outlier handling in X_train:\\n{X_train.isnull().sum()}\", step_name, 'debug')\n",
    "\n",
    "        return X_train, y_train\n",
    "\n",
    "    def test_normality(self, X_train: pd.DataFrame) -> Dict[str, Dict]:\n",
    "        \"\"\"\n",
    "        Test normality for numerical features based on normality tests and user options.\n",
    "\n",
    "        Args:\n",
    "            X_train (pd.DataFrame): Training features.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, Dict]: Dictionary with normality test results for each numerical feature.\n",
    "        \"\"\"\n",
    "        step_name = \"Test for Normality\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "        debug_flag = self.get_debug_flag('debug_test_normality')\n",
    "        normality_results = {}\n",
    "\n",
    "        # Fetch user-defined normality test options or set defaults\n",
    "        normality_options = self.options.get('test_normality', {})\n",
    "        p_value_threshold = normality_options.get('p_value_threshold', 0.05)\n",
    "        skewness_threshold = normality_options.get('skewness_threshold', 1.0)\n",
    "        additional_tests = normality_options.get('additional_tests', [])  # e.g., ['anderson-darling']\n",
    "\n",
    "        for col in self.numericals:\n",
    "            data = X_train[col].dropna()\n",
    "            skewness = data.skew()\n",
    "            kurtosis = data.kurtosis()\n",
    "\n",
    "            # Determine which normality test to use based on sample size and user options\n",
    "            test_used = 'Shapiro-Wilk'\n",
    "            p_value = 0.0\n",
    "\n",
    "            if len(data) <= 5000:\n",
    "                from scipy.stats import shapiro\n",
    "                stat, p_val = shapiro(data)\n",
    "                test_used = 'Shapiro-Wilk'\n",
    "                p_value = p_val\n",
    "            else:\n",
    "                from scipy.stats import anderson\n",
    "                result = anderson(data)\n",
    "                test_used = 'Anderson-Darling'\n",
    "                # Determine p-value based on critical values\n",
    "                p_value = 0.0  # Default to 0\n",
    "                for cv, sig in zip(result.critical_values, result.significance_level):\n",
    "                    if result.statistic < cv:\n",
    "                        p_value = sig / 100\n",
    "                        break\n",
    "\n",
    "            # Apply user-defined or default criteria\n",
    "            if self.model_category in ['regression', 'classification', 'clustering']:\n",
    "                # Linear, Logistic Regression, and Clustering: Use p-value and skewness\n",
    "                needs_transform = (p_value < p_value_threshold) or (abs(skewness) > skewness_threshold)\n",
    "            else:\n",
    "                # Other models: Use skewness, and optionally p-values based on options\n",
    "                use_p_value = normality_options.get('use_p_value_other_models', False)\n",
    "                if use_p_value:\n",
    "                    needs_transform = (p_value < p_value_threshold) or (abs(skewness) > skewness_threshold)\n",
    "                else:\n",
    "                    needs_transform = abs(skewness) > skewness_threshold\n",
    "\n",
    "            normality_results[col] = {\n",
    "                'skewness': skewness,\n",
    "                'kurtosis': kurtosis,\n",
    "                'p_value': p_value,\n",
    "                'test_used': test_used,\n",
    "                'needs_transform': needs_transform\n",
    "            }\n",
    "\n",
    "            # Conditional Detailed Logging\n",
    "            if debug_flag:\n",
    "                self._log(f\"Feature '{col}': p-value={p_value:.4f}, skewness={skewness:.4f}, needs_transform={needs_transform}\", step_name, 'debug')\n",
    "\n",
    "        self.normality_results = normality_results\n",
    "        self.preprocessing_steps.append(step_name)\n",
    "\n",
    "        # Completion Logging\n",
    "        if debug_flag:\n",
    "            self._log(f\"Completed: {step_name}. Normality results computed.\", step_name, 'debug')\n",
    "        else:\n",
    "            self.logger.info(f\"Step '{step_name}' completed: Normality results computed.\")\n",
    "\n",
    "        return normality_results\n",
    "\n",
    "    def encode_categorical_variables(self, X_train: pd.DataFrame, X_test: Optional[pd.DataFrame] = None) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n",
    "        \"\"\"\n",
    "        Encode categorical variables using user-specified encoding strategies.\n",
    "        \"\"\"\n",
    "        step_name = \"encode_categorical_variables\"\n",
    "        self.logger.info(\"Step: Encode Categorical Variables\")\n",
    "        self._log(\"Starting categorical variable encoding.\", step_name, 'debug')\n",
    "\n",
    "        # Fetch user-defined encoding options or set defaults\n",
    "        encoding_options = self.options.get('encode_categoricals', {})\n",
    "        ordinal_encoding = encoding_options.get('ordinal_encoding', 'OrdinalEncoder')  # Options: 'OrdinalEncoder', 'None'\n",
    "        nominal_encoding = encoding_options.get('nominal_encoding', 'OneHotEncoder')  # Changed from 'OneHotEncoder' to 'OrdinalEncoder'\n",
    "        handle_unknown = encoding_options.get('handle_unknown', 'use_encoded_value')  # Adjusted for OrdinalEncoder\n",
    "\n",
    "        # Determine if SMOTENC is being used\n",
    "        smote_variant = self.options.get('implement_smote', {}).get('variant', None)\n",
    "        if smote_variant == 'SMOTENC':\n",
    "            nominal_encoding = 'OrdinalEncoder'  # Ensure compatibility\n",
    "\n",
    "        transformers = []\n",
    "        new_columns = []\n",
    "        if self.ordinal_categoricals and ordinal_encoding != 'None':\n",
    "            if ordinal_encoding == 'OrdinalEncoder':\n",
    "                transformers.append(\n",
    "                    ('ordinal', OrdinalEncoder(), self.ordinal_categoricals)\n",
    "                )\n",
    "                self._log(f\"Added OrdinalEncoder for features: {self.ordinal_categoricals}\", step_name, 'debug')\n",
    "            else:\n",
    "                self.logger.error(f\"Ordinal encoding method '{ordinal_encoding}' is not supported.\")\n",
    "                raise ValueError(f\"Ordinal encoding method '{ordinal_encoding}' is not supported.\")\n",
    "        if self.nominal_categoricals and nominal_encoding != 'None':\n",
    "            if nominal_encoding == 'OrdinalEncoder':\n",
    "                transformers.append(\n",
    "                    ('nominal', OrdinalEncoder(handle_unknown=handle_unknown), self.nominal_categoricals)\n",
    "                )\n",
    "                self._log(f\"Added OrdinalEncoder for features: {self.nominal_categoricals}\", step_name, 'debug')\n",
    "            elif nominal_encoding == 'FrequencyEncoder':\n",
    "                # Custom Frequency Encoding\n",
    "                for col in self.nominal_categoricals:\n",
    "                    freq = X_train[col].value_counts(normalize=True)\n",
    "                    X_train[col] = X_train[col].map(freq)\n",
    "                    if X_test is not None:\n",
    "                        X_test[col] = X_test[col].map(freq).fillna(0)\n",
    "                    self.feature_reasons[col] += 'Encoded with Frequency Encoding | '\n",
    "                    self._log(f\"Applied Frequency Encoding to '{col}'.\", step_name, 'debug')\n",
    "            else:\n",
    "                self.logger.error(f\"Nominal encoding method '{nominal_encoding}' is not supported.\")\n",
    "                raise ValueError(f\"Nominal encoding method '{nominal_encoding}' is not supported.\")\n",
    "\n",
    "        if not transformers and 'FrequencyEncoder' not in nominal_encoding:\n",
    "            self.logger.info(\"No categorical variables to encode.\")\n",
    "            self.preprocessing_steps.append(\"Encode Categorical Variables\")\n",
    "            self._log(f\"Completed: Encode Categorical Variables. No encoding was applied.\", step_name, 'debug')\n",
    "            return X_train, X_test\n",
    "\n",
    "        if transformers:\n",
    "            self.preprocessor = ColumnTransformer(\n",
    "                transformers=transformers,\n",
    "                remainder='passthrough',\n",
    "                verbose_feature_names_out=False  # Disable prefixing\n",
    "            )\n",
    "\n",
    "            # Fit and transform training data\n",
    "            X_train_encoded = self.preprocessor.fit_transform(X_train)\n",
    "            self._log(\"Fitted and transformed X_train with ColumnTransformer.\", step_name, 'debug')\n",
    "\n",
    "            # Transform testing data\n",
    "            if X_test is not None:\n",
    "                X_test_encoded = self.preprocessor.transform(X_test)\n",
    "                self._log(\"Transformed X_test with fitted ColumnTransformer.\", step_name, 'debug')\n",
    "            else:\n",
    "                X_test_encoded = None\n",
    "\n",
    "            # Retrieve feature names after encoding\n",
    "            encoded_feature_names = []\n",
    "            if self.ordinal_categoricals and ordinal_encoding == 'OrdinalEncoder':\n",
    "                encoded_feature_names += self.ordinal_categoricals\n",
    "            if self.nominal_categoricals and nominal_encoding == 'OrdinalEncoder':\n",
    "                encoded_feature_names += self.nominal_categoricals\n",
    "            elif self.nominal_categoricals and nominal_encoding == 'FrequencyEncoder':\n",
    "                encoded_feature_names += self.nominal_categoricals\n",
    "            passthrough_features = [col for col in X_train.columns if col not in self.ordinal_categoricals + self.nominal_categoricals]\n",
    "            encoded_feature_names += passthrough_features\n",
    "            new_columns.extend(encoded_feature_names)\n",
    "\n",
    "            # Convert numpy arrays back to DataFrames\n",
    "            X_train_encoded_df = pd.DataFrame(X_train_encoded, columns=encoded_feature_names, index=X_train.index)\n",
    "            if X_test_encoded is not None:\n",
    "                X_test_encoded_df = pd.DataFrame(X_test_encoded, columns=encoded_feature_names, index=X_test.index)\n",
    "            else:\n",
    "                X_test_encoded_df = None\n",
    "\n",
    "            # Store encoders for inverse transformation\n",
    "            self.ordinal_encoder = self.preprocessor.named_transformers_.get('ordinal', None)\n",
    "            self.nominal_encoder = self.preprocessor.named_transformers_.get('nominal', None)\n",
    "\n",
    "            self.preprocessing_steps.append(\"Encode Categorical Variables\")\n",
    "            self._log(f\"Completed: Encode Categorical Variables. X_train_encoded shape: {X_train_encoded_df.shape}\", step_name, 'debug')\n",
    "            self._log(f\"Columns after encoding: {encoded_feature_names}\", step_name, 'debug')\n",
    "            self._log(f\"Sample of encoded X_train:\\n{X_train_encoded_df.head()}\", step_name, 'debug')\n",
    "            self._log(f\"New columns added: {new_columns}\", step_name, 'debug')\n",
    "\n",
    "            return X_train_encoded_df, X_test_encoded_df\n",
    "\n",
    "    def generate_recommendations(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Generate a table of preprocessing recommendations based on the model type, data, and user options.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame containing recommendations for each feature.\n",
    "        \"\"\"\n",
    "        step_name = \"Generate Preprocessor Recommendations\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "        debug_flag = self.get_debug_flag('debug_generate_recommendations')\n",
    "\n",
    "        # Generate recommendations based on feature reasons\n",
    "        recommendations = {}\n",
    "        for col in self.ordinal_categoricals + self.nominal_categoricals + self.numericals:\n",
    "            reasons = self.feature_reasons.get(col, '').strip(' | ')\n",
    "            recommendations[col] = reasons\n",
    "\n",
    "        recommendations_table = pd.DataFrame.from_dict(\n",
    "            recommendations, \n",
    "            orient='index', \n",
    "            columns=['Preprocessing Reason']\n",
    "        )\n",
    "        if debug_flag:\n",
    "            self.logger.debug(f\"Preprocessing Recommendations:\\n{recommendations_table}\")\n",
    "        else:\n",
    "            self.logger.info(\"Preprocessing Recommendations generated.\")\n",
    "\n",
    "        self.preprocessing_steps.append(step_name)\n",
    "\n",
    "        # Completion Logging\n",
    "        if debug_flag:\n",
    "            self._log(f\"Completed: {step_name}. Recommendations generated.\", step_name, 'debug')\n",
    "        else:\n",
    "            self.logger.info(f\"Step '{step_name}' completed: Recommendations generated.\")\n",
    "\n",
    "        return recommendations_table\n",
    "\n",
    "    def save_transformers(self):\n",
    "        step_name = \"Save Transformers\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "        debug_flag = self.get_debug_flag('debug_save_transformers')\n",
    "        \n",
    "        # Ensure the transformers directory exists\n",
    "        os.makedirs(self.transformers_dir, exist_ok=True)\n",
    "        transformers_path = os.path.join(self.transformers_dir, 'transformers.pkl')  # Consistent file path\n",
    "        \n",
    "        transformers = {\n",
    "            'numerical_imputer': getattr(self, 'numerical_imputer', None),\n",
    "            'categorical_imputer': getattr(self, 'categorical_imputer', None),\n",
    "            'preprocessor': self.pipeline,   # Includes all preprocessing steps\n",
    "            'smote': self.smote,\n",
    "            'final_feature_order': self.final_feature_order,\n",
    "            'categorical_indices': self.categorical_indices\n",
    "        }\n",
    "        try:\n",
    "            joblib.dump(transformers, transformers_path)\n",
    "            if debug_flag:\n",
    "                self._log(f\"Transformers saved at '{transformers_path}'.\", step_name, 'debug')\n",
    "            else:\n",
    "                self.logger.info(f\"Transformers saved at '{transformers_path}'.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Failed to save transformers: {e}\")\n",
    "            raise\n",
    "\n",
    "        self.preprocessing_steps.append(step_name)\n",
    "\n",
    "    def load_transformers(self) -> dict:\n",
    "        step_name = \"Load Transformers\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "        debug_flag = self.get_debug_flag('debug_load_transformers')  # Assuming a step-specific debug flag\n",
    "        transformers_path = os.path.join(self.transformers_dir, 'transformers.pkl')  # Correct path\n",
    "\n",
    "        # Debug log\n",
    "        self.logger.debug(f\"Loading transformers from: {transformers_path}\")\n",
    "\n",
    "        if not os.path.exists(transformers_path):\n",
    "            self.logger.error(f\"❌ Transformers file not found at '{transformers_path}'. Cannot proceed with prediction.\")\n",
    "            raise FileNotFoundError(f\"Transformers file not found at '{transformers_path}'.\")\n",
    "\n",
    "        try:\n",
    "            transformers = joblib.load(transformers_path)\n",
    "\n",
    "            # Extract transformers\n",
    "            numerical_imputer = transformers.get('numerical_imputer')\n",
    "            categorical_imputer = transformers.get('categorical_imputer')\n",
    "            preprocessor = transformers.get('preprocessor')\n",
    "            smote = transformers.get('smote', None)\n",
    "            final_feature_order = transformers.get('final_feature_order', [])\n",
    "            categorical_indices = transformers.get('categorical_indices', [])\n",
    "            self.categorical_indices = categorical_indices  # Set the attribute\n",
    "\n",
    "            # **Post-Loading Debugging:**\n",
    "            if preprocessor is not None:\n",
    "                try:\n",
    "                    # Do not attempt to transform dummy data here\n",
    "                    self.logger.debug(f\"Pipeline loaded. Ready to transform new data.\")\n",
    "                except AttributeError as e:\n",
    "                    self.logger.error(f\"Pipeline's get_feature_names_out is not available: {e}\")\n",
    "                    expected_features = []\n",
    "            else:\n",
    "                self.logger.error(\"❌ Preprocessor is not loaded.\")\n",
    "                raise AttributeError(\"Preprocessor is not loaded.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Failed to load transformers: {e}\")\n",
    "            raise\n",
    "\n",
    "        self.preprocessing_steps.append(step_name)\n",
    "\n",
    "        # Additional checks\n",
    "        if preprocessor is None:\n",
    "            self.logger.error(\"❌ Preprocessor is not loaded.\")\n",
    "\n",
    "        if debug_flag:\n",
    "            self._log(f\"Transformers loaded successfully from '{transformers_path}'.\", step_name, 'debug')\n",
    "        else:\n",
    "            self.logger.info(f\"Transformers loaded successfully from '{transformers_path}'.\")\n",
    "\n",
    "        # Set the pipeline\n",
    "        self.pipeline = preprocessor\n",
    "\n",
    "        # Return the transformers as a dictionary\n",
    "        return {\n",
    "            'numerical_imputer': numerical_imputer,\n",
    "            'categorical_imputer': categorical_imputer,\n",
    "            'preprocessor': preprocessor,\n",
    "            'smote': smote,\n",
    "            'final_feature_order': final_feature_order,\n",
    "            'categorical_indices': categorical_indices\n",
    "        }\n",
    "\n",
    "    def apply_scaling(self, X_train: pd.DataFrame, X_test: Optional[pd.DataFrame] = None) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n",
    "        \"\"\"\n",
    "        Apply scaling based on the model type and user options.\n",
    "\n",
    "        Args:\n",
    "            X_train (pd.DataFrame): Training features.\n",
    "            X_test (Optional[pd.DataFrame]): Testing features.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, Optional[pd.DataFrame]]: Scaled X_train and X_test.\n",
    "        \"\"\"\n",
    "        step_name = \"Apply Scaling (If Needed by Model)\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "        debug_flag = self.get_debug_flag('debug_apply_scaling')\n",
    "\n",
    "        # Fetch user-defined scaling options or set defaults\n",
    "        scaling_options = self.options.get('apply_scaling', {})\n",
    "        scaling_method = scaling_options.get('method', None)  # 'StandardScaler', 'MinMaxScaler', 'RobustScaler', 'None'\n",
    "        features_to_scale = scaling_options.get('features', self.numericals)\n",
    "\n",
    "        scaler = None\n",
    "        scaling_type = 'None'\n",
    "\n",
    "        if scaling_method is None:\n",
    "            # Default scaling based on model category\n",
    "            if self.model_category in ['regression', 'classification', 'clustering']:\n",
    "                # For clustering, MinMaxScaler is generally preferred\n",
    "                if self.model_category == 'clustering':\n",
    "                    scaler = MinMaxScaler()\n",
    "                    scaling_type = 'MinMaxScaler'\n",
    "                else:\n",
    "                    scaler = StandardScaler()\n",
    "                    scaling_type = 'StandardScaler'\n",
    "            else:\n",
    "                scaler = None\n",
    "                scaling_type = 'None'\n",
    "        else:\n",
    "            # Normalize the scaling_method string to handle case-insensitivity\n",
    "            scaling_method_normalized = scaling_method.lower()\n",
    "            if scaling_method_normalized == 'standardscaler':\n",
    "                scaler = StandardScaler()\n",
    "                scaling_type = 'StandardScaler'\n",
    "            elif scaling_method_normalized == 'minmaxscaler':\n",
    "                scaler = MinMaxScaler()\n",
    "                scaling_type = 'MinMaxScaler'\n",
    "            elif scaling_method_normalized == 'robustscaler':\n",
    "                scaler = RobustScaler()\n",
    "                scaling_type = 'RobustScaler'\n",
    "            elif scaling_method_normalized == 'none':\n",
    "                scaler = None\n",
    "                scaling_type = 'None'\n",
    "            else:\n",
    "                self.logger.error(f\"Scaling method '{scaling_method}' is not supported.\")\n",
    "                raise ValueError(f\"Scaling method '{scaling_method}' is not supported.\")\n",
    "\n",
    "        # Apply scaling if scaler is defined\n",
    "        if scaler is not None and features_to_scale:\n",
    "            self.scaler = scaler\n",
    "            if debug_flag:\n",
    "                self._log(f\"Features to scale: {features_to_scale}\", step_name, 'debug')\n",
    "\n",
    "            # Check if features exist in the dataset\n",
    "            missing_features = [feat for feat in features_to_scale if feat not in X_train.columns]\n",
    "            if missing_features:\n",
    "                self.logger.error(f\"The following features specified for scaling are missing in the dataset: {missing_features}\")\n",
    "                raise KeyError(f\"The following features specified for scaling are missing in the dataset: {missing_features}\")\n",
    "\n",
    "            X_train[features_to_scale] = scaler.fit_transform(X_train[features_to_scale])\n",
    "            if X_test is not None:\n",
    "                X_test[features_to_scale] = scaler.transform(X_test[features_to_scale])\n",
    "\n",
    "            for col in features_to_scale:\n",
    "                self.feature_reasons[col] += f'Scaling Applied: {scaling_type} | '\n",
    "\n",
    "            self.preprocessing_steps.append(step_name)\n",
    "            if debug_flag:\n",
    "                self._log(f\"Applied {scaling_type} to features: {features_to_scale}\", step_name, 'debug')\n",
    "                if hasattr(scaler, 'mean_'):\n",
    "                    self._log(f\"Scaler Parameters: mean={scaler.mean_}\", step_name, 'debug')\n",
    "                if hasattr(scaler, 'scale_'):\n",
    "                    self._log(f\"Scaler Parameters: scale={scaler.scale_}\", step_name, 'debug')\n",
    "                self._log(f\"Sample of scaled X_train:\\n{X_train[features_to_scale].head()}\", step_name, 'debug')\n",
    "                if X_test is not None:\n",
    "                    self._log(f\"Sample of scaled X_test:\\n{X_test[features_to_scale].head()}\", step_name, 'debug')\n",
    "            else:\n",
    "                self.logger.info(f\"Step '{step_name}' completed: Applied {scaling_type} to features: {features_to_scale}\")\n",
    "        else:\n",
    "            self.logger.info(\"No scaling applied based on user options or no features specified.\")\n",
    "            self.preprocessing_steps.append(step_name)\n",
    "            if debug_flag:\n",
    "                self._log(f\"Completed: {step_name}. No scaling was applied.\", step_name, 'debug')\n",
    "            else:\n",
    "                self.logger.info(f\"Step '{step_name}' completed: No scaling was applied.\")\n",
    "\n",
    "        return X_train, X_test\n",
    "\n",
    "    def determine_n_neighbors(self, minority_count: int, default_neighbors: int = 5) -> int:\n",
    "        \"\"\"\n",
    "        Determine the appropriate number of neighbors for SMOTE based on minority class size.\n",
    "\n",
    "        Args:\n",
    "            minority_count (int): Number of samples in the minority class.\n",
    "            default_neighbors (int): Default number of neighbors to use if possible.\n",
    "\n",
    "        Returns:\n",
    "            int: Determined number of neighbors for SMOTE.\n",
    "        \"\"\"\n",
    "        if minority_count <= 1:\n",
    "            raise ValueError(\"SMOTE cannot be applied when the minority class has less than 2 samples.\")\n",
    "        \n",
    "        # Ensure n_neighbors does not exceed minority_count - 1\n",
    "        n_neighbors = min(default_neighbors, minority_count - 1)\n",
    "        return n_neighbors\n",
    "\n",
    "    def implement_smote(self, X_train: pd.DataFrame, y_train: pd.Series) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "        \"\"\"\n",
    "        Implement SMOTE or its variants based on class imbalance with automated n_neighbors selection.\n",
    "\n",
    "        Args:\n",
    "            X_train (pd.DataFrame): Training features (transformed).\n",
    "            y_train (pd.Series): Training target.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, pd.Series]: Resampled X_train and y_train.\n",
    "        \"\"\"\n",
    "        step_name = \"Implement SMOTE (Train Only)\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "\n",
    "        # Check if classification\n",
    "        if self.model_category != 'classification':\n",
    "            self.logger.info(\"SMOTE not applicable: Not a classification model.\")\n",
    "            self.preprocessing_steps.append(\"SMOTE Skipped\")\n",
    "            return X_train, y_train\n",
    "\n",
    "        # Calculate class distribution\n",
    "        class_counts = y_train.value_counts()\n",
    "        if len(class_counts) < 2:\n",
    "            self.logger.warning(\"SMOTE not applicable: Only one class present.\")\n",
    "            self.preprocessing_steps.append(\"SMOTE Skipped\")\n",
    "            return X_train, y_train\n",
    "\n",
    "        majority_class = class_counts.idxmax()\n",
    "        minority_class = class_counts.idxmin()\n",
    "        majority_count = class_counts.max()\n",
    "        minority_count = class_counts.min()\n",
    "        imbalance_ratio = minority_count / majority_count\n",
    "        self.logger.info(f\"Class Distribution before SMOTE: {class_counts.to_dict()}\")\n",
    "        self.logger.info(f\"Imbalance Ratio (Minority/Majority): {imbalance_ratio:.4f}\")\n",
    "\n",
    "        # Determine SMOTE variant based on dataset composition\n",
    "        has_numericals = len(self.numericals) > 0\n",
    "        has_categoricals = len(self.ordinal_categoricals) + len(self.nominal_categoricals) > 0\n",
    "\n",
    "        # Automatically select SMOTE variant\n",
    "        if has_numericals and has_categoricals:\n",
    "            smote_variant = 'SMOTENC'\n",
    "            self.logger.info(\"Dataset contains both numerical and categorical features. Using SMOTENC.\")\n",
    "        elif has_numericals and not has_categoricals:\n",
    "            smote_variant = 'SMOTE'\n",
    "            self.logger.info(\"Dataset contains only numerical features. Using SMOTE.\")\n",
    "        elif has_categoricals and not has_numericals:\n",
    "            smote_variant = 'SMOTEN'\n",
    "            self.logger.info(\"Dataset contains only categorical features. Using SMOTEN.\")\n",
    "        else:\n",
    "            smote_variant = 'SMOTE'  # Fallback\n",
    "            self.logger.info(\"Feature composition unclear. Using SMOTE as default.\")\n",
    "\n",
    "        # Initialize SMOTE based on the variant\n",
    "        try:\n",
    "            if smote_variant == 'SMOTENC':\n",
    "                if not self.categorical_indices:\n",
    "                    categorical_features = []\n",
    "                    offset = 0\n",
    "                    # Use running offset to calculate correct indices in the concatenated output\n",
    "                    for name, transformer, features in self.pipeline.transformers_:\n",
    "                        # Determine number of output columns from this transformer\n",
    "                        if hasattr(transformer, 'get_feature_names_out'):\n",
    "                            out_features = transformer.get_feature_names_out(features)\n",
    "                            n_cols = len(out_features)\n",
    "                        else:\n",
    "                            n_cols = len(features)  # fallback\n",
    "                        if name in ['ord', 'nominal', 'nominal_ord']:\n",
    "                            categorical_features.extend(list(range(offset, offset + n_cols)))\n",
    "                        offset += n_cols\n",
    "                    self.categorical_indices = categorical_features\n",
    "                    self.logger.debug(f\"Categorical feature indices for SMOTENC: {self.categorical_indices}\")\n",
    "                n_neighbors = self.determine_n_neighbors(minority_count, default_neighbors=5)\n",
    "                smote = SMOTENC(categorical_features=self.categorical_indices, random_state=42, k_neighbors=n_neighbors)\n",
    "                self.logger.debug(f\"Initialized SMOTENC with categorical features indices: {self.categorical_indices} and n_neighbors={n_neighbors}\")\n",
    "            elif smote_variant == 'SMOTEN':\n",
    "                n_neighbors = self.determine_n_neighbors(minority_count, default_neighbors=5)\n",
    "                smote = SMOTEN(random_state=42, n_neighbors=n_neighbors)\n",
    "                self.logger.debug(f\"Initialized SMOTEN with n_neighbors={n_neighbors}\")\n",
    "            else:\n",
    "                n_neighbors = self.determine_n_neighbors(minority_count, default_neighbors=5)\n",
    "                smote = SMOTE(random_state=42, k_neighbors=n_neighbors)\n",
    "                self.logger.debug(f\"Initialized SMOTE with n_neighbors={n_neighbors}\")\n",
    "        except ValueError as ve:\n",
    "            self.logger.error(f\"❌ SMOTE initialization failed: {ve}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Unexpected error during SMOTE initialization: {e}\")\n",
    "            raise\n",
    "\n",
    "        # Apply SMOTE\n",
    "        try:\n",
    "            X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "            self.logger.info(f\"Applied {smote_variant}. Resampled dataset shape: {X_resampled.shape}\")\n",
    "            self.preprocessing_steps.append(\"Implement SMOTE\")\n",
    "            self.smote = smote  # Save SMOTE instance for later use\n",
    "            self.logger.debug(f\"Selected n_neighbors for SMOTE: {n_neighbors}\")\n",
    "            return X_resampled, y_resampled\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ SMOTE application failed: {e}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "    def inverse_transform_data(self, X_transformed: np.ndarray, original_data: Optional[pd.DataFrame] = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Perform inverse transformation on the transformed data to reconstruct original feature values.\n",
    "\n",
    "        Args:\n",
    "            X_transformed (np.ndarray): The transformed feature data.\n",
    "            original_data (Optional[pd.DataFrame]): The original data before transformation.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: The inverse-transformed DataFrame including passthrough columns.\n",
    "        \"\"\"\n",
    "        if self.pipeline is None:\n",
    "            self.logger.error(\"Preprocessing pipeline has not been fitted. Cannot perform inverse transformation.\")\n",
    "            raise AttributeError(\"Preprocessing pipeline has not been fitted. Cannot perform inverse transformation.\")\n",
    "\n",
    "        preprocessor = self.pipeline\n",
    "        logger = logging.getLogger('InverseTransform')\n",
    "        if self.debug or self.get_debug_flag('debug_final_inverse_transformations'):\n",
    "            logger.setLevel(logging.DEBUG)\n",
    "        else:\n",
    "            logger.setLevel(logging.INFO)\n",
    "\n",
    "        logger.debug(f\"[DEBUG Inverse] Starting inverse transformation. Input shape: {X_transformed.shape}\")\n",
    "\n",
    "        # Initialize variables\n",
    "        inverse_data = {}\n",
    "        transformations_applied = False  # Flag to check if any transformations are applied\n",
    "        start_idx = 0  # Starting index for slicing\n",
    "\n",
    "        # Iterate over each transformer in the ColumnTransformer\n",
    "        for name, transformer, features in preprocessor.transformers_:\n",
    "            if name == 'remainder':\n",
    "                logger.debug(f\"[DEBUG Inverse] Skipping 'remainder' transformer (passthrough columns).\")\n",
    "                continue  # Skip passthrough columns\n",
    "\n",
    "            end_idx = start_idx + len(features)\n",
    "            logger.debug(f\"[DEBUG Inverse] Transformer '{name}' handling features {features} with slice {start_idx}:{end_idx}\")\n",
    "\n",
    "            # Check if the transformer has an inverse_transform method\n",
    "            if hasattr(transformer, 'named_steps'):\n",
    "                # Access the last step in the pipeline (e.g., scaler or encoder)\n",
    "                last_step = list(transformer.named_steps.keys())[-1]\n",
    "                inverse_transformer = transformer.named_steps[last_step]\n",
    "\n",
    "                if hasattr(inverse_transformer, 'inverse_transform'):\n",
    "                    transformed_slice = X_transformed[:, start_idx:end_idx]\n",
    "                    inverse_slice = inverse_transformer.inverse_transform(transformed_slice)\n",
    "\n",
    "                    # Assign inverse-transformed data to the corresponding feature names\n",
    "                    for idx, feature in enumerate(features):\n",
    "                        inverse_data[feature] = inverse_slice[:, idx]\n",
    "\n",
    "                    logger.debug(f\"[DEBUG Inverse] Applied inverse_transform on transformer '{last_step}' for features {features}.\")\n",
    "                    transformations_applied = True\n",
    "                else:\n",
    "                    logger.debug(f\"[DEBUG Inverse] Transformer '{last_step}' does not support inverse_transform. Skipping.\")\n",
    "            else:\n",
    "                logger.debug(f\"[DEBUG Inverse] Transformer '{name}' does not have 'named_steps'. Skipping.\")\n",
    "\n",
    "            start_idx = end_idx  # Update starting index for next transformer\n",
    "\n",
    "        # Convert the inverse_data dictionary to a DataFrame\n",
    "        if transformations_applied:\n",
    "            inverse_df = pd.DataFrame(inverse_data, index=original_data.index if original_data is not None else None)\n",
    "            logger.debug(f\"[DEBUG Inverse] Inverse DataFrame shape (transformed columns): {inverse_df.shape}\")\n",
    "            logger.debug(f\"[DEBUG Inverse] Sample of inverse-transformed data:\\n{inverse_df.head()}\")\n",
    "        else:\n",
    "            if original_data is not None:\n",
    "                logger.warning(\"⚠️ No reversible transformations were applied. Returning original data.\")\n",
    "                inverse_df = original_data.copy()\n",
    "                logger.debug(f\"[DEBUG Inverse] Returning a copy of original_data with shape: {inverse_df.shape}\")\n",
    "            else:\n",
    "                logger.error(\"❌ No transformations were applied and original_data was not provided. Cannot perform inverse transformation.\")\n",
    "                raise ValueError(\"No transformations were applied and original_data was not provided.\")\n",
    "\n",
    "        # Identify passthrough columns by excluding transformed features\n",
    "        if original_data is not None and transformations_applied:\n",
    "            transformed_features = set(inverse_data.keys())\n",
    "            all_original_features = set(original_data.columns)\n",
    "            passthrough_columns = list(all_original_features - transformed_features)\n",
    "            logger.debug(f\"[DEBUG Inverse] Inverse DataFrame columns before pass-through merge: {inverse_df.columns.tolist()}\")\n",
    "            logger.debug(f\"[DEBUG Inverse] all_original_features: {list(all_original_features)}\")\n",
    "            logger.debug(f\"[DEBUG Inverse] passthrough_columns: {passthrough_columns}\")\n",
    "\n",
    "            if passthrough_columns:\n",
    "                logger.debug(f\"[DEBUG Inverse] Passthrough columns to merge: {passthrough_columns}\")\n",
    "                passthrough_data = original_data[passthrough_columns].copy()\n",
    "                inverse_df = pd.concat([inverse_df, passthrough_data], axis=1)\n",
    "\n",
    "                # Ensure the final DataFrame has the same column order as original_data\n",
    "                inverse_df = inverse_df[original_data.columns]\n",
    "                logger.debug(f\"[DEBUG Inverse] Final inverse DataFrame shape: {inverse_df.shape}\")\n",
    "                \n",
    "                # Check for missing columns after inverse transform\n",
    "                expected_columns = set(original_data.columns)\n",
    "                final_columns = set(inverse_df.columns)\n",
    "                missing_after_inverse = expected_columns - final_columns\n",
    "\n",
    "                if missing_after_inverse:\n",
    "                    err_msg = (\n",
    "                    f\"Inverse transform error: The following columns are missing \"\n",
    "                    f\"after inverse transform: {missing_after_inverse}\"\n",
    "                    )\n",
    "                    logger.error(err_msg)\n",
    "                    raise ValueError(err_msg)\n",
    "            else:\n",
    "                logger.debug(\"[DEBUG Inverse] No passthrough columns to merge.\")\n",
    "        else:\n",
    "            logger.debug(\"[DEBUG Inverse] Either no original_data provided or no transformations were applied.\")\n",
    "\n",
    "        return inverse_df\n",
    "\n",
    "\n",
    "\n",
    "    def build_pipeline(self, X_train: pd.DataFrame) -> ColumnTransformer:\n",
    "        transformers = []\n",
    "\n",
    "        # Handle Numerical Features\n",
    "        if self.numericals:\n",
    "            numerical_strategy = self.options.get('handle_missing_values', {}).get('numerical_strategy', {}).get('strategy', 'median')\n",
    "            numerical_imputer = self.options.get('handle_missing_values', {}).get('numerical_strategy', {}).get('imputer', 'SimpleImputer')\n",
    "\n",
    "            if numerical_imputer == 'SimpleImputer':\n",
    "                num_imputer = SimpleImputer(strategy=numerical_strategy)\n",
    "            elif numerical_imputer == 'KNNImputer':\n",
    "                knn_neighbors = self.options.get('handle_missing_values', {}).get('numerical_strategy', {}).get('knn_neighbors', 5)\n",
    "                num_imputer = KNNImputer(n_neighbors=knn_neighbors)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported numerical imputer type: {numerical_imputer}\")\n",
    "\n",
    "            # Determine scaling method\n",
    "            scaling_method = self.options.get('apply_scaling', {}).get('method', None)\n",
    "            if scaling_method is None:\n",
    "                # Default scaling based on model category\n",
    "                if self.model_category in ['regression', 'classification', 'clustering']:\n",
    "                    # For clustering, MinMaxScaler is generally preferred\n",
    "                    if self.model_category == 'clustering':\n",
    "                        scaler = MinMaxScaler()\n",
    "                        scaling_type = 'MinMaxScaler'\n",
    "                    else:\n",
    "                        scaler = StandardScaler()\n",
    "                        scaling_type = 'StandardScaler'\n",
    "                else:\n",
    "                    scaler = 'passthrough'\n",
    "                    scaling_type = 'None'\n",
    "            else:\n",
    "                # Normalize the scaling_method string to handle case-insensitivity\n",
    "                scaling_method_normalized = scaling_method.lower()\n",
    "                if scaling_method_normalized == 'standardscaler':\n",
    "                    scaler = StandardScaler()\n",
    "                    scaling_type = 'StandardScaler'\n",
    "                elif scaling_method_normalized == 'minmaxscaler':\n",
    "                    scaler = MinMaxScaler()\n",
    "                    scaling_type = 'MinMaxScaler'\n",
    "                elif scaling_method_normalized == 'robustscaler':\n",
    "                    scaler = RobustScaler()\n",
    "                    scaling_type = 'RobustScaler'\n",
    "                elif scaling_method_normalized == 'none':\n",
    "                    scaler = 'passthrough'\n",
    "                    scaling_type = 'None'\n",
    "                else:\n",
    "                    raise ValueError(f\"Unsupported scaling method: {scaling_method}\")\n",
    "\n",
    "            numerical_transformer = Pipeline(steps=[\n",
    "                ('imputer', num_imputer),\n",
    "                ('scaler', scaler)\n",
    "            ])\n",
    "\n",
    "            transformers.append(('num', numerical_transformer, self.numericals))\n",
    "            self.logger.debug(f\"Numerical transformer added with imputer '{numerical_imputer}' and scaler '{scaling_type}'.\")\n",
    "\n",
    "        # Handle Ordinal Categorical Features\n",
    "        if self.ordinal_categoricals:\n",
    "            ordinal_strategy = self.options.get('encode_categoricals', {}).get('ordinal_encoding', 'OrdinalEncoder')\n",
    "            if ordinal_strategy == 'OrdinalEncoder':\n",
    "                ordinal_transformer = Pipeline(steps=[\n",
    "                    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                    ('ordinal_encoder', OrdinalEncoder())\n",
    "                ])\n",
    "                transformers.append(('ord', ordinal_transformer, self.ordinal_categoricals))\n",
    "                self.logger.debug(\"Ordinal transformer added with OrdinalEncoder.\")\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported ordinal encoding strategy: {ordinal_strategy}\")\n",
    "\n",
    "        # Handle Nominal Categorical Features\n",
    "        if self.nominal_categoricals:\n",
    "            nominal_strategy = self.options.get('encode_categoricals', {}).get('nominal_encoding', 'OneHotEncoder')\n",
    "            if nominal_strategy == 'OneHotEncoder':\n",
    "                nominal_transformer = Pipeline(steps=[\n",
    "                    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                    ('onehot_encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "                ])\n",
    "                transformers.append(('nominal', nominal_transformer, self.nominal_categoricals))\n",
    "                self.logger.debug(\"Nominal transformer added with OneHotEncoder.\")\n",
    "            elif nominal_strategy == 'OrdinalEncoder':\n",
    "                nominal_transformer = Pipeline(steps=[\n",
    "                    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                    ('ordinal_encoder', OrdinalEncoder())\n",
    "                ])\n",
    "                transformers.append(('nominal_ord', nominal_transformer, self.nominal_categoricals))\n",
    "                self.logger.debug(\"Nominal transformer added with OrdinalEncoder.\")\n",
    "            elif nominal_strategy == 'FrequencyEncoder':\n",
    "                # Implement custom Frequency Encoding\n",
    "                for feature in self.nominal_categoricals:\n",
    "                    freq = X_train[feature].value_counts(normalize=True)\n",
    "                    X_train[feature] = X_train[feature].map(freq)\n",
    "                    self.feature_reasons[feature] += 'Frequency Encoding applied | '\n",
    "                    self.logger.debug(f\"Frequency Encoding applied to '{feature}'.\")\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported nominal encoding strategy: {nominal_strategy}\")\n",
    "\n",
    "        if not transformers and 'FrequencyEncoder' not in nominal_strategy:\n",
    "            self.logger.error(\"No transformers added to the pipeline. Check feature categorization and configuration.\")\n",
    "            raise ValueError(\"No transformers added to the pipeline. Check feature categorization and configuration.\")\n",
    "\n",
    "        preprocessor = ColumnTransformer(transformers=transformers, remainder='passthrough')\n",
    "        self.logger.debug(\"ColumnTransformer constructed with the following transformers:\")\n",
    "        for t in transformers:\n",
    "            self.logger.debug(t)\n",
    "\n",
    "        preprocessor.fit(X_train)\n",
    "        self.logger.info(\"✅ Preprocessor fitted on training data.\")\n",
    "\n",
    "        if self.options.get('implement_smote', {}).get('variant', None) == 'SMOTENC':\n",
    "            if not self.categorical_indices:\n",
    "                categorical_features = []\n",
    "                offset = 0\n",
    "                for name, transformer, features in self.pipeline.transformers_:\n",
    "                    # Get the number of output columns for this transformer.\n",
    "                    # If the transformer is a Pipeline and has get_feature_names_out, use it.\n",
    "                    if hasattr(transformer, 'get_feature_names_out'):\n",
    "                        out_features = transformer.get_feature_names_out(features)\n",
    "                        n_cols = len(out_features)\n",
    "                    else:\n",
    "                        n_cols = len(features)  # Fallback\n",
    "                    if name in ['ord', 'nominal', 'nominal_ord']:\n",
    "                        # The categorical indices for this block are offset to offset+n_cols\n",
    "                        categorical_features.extend(list(range(offset, offset + n_cols)))\n",
    "                    offset += n_cols\n",
    "                self.categorical_indices = categorical_features\n",
    "                self.logger.debug(f\"Categorical feature indices for SMOTENC: {self.categorical_indices}\")\n",
    "\n",
    "\n",
    "        return preprocessor\n",
    "\n",
    "\n",
    "    def preprocess_train(self, X: pd.DataFrame, y: pd.Series) -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series, pd.DataFrame, Optional[pd.DataFrame]]:\n",
    "        # Step 1: Split Dataset\n",
    "        X_train_original, X_test_original, y_train_original, y_test = self.split_dataset(X, y)\n",
    "\n",
    "        # Step 2: Handle Missing Values\n",
    "        X_train_missing_values, X_test_missing_values = self.handle_missing_values(X_train_original, X_test_original)\n",
    "\n",
    "        # Step 3: Test for Normality\n",
    "        if self.model_category in ['regression', 'classification', 'clustering']:\n",
    "            self.test_normality(X_train_missing_values)\n",
    "\n",
    "        # Step 4: Handle Outliers\n",
    "        X_train_outliers_handled, y_train_outliers_handled = self.handle_outliers(X_train_missing_values, y_train_original)\n",
    "\n",
    "        # Retain a copy of X_test without outliers for reference\n",
    "        X_test_outliers_handled = X_test_missing_values.copy() if X_test_missing_values is not None else None\n",
    "\n",
    "        # Step 5: Generate Preprocessing Recommendations\n",
    "        recommendations = self.generate_recommendations()\n",
    "\n",
    "        # Step 6: Build and Fit the Pipeline\n",
    "        self.pipeline = self.build_pipeline(X_train_outliers_handled)\n",
    "\n",
    "        # Fit and transform training data using the pipeline\n",
    "        X_train_preprocessed = self.pipeline.fit_transform(X_train_outliers_handled)\n",
    "\n",
    "        # Transform test data\n",
    "        X_test_preprocessed = self.pipeline.transform(X_test_outliers_handled) if X_test_outliers_handled is not None else None\n",
    "\n",
    "        # Step 7: Implement SMOTE Variant (Train Only for Classification)\n",
    "        if self.model_category == 'classification':\n",
    "            try:\n",
    "                X_train_smoted, y_train_smoted = self.implement_smote(X_train_preprocessed, y_train_outliers_handled)\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"❌ SMOTE application failed: {e}\")\n",
    "                raise  # Reraise exception to prevent saving incomplete transformers\n",
    "        else:\n",
    "            X_train_smoted, y_train_smoted = X_train_preprocessed, y_train_outliers_handled\n",
    "            self.logger.info(\"⚠️ SMOTE not applied: Not a classification model.\")\n",
    "\n",
    "        # Step 8: Save Transformers (Including the Pipeline)\n",
    "        self.final_feature_order = list(self.pipeline.get_feature_names_out())\n",
    "        X_train_final = pd.DataFrame(X_train_smoted, columns=self.final_feature_order)\n",
    "        X_test_final = pd.DataFrame(X_test_preprocessed, columns=self.final_feature_order, index=X_test_original.index) if X_test_preprocessed is not None else None\n",
    "\n",
    "        try:\n",
    "            self.save_transformers()\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Saving transformers failed: {e}\")\n",
    "            raise  # Prevent further steps if saving fails\n",
    "\n",
    "        # Inverse transformations (optional, for interpretability)\n",
    "        try:\n",
    "            # Use the final test dataset (fully transformed) for inverse transformations\n",
    "            if X_test_final is not None:\n",
    "                X_test_inverse = self.inverse_transform_data(X_test_final.values, original_data=X_test_original)\n",
    "                self.logger.info(\"✅ Inverse transformations applied successfully.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Inverse transformations failed: {e}\")\n",
    "            X_test_inverse = None\n",
    "\n",
    "        # Return processed datasets\n",
    "        return X_train_final, X_test_final, y_train_smoted, y_test, recommendations, X_test_inverse\n",
    "\n",
    "    def transform(self, X: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Transform new data using the fitted preprocessing pipeline.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): New data to transform.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Preprocessed data.\n",
    "        \"\"\"\n",
    "        if self.pipeline is None:\n",
    "            self.logger.error(\"Preprocessing pipeline has not been fitted. Cannot perform inverse transformation.\")\n",
    "            raise AttributeError(\"Preprocessing pipeline has not been fitted. Cannot perform inverse transformation.\")\n",
    "        self.logger.debug(\"Transforming new data.\")\n",
    "        X_preprocessed = self.pipeline.transform(X)\n",
    "        if self.debug:\n",
    "            self.logger.debug(f\"Transformed data shape: {X_preprocessed.shape}\")\n",
    "        else:\n",
    "            self.logger.info(\"Data transformed.\")\n",
    "        return X_preprocessed\n",
    "\n",
    "    def preprocess_predict(self, X: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, Optional[pd.DataFrame]]:\n",
    "        \"\"\"\n",
    "        Preprocess new data for prediction.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): New data for prediction.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, pd.DataFrame, Optional[pd.DataFrame]]: X_preprocessed, recommendations, X_inversed\n",
    "        \"\"\"\n",
    "        step_name = \"Preprocess Predict\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "\n",
    "        # Log initial columns and feature count\n",
    "        self.logger.debug(f\"Initial columns in prediction data: {X.columns.tolist()}\")\n",
    "        self.logger.debug(f\"Initial number of features: {X.shape[1]}\")\n",
    "\n",
    "        # Load transformers\n",
    "        try:\n",
    "            transformers = self.load_transformers()\n",
    "            self.logger.debug(\"Transformers loaded successfully.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Failed to load transformers: {e}\")\n",
    "            raise\n",
    "\n",
    "        # Filter columns based on raw feature names\n",
    "        try:\n",
    "            X_filtered = self.filter_columns(X)\n",
    "            self.logger.debug(f\"Columns after filtering: {X_filtered.columns.tolist()}\")\n",
    "            self.logger.debug(f\"Number of features after filtering: {X_filtered.shape[1]}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Failed during column filtering: {e}\")\n",
    "            raise\n",
    "\n",
    "        # Handle missing values\n",
    "        try:\n",
    "            X_filtered, _ = self.handle_missing_values(X_filtered)\n",
    "            self.logger.debug(f\"Columns after handling missing values: {X_filtered.columns.tolist()}\")\n",
    "            self.logger.debug(f\"Number of features after handling missing values: {X_filtered.shape[1]}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Failed during missing value handling: {e}\")\n",
    "            raise\n",
    "\n",
    "        # Ensure all expected raw features are present\n",
    "        expected_raw_features = self.numericals + self.ordinal_categoricals + self.nominal_categoricals\n",
    "        provided_features = X_filtered.columns.tolist()\n",
    "\n",
    "        self.logger.debug(f\"Expected raw features: {expected_raw_features}\")\n",
    "        self.logger.debug(f\"Provided features: {provided_features}\")\n",
    "\n",
    "        missing_raw_features = set(expected_raw_features) - set(provided_features)\n",
    "        if missing_raw_features:\n",
    "            self.logger.error(f\"❌ Missing required raw feature columns in prediction data: {missing_raw_features}\")\n",
    "            raise ValueError(f\"Missing required raw feature columns in prediction data: {missing_raw_features}\")\n",
    "\n",
    "        # Handle unexpected columns (optional: ignore or log)\n",
    "        unexpected_features = set(provided_features) - set(expected_raw_features)\n",
    "        if unexpected_features:\n",
    "            self.logger.warning(f\"⚠️ Unexpected columns in prediction data that will be ignored: {unexpected_features}\")\n",
    "\n",
    "        # Ensure the order of columns matches the pipeline's expectation (optional)\n",
    "        X_filtered = X_filtered[expected_raw_features]\n",
    "        self.logger.debug(\"Reordered columns to match the pipeline's raw feature expectations.\")\n",
    "\n",
    "        # Transform data using the loaded pipeline\n",
    "        try:\n",
    "            X_preprocessed_np = self.pipeline.transform(X_filtered)\n",
    "            self.logger.debug(f\"Transformed data shape: {X_preprocessed_np.shape}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Transformation failed: {e}\")\n",
    "            raise\n",
    "\n",
    "        # Retrieve feature names from the pipeline or use stored final_feature_order\n",
    "        if hasattr(self.pipeline, 'get_feature_names_out'):\n",
    "            try:\n",
    "                columns = self.pipeline.get_feature_names_out()\n",
    "                self.logger.debug(f\"Derived feature names from pipeline: {columns.tolist()}\")\n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"Could not retrieve feature names from pipeline: {e}\")\n",
    "                columns = self.final_feature_order\n",
    "                self.logger.debug(f\"Using stored final_feature_order for column names: {columns}\")\n",
    "        else:\n",
    "            columns = self.final_feature_order\n",
    "            self.logger.debug(f\"Using stored final_feature_order for column names: {columns}\")\n",
    "\n",
    "        # Convert NumPy array back to DataFrame with correct column names\n",
    "        try:\n",
    "            X_preprocessed_df = pd.DataFrame(X_preprocessed_np, columns=columns, index=X_filtered.index)\n",
    "            self.logger.debug(f\"X_preprocessed_df columns: {X_preprocessed_df.columns.tolist()}\")\n",
    "            self.logger.debug(f\"Sample of X_preprocessed_df:\\n{X_preprocessed_df.head()}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Failed to convert transformed data to DataFrame: {e}\")\n",
    "            raise\n",
    "\n",
    "        # Inverse transform for interpretability (optional, for interpretability)\n",
    "        try:\n",
    "            self.logger.debug(f\"[DEBUG] Original data shape before inverse transform: {X.shape}\")\n",
    "            X_inversed = self.inverse_transform_data(X_preprocessed_np, original_data=X)\n",
    "            self.logger.debug(f\"[DEBUG] Inversed data shape: {X_inversed.shape}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Inverse transformation failed: {e}\")\n",
    "            X_inversed = None\n",
    "\n",
    "        # Generate recommendations (if applicable)\n",
    "        try:\n",
    "            recommendations = self.generate_recommendations()\n",
    "            self.logger.debug(\"Generated preprocessing recommendations.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Failed to generate recommendations: {e}\")\n",
    "            recommendations = pd.DataFrame()\n",
    "\n",
    "        # Prepare outputs\n",
    "        return X_preprocessed_df, recommendations, X_inversed\n",
    "\n",
    "    def preprocess_clustering(self, X: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Preprocess data for clustering mode.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): Input features for clustering.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, pd.DataFrame]: X_processed, recommendations.\n",
    "        \"\"\"\n",
    "        step_name = \"Preprocess Clustering\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "        debug_flag = self.get_debug_flag('debug_handle_missing_values')  # Use relevant debug flags\n",
    "\n",
    "        # Handle Missing Values\n",
    "        X_missing, _ = self.handle_missing_values(X, None)\n",
    "        self.logger.debug(f\"After handling missing values: X_missing.shape={X_missing.shape}\")\n",
    "\n",
    "        # Handle Outliers\n",
    "        X_outliers_handled, _ = self.handle_outliers(X_missing, None)\n",
    "        self.logger.debug(f\"After handling outliers: X_outliers_handled.shape={X_outliers_handled.shape}\")\n",
    "\n",
    "        # Test Normality (optional for clustering)\n",
    "        if self.model_category in ['clustering']:\n",
    "            self.logger.info(\"Skipping normality tests for clustering.\")\n",
    "        else:\n",
    "            self.test_normality(X_outliers_handled)\n",
    "\n",
    "        # Generate Preprocessing Recommendations\n",
    "        recommendations = self.generate_recommendations()\n",
    "\n",
    "        # Build and Fit the Pipeline\n",
    "        self.pipeline = self.build_pipeline(X_outliers_handled)\n",
    "        self.logger.debug(\"Pipeline built and fitted.\")\n",
    "\n",
    "        # Transform the data\n",
    "        X_processed = self.pipeline.transform(X_outliers_handled)\n",
    "        self.logger.debug(f\"After pipeline transform: X_processed.shape={X_processed.shape}\")\n",
    "\n",
    "        # Optionally, inverse transformations can be handled if necessary\n",
    "\n",
    "        # Save Transformers (if needed)\n",
    "        # Not strictly necessary for clustering unless you plan to apply the same preprocessing on new data\n",
    "        self.save_transformers()\n",
    "\n",
    "        self.logger.info(\"✅ Clustering data preprocessed successfully.\")\n",
    "\n",
    "        return X_processed, recommendations\n",
    "\n",
    "    def final_preprocessing(\n",
    "        self, \n",
    "        data: pd.DataFrame\n",
    "    ) -> Tuple:\n",
    "        \"\"\"\n",
    "        Execute the full preprocessing pipeline based on the mode.\n",
    "\n",
    "        Args:\n",
    "            data (pd.DataFrame): Input dataset containing features and possibly the target variable.\n",
    "\n",
    "        Returns:\n",
    "            Tuple: Depending on mode:\n",
    "                - 'train': X_train, X_test, y_train, y_test, recommendations, X_test_inverse\n",
    "                - 'predict': X_preprocessed, recommendations, X_inverse\n",
    "                - 'clustering': X_processed, recommendations\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"Starting: Final Preprocessing Pipeline in '{self.mode}' mode.\")\n",
    "\n",
    "        # Step 0: Filter Columns\n",
    "        try:\n",
    "            data = self.filter_columns(data)\n",
    "            self.logger.info(\"✅ Column filtering completed successfully.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Column filtering failed: {e}\")\n",
    "            raise\n",
    "\n",
    "        if self.mode == 'train':\n",
    "            # Ensure y_variable is present in the data\n",
    "            if not all(col in data.columns for col in self.y_variable):\n",
    "                missing_y = [col for col in self.y_variable if col not in data.columns]\n",
    "                raise ValueError(f\"Target variable(s) {missing_y} not found in the dataset.\")\n",
    "\n",
    "            # Separate X and y\n",
    "            X = data.drop(self.y_variable, axis=1)\n",
    "            y = data[self.y_variable].iloc[:, 0] if len(self.y_variable) == 1 else data[self.y_variable]\n",
    "\n",
    "            if y is None:\n",
    "                raise ValueError(\"Target variable 'y' must be provided in train mode.\")\n",
    "            return self.preprocess_train(X, y)\n",
    "\n",
    "        elif self.mode == 'predict':\n",
    "            # For predict mode, y_variable is not used\n",
    "            X = data.copy()\n",
    "            # Ensure that transformers are loaded\n",
    "            transformers_path = os.path.join(self.transformers_dir, 'transformers.pkl')\n",
    "            if not os.path.exists(transformers_path):\n",
    "                self.logger.error(f\"❌ Transformers file not found at '{self.transformers_dir}'. Cannot proceed with prediction.\")\n",
    "                raise FileNotFoundError(f\"Transformers file not found at '{self.transformers_dir}'.\")\n",
    "\n",
    "            # Preprocess the data\n",
    "            X_preprocessed, recommendations, X_inversed = self.preprocess_predict(X)\n",
    "            self.logger.info(\"✅ Preprocessing completed successfully in predict mode.\")\n",
    "\n",
    "            return X_preprocessed, recommendations, X_inversed\n",
    "\n",
    "        elif self.mode == 'clustering':\n",
    "            # Clustering mode: Use all data as X; y is not used\n",
    "            X = data.copy()\n",
    "            return self.preprocess_clustering(X)\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Mode '{self.mode}' is not implemented.\")\n",
    "\n",
    "    # Optionally, implement a method to display column info for debugging\n",
    "    def _debug_column_info(self, df: pd.DataFrame, step: str = \"Debug Column Info\"):\n",
    "        \"\"\"\n",
    "        Display information about DataFrame columns for debugging purposes.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): The DataFrame to inspect.\n",
    "            step (str, optional): Description of the current step. Defaults to \"Debug Column Info\".\n",
    "        \"\"\"\n",
    "        self.logger.debug(f\"\\n📊 {step}: Column Information\")\n",
    "        for col in df.columns:\n",
    "            self.logger.debug(f\"Column '{col}': {df[col].dtype}, Unique Values: {df[col].nunique()}\")\n",
    "        self.logger.debug(\"\\n\")\n",
    "        \n",
    "        \n",
    "# --- Training Functions for Different Models ---\n",
    "def train_overall_injury_risk_model(data, feature_dir, timesteps=5):\n",
    "    # Load features for overall injury risk\n",
    "    features_injury = load_top_features('injury_risk', feature_dir=feature_dir)\n",
    "    print(\"[DEBUG] Features loaded for injury_risk:\", features_injury)  # Add this\n",
    "    y_variable_injury = ['injury_risk']\n",
    "\n",
    "    assert len(features_injury) > 0, \"No features loaded for injury_risk model!\"\n",
    "\n",
    "    # Initialize classifier preprocessor\n",
    "    ordinal_categoricals = ['player_height_in_meters', 'player_weight__in_kg']\n",
    "    nominal_categoricals = ['trial_id']\n",
    "    # Save the transformers for the joint model.\n",
    "    # Initialize DataPreprocessor\n",
    "    preprocessor = DataPreprocessor(\n",
    "        model_type=\"Tree Based Classifier\",\n",
    "        y_variable=y_variable_injury,\n",
    "        ordinal_categoricals=ordinal_categoricals,\n",
    "        nominal_categoricals=nominal_categoricals,\n",
    "        numericals=features_injury,\n",
    "        mode='train',\n",
    "        normalize_debug=False,\n",
    "        normalize_graphs_output=False,\n",
    "        graphs_output_dir=Path(\"../../data/Deep_Learning_Final/transformers\"),\n",
    "        transformers_dir=Path(\"../../data/Deep_Learning_Final/transformers\")\n",
    "    )\n",
    "\n",
    "    classifier_preprocessor = DataPreprocessor(\n",
    "        model_type=\"Tree Based Classifier\",\n",
    "        y_variable=y_variable_injury,\n",
    "        ordinal_categoricals=ordinal_categoricals,\n",
    "        nominal_categoricals=nominal_categoricals,\n",
    "        numericals=features_injury,\n",
    "        mode='predict',\n",
    "        debug=True,\n",
    "        normalize_debug=False,\n",
    "        normalize_graphs_output=False,\n",
    "        graphs_output_dir=Path(\"../../data/Deep_Learning_Final/transformers\"),\n",
    "        transformers_dir=Path(\"../../data/Deep_Learning_Final/transformers\")\n",
    "    )\n",
    "    \n",
    "    # Split the data\n",
    "    train_data, test_data = temporal_train_test_split(data, test_size=0.2)\n",
    "    X_train, X_test, y_train, y_test, recommendations, X_test_inverse = preprocessor.final_preprocessing(data)\n",
    "    \n",
    "    X_train, y_train, _, _ = classifier_preprocessor.final_preprocessing(train_data)\n",
    "    print(\"X_train shape:\", X_train.shape)\n",
    "    print(\"Feature order:\", classifier_preprocessor.final_feature_order)\n",
    "    X_test, y_test, _, _ = classifier_preprocessor.final_preprocessing(test_data)\n",
    "    \n",
    "    # Create LSTM sequences\n",
    "    X_lstm_train, y_lstm_train = create_sequences(X_train, y_train, timesteps)\n",
    "    X_lstm_test, y_lstm_test = create_sequences(X_test, y_test, timesteps)\n",
    "    \n",
    "    # Build and train the injury risk model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(64, input_shape=(X_lstm_train.shape[1], X_lstm_train.shape[2]), return_sequences=False))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=5)\n",
    "    logging.info(\"Training overall injury risk model (Classification)...\")\n",
    "    model.fit(X_lstm_train, y_lstm_train, epochs=50, batch_size=32, validation_data=(X_lstm_test, y_lstm_test), callbacks=[early_stop])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_overall_exhaustion_model(data, feature_dir, timesteps=5):\n",
    "    features_exhaustion = load_top_features('by_trial_exhaustion_score', feature_dir=feature_dir)\n",
    "    y_variable_exh = 'by_trial_exhaustion_score'\n",
    "    \n",
    "    ordinal_categoricals = ['player_height_in_meters', 'player_weight__in_kg']\n",
    "    nominal_categoricals = ['trial_id']\n",
    "    # Save the transformers for the joint model.\n",
    "    # Initialize DataPreprocessor\n",
    "    preprocessor = DataPreprocessor(\n",
    "        model_type=\"Linear Regression\",\n",
    "        y_variable=y_variable_exh,\n",
    "        ordinal_categoricals=ordinal_categoricals,\n",
    "        nominal_categoricals=nominal_categoricals,\n",
    "        numericals=features_exhaustion,\n",
    "        mode='train',\n",
    "        normalize_debug=False,\n",
    "        normalize_graphs_output=False,\n",
    "        graphs_output_dir=Path(\"../../data/Deep_Learning_Final/transformers\"),\n",
    "        transformers_dir=Path(\"../../data/Deep_Learning_Final/transformers\")\n",
    "    )\n",
    "\n",
    "    regressor_preprocessor = DataPreprocessor(\n",
    "        model_type=\"Linear Regression\",\n",
    "        y_variable=y_variable_exh,\n",
    "        ordinal_categoricals=ordinal_categoricals,\n",
    "        nominal_categoricals=nominal_categoricals,\n",
    "        numericals=features_exhaustion,\n",
    "        mode='test',\n",
    "        debug=True,\n",
    "        normalize_debug=False,\n",
    "        normalize_graphs_output=False,\n",
    "        graphs_output_dir=Path(\"../../data/Deep_Learning_Final/transformers\"),\n",
    "        transformers_dir=Path(\"../../data/Deep_Learning_Final/transformers\")\n",
    "    )\n",
    "    \n",
    "    train_data, test_data = temporal_train_test_split(data, test_size=0.2)\n",
    "    X_train, X_test, y_train, y_test, recommendations, X_test_inverse = preprocessor.final_preprocessing(data)\n",
    "    \n",
    "    X_train, y_train, _, _ = regressor_preprocessor.final_preprocessing(train_data)\n",
    "    X_test, y_test, _, _ = regressor_preprocessor.final_preprocessing(test_data)\n",
    "    \n",
    "    X_lstm_train, y_lstm_train = create_sequences(X_train, y_train, timesteps)\n",
    "    X_lstm_test, y_lstm_test = create_sequences(X_test, y_test, timesteps)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(64, input_shape=(X_lstm_train.shape[1], X_lstm_train.shape[2]), return_sequences=False))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=5)\n",
    "    logging.info(\"Training overall exhaustion model (Regression)...\")\n",
    "    model.fit(X_lstm_train, y_lstm_train, epochs=50, batch_size=32, validation_data=(X_lstm_test, y_lstm_test), callbacks=[early_stop])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_joint_injury_risk_models(data, feature_dir, timesteps=5):\n",
    "    joints = ['ANKLE', 'WRIST', 'ELBOW', 'KNEE', 'HIP']\n",
    "    joint_models = {}\n",
    "    for joint in joints:\n",
    "        # Load aggregated joint features for the current joint.\n",
    "        joint_features = load_top_features(joint, feature_dir=feature_dir, data=data, joint_level=True)\n",
    "        for side in ['L', 'R']:\n",
    "            target_joint = f\"{side}_{joint}_injury_risk\"\n",
    "            logging.info(f\"Training model for {target_joint}...\")\n",
    "            \n",
    "            # Reload overall injury risk features as baseline.\n",
    "            features_injury = load_top_features('injury_risk', feature_dir=feature_dir)\n",
    "            y_variable_injury = ['injury_risk']\n",
    "            # Combine the aggregated joint features with the target column name.\n",
    "            all_features = joint_features + [target_joint]\n",
    "            \n",
    "            # Subset data to the relevant columns.\n",
    "            train_subset = data.loc[:, all_features]\n",
    "            from sklearn.model_selection import train_test_split\n",
    "            train_data_subset, test_data_subset = train_test_split(train_subset, test_size=0.2, shuffle=False)\n",
    "            \n",
    "            # Save the transformers for the joint model.\n",
    "            # Initialize DataPreprocessor\n",
    "            preprocessor = DataPreprocessor(\n",
    "                model_type=\"Tree Based Classifier\",\n",
    "                y_variable=y_variable_injury,\n",
    "                ordinal_categoricals=['player_height_in_meters', 'player_weight__in_kg'],\n",
    "                nominal_categoricals=['trial_id'],\n",
    "                numericals=features_injury,\n",
    "                mode='train',\n",
    "                normalize_debug=False,\n",
    "                normalize_graphs_output=False,\n",
    "                graphs_output_dir=Path(\"../../data/Deep_Learning_Final/transformers\"),\n",
    "                transformers_dir=Path(\"../../data/Deep_Learning_Final/transformers\")\n",
    "            )\n",
    "\n",
    "            # load trasnformers\n",
    "            classifier_preprocessor = DataPreprocessor(\n",
    "                model_type=\"Tree Based Classifier\",\n",
    "                y_variable=y_variable_injury,\n",
    "                ordinal_categoricals=['player_height_in_meters', 'player_weight__in_kg'],\n",
    "                nominal_categoricals=['trial_id'],\n",
    "                numericals=features_injury,\n",
    "                mode='predict',\n",
    "                debug=True,\n",
    "                normalize_debug=False,\n",
    "                normalize_graphs_output=False,\n",
    "                graphs_output_dir=Path(\"../../data/Deep_Learning_Final/transformers\"),\n",
    "                transformers_dir=Path(\"../../data/Deep_Learning_Final/transformers\")\n",
    "            )\n",
    "            X_train, X_test, y_train, y_test, recommendations, X_test_inverse = preprocessor.final_preprocessing(train_subset)\n",
    "            X_train, y_train, _, _ = classifier_preprocessor.final_preprocessing(train_data_subset)\n",
    "            X_test, y_test, _, _ = classifier_preprocessor.final_preprocessing(test_data_subset)\n",
    "            \n",
    "            X_lstm, y_lstm = create_sequences(X_train, y_train, timesteps)\n",
    "            X_lstm_val, y_lstm_val = create_sequences(X_test, y_test, timesteps)\n",
    "            \n",
    "            model = Sequential()\n",
    "            model.add(LSTM(64, input_shape=(X_lstm.shape[1], X_lstm.shape[2]), return_sequences=False))\n",
    "            model.add(Dropout(0.2))\n",
    "            model.add(Dense(1, activation='sigmoid'))\n",
    "            model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "            \n",
    "            model.fit(X_lstm, y_lstm, epochs=50, batch_size=32, validation_data=(X_lstm_val, y_lstm_val))\n",
    "            \n",
    "            joint_models[target_joint] = {\n",
    "                'model': model,\n",
    "                'features': joint_features\n",
    "            }\n",
    "    return joint_models\n",
    "\n",
    "def run_training_pipeline():\n",
    "    logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "    \n",
    "    csv_path = \"../../data/processed/final_granular_dataset.csv\"\n",
    "    json_path = \"../../data/basketball/freethrow/participant_information.json\"\n",
    "    feature_dir = \"../../data/Deep_Learning_Final\"\n",
    "    import os \n",
    "    print(os.getcwd())\n",
    "    # Assumes these functions are defined in a module (e.g., load_data_module.py)\n",
    "    data = load_data(csv_path, json_path)\n",
    "    data = prepare_joint_features(data)\n",
    "    data = feature_engineering(data)\n",
    "    print(\"Columns in data:\", data.columns.tolist())\n",
    "    \n",
    "    model_injury = train_overall_injury_risk_model(data, feature_dir)\n",
    "    # model_exhaustion = train_overall_exhaustion_model(data, feature_dir)\n",
    "    # joint_models = train_joint_injury_risk_models(data, feature_dir)\n",
    "    \n",
    "    # Optionally, you can save the models to disk here.\n",
    "    return model_injury#, model_exhaustion, joint_models\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_training_pipeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forecast.py\n",
    "import logging\n",
    "from utils import forecast_and_plot\n",
    "\n",
    "def run_forecast(model, data_series, scaler, timesteps, future_steps=10, title=\"Forecast\"):\n",
    "    logging.info(\"Running forecast...\")\n",
    "    forecast_and_plot(model, data_series, scaler, timesteps, future_steps, title)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage:\n",
    "    # Load your trained model and data_series, then call run_forecast().\n",
    "    # For example:\n",
    "    # from tensorflow.keras.models import load_model\n",
    "    # model = load_model('path_to_saved_model.h5')\n",
    "    # data_series = ...  # load or prepare your time series data\n",
    "    # run_forecast(model, data_series, scaler=None, timesteps=5, future_steps=10, title=\"Forecast Example\")\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict.py\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import load_model\n",
    "from utils import create_sequences, temporal_train_test_split\n",
    "\n",
    "def predict(model, data, timesteps):\n",
    "    # Perform any preprocessing required here (this is a placeholder)\n",
    "    train_data, test_data = temporal_train_test_split(data)\n",
    "    # For example, assume we simply create sequences from the test_data\n",
    "    X_seq, _ = create_sequences(test_data, test_data, timesteps)\n",
    "    predictions = model.predict(X_seq)\n",
    "    return predictions\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage:\n",
    "    # model = load_model('path_to_saved_model.h5')\n",
    "    # data = pd.read_csv('path_to_new_data.csv')\n",
    "    # preds = predict(model, data, timesteps=5)\n",
    "    # print(preds)\n",
    "    pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science_ft_bio_predictions",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
