{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final full idea put together\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ml/load_and_prepare_data/load_data_and_analyze.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ml/load_and_prepare_data/load_data_and_analyze.py\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import sys\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error, r2_score, accuracy_score,\n",
    "    precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    ")\n",
    "import shap\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# HELPER FUNCTION FOR DEBUG OUTPUTS\n",
    "###############################################################################\n",
    "def _print_debug_info(step_name, df, new_columns=None, debug=False):\n",
    "    \"\"\"\n",
    "    Prints debug information about a DataFrame after a processing step.\n",
    "    \n",
    "    When debug=True, prints:\n",
    "      - The step name.\n",
    "      - The DataFrame shape.\n",
    "      - If new_columns is provided (a list of column names), prints for each:\n",
    "          • Data type and a sample of unique values (up to 5).\n",
    "    When debug=False, prints a single-line message indicating step completion.\n",
    "    \"\"\"\n",
    "    if debug:\n",
    "        logging.info(f\"Step [{step_name}]: DataFrame shape = {df.shape}\")\n",
    "        if new_columns:\n",
    "            logging.info(f\"New columns added: {new_columns}\")\n",
    "            for col in new_columns:\n",
    "                sample = df[col].dropna().unique()[:5]\n",
    "                logging.info(f\" - {col}: dtype={df[col].dtype}, sample values={sample}\")\n",
    "    else:\n",
    "        logging.info(f\"Step [{step_name}] completed.\")\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# FUNCTION DEFINITIONS\n",
    "###############################################################################\n",
    "def load_data(csv_path, json_path, participant_id='P0001', debug=False):\n",
    "    \"\"\"\n",
    "    Loads the main dataset and participant information, then merges them.\n",
    "    \n",
    "    Parameters:\n",
    "      - csv_path (str): Path to the main CSV file.\n",
    "      - json_path (str): Path to the participant information JSON file.\n",
    "      - participant_id (str): Participant identifier.\n",
    "      - debug (bool): If True, prints detailed debug info.\n",
    "    \n",
    "    Returns:\n",
    "      - data (pd.DataFrame): Merged DataFrame.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = pd.read_csv(csv_path)\n",
    "        logging.info(f\"Loaded data from {csv_path} with shape {data.shape}\")\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"File not found: {csv_path}\")\n",
    "        sys.exit(1)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading {csv_path}: {e}\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    data['participant_id'] = participant_id\n",
    "    logging.info(f\"Added 'participant_id' column with value '{participant_id}'\")\n",
    "    \n",
    "    try:\n",
    "        with open(json_path, 'r') as file:\n",
    "            participant_info = json.load(file)\n",
    "        participant_df = pd.DataFrame([participant_info])\n",
    "        logging.info(f\"Loaded participant information from {json_path}\")\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"File not found: {json_path}\")\n",
    "        sys.exit(1)\n",
    "    except json.JSONDecodeError:\n",
    "        logging.error(f\"Invalid JSON format in {json_path}\")\n",
    "        sys.exit(1)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading {json_path}: {e}\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    data = pd.merge(data, participant_df, on='participant_id', how='left')\n",
    "    logging.info(f\"Merged participant data. New shape: {data.shape}\")\n",
    "    _print_debug_info(\"load_data\", data, debug=debug)\n",
    "    \n",
    "    # remove these columns\n",
    "    data = data.drop(columns=['event_idx_leg', 'event_idx_elbow', 'event_idx_release', 'event_idx_wrist'])\n",
    "    # filter out null shooting_phases\n",
    "    if 'shooting_phases' in data.columns:\n",
    "        data = data[data['shooting_phases'].notnull()]\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "def calculate_joint_angles(df, connections, debug=False):\n",
    "    \"\"\"\n",
    "    Calculates joint angles from coordinate data using vector mathematics.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame containing joint coordinates.\n",
    "        connections (list): Joint connections defining biomechanical segments.\n",
    "        debug (bool): Enable debug logging.\n",
    "        \n",
    "    Returns:\n",
    "        df (pd.DataFrame): Updated DataFrame with new angle columns.\n",
    "    \"\"\"\n",
    "    angle_columns = []\n",
    "    \n",
    "    # Define angle calculation points for key joints\n",
    "    # Note: The new \"KNEE\" definition uses hip, knee, and ankle as the points.\n",
    "    angle_definitions = {\n",
    "        'SHOULDER': {\n",
    "            'left': ['L_HIP', 'L_SHOULDER', 'L_ELBOW'],\n",
    "            'right': ['R_HIP', 'R_SHOULDER', 'R_ELBOW']\n",
    "        },\n",
    "        'HIP': {\n",
    "            'left': ['L_SHOULDER', 'L_HIP', 'L_KNEE'],\n",
    "            'right': ['R_SHOULDER', 'R_HIP', 'R_KNEE']\n",
    "        },\n",
    "        'KNEE': {\n",
    "            'left': ['L_HIP', 'L_KNEE', 'L_ANKLE'],\n",
    "            'right': ['R_HIP', 'R_KNEE', 'R_ANKLE']\n",
    "        },\n",
    "        'ANKLE': {\n",
    "            'left': ['L_KNEE', 'L_ANKLE', 'L_5THTOE'],\n",
    "            'right': ['R_KNEE', 'R_ANKLE', 'R_5THTOE']\n",
    "        }\n",
    "    }\n",
    "\n",
    "    for joint, sides in angle_definitions.items():\n",
    "        for side in ['left', 'right']:\n",
    "            points = sides[side]\n",
    "            prefix = 'L' if side == 'left' else 'R'\n",
    "            \n",
    "            # Build list of required coordinate columns for this calculation\n",
    "            required_cols = []\n",
    "            for point in points:\n",
    "                required_cols += [f'{point}_x', f'{point}_y', f'{point}_z']\n",
    "                \n",
    "            if all(col in df.columns for col in required_cols):\n",
    "                # Calculate the vectors needed for the angle\n",
    "                vec1 = df[[f'{points[0]}_x', f'{points[0]}_y', f'{points[0]}_z']].values - \\\n",
    "                       df[[f'{points[1]}_x', f'{points[1]}_y', f'{points[1]}_z']].values\n",
    "                vec2 = df[[f'{points[2]}_x', f'{points[2]}_y', f'{points[2]}_z']].values - \\\n",
    "                       df[[f'{points[1]}_x', f'{points[1]}_y', f'{points[1]}_z']].values\n",
    "\n",
    "                # Compute the dot product and the norms of the vectors\n",
    "                dot_product = np.sum(vec1 * vec2, axis=1)\n",
    "                norm_product = np.linalg.norm(vec1, axis=1) * np.linalg.norm(vec2, axis=1)\n",
    "                \n",
    "                # Compute the angle (in degrees) and add a small epsilon to avoid division by zero\n",
    "                angles = np.degrees(np.arccos(dot_product / (norm_product + 1e-8)))\n",
    "                \n",
    "                col_name = f'{prefix}_{joint}_angle'\n",
    "                df[col_name] = angles\n",
    "                angle_columns.append(col_name)\n",
    "                \n",
    "                if debug:\n",
    "                    logging.info(f\"Calculated {col_name} with mean: {angles.mean():.2f}°\")\n",
    "            else:\n",
    "                logging.warning(f\"Missing coordinates for {prefix}_{joint} angle calculation\")\n",
    "\n",
    "    _print_debug_info(\"calculate_joint_angles\", df, new_columns=angle_columns, debug=debug)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def prepare_joint_features(data, debug=False, group_trial=False, group_shot_phase=False):\n",
    "    \"\"\"\n",
    "    Aggregates joint-level energy and power, creates additional biomechanical features,\n",
    "    and adds new features:\n",
    "      - energy_acceleration: instantaneous rate of change of joint_energy.\n",
    "      - ankle_power_ratio: ratio of left to right ankle ongoing power.\n",
    "      - Additional asymmetry metrics.\n",
    "      - Power ratios for all joint pairs.\n",
    "      - Side-Specific Range-of-Motion (ROM) metrics (ROM, deviation, and binary extreme flag).\n",
    "      - Removal of the wrist_angle_release column if present.\n",
    "      \n",
    "    NEW GROUPING FEATURE (optional):\n",
    "      - If group_trial is True, computes aggregated trial-level features (e.g., trial mean exhaustion and total joint energy)\n",
    "        and merges them into the DataFrame.\n",
    "      - If group_shot_phase is True, and if the column 'shooting_phases' exists, computes shot-phase level aggregates\n",
    "        (based on both 'trial_id' and 'shooting_phases') and merges them in.\n",
    "    \n",
    "    Parameters:\n",
    "      - data (pd.DataFrame): Input DataFrame.\n",
    "      - debug (bool): If True, prints detailed debug outputs.\n",
    "      - group_trial (bool): If True, add trial-level aggregated features.\n",
    "      - group_shot_phase (bool): If True, add shot-phase-level aggregated features.\n",
    "    \n",
    "    Returns:\n",
    "      - data (pd.DataFrame): Updated DataFrame with new features and, optionally, grouping-based extra columns.\n",
    "    \"\"\"\n",
    "    step = \"prepare_joint_features\"\n",
    "    new_cols = []\n",
    "    connections = [\n",
    "        (\"R_EYE\", \"L_EYE\"), (\"R_EYE\", \"NOSE\"), (\"L_EYE\", \"NOSE\"),\n",
    "        (\"R_EYE\", \"R_EAR\"), (\"L_EYE\", \"L_EAR\"), (\"R_SHOULDER\", \"L_SHOULDER\"),\n",
    "        (\"R_SHOULDER\", \"R_ELBOW\"), (\"L_SHOULDER\", \"L_ELBOW\"), (\"R_ELBOW\", \"R_WRIST\"),\n",
    "        (\"L_ELBOW\", \"L_WRIST\"), (\"R_SHOULDER\", \"R_HIP\"), (\"L_SHOULDER\", \"L_HIP\"),\n",
    "        (\"R_HIP\", \"L_HIP\"), (\"R_HIP\", \"R_KNEE\"), (\"L_HIP\", \"L_KNEE\"),\n",
    "        (\"R_KNEE\", \"R_ANKLE\"), (\"L_KNEE\", \"L_ANKLE\"), (\"R_WRIST\", \"R_1STFINGER\"),\n",
    "        (\"R_WRIST\", \"R_5THFINGER\"), (\"L_WRIST\", \"L_1STFINGER\"), (\"L_WRIST\", \"L_5THFINGER\"),\n",
    "        (\"R_ANKLE\", \"R_1STTOE\"), (\"R_ANKLE\", \"R_5THTOE\"), (\"L_ANKLE\", \"L_1STTOE\"),\n",
    "        (\"L_ANKLE\", \"L_5THTOE\"), (\"R_ANKLE\", \"R_CALC\"), (\"L_ANKLE\", \"L_CALC\"),\n",
    "        (\"R_1STTOE\", \"R_5THTOE\"), (\"L_1STTOE\", \"L_5THTOE\"), (\"R_1STTOE\", \"R_CALC\"),\n",
    "        (\"L_1STTOE\", \"L_CALC\"), (\"R_5THTOE\", \"R_CALC\"), (\"L_5THTOE\", \"L_CALC\"),\n",
    "        (\"R_1STFINGER\", \"R_5THFINGER\"), (\"L_1STFINGER\", \"L_5THFINGER\")\n",
    "    ]\n",
    "    # Compute joint angles first.\n",
    "    data = calculate_joint_angles(data, connections, debug=debug)\n",
    "    # Example: Assume a base datetime for when the recording started.\n",
    "    base_datetime = pd.Timestamp('2025-01-01 00:00:00')\n",
    "\n",
    "    # Determine the unit of continuous_frame_time:\n",
    "    # If continuous_frame_time is in milliseconds, then:\n",
    "    data['datetime'] = base_datetime + pd.to_timedelta(data['continuous_frame_time'], unit='ms')\n",
    "\n",
    "    # Rename participant anthropometrics if available.\n",
    "    if 'height_in_meters' in data.columns and 'weight__in_kg' in data.columns:\n",
    "        data['player_height_in_meters'] = data['height_in_meters']\n",
    "        data['player_weight__in_kg'] = data['weight__in_kg']\n",
    "        data.drop(['height_in_meters', 'weight__in_kg'], axis=1, inplace=True, errors='ignore')\n",
    "        new_cols.extend(['player_height_in_meters', 'player_weight__in_kg'])\n",
    "        logging.info(\"Renamed participant anthropometrics.\")\n",
    "    else:\n",
    "        logging.warning(\"Participant anthropometric columns not found during renaming.\")\n",
    "\n",
    "    # Identify joint energy and power columns.\n",
    "    joint_energy_columns = [col for col in data.columns if '_energy' in col and not ('by_trial' in col or 'overall' in col)]\n",
    "    print(\"Joint energy columns: \", joint_energy_columns)\n",
    "    joint_power_columns = [col for col in data.columns if '_ongoing_power' in col]\n",
    "    print(\"Joint power columns: \", joint_power_columns)\n",
    "    print(\"All angle columns: \", [col for col in data.columns if 'angle' in col])\n",
    "    logging.info(f\"Identified {len(joint_energy_columns)} joint energy and {len(joint_power_columns)} joint power columns.\")\n",
    "    if not joint_energy_columns:\n",
    "        logging.error(\"No joint energy columns found. Check naming conventions.\")\n",
    "        sys.exit(1)\n",
    "    if not joint_power_columns:\n",
    "        logging.error(\"No joint power columns found. Check naming conventions.\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # Create aggregated columns.\n",
    "    data['joint_energy'] = data[joint_energy_columns].sum(axis=1)\n",
    "    data['joint_power'] = data[joint_power_columns].sum(axis=1)\n",
    "    new_cols.extend(['joint_energy', 'joint_power'])\n",
    "    logging.info(\"Created aggregated 'joint_energy' and 'joint_power'.\")\n",
    "\n",
    "    # --- NEW FEATURE: Energy Acceleration ---\n",
    "    if 'continuous_frame_time' in data.columns:\n",
    "        time_diff = data['continuous_frame_time'].diff().replace(0, 1e-6)  # Avoid division by zero\n",
    "        data['energy_acceleration'] = data['joint_energy'].diff() / time_diff\n",
    "        data['energy_acceleration'] = data['energy_acceleration'].replace([np.inf, -np.inf], np.nan)\n",
    "        new_cols.append('energy_acceleration')\n",
    "        logging.info(\"Created 'energy_acceleration' as derivative of joint_energy over time.\")\n",
    "    else:\n",
    "        logging.error(\"Missing 'continuous_frame_time' for energy_acceleration calculation.\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # --- NEW FEATURE: Ankle Power Ratio ---\n",
    "    if 'L_ANKLE_ongoing_power' in data.columns and 'R_ANKLE_ongoing_power' in data.columns:\n",
    "        data['ankle_power_ratio'] = data['L_ANKLE_ongoing_power'] / (data['R_ANKLE_ongoing_power'] + 1e-6)\n",
    "        new_cols.append('ankle_power_ratio')\n",
    "        logging.info(\"Created 'ankle_power_ratio' feature comparing left to right ankle ongoing power.\")\n",
    "    else:\n",
    "        logging.warning(\"Ankle ongoing power columns not found; 'ankle_power_ratio' not created.\")\n",
    "\n",
    "    # --- NEW FEATURES: Additional Asymmetry Metrics ---\n",
    "    additional_asymmetry_joints = ['hip', 'ankle', 'wrist', 'elbow', 'knee', '1stfinger', '5thfinger']\n",
    "    for joint in additional_asymmetry_joints:\n",
    "        left_col = f\"L_{joint.upper()}_energy\"\n",
    "        right_col = f\"R_{joint.upper()}_energy\"\n",
    "        if left_col in data.columns and right_col in data.columns:\n",
    "            col_name = f\"{joint}_asymmetry\"\n",
    "            data[col_name] = np.abs(data[left_col] - data[right_col])\n",
    "            new_cols.append(col_name)\n",
    "            logging.info(f\"Created asymmetry feature: {col_name}\")\n",
    "        else:\n",
    "            logging.warning(f\"Columns {left_col} and/or {right_col} not found; skipping {joint}_asymmetry.\")\n",
    "\n",
    "    # --- NEW FEATURES: Power Ratios for All Joints ---\n",
    "    joints_for_power_ratio = additional_asymmetry_joints.copy()\n",
    "    if 'knee' not in joints_for_power_ratio:\n",
    "        joints_for_power_ratio.append('knee')\n",
    "    for joint in joints_for_power_ratio:\n",
    "        if joint == 'foot':\n",
    "            left_col = 'left_foot_power'\n",
    "            right_col = 'right_foot_power'\n",
    "        else:\n",
    "            left_col = f\"L_{joint.upper()}_ongoing_power\"\n",
    "            right_col = f\"R_{joint.upper()}_ongoing_power\"\n",
    "        logging.debug(f\"Expecting power columns: {left_col} and {right_col}\")\n",
    "        if left_col in data.columns and right_col in data.columns:\n",
    "            ratio_col = f\"{joint}_power_ratio\"\n",
    "            data[ratio_col] = data[left_col] / (data[right_col] + 1e-6)\n",
    "            new_cols.append(ratio_col)\n",
    "            logging.info(f\"Created power ratio feature: {ratio_col} using columns {left_col} and {right_col}\")\n",
    "        else:\n",
    "            logging.warning(f\"Columns {left_col} and/or {right_col} not found; skipping {joint}_power_ratio.\")\n",
    "\n",
    "    # --- NEW FEATURES: Side-Specific Range-of-Motion (ROM) Metrics ---\n",
    "    rom_joints = {\n",
    "        'KNEE': {'min': 120, 'max': 135},\n",
    "        'SHOULDER': {'min': 0,  'max': 150},\n",
    "        'HIP': {'min': 0,  'max': 120},\n",
    "        'ANKLE': {'min': 0,  'max': 20},\n",
    "        'WRIST': {'min': 0,  'max': 80}\n",
    "    }\n",
    "    for joint, thresholds in rom_joints.items():\n",
    "        for side in ['L', 'R']:\n",
    "            angle_col = f\"{side}_{joint}_angle\"\n",
    "            if angle_col in data.columns:\n",
    "                rom_col = f\"{side}_{joint}_ROM\"\n",
    "                data[rom_col] = data.groupby('trial_id')[angle_col].transform(lambda x: x.max() - x.min())\n",
    "                new_cols.append(rom_col)\n",
    "                logging.info(f\"Computed ROM for {side} {joint} as {rom_col}\")\n",
    "\n",
    "                deviation_col = f\"{side}_{joint}_ROM_deviation\"\n",
    "                normal_min = thresholds['min']\n",
    "                normal_max = thresholds['max']\n",
    "                data[deviation_col] = np.maximum(0, normal_min - data[rom_col]) + np.maximum(0, data[rom_col] - normal_max)\n",
    "                new_cols.append(deviation_col)\n",
    "                logging.info(f\"Computed ROM deviation for {side} {joint} as {deviation_col}\")\n",
    "\n",
    "                extreme_col = f\"{side}_{joint}_ROM_extreme\"\n",
    "                data[extreme_col] = ((data[rom_col] < normal_min) | (data[rom_col] > normal_max)).astype(int)\n",
    "                new_cols.append(extreme_col)\n",
    "                logging.info(f\"Created binary flag for {side} {joint} ROM extremes: {extreme_col}\")\n",
    "            else:\n",
    "                logging.info(f\"Angle column '{angle_col}' not found; skipping ROM metrics for {side} {joint}.\")\n",
    "\n",
    "    # --- Removal of Non-Contributing Features ---\n",
    "    if 'wrist_angle_release' in data.columns:\n",
    "        data.drop(columns=['wrist_angle_release'], inplace=True)\n",
    "        logging.info(\"Dropped 'wrist_angle_release' column as it is not helpful for the model.\")\n",
    "    \n",
    "    # --- Sort Data ---\n",
    "    if 'continuous_frame_time' in data.columns and 'participant_id' in data.columns:\n",
    "        data.sort_values(by=['participant_id', 'continuous_frame_time'], inplace=True)\n",
    "        data.reset_index(drop=True, inplace=True)\n",
    "        logging.info(\"Sorted data by 'participant_id' and 'continuous_frame_time'.\")\n",
    "    else:\n",
    "        logging.error(\"Missing required columns for sorting ('participant_id', 'continuous_frame_time').\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # --- Create Exhaustion Rate ---\n",
    "    if 'by_trial_exhaustion_score' in data.columns and 'by_trial_time' in data.columns:\n",
    "        data['exhaustion_rate'] = data['by_trial_exhaustion_score'].diff() / data['by_trial_time'].diff()\n",
    "        print(\"print all the columns with by_trial_exhaustion_score: \", [col for col in data.columns if 'by_trial_exhaustion_score' in col])\n",
    "        new_cols.append('exhaustion_rate')\n",
    "        logging.info(\"Created 'exhaustion_rate' feature.\")\n",
    "    else:\n",
    "        logging.error(\"Missing columns for 'exhaustion_rate' calculation.\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # --- Create Simulated Heart Rate ---\n",
    "    if 'by_trial_exhaustion_score' in data.columns and 'joint_energy' in data.columns:\n",
    "        data['simulated_HR'] = 60 + (data['by_trial_exhaustion_score'] * 1.5) + (data['joint_energy'] * 0.3)\n",
    "        new_cols.append('simulated_HR')\n",
    "        logging.info(\"Created 'simulated_HR' feature.\")\n",
    "    else:\n",
    "        logging.error(\"Missing columns for 'simulated_HR' calculation.\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # ----- NEW: Add Grouping-Based Aggregation Features -----\n",
    "    # Trial-level aggregations\n",
    "    if group_trial:\n",
    "        # For example, compute the trial mean exhaustion score and trial total joint energy.\n",
    "        trial_aggs = data.groupby('trial_id').agg({\n",
    "            'by_trial_exhaustion_score': 'mean',\n",
    "            'joint_energy': 'sum'\n",
    "        }).rename(columns={\n",
    "            'by_trial_exhaustion_score': 'trial_mean_exhaustion',\n",
    "            'joint_energy': 'trial_total_joint_energy'\n",
    "        }).reset_index()\n",
    "        data = data.merge(trial_aggs, on='trial_id', how='left')\n",
    "        new_cols.extend(['trial_mean_exhaustion', 'trial_total_joint_energy'])\n",
    "        logging.info(\"Added trial-level aggregated features: trial_mean_exhaustion, trial_total_joint_energy.\")\n",
    "\n",
    "    # Shot-phase-level aggregations (requires shooting_phases column)\n",
    "    if group_shot_phase:\n",
    "        if 'shooting_phases' in data.columns:\n",
    "            shot_aggs = data.groupby(['trial_id', 'shooting_phases']).agg({\n",
    "                'by_trial_exhaustion_score': 'mean',\n",
    "                'joint_energy': 'sum'\n",
    "            }).rename(columns={\n",
    "                'by_trial_exhaustion_score': 'shot_phase_mean_exhaustion',\n",
    "                'joint_energy': 'shot_phase_total_joint_energy'\n",
    "            }).reset_index()\n",
    "            data = data.merge(shot_aggs, on=['trial_id', 'shooting_phases'], how='left')\n",
    "            new_cols.extend(['shot_phase_mean_exhaustion', 'shot_phase_total_joint_energy'])\n",
    "            logging.info(\"Added shot-phase-level aggregated features: shot_phase_mean_exhaustion, shot_phase_total_joint_energy.\")\n",
    "        else:\n",
    "            logging.warning(\"Column 'shooting_phases' not found; skipping shot-phase aggregation features.\")\n",
    "    \n",
    "\n",
    "    \n",
    "    _print_debug_info(step, data, new_columns=new_cols, debug=debug)\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "def summarize_data(data, groupby_cols, lag_columns=None, rolling_window=3, \n",
    "                   agg_columns=None, phase_list=None, global_lag=False, debug=False):\n",
    "    \"\"\"\n",
    "    Summarize the dataset with advanced feature engineering by grouping on specified columns.\n",
    "    \n",
    "    This function computes:\n",
    "      - The mean of selected numeric columns (except for 'injury_risk', which is aggregated using max).\n",
    "      - The standard deviation of these columns (with a '_std' suffix).\n",
    "      - The count of records in each group.\n",
    "      - A computed duration (e.g., frame_count * 0.33 seconds if applicable).\n",
    "    \n",
    "    Additionally, for the columns specified in lag_columns, it computes:\n",
    "      - A lag feature.\n",
    "      - A delta feature (current value minus the lag).\n",
    "      - A rolling average (using the specified rolling window).\n",
    "    \n",
    "    If a phase_list is provided and 'shooting_phases' is in groupby_cols,\n",
    "    the function forces the final DataFrame to include all combinations of trial_id and\n",
    "    the specified phases (Cartesian product).\n",
    "    \n",
    "    Parameters:\n",
    "        global_lag (bool): If True, lag is computed over the entire DataFrame (across all trials);\n",
    "                           if False, lag is computed within groups defined by the last groupby column.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Aggregated DataFrame with additional lag features.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    # 1) Set default aggregation and lag columns if not provided.\n",
    "    default_agg_columns = [\n",
    "        'joint_energy', 'L_ELBOW_energy', 'R_ELBOW_energy',\n",
    "        'L_WRIST_energy', 'R_WRIST_energy', 'L_KNEE_energy', 'R_KNEE_energy',\n",
    "        'L_HIP_energy', 'R_HIP_energy',\n",
    "        'joint_power', 'L_ELBOW_ongoing_power', 'R_ELBOW_ongoing_power',\n",
    "        'L_WRIST_ongoing_power', 'R_WRIST_ongoing_power',\n",
    "        'L_KNEE_ongoing_power', 'R_KNEE_ongoing_power', 'L_HIP_ongoing_power', 'R_HIP_ongoing_power',\n",
    "        'elbow_asymmetry', 'wrist_asymmetry', 'knee_asymmetry', 'hip_asymmetry',\n",
    "        'L_ELBOW_angle', 'R_ELBOW_angle', 'L_WRIST_angle', 'R_WRIST_angle',\n",
    "        'L_KNEE_angle', 'R_KNEE_angle',\n",
    "        'L_SHOULDER_ROM', 'R_SHOULDER_ROM', 'L_WRIST_ROM', 'R_WRIST_ROM',\n",
    "        'L_KNEE_ROM', 'R_KNEE_ROM', 'L_HIP_ROM', 'R_HIP_ROM',\n",
    "        'exhaustion_rate', 'by_trial_exhaustion_score', 'injury_risk',\n",
    "        'energy_acceleration', 'power_avg_5', 'rolling_power_std', 'rolling_hr_mean',\n",
    "        'simulated_HR',\n",
    "        'player_height_in_meters', 'player_weight__in_kg'\n",
    "    ]\n",
    "    default_lag_columns = [\n",
    "        'joint_energy', 'joint_power',\n",
    "        'L_ELBOW_energy', 'R_ELBOW_energy', 'L_WRIST_energy', 'R_WRIST_energy',\n",
    "        'elbow_asymmetry', 'wrist_asymmetry',\n",
    "        'L_ELBOW_angle', 'R_ELBOW_angle', 'L_WRIST_angle', 'R_WRIST_angle',\n",
    "        'exhaustion_rate', 'by_trial_exhaustion_score', 'simulated_HR'\n",
    "    ]\n",
    "    if agg_columns is None:\n",
    "        agg_columns = default_agg_columns.copy()\n",
    "    if lag_columns is None:\n",
    "        lag_columns = default_lag_columns.copy()\n",
    "\n",
    "    if debug:\n",
    "        print(\"\\n--- DEBUG: summarize_data START ---\")\n",
    "        print(f\"Initial data shape: {data.shape}\")\n",
    "        print(f\"Grouping by: {groupby_cols}\")\n",
    "        print(f\"Aggregation columns: {agg_columns}\")\n",
    "        print(f\"Lag columns: {lag_columns}\")\n",
    "        print(f\"Rolling window: {rolling_window}\")\n",
    "        print(f\"Global lag: {global_lag}\")\n",
    "        if phase_list is not None:\n",
    "            print(f\"Forced phase list: {phase_list}\")\n",
    "        else:\n",
    "            print(\"No forced phase list provided.\")\n",
    "\n",
    "    # 2) Build aggregation dictionary.\n",
    "    # Use 'mean' for all columns by default, but for 'injury_risk' use 'max' to force binary outcome.\n",
    "    agg_dict = {col: 'mean' for col in agg_columns}\n",
    "    if 'injury_risk' in agg_columns:\n",
    "        agg_dict['injury_risk'] = 'max'\n",
    "    \n",
    "    # 3) Group by the specified columns and compute aggregates.\n",
    "    grouped = data.groupby(groupby_cols)\n",
    "    mean_df = grouped.agg(agg_dict).reset_index()\n",
    "    std_df = grouped[agg_columns].std(ddof=0).reset_index()\n",
    "    count_df = grouped.size().reset_index(name='frame_count')\n",
    "\n",
    "    # 4) Merge aggregates.\n",
    "    summary = pd.merge(mean_df, std_df, on=groupby_cols, suffixes=(\"\", \"_std\"))\n",
    "    summary = pd.merge(summary, count_df, on=groupby_cols)\n",
    "\n",
    "    # 5) Compute duration metric.\n",
    "    if 'frame_count' in summary.columns:\n",
    "        summary['phase_duration'] = summary['frame_count'] * 0.33\n",
    "\n",
    "    # 6) Sort the summary by the grouping columns.\n",
    "    summary = summary.sort_values(groupby_cols)\n",
    "\n",
    "    # 7) Compute lag, delta, and rolling average features.\n",
    "    for col in lag_columns:\n",
    "        if col in summary.columns:\n",
    "            if global_lag:\n",
    "                summary[f\"{col}_lag1\"] = summary[col].shift(1)\n",
    "                summary[f\"{col}_rolling_avg\"] = summary[col].rolling(window=rolling_window, min_periods=1).mean()\n",
    "            else:\n",
    "                group_key = groupby_cols[-1]\n",
    "                summary[f\"{col}_lag1\"] = summary.groupby(group_key)[col].shift(1)\n",
    "                summary[f\"{col}_rolling_avg\"] = (\n",
    "                    summary.groupby(group_key)[col]\n",
    "                    .rolling(window=rolling_window, min_periods=1)\n",
    "                    .mean()\n",
    "                    .reset_index(level=0, drop=True)\n",
    "                )\n",
    "            summary[f\"{col}_delta\"] = summary[col] - summary[f\"{col}_lag1\"]\n",
    "            if debug:\n",
    "                print(f\"\\nComputed lag features for '{col}' with global_lag={global_lag}\")\n",
    "\n",
    "    # 8) Impute NaN values in lag and delta columns.\n",
    "    lag_feature_cols = []\n",
    "    for col in lag_columns:\n",
    "        for suffix in ['_lag1', '_delta']:\n",
    "            new_col = f\"{col}{suffix}\"\n",
    "            if new_col in summary.columns:\n",
    "                lag_feature_cols.append(new_col)\n",
    "    for lag_col in lag_feature_cols:\n",
    "        overall_mean = summary[lag_col].mean(skipna=True)\n",
    "        summary[lag_col] = summary[lag_col].fillna(overall_mean)\n",
    "        if debug:\n",
    "            nan_count = summary[lag_col].isna().sum()\n",
    "            print(f\"Imputed {nan_count} NaN(s) in column '{lag_col}' with overall mean {overall_mean:.4f}\")\n",
    "\n",
    "    # 9) Handle forced phase list for shot phase data.\n",
    "    if phase_list is not None and 'shooting_phases' in groupby_cols:\n",
    "        trial_ids = data['trial_id'].unique()\n",
    "        phase_list = np.array(phase_list)\n",
    "        all_combinations = pd.MultiIndex.from_product([trial_ids, phase_list],\n",
    "                                                        names=['trial_id', 'shooting_phases']).to_frame(index=False)\n",
    "        summary = pd.merge(all_combinations, summary, on=['trial_id', 'shooting_phases'], how='left')\n",
    "        summary = summary[summary['shooting_phases'].notnull()]\n",
    "\n",
    "    # 10) Final sort and re-index.\n",
    "    summary = summary.sort_values(groupby_cols).reset_index(drop=True)\n",
    "    if debug:\n",
    "        print(f\"\\n--- Final debug: summary at end of function ---\")\n",
    "        print(\"Final summary shape:\", summary.shape)\n",
    "        print(\"Final summary columns:\", summary.columns.tolist())\n",
    "        print(\"Sample final summary rows:\\n\", summary.head(10))\n",
    "        print(\"--- DEBUG: summarize_data END ---\\n\")\n",
    "\n",
    "    return summary\n",
    "\n",
    "\n",
    "\n",
    "def add_datetime_column(data, \n",
    "                        base_datetime=pd.Timestamp('2025-01-01 00:00:00'),\n",
    "                        break_seconds=10,\n",
    "                        trial_id_col='trial_id',\n",
    "                        freq='33ms'):\n",
    "    \"\"\"\n",
    "    Generates a new timestamp column for the data such that within each trial the timestamps are evenly spaced\n",
    "    with the specified frequency, and a fixed break (in seconds) is added between trials.\n",
    "    \n",
    "    Parameters:\n",
    "      data (pd.DataFrame): DataFrame containing at least the trial identifier column.\n",
    "      base_datetime (pd.Timestamp): The starting datetime reference.\n",
    "      break_seconds (int): Number of seconds to insert as a break between trials.\n",
    "      trial_id_col (str): The column in the DataFrame that identifies trials.\n",
    "      freq (str): Frequency string (compatible with pd.date_range) for timestamps within each trial.\n",
    "    \n",
    "    Returns:\n",
    "      pd.DataFrame: The original DataFrame with a new 'timestamp' column that contains evenly spaced timestamps \n",
    "                    for each trial and includes the specified break between trials.\n",
    "    \"\"\"\n",
    "    # Get sorted unique trial identifiers. Zero-padded trial IDs sort lexicographically.\n",
    "    unique_trials = sorted(data[trial_id_col].unique())\n",
    "    \n",
    "    # List to hold timestamp ranges for each trial.\n",
    "    timestamp_list = []\n",
    "    \n",
    "    # Initialize the current trial start.\n",
    "    current_time = base_datetime\n",
    "    \n",
    "    # Process each trial individually.\n",
    "    for trial in unique_trials:\n",
    "        # Select the rows for this trial.\n",
    "        trial_mask = data[trial_id_col] == trial\n",
    "        trial_data = data[trial_mask]\n",
    "        n = len(trial_data)\n",
    "        \n",
    "        # Generate a date_range for this trial with n periods at the specified frequency.\n",
    "        trial_timestamps = pd.date_range(start=current_time, periods=n, freq=freq)\n",
    "        timestamp_list.append(trial_timestamps)\n",
    "        \n",
    "        # Set the start time for the next trial:\n",
    "        # current_time becomes the last timestamp of this trial plus the fixed break.\n",
    "        current_time = trial_timestamps[-1] + pd.Timedelta(seconds=break_seconds) + pd.Timedelta(milliseconds=33)\n",
    "    \n",
    "    # Construct a single timestamp Series aligned with the original data.\n",
    "    timestamps = pd.Series(index=data.index, dtype='datetime64[ns]')\n",
    "    for trial, trial_timestamps in zip(unique_trials, timestamp_list):\n",
    "        indices = data.index[data[trial_id_col] == trial]\n",
    "        timestamps.loc[indices] = trial_timestamps\n",
    "        \n",
    "\n",
    "    data = data.copy()\n",
    "    data['timestamp'] = timestamps\n",
    "    # Set the 'datetime' column as index, then force a uniform frequency.\n",
    "    # After setting the index and applying asfreq:\n",
    "    # data = data.set_index('timestamp')\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "def feature_engineering(data, window_size=5, debug=False, group_trial=False, group_shot_phase=False):\n",
    "    \"\"\"Optimized feature engineering with vectorized operations.\n",
    "    \n",
    "    NEW GROUPING FEATURE (optional):\n",
    "      - If group_trial is True, adds trial-level aggregated features (mean exhaustion and injury rate).\n",
    "      - If group_shot_phase is True, and if 'shooting_phases' exists, adds shot-phase-level aggregated features.\n",
    "    \n",
    "    Parameters:\n",
    "      - data (pd.DataFrame): Input DataFrame.\n",
    "      - window_size (int): Window size for some rolling computations.\n",
    "      - debug (bool): If True, prints detailed debug outputs.\n",
    "      - group_trial (bool): If True, add trial-level aggregated features.\n",
    "      - group_shot_phase (bool): If True, add shot-phase-level aggregated features.\n",
    "    \n",
    "    Returns:\n",
    "      - data (pd.DataFrame): Updated DataFrame with engineered features and optional grouping-based columns.\n",
    "    \"\"\"\n",
    "    step = \"feature_engineering\"\n",
    "    new_cols = []\n",
    "    rolling_window = 20\n",
    "    required_columns = {\n",
    "        'base': ['by_trial_exhaustion_score', 'joint_power', 'simulated_HR', 'continuous_frame_time'],\n",
    "        'joints': ['by_trial_time']\n",
    "    }\n",
    "    \n",
    "    # Validate required columns\n",
    "    missing = [col for col in required_columns['base'] if col not in data.columns]\n",
    "    if missing:\n",
    "        logging.error(f\"Missing required columns: {missing}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Vectorized temporal features\n",
    "    data['time_since_start'] = data['continuous_frame_time'] - data['continuous_frame_time'].min()\n",
    "    new_cols.append('time_since_start')\n",
    "    \n",
    "    # Fill ball-related columns with 0 when not in play\n",
    "    ball_cols = ['ball_speed', 'ball_velocity_x', 'ball_velocity_y', 'ball_velocity_z']\n",
    "    data[ball_cols] = data[ball_cols].fillna(0)\n",
    "\n",
    "    # For motion columns, forward-fill missing values\n",
    "    motion_cols = ['dx', 'dy', 'dz']\n",
    "    data[motion_cols] = data[motion_cols].ffill().fillna(0)\n",
    "\n",
    "\n",
    "    # Rolling features\n",
    "    roll_config = {\n",
    "        'power_avg_5': ('joint_power', 'mean'),\n",
    "        'rolling_power_std': ('joint_power', 'std'),\n",
    "        'rolling_hr_mean': ('simulated_HR', 'mean')\n",
    "    }\n",
    "    for new_col, (base_col, func) in roll_config.items():\n",
    "        data[new_col] = getattr(data[base_col].rolling(window_size, min_periods=1), func)()\n",
    "    \n",
    "    # Safe expanding quantile function\n",
    "    def safe_expanding_quantile(s):\n",
    "        return s.expanding().quantile(0.75).shift().fillna(0)\n",
    "    \n",
    "    # Optional new feature: Rolling Energy Standard Deviation\n",
    "    if 'joint_energy' in data.columns:\n",
    "        data['rolling_energy_std'] = data['joint_energy'].rolling(window=window_size, min_periods=1).std(ddof=0)\n",
    "        logging.info(f\"Created 'rolling_energy_std' with sample: {data['rolling_energy_std'].head(10).tolist()}\")\n",
    "        logging.info(f\"Created 'rolling_energy_std' with window {window_size}.\")\n",
    "    else:\n",
    "        logging.warning(\"Column 'joint_energy' missing for 'rolling_energy_std'.\")\n",
    "    new_cols.append('rolling_energy_std')\n",
    "    \n",
    "    # Vectorized exhaustion features\n",
    "    data['exhaustion_lag1'] = data['by_trial_exhaustion_score'].shift(1)\n",
    "    data['ema_exhaustion'] = data['by_trial_exhaustion_score'].ewm(span=10, adjust=False).mean()\n",
    "    data['rolling_exhaustion'] = data['by_trial_exhaustion_score'].rolling(rolling_window, min_periods=1).sum()\n",
    "    \n",
    "    # Injury risk calculation\n",
    "    data['injury_risk'] = (data['rolling_exhaustion'] > safe_expanding_quantile(data['rolling_exhaustion'])).astype(int)\n",
    "    new_cols += ['exhaustion_lag1', 'ema_exhaustion', 'rolling_exhaustion', 'injury_risk']\n",
    "\n",
    "    # Joint features computed per joint & side\n",
    "    joints = ['ANKLE', 'WRIST', 'ELBOW', 'KNEE', 'HIP']\n",
    "    sides = ['L', 'R']\n",
    "    dt = data['by_trial_time'].diff().replace(0, np.nan)\n",
    "    for joint in joints:\n",
    "        for side in sides:\n",
    "            joint_name = f\"{side}_{joint}\"\n",
    "            score_col = f'{joint_name}_energy_by_trial_exhaustion_score'\n",
    "            if score_col not in data.columns:\n",
    "                continue\n",
    "            data[f'{joint_name}_exhaustion_rate'] = data[score_col].diff() / dt\n",
    "            data[f'{joint_name}_rolling_exhaustion'] = data[score_col].rolling(rolling_window, min_periods=1).sum()\n",
    "            rolling_series = data[f'{joint_name}_rolling_exhaustion']\n",
    "            data[f'{joint_name}_injury_risk'] = (rolling_series > safe_expanding_quantile(rolling_series)).astype(int)\n",
    "            new_cols.extend([f'{joint_name}_exhaustion_rate', f'{joint_name}_rolling_exhaustion', f'{joint_name}_injury_risk'])\n",
    "\n",
    "    # Drop rows with NA in exhaustion_lag1\n",
    "    data.dropna(subset=['exhaustion_lag1'], inplace=True)\n",
    "    \n",
    "    # ----- NEW: Add Grouping-Based Aggregation Features in Feature Engineering -----\n",
    "    # Trial-level aggregations for feature engineering\n",
    "    if group_trial:\n",
    "        trial_aggs = data.groupby('trial_id').agg({\n",
    "            'by_trial_exhaustion_score': 'mean',\n",
    "            'injury_risk': 'mean'\n",
    "        }).rename(columns={\n",
    "            'by_trial_exhaustion_score': 'trial_mean_exhaustion_fe',\n",
    "            'injury_risk': 'trial_injury_rate_fe'\n",
    "        }).reset_index()\n",
    "        data = data.merge(trial_aggs, on='trial_id', how='left')\n",
    "        new_cols.extend(['trial_mean_exhaustion_fe', 'trial_injury_rate_fe'])\n",
    "        logging.info(\"Added trial-level aggregated features in feature_engineering: trial_mean_exhaustion_fe, trial_injury_rate_fe.\")\n",
    "\n",
    "    # Shot-phase-level aggregations\n",
    "    if group_shot_phase:\n",
    "        if 'shooting_phases' in data.columns:\n",
    "            shot_aggs = data.groupby(['trial_id', 'shooting_phases']).agg({\n",
    "                'by_trial_exhaustion_score': 'mean',\n",
    "                'injury_risk': 'mean'\n",
    "            }).rename(columns={\n",
    "                'by_trial_exhaustion_score': 'shot_phase_mean_exhaustion_fe',\n",
    "                'injury_risk': 'shot_phase_injury_rate_fe'\n",
    "            }).reset_index()\n",
    "            data = data.merge(shot_aggs, on=['trial_id', 'shooting_phases'], how='left')\n",
    "            new_cols.extend(['shot_phase_mean_exhaustion_fe', 'shot_phase_injury_rate_fe'])\n",
    "            logging.info(\"Added shot-phase-level aggregated features in feature_engineering: shot_phase_mean_exhaustion_fe, shot_phase_injury_rate_fe.\")\n",
    "        else:\n",
    "            logging.warning(\"Column 'shooting_phases' not found; skipping shot-phase grouping features in feature_engineering.\")\n",
    "    \n",
    "    # Remove duplicate columns, if any.\n",
    "    data = data.loc[:, ~data.columns.duplicated()]\n",
    "\n",
    "    ## Add datetime column using our new function\n",
    "    data = add_datetime_column(data, base_datetime=pd.Timestamp('2025-01-01 00:00:00'), \n",
    "                               break_seconds=0, trial_id_col='trial_id', freq='33ms')\n",
    "    print(\"Data columns for Darts processing:\", data.columns.tolist())\n",
    "        \n",
    "    if debug:\n",
    "        _print_debug_info(step, data, new_columns=new_cols, debug=debug)\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def make_exhaustion_monotonic_and_time_to_zero(data):\n",
    "    # (A) Cumulative exhaustion example\n",
    "    data['cumulative_exhaustion'] = (\n",
    "        data.groupby('participant_id')['by_trial_exhaustion_score']\n",
    "            .cumsum()\n",
    "    )\n",
    "    \n",
    "    # (B) Invert the raw exhaustion so that 1=Fresh, 0=Exhausted\n",
    "    data['remaining_capacity'] = 1.0 - data['by_trial_exhaustion_score']\n",
    "    \n",
    "    # (C) Compute \"time to 0 exhaustion\"\n",
    "    data = data.sort_values(['participant_id', 'continuous_frame_time']).reset_index(drop=True)\n",
    "    times = data['continuous_frame_time'].values\n",
    "    exhaustion = data['by_trial_exhaustion_score'].values\n",
    "    time_to_zero = np.full(len(data), np.nan)\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        if exhaustion[i] <= 0.0:\n",
    "            time_to_zero[i] = 0.0\n",
    "        else:\n",
    "            future_idxs = np.where(exhaustion[i:] <= 0.0)[0]\n",
    "            if len(future_idxs) > 0:\n",
    "                j = i + future_idxs[0]\n",
    "                time_to_zero[i] = times[j] - times[i]\n",
    "            else:\n",
    "                # If it never reaches 0 in the future, leave it as NaN or set a default\n",
    "                time_to_zero[i] = np.nan\n",
    "\n",
    "    data['time_to_zero_exhaustion'] = time_to_zero\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "def add_simulated_player_metrics(df, window=5, debug=False):\n",
    "    \"\"\"\n",
    "    Adds simulated player metrics to mimic heart rate and fatigue.\n",
    "    \n",
    "    New Metrics:\n",
    "      - simulated_HR_fake: Alternative simulated heart rate.\n",
    "      - fatigue_index_fake: Combined fatigue index.\n",
    "      - fatigue_rate_fake: Frame-by-frame rate of change of fatigue_index_fake.\n",
    "      - HR_variability_fake: Rolling standard deviation of simulated_HR_fake.\n",
    "    \n",
    "    Parameters:\n",
    "      - df (pd.DataFrame): DataFrame with required columns (e.g., by_trial_exhaustion_score, joint_energy, overall_exhaustion_score, dt).\n",
    "      - window (int): Rolling window size for HR variability.\n",
    "      - debug (bool): If True, prints detailed debug outputs.\n",
    "    \n",
    "    Returns:\n",
    "      - df (pd.DataFrame): DataFrame with new simulated metrics.\n",
    "    \"\"\"\n",
    "    step = \"add_simulated_player_metrics\"\n",
    "    new_cols = []\n",
    "    \n",
    "    # Use maximum joint_energy for scaling\n",
    "    max_joint_energy = df['joint_energy'].max() if 'joint_energy' in df.columns else 1\n",
    "    df['simulated_HR_fake'] = 60 + (df['by_trial_exhaustion_score'] * 2.0) + ((df['joint_energy'] / max_joint_energy) * 20)\n",
    "    new_cols.append('simulated_HR_fake')\n",
    "    \n",
    "    df['fatigue_index_fake'] = df['overall_exhaustion_score'] + ((df['simulated_HR_fake'] - 60) / 100)\n",
    "    new_cols.append('fatigue_index_fake')\n",
    "    \n",
    "    df['fatigue_rate_fake'] = df['fatigue_index_fake'].diff() / df['dt']\n",
    "    df['fatigue_rate_fake'] = df['fatigue_rate_fake'].fillna(0)\n",
    "    new_cols.append('fatigue_rate_fake')\n",
    "    \n",
    "    df['HR_variability_fake'] = df['simulated_HR_fake'].rolling(window=window, min_periods=1).std()\n",
    "    new_cols.append('HR_variability_fake')\n",
    "    \n",
    "    _print_debug_info(step, df, new_columns=new_cols, debug=debug)\n",
    "    return df\n",
    "\n",
    "\n",
    "def check_and_drop_nulls(df, columns_to_drop=None, df_name=\"DataFrame\", debug=False):\n",
    "    \"\"\"\n",
    "    Prints a summary of null values for all columns in the DataFrame.\n",
    "    Optionally drops rows with nulls in the specified columns and then prints\n",
    "    the updated null summary.\n",
    "\n",
    "    Parameters:\n",
    "      df (pd.DataFrame): The DataFrame to check.\n",
    "      columns_to_drop (list, optional): List of columns in which, if nulls are present,\n",
    "                                        the corresponding rows should be dropped.\n",
    "                                        If None, no rows will be dropped.\n",
    "      df_name (str, optional): Name for the DataFrame used in print messages.\n",
    "\n",
    "    Returns:\n",
    "      pd.DataFrame: The resulting DataFrame after dropping rows (if columns_to_drop is provided).\n",
    "    \"\"\"\n",
    "    total_rows = len(df)\n",
    "    print(f\"\\nNull summary for {df_name}: Total Rows = {total_rows}\")\n",
    "    \n",
    "    # Print summary for all columns.\n",
    "    null_sums = df.isnull().sum()\n",
    "    for col, count in null_sums.items():\n",
    "        percent = (count / total_rows) * 100\n",
    "        if debug:\n",
    "            print(f\"{col}: {count} nulls, {percent:.2f}% null\")\n",
    "    \n",
    "    if columns_to_drop:\n",
    "        df = df.dropna(subset=columns_to_drop)\n",
    "        print(f\"\\nAfter dropping rows with nulls in columns: {columns_to_drop}\")\n",
    "        total_rows = len(df)\n",
    "        print(f\"Total Rows = {total_rows}\")\n",
    "        null_sums = df.isnull().sum()\n",
    "        for col, count in null_sums.items():\n",
    "            percent = (count / total_rows) * 100\n",
    "            print(f\"{col}: {count} nulls, {percent:.2f}% null\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def clip_outliers(df, column, k=3.0, min_value=0.0):\n",
    "    \"\"\"\n",
    "    Clips outliers in a given column to the IQR-based range [max(Q1 - k*IQR, min_value), Q3 + k*IQR].\n",
    "    Ensures the lower bound is at least `min_value`, preventing negative values if min_value=0.0.\n",
    "    \n",
    "    Parameters:\n",
    "      - df (pd.DataFrame): The DataFrame with data.\n",
    "      - column (str): Name of the column to clip.\n",
    "      - k (float): Multiplier for the IQR to define the clip boundaries.\n",
    "      - min_value (float): Minimum allowed value (defaults to 0.0).\n",
    "    \n",
    "    Returns:\n",
    "      - df (pd.DataFrame): Modified DataFrame with clipped column values.\n",
    "    \"\"\"\n",
    "    q1 = df[column].quantile(0.25)\n",
    "    q3 = df[column].quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    \n",
    "    # Compute the IQR-based bounds\n",
    "    lower_bound = q1 - k * iqr\n",
    "    upper_bound = q3 + k * iqr\n",
    "    \n",
    "    # Enforce a hard floor at min_value (e.g., 0.0)\n",
    "    lower_bound = max(lower_bound, min_value)\n",
    "    \n",
    "    df[column] = df[column].clip(lower_bound, upper_bound)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def prepare_base_datasets(csv_path, json_path, debug=False):\n",
    "    # 1) Load and base‐engineer:\n",
    "    data = load_data(csv_path, json_path, debug=debug)\n",
    "    data = prepare_joint_features(data, debug=debug)\n",
    "    data = feature_engineering(data, debug=debug)\n",
    "    data = check_and_drop_nulls(data,\n",
    "                                columns_to_drop=['energy_acceleration', 'exhaustion_rate'],\n",
    "                                df_name=\"Final Data\")\n",
    "\n",
    "    # 2) Trial‐summary:\n",
    "    trial = prepare_joint_features(data, debug=debug, group_trial=True)\n",
    "    trial = feature_engineering(trial, debug=debug, group_trial=True)\n",
    "    trial_summary = summarize_data(\n",
    "        data,\n",
    "        groupby_cols=['trial_id'],\n",
    "        lag_columns=[\n",
    "        'joint_energy', 'L_ELBOW_energy', 'R_ELBOW_energy', 'L_WRIST_energy', 'R_WRIST_energy',\n",
    "        'L_KNEE_energy', 'R_KNEE_energy', 'L_HIP_energy', 'R_HIP_energy',\n",
    "        'joint_power', 'L_ELBOW_ongoing_power', 'R_ELBOW_ongoing_power', \n",
    "        'L_WRIST_ongoing_power', 'R_WRIST_ongoing_power',\n",
    "        'L_KNEE_ongoing_power', 'R_KNEE_ongoing_power', 'L_HIP_ongoing_power', 'R_HIP_ongoing_power',\n",
    "        'elbow_asymmetry', 'wrist_asymmetry', 'knee_asymmetry', 'hip_asymmetry',\n",
    "        'L_ELBOW_angle', 'R_ELBOW_angle', 'L_WRIST_angle', 'R_WRIST_angle', \n",
    "        'L_KNEE_angle', 'R_KNEE_angle',\n",
    "        'L_SHOULDER_ROM', 'R_SHOULDER_ROM', 'L_WRIST_ROM', 'R_WRIST_ROM',\n",
    "        'L_KNEE_ROM', 'R_KNEE_ROM', 'L_HIP_ROM', 'R_HIP_ROM',\n",
    "        'exhaustion_rate', 'by_trial_exhaustion_score', 'injury_risk',\n",
    "        'energy_acceleration', 'power_avg_5', 'rolling_power_std', 'rolling_hr_mean',\n",
    "        'simulated_HR',\n",
    "        'player_height_in_meters', 'player_weight__in_kg'\n",
    "    ],\n",
    "        rolling_window=3,\n",
    "        agg_columns= [\n",
    "        'joint_energy', 'L_ELBOW_energy', 'R_ELBOW_energy', 'L_WRIST_energy', 'R_WRIST_energy',\n",
    "        'L_KNEE_energy', 'R_KNEE_energy', 'L_HIP_energy', 'R_HIP_energy',\n",
    "        'joint_power', 'L_ELBOW_ongoing_power', 'R_ELBOW_ongoing_power', \n",
    "        'L_WRIST_ongoing_power', 'R_WRIST_ongoing_power',\n",
    "        'L_KNEE_ongoing_power', 'R_KNEE_ongoing_power', 'L_HIP_ongoing_power', 'R_HIP_ongoing_power',\n",
    "        'elbow_asymmetry', 'wrist_asymmetry', 'knee_asymmetry', 'hip_asymmetry',\n",
    "        'L_ELBOW_angle', 'R_ELBOW_angle', 'L_WRIST_angle', 'R_WRIST_angle', \n",
    "        'L_KNEE_angle', 'R_KNEE_angle',\n",
    "        'L_SHOULDER_ROM', 'R_SHOULDER_ROM', 'L_WRIST_ROM', 'R_WRIST_ROM',\n",
    "        'L_KNEE_ROM', 'R_KNEE_ROM', 'L_HIP_ROM', 'R_HIP_ROM',\n",
    "        'exhaustion_rate', 'by_trial_exhaustion_score', 'injury_risk',\n",
    "        'energy_acceleration', 'power_avg_5', 'rolling_power_std', 'rolling_hr_mean',\n",
    "        'simulated_HR',\n",
    "        'player_height_in_meters', 'player_weight__in_kg'\n",
    "    ],\n",
    "        global_lag=True,\n",
    "        debug=debug\n",
    "    )\n",
    "\n",
    "    # 3) Shot‐phase‐summary:\n",
    "    shot = prepare_joint_features(data, debug=debug, group_trial=True, group_shot_phase=True)\n",
    "    shot = feature_engineering(shot, debug=debug, group_trial=True, group_shot_phase=True)\n",
    "    shot_summary = summarize_data(\n",
    "        shot,\n",
    "        groupby_cols=['trial_id', 'shooting_phases'],\n",
    "        lag_columns=[\n",
    "        'joint_energy', 'L_ELBOW_energy', 'R_ELBOW_energy', 'L_WRIST_energy', 'R_WRIST_energy',\n",
    "        'L_KNEE_energy', 'R_KNEE_energy', 'L_HIP_energy', 'R_HIP_energy',\n",
    "        'joint_power', 'L_ELBOW_ongoing_power', 'R_ELBOW_ongoing_power', \n",
    "        'L_WRIST_ongoing_power', 'R_WRIST_ongoing_power',\n",
    "        'L_KNEE_ongoing_power', 'R_KNEE_ongoing_power', 'L_HIP_ongoing_power', 'R_HIP_ongoing_power',\n",
    "        'elbow_asymmetry', 'wrist_asymmetry', 'knee_asymmetry', 'hip_asymmetry',\n",
    "        'L_ELBOW_angle', 'R_ELBOW_angle', 'L_WRIST_angle', 'R_WRIST_angle', \n",
    "        'L_KNEE_angle', 'R_KNEE_angle',\n",
    "        'L_SHOULDER_ROM', 'R_SHOULDER_ROM', 'L_WRIST_ROM', 'R_WRIST_ROM',\n",
    "        'L_KNEE_ROM', 'R_KNEE_ROM', 'L_HIP_ROM', 'R_HIP_ROM',\n",
    "        'exhaustion_rate', 'by_trial_exhaustion_score', 'injury_risk',\n",
    "        'energy_acceleration', 'power_avg_5', 'rolling_power_std', 'rolling_hr_mean',\n",
    "        'simulated_HR',\n",
    "        'player_height_in_meters', 'player_weight__in_kg'\n",
    "    ],\n",
    "        rolling_window=3,\n",
    "        agg_columns= [\n",
    "        'joint_energy', 'L_ELBOW_energy', 'R_ELBOW_energy', 'L_WRIST_energy', 'R_WRIST_energy',\n",
    "        'L_KNEE_energy', 'R_KNEE_energy', 'L_HIP_energy', 'R_HIP_energy',\n",
    "        'joint_power', 'L_ELBOW_ongoing_power', 'R_ELBOW_ongoing_power', \n",
    "        'L_WRIST_ongoing_power', 'R_WRIST_ongoing_power',\n",
    "        'L_KNEE_ongoing_power', 'R_KNEE_ongoing_power', 'L_HIP_ongoing_power', 'R_HIP_ongoing_power',\n",
    "        'elbow_asymmetry', 'wrist_asymmetry', 'knee_asymmetry', 'hip_asymmetry',\n",
    "        'L_ELBOW_angle', 'R_ELBOW_angle', 'L_WRIST_angle', 'R_WRIST_angle', \n",
    "        'L_KNEE_angle', 'R_KNEE_angle',\n",
    "        'L_SHOULDER_ROM', 'R_SHOULDER_ROM', 'L_WRIST_ROM', 'R_WRIST_ROM',\n",
    "        'L_KNEE_ROM', 'R_KNEE_ROM', 'L_HIP_ROM', 'R_HIP_ROM',\n",
    "        'exhaustion_rate', 'by_trial_exhaustion_score', 'injury_risk',\n",
    "        'energy_acceleration', 'power_avg_5', 'rolling_power_std', 'rolling_hr_mean',\n",
    "        'simulated_HR',\n",
    "        'player_height_in_meters', 'player_weight__in_kg'\n",
    "    ],\n",
    "        phase_list=[\"arm_cock\", \"arm_release\", \"leg_cock\", \"wrist_release\"],\n",
    "        debug=debug\n",
    "    )\n",
    "    data = check_and_drop_nulls(data, columns_to_drop=['energy_acceleration', 'exhaustion_rate'], df_name=\"Final Data\")\n",
    "    # Example usage\n",
    "    joints = ['ANKLE', 'WRIST', 'ELBOW', 'KNEE', 'HIP']\n",
    "    sides = ['L', 'R']\n",
    "    data = clip_outliers(data, 'exhaustion_rate', k=3.0, min_value=0.0)\n",
    "    data = clip_outliers(data, 'joint_power', k=3.0, min_value=0.0)\n",
    "    data = clip_outliers(data, 'joint_energy', k=3.0, min_value=0.0)\n",
    "    for joint in joints:\n",
    "        for side in sides:\n",
    "            data = clip_outliers(data, f'{side}_{joint}_exhaustion_rate', k=3.0, min_value=0.0)\n",
    "            \n",
    "    return data, trial_summary, shot_summary\n",
    "\n",
    "\n",
    "\n",
    "def joint_specific_analysis(data, joint_energy_columns, debug=False):\n",
    "    \"\"\"\n",
    "    Performs joint-specific analysis including:\n",
    "      - Energy distribution per joint.\n",
    "      - Injury risk analysis for each joint.\n",
    "      - Cumulative energy accumulation patterns.\n",
    "    \n",
    "    Parameters:\n",
    "      - data (pd.DataFrame): Input DataFrame.\n",
    "      - joint_energy_columns (list): List of joint energy column names.\n",
    "      - debug (bool): If True, prints debug information.\n",
    "    \"\"\"\n",
    "    step = \"joint_specific_analysis\"\n",
    "    # Energy distribution across joints\n",
    "    joint_energy_melted = data[joint_energy_columns].melt(var_name='Joint', value_name='Energy')\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    order = joint_energy_melted.groupby('Joint')['Energy'].median().sort_values().index\n",
    "    sns.boxplot(x='Joint', y='Energy', data=joint_energy_melted, order=order)\n",
    "    plt.title('Joint Energy Distributions (Sorted by Median Energy)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "    if debug:\n",
    "        logging.info(\"Displayed boxplot for joint energy distributions.\")\n",
    "    else:\n",
    "        logging.info(\"Energy distribution plot displayed.\")\n",
    "    \n",
    "    # Injury risk analysis: only run if 'injury_risk' exists.\n",
    "    if 'injury_risk' not in data.columns:\n",
    "        logging.warning(\"Column 'injury_risk' not found; skipping injury risk analysis in joint_specific_analysis.\")\n",
    "    else:\n",
    "        num_plots = len(joint_energy_columns)\n",
    "        ncols = 4\n",
    "        nrows = int(np.ceil(num_plots / ncols))\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        for i, joint in enumerate(joint_energy_columns, 1):\n",
    "            plt.subplot(nrows, ncols, i)\n",
    "            sns.boxplot(x='injury_risk', y=joint, data=data)\n",
    "            plt.title(f'{joint.split(\"_\")[0].title()} Energy')\n",
    "            plt.tight_layout()\n",
    "        plt.suptitle('Joint Energy Distributions by Injury Risk', y=1.02)\n",
    "        plt.show()\n",
    "        if debug:\n",
    "            logging.info(\"Displayed injury risk analysis plots for joint energy.\")\n",
    "        else:\n",
    "            logging.info(\"Injury risk analysis plots displayed.\")\n",
    "    \n",
    "    # Cumulative energy accumulation patterns\n",
    "    joint_cumulative = data.groupby('participant_id')[joint_energy_columns].cumsum()\n",
    "    joint_cumulative['time'] = data['continuous_frame_time']\n",
    "    joint_cumulative_melted = joint_cumulative.melt(id_vars='time', var_name='Joint', value_name='Cumulative Energy')\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    sns.lineplot(x='time', y='Cumulative Energy', hue='Joint', \n",
    "                 data=joint_cumulative_melted, estimator='median', errorbar=None)\n",
    "    plt.title('Cumulative Joint Energy Over Time (Median Across Participants)')\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('Cumulative Energy')\n",
    "    plt.show()\n",
    "    if debug:\n",
    "        logging.info(\"Displayed cumulative joint energy plot.\")\n",
    "    else:\n",
    "        logging.info(\"Cumulative energy plot displayed.\")\n",
    "    \n",
    "    _print_debug_info(step, data, debug=debug)\n",
    "\n",
    "\n",
    "def movement_pattern_analysis(data, debug=False):\n",
    "    \"\"\"\n",
    "    Performs movement pattern analysis:\n",
    "      - Angular velocity histograms with KDE.\n",
    "      - Asymmetry analysis via pairplot.\n",
    "    \n",
    "    Parameters:\n",
    "      - data (pd.DataFrame): Input DataFrame.\n",
    "      - debug (bool): If True, prints debug information.\n",
    "    \"\"\"\n",
    "    step = \"movement_pattern_analysis\"\n",
    "    # Angular velocity analysis\n",
    "    angular_columns = [col for col in data.columns if '_angular_velocity' in col]\n",
    "    if angular_columns:\n",
    "        fig, axes = plt.subplots(nrows=4, ncols=4, figsize=(20, 15))\n",
    "        axes = axes.flatten()\n",
    "        for ax, col in zip(axes, angular_columns):\n",
    "            sns.histplot(data[col], ax=ax, kde=True)\n",
    "            ax.set_title(f'{col.split(\"_\")[0].title()} Angular Velocity')\n",
    "        for j in range(len(angular_columns), len(axes)):\n",
    "            fig.delaxes(axes[j])\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        logging.info(\"Displayed angular velocity histograms.\")\n",
    "    else:\n",
    "        logging.info(\"No angular velocity columns found.\")\n",
    "    \n",
    "    # Asymmetry analysis\n",
    "    asymmetry_metrics = [col for col in data.columns if 'asymmetry' in col]\n",
    "    if 'injury_risk' in data.columns and asymmetry_metrics:\n",
    "        sns.pairplot(data[asymmetry_metrics + ['injury_risk']], hue='injury_risk', corner=True)\n",
    "        plt.suptitle('Joint Asymmetry Relationships with Injury Risk', y=1.02)\n",
    "        plt.show()\n",
    "        logging.info(\"Displayed asymmetry pairplot.\")\n",
    "    else:\n",
    "        logging.info(\"Required columns for asymmetry analysis not found.\")\n",
    "    \n",
    "    _print_debug_info(step, data, debug=debug)\n",
    "\n",
    "\n",
    "def temporal_analysis_enhancements(data, debug=False):\n",
    "    \"\"\"\n",
    "    Performs temporal analysis enhancements:\n",
    "      - Computes lagged correlations between joint energy and exhaustion score.\n",
    "      - Plots autocorrelation of joint energy.\n",
    "    \n",
    "    Parameters:\n",
    "      - data (pd.DataFrame): Input DataFrame.\n",
    "      - debug (bool): If True, prints debug information.\n",
    "    \"\"\"\n",
    "    step = \"temporal_analysis_enhancements\"\n",
    "    max_lag = 10\n",
    "    lagged_corrs = []\n",
    "    for lag in range(1, max_lag + 1):\n",
    "        corr_val = data['joint_energy'].corr(data['by_trial_exhaustion_score'].shift(lag))\n",
    "        lagged_corrs.append(corr_val)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, max_lag + 1), lagged_corrs, marker='o')\n",
    "    plt.title('Lagged Correlation Between Joint Energy and Exhaustion Score')\n",
    "    plt.xlabel('Time Lag (periods)')\n",
    "    plt.ylabel('Correlation Coefficient')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    from statsmodels.graphics.tsaplots import plot_acf\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plot_acf(data['joint_energy'].dropna(), lags=50, alpha=0.05)\n",
    "    plt.title('Joint Energy Autocorrelation')\n",
    "    plt.xlabel('Lags')\n",
    "    plt.ylabel('Autocorrelation')\n",
    "    plt.show()\n",
    "    \n",
    "    _print_debug_info(step, data, debug=debug)\n",
    "\n",
    "\n",
    "def multivariate_analysis(data, joint_energy_columns, debug=False):\n",
    "    \"\"\"\n",
    "    Performs multivariate analysis separately for left- and right-sided joints:\n",
    "      - 3D visualization of joint energy interactions for each side.\n",
    "      - KMeans clustering on selected features for each side.\n",
    "    \n",
    "    Parameters:\n",
    "      - data (pd.DataFrame): Input DataFrame.\n",
    "      - joint_energy_columns (list): List of all joint energy columns.\n",
    "      - debug (bool): If True, prints debug information.\n",
    "    \"\"\"\n",
    "    step = \"multivariate_analysis\"\n",
    "\n",
    "    # --- 3D Visualization: Left Side ---\n",
    "    required_left = ['L_ELBOW_energy', 'L_KNEE_energy', 'L_ANKLE_energy', 'injury_risk']\n",
    "    if all(col in data.columns for col in required_left):\n",
    "        from mpl_toolkits.mplot3d import Axes3D\n",
    "        fig = plt.figure(figsize=(12, 8))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        scatter = ax.scatter(data['L_ELBOW_energy'], \n",
    "                             data['L_KNEE_energy'], \n",
    "                             data['L_ANKLE_energy'], \n",
    "                             c=data['injury_risk'],\n",
    "                             cmap='viridis',\n",
    "                             alpha=0.6)\n",
    "        ax.set_xlabel('L Elbow Energy')\n",
    "        ax.set_ylabel('L Knee Energy')\n",
    "        ax.set_zlabel('L Ankle Energy')\n",
    "        plt.title('3D Joint Energy Space (Left Side) with Injury Risk Coloring')\n",
    "        plt.colorbar(scatter, label='Injury Risk')\n",
    "        plt.show()\n",
    "    else:\n",
    "        logging.info(\"Required left-side columns for 3D analysis not found; skipping left side 3D plot.\")\n",
    "\n",
    "    # --- 3D Visualization: Right Side ---\n",
    "    required_right = ['R_ELBOW_energy', 'R_KNEE_energy', 'R_ANKLE_energy', 'injury_risk']\n",
    "    if all(col in data.columns for col in required_right):\n",
    "        from mpl_toolkits.mplot3d import Axes3D\n",
    "        fig = plt.figure(figsize=(12, 8))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        scatter = ax.scatter(data['R_ELBOW_energy'], \n",
    "                             data['R_KNEE_energy'], \n",
    "                             data['R_ANKLE_energy'], \n",
    "                             c=data['injury_risk'],\n",
    "                             cmap='viridis',\n",
    "                             alpha=0.6)\n",
    "        ax.set_xlabel('R Elbow Energy')\n",
    "        ax.set_ylabel('R Knee Energy')\n",
    "        ax.set_zlabel('R Ankle Energy')\n",
    "        plt.title('3D Joint Energy Space (Right Side) with Injury Risk Coloring')\n",
    "        plt.colorbar(scatter, label='Injury Risk')\n",
    "        plt.show()\n",
    "    else:\n",
    "        logging.info(\"Required right-side columns for 3D analysis not found; skipping right side 3D plot.\")\n",
    "\n",
    "    # --- Clustering Analysis: Left Side ---\n",
    "    left_features = ['L_ELBOW_energy', 'L_KNEE_energy', 'L_ANKLE_energy']\n",
    "    # Optionally include asymmetry features if desired (they compare L vs R)\n",
    "    left_features = [feat for feat in left_features if feat in data.columns]\n",
    "    if left_features:\n",
    "        from sklearn.cluster import KMeans\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        X_left = data[left_features].dropna()\n",
    "        X_left_scaled = StandardScaler().fit_transform(X_left)\n",
    "        kmeans_left = KMeans(n_clusters=3, random_state=42).fit(X_left_scaled)\n",
    "        data.loc[X_left.index, 'left_movement_cluster'] = kmeans_left.labels_\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.scatterplot(x='L_ELBOW_energy', y='L_KNEE_energy', hue='left_movement_cluster', \n",
    "                        data=data, palette='viridis', alpha=0.6)\n",
    "        plt.title('Left Side Movement Clusters in Elbow-Knee Energy Space')\n",
    "        plt.xlabel('L Elbow Energy')\n",
    "        plt.ylabel('L Knee Energy')\n",
    "        plt.show()\n",
    "    else:\n",
    "        logging.info(\"Not enough left-side features available for clustering analysis.\")\n",
    "\n",
    "    # --- Clustering Analysis: Right Side ---\n",
    "    right_features = ['R_ELBOW_energy', 'R_KNEE_energy', 'R_ANKLE_energy']\n",
    "    right_features = [feat for feat in right_features if feat in data.columns]\n",
    "    if right_features:\n",
    "        from sklearn.cluster import KMeans\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        X_right = data[right_features].dropna()\n",
    "        X_right_scaled = StandardScaler().fit_transform(X_right)\n",
    "        kmeans_right = KMeans(n_clusters=3, random_state=42).fit(X_right_scaled)\n",
    "        data.loc[X_right.index, 'right_movement_cluster'] = kmeans_right.labels_\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.scatterplot(x='R_ELBOW_energy', y='R_KNEE_energy', hue='right_movement_cluster', \n",
    "                        data=data, palette='viridis', alpha=0.6)\n",
    "        plt.title('Right Side Movement Clusters in Elbow-Knee Energy Space')\n",
    "        plt.xlabel('R Elbow Energy')\n",
    "        plt.ylabel('R Knee Energy')\n",
    "        plt.show()\n",
    "    else:\n",
    "        logging.info(\"Not enough right-side features available for clustering analysis.\")\n",
    "\n",
    "    _print_debug_info(step, data, debug=debug)\n",
    "\n",
    "\n",
    "\n",
    "def statistical_testing(data, joint_energy_columns, debug=False):\n",
    "    \"\"\"\n",
    "    Performs Mann-Whitney U tests on each joint energy metric between low and high injury risk groups.\n",
    "    \n",
    "    Parameters:\n",
    "      - data (pd.DataFrame): Input DataFrame.\n",
    "      - joint_energy_columns (list): List of joint energy column names.\n",
    "      - debug (bool): If True, prints detailed test outputs.\n",
    "    \n",
    "    Returns:\n",
    "      - results_df (pd.DataFrame): Summary table of test statistics.\n",
    "    \"\"\"\n",
    "    from scipy.stats import mannwhitneyu\n",
    "    step = \"statistical_testing\"\n",
    "    results = []\n",
    "    for joint in joint_energy_columns:\n",
    "        if joint in data.columns and 'injury_risk' in data.columns:\n",
    "            low_risk = data[data['injury_risk'] == 0][joint]\n",
    "            high_risk = data[data['injury_risk'] == 1][joint]\n",
    "            stat, p = mannwhitneyu(low_risk, high_risk, alternative='two-sided')\n",
    "            effect_size = stat / (len(low_risk) * len(high_risk)) if (len(low_risk) * len(high_risk)) > 0 else np.nan\n",
    "            results.append({\n",
    "                'Joint': joint.split('_')[0],\n",
    "                'U Statistic': stat,\n",
    "                'p-value': p,\n",
    "                'Effect Size': effect_size\n",
    "            })\n",
    "    results_df = pd.DataFrame(results).sort_values('p-value')\n",
    "    logging.info(\"Mann-Whitney U Test Results:\")\n",
    "    logging.info(results_df)\n",
    "    _print_debug_info(step, data, debug=debug)\n",
    "    return results_df\n",
    "\n",
    "\n",
    "\n",
    "## Utility function to print debug information\n",
    "def validate_target_column(target_array, target_name, expected_type):\n",
    "    \"\"\"\n",
    "    Validates that the target array meets the expected type.\n",
    "    \n",
    "    Parameters:\n",
    "      - target_array (np.ndarray): The target values.\n",
    "      - target_name (str): Name of the target (for logging).\n",
    "      - expected_type (str): Either \"binary\" or \"continuous\".\n",
    "    \n",
    "    Raises:\n",
    "      - ValueError: If the target values do not match the expectation.\n",
    "    \"\"\"\n",
    "    unique_vals = np.unique(target_array)\n",
    "    logging.info(f\"Validating target '{target_name}': unique values = {unique_vals}\")\n",
    "    \n",
    "    if expected_type == \"binary\":\n",
    "        # Check that the unique values are a subset of {0, 1}\n",
    "        if not set(unique_vals).issubset({0, 1, 0.0, 1.0}):\n",
    "            raise ValueError(f\"Target '{target_name}' expected to be binary, but got values: {unique_vals}\")\n",
    "    elif expected_type == \"continuous\":\n",
    "        # For continuous targets, if there are only two unique values (0 and 1), warn or error.\n",
    "        if set(unique_vals).issubset({0, 1, 0.0, 1.0}):\n",
    "            raise ValueError(f\"Target '{target_name}' expected to be continuous, but appears binary: {unique_vals}\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown expected_type '{expected_type}' for target '{target_name}'\")\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import shapiro, probplot\n",
    "\n",
    "def check_distribution_and_zscore(df, col_list, zscore_suffix='_zscore'):\n",
    "    \"\"\"\n",
    "    For each column in col_list, this function:\n",
    "      - Prints summary stats\n",
    "      - Plots histogram + KDE\n",
    "      - Performs Shapiro-Wilk test for normality\n",
    "      - Plots QQ-plot\n",
    "      - Creates a z-score column in the DataFrame\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for col in col_list:\n",
    "        print(f\"\\n--- Analyzing Column: {col} ---\")\n",
    "\n",
    "        # 1. Summary statistics\n",
    "        desc = df[col].describe()\n",
    "        print(desc)\n",
    "\n",
    "        # 2. Plot histogram and KDE\n",
    "        plt.figure(figsize=(7,4))\n",
    "        df[col].dropna().hist(bins=30, alpha=0.5, density=True, label=f\"{col} Histogram\")\n",
    "        df[col].dropna().plot(kind='kde', label=f\"{col} KDE\")\n",
    "        plt.title(f\"Distribution of {col}\")\n",
    "        plt.xlabel(col)\n",
    "        plt.ylabel(\"Density\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "        # 3. Normality check (Shapiro-Wilk)\n",
    "        #    Note: Shapiro test can be sensitive to large n. If you have a lot of rows,\n",
    "        #    you might want to sample or use another normality test.\n",
    "        sample_for_shapiro = df[col].dropna()\n",
    "        if len(sample_for_shapiro) > 3:\n",
    "            stat, p_value = shapiro(sample_for_shapiro)\n",
    "            print(f\"Shapiro-Wilk p-value for {col}: {p_value:.4f}\")\n",
    "        else:\n",
    "            stat, p_value = np.nan, np.nan\n",
    "            print(f\"Not enough non-null values for Shapiro test in {col}.\")\n",
    "\n",
    "        # 4. QQ-plot (to visually check normality)\n",
    "        plt.figure(figsize=(5,5))\n",
    "        probplot(sample_for_shapiro, dist=\"norm\", plot=plt)\n",
    "        plt.title(f\"QQ-Plot of {col}\")\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "        # 5. Compute z-score and attach to DataFrame\n",
    "        mean_val = df[col].mean()\n",
    "        std_val = df[col].std(ddof=0)  # population-like; use ddof=1 for sample\n",
    "        z_column = col + zscore_suffix\n",
    "        df[z_column] = (df[col] - mean_val) / (std_val if std_val != 0 else 1e-8)\n",
    "\n",
    "        # identify potential outliers: e.g., absolute z-score > 3\n",
    "        outliers = df[z_column].abs() > 3\n",
    "        outlier_count = outliers.sum()\n",
    "        outlier_pct = 100.0 * outlier_count / len(df)\n",
    "        print(f\"Number of potential outliers for {col} (|z|>3): {outlier_count} ({outlier_pct:.2f}%)\")\n",
    "\n",
    "        # Store stats in a results dictionary if you want to use them later\n",
    "        results[col] = {\n",
    "            'mean': mean_val,\n",
    "            'std': std_val,\n",
    "            'shapiro_stat': stat,\n",
    "            'shapiro_p': p_value,\n",
    "            'outlier_count': outlier_count,\n",
    "            'outlier_pct': outlier_pct\n",
    "        }\n",
    "    return df, pd.DataFrame(results).T\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# NEW FUNCTION FOR SCALING DATA BEFORE LSTM\n",
    "###############################################################################\n",
    "def scale_for_lstm(df, columns_to_scale, method=\"standard\", debug=False):\n",
    "    \"\"\"\n",
    "    Scales specified columns for LSTM training, either by z-score (StandardScaler)\n",
    "    or min-max normalization (MinMaxScaler).\n",
    "    \n",
    "    Parameters:\n",
    "      - df (pd.DataFrame): The DataFrame containing the features to be scaled.\n",
    "      - columns_to_scale (list of str): Columns to scale.\n",
    "      - method (str): \"standard\" for StandardScaler (z-score), \"minmax\" for MinMaxScaler.\n",
    "      - debug (bool): If True, prints debug information about scaling.\n",
    "      \n",
    "    Returns:\n",
    "      - scaled_df (pd.DataFrame): Copy of the original DataFrame with scaled columns.\n",
    "      - scaler (object): The fitted scaler object (useful if you need inverse transforms later).\n",
    "    \n",
    "    Note:\n",
    "      This function does NOT drop or rename columns; it overwrites them with scaled values\n",
    "      to keep them numeric but on a uniform scale.\n",
    "    \"\"\"\n",
    "    from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "    \n",
    "    # 1) Make a copy so we don't mutate the original\n",
    "    scaled_df = df.copy()\n",
    "    \n",
    "    # 2) Choose which scaler to use\n",
    "    if method == \"standard\":\n",
    "        scaler = StandardScaler()\n",
    "        if debug:\n",
    "            print(\"Using StandardScaler for columns:\", columns_to_scale)\n",
    "    elif method == \"minmax\":\n",
    "        scaler = MinMaxScaler()\n",
    "        if debug:\n",
    "            print(\"Using MinMaxScaler for columns:\", columns_to_scale)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown scaling method '{method}'. Choose 'standard' or 'minmax'.\")\n",
    "    \n",
    "    # 3) Fit and transform the specified columns\n",
    "    # Drop NaNs to avoid issues with partial columns, then reassign\n",
    "    subset = scaled_df[columns_to_scale].dropna()\n",
    "    scaled_values = scaler.fit_transform(subset)\n",
    "    \n",
    "    # Put scaled values back into the DataFrame (at the same indices)\n",
    "    scaled_df.loc[subset.index, columns_to_scale] = scaled_values\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"Scaler mean_ (if standard) or data_min_ (if minmax): {scaler.mean_ if method=='standard' else scaler.data_min_}\")\n",
    "        print(f\"Scaler scale_ (if standard) or data_range_ (if minmax): {scaler.scale_ if method=='standard' else scaler.data_range_}\")\n",
    "        print(\"Sample after scaling:\\n\", scaled_df[columns_to_scale].head(5))\n",
    "    \n",
    "    return scaled_df, scaler\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# MAIN SCRIPT\n",
    "###############################################################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the main pipeline with debug output enabled.\n",
    "\n",
    "    debug=True\n",
    " \n",
    "    # \"\"\"\n",
    "    # Main processing pipeline:\n",
    "    #   1. Loads and merges data.\n",
    "    #   2. Prepares joint features.\n",
    "    #   3. Performs feature engineering.\n",
    "    #   4. Adds simulated player metrics.\n",
    "    #   5. Executes various analyses (joint-specific, movement pattern, temporal, multivariate, statistical, and fatigue-injury interaction).\n",
    "    \n",
    "    # Parameters:\n",
    "    #   - debug (bool): Controls verbose debug output.\n",
    "    #   - csv_path (str): Path to input CSV file.\n",
    "    #   - json_path (str): Path to participant info JSON.\n",
    "    # \"\"\"\n",
    "    csv_path=\"../../data/processed/final_granular_dataset.csv\"\n",
    "    json_path=\"../../data/basketball/freethrow/participant_information.json\"\n",
    "    data, trial_summary_data, shot_phase_summary_data = prepare_base_datasets(csv_path, json_path, debug=debug)\n",
    "        \n",
    "\n",
    "    # data = add_simulated_player_metrics(data, window=5, debug=debug)\n",
    "\n",
    "    # === NEW ADDITION: Validate Target Columns ===\n",
    "    try:\n",
    "        # 'by_trial_exhaustion_score' should be continuous\n",
    "        validate_target_column(data['by_trial_exhaustion_score'].values, \n",
    "                               'by_trial_exhaustion_score', 'continuous')\n",
    "        # 'injury_risk' should be binary\n",
    "        validate_target_column(data['injury_risk'].values, \n",
    "                               'injury_risk', 'binary')\n",
    "        # 'by_trial_exhaustion_score' should be continuous\n",
    "        validate_target_column(trial_summary_data['by_trial_exhaustion_score'].values, \n",
    "                               'by_trial_exhaustion_score', 'continuous')\n",
    "        # 'injury_risk' should be binary\n",
    "        validate_target_column(trial_summary_data['injury_risk'].values, \n",
    "                               'injury_risk', 'binary')\n",
    "        # 'by_trial_exhaustion_score' should be continuous\n",
    "        validate_target_column(shot_phase_summary_data['by_trial_exhaustion_score'].values, \n",
    "                               'by_trial_exhaustion_score', 'continuous')\n",
    "        # 'injury_risk' should be binary\n",
    "        validate_target_column(shot_phase_summary_data['injury_risk'].values, \n",
    "                               'injury_risk', 'binary')\n",
    "    except ValueError as e:\n",
    "        logging.error(f\"Target validation error: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "\n",
    "    # For demonstration, define features/targets (you can adjust these as needed)\n",
    "    features_exhaustion = [\n",
    "        'joint_power', \n",
    "        'joint_energy', \n",
    "        'elbow_asymmetry',  \n",
    "        'L_WRIST_angle', 'R_WRIST_angle',  # Updated: removed \"wrist_angle\"\n",
    "        'exhaustion_lag1', \n",
    "        'power_avg_5',\n",
    "        'simulated_HR',\n",
    "        'player_height_in_meters',\n",
    "        'player_weight__in_kg'\n",
    "    ]\n",
    "    target_exhaustion = 'by_trial_exhaustion_score'\n",
    "\n",
    "    features_injury = [\n",
    "        'joint_power', \n",
    "        'joint_energy', \n",
    "        'elbow_asymmetry',  \n",
    "        'knee_asymmetry', \n",
    "        'L_WRIST_angle', 'R_WRIST_angle',  # Updated: removed \"wrist_angle\"\n",
    "        'exhaustion_lag1', \n",
    "        'power_avg_5',\n",
    "        'simulated_HR',\n",
    "        'player_height_in_meters',\n",
    "        'player_weight__in_kg'\n",
    "    ]\n",
    "    target_injury = 'injury_risk'\n",
    "\n",
    "    \n",
    "    # Identify joint energy columns (excluding the aggregated 'joint_energy')\n",
    "    joint_energy_columns = [\n",
    "        col for col in data.columns\n",
    "        if '_energy' in col and not ('by_trial' in col or 'overall' in col) and col != 'joint_energy'\n",
    "    ]\n",
    "    logging.info(f\"Joint Energy Columns after excluding 'joint_energy' ({len(joint_energy_columns)}): {joint_energy_columns}\")\n",
    "    \n",
    "    # # Execute analysis functions\n",
    "    # joint_specific_analysis(data, joint_energy_columns, debug=debug)\n",
    "    # movement_pattern_analysis(data, debug=debug)\n",
    "    # temporal_analysis_enhancements(data, debug=debug)\n",
    "    # multivariate_analysis(data, joint_energy_columns, debug=debug)\n",
    "    # statistical_testing(data, joint_energy_columns, debug=debug)\n",
    "    # Drop rows with NaNs in the key columns (e.g., energy_acceleration and exhaustion_rate)\n",
    "\n",
    "    #check if energy_acceleration and exhaustion_rate are dropped\n",
    "    print(\"Columns in the final data:\", data.columns.tolist())\n",
    "    print(\"Number of NaNs in energy_acceleration:\", data['energy_acceleration'].isna().sum())\n",
    "    print(\"Number of NaNs in exhaustion_rate:\", data['exhaustion_rate'].isna().sum())\n",
    "    logging.info(\"Processing pipeline completed successfully.\")\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    # right after feature_engineering(data, ...) or wherever both columns exist:\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # choose the joint‐specific exhaustion rate you want to check:\n",
    "    x = data['joint_power']\n",
    "    y = data['exhaustion_rate']  # or any other '<side>_<JOINT>_exhaustion_rate'\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.scatter(x, y, alpha=0.6)\n",
    "    plt.xlabel('Total Joint Power')\n",
    "    plt.ylabel('L Elbow Exhaustion Rate')\n",
    "    plt.title('Scatter: Joint Power vs. L Elbow Exhaustion Rate')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    data[['L_ELBOW_energy_by_trial_exhaustion_score','by_trial_time']].head(20)\n",
    "\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Example usage with your DataFrame and “x”, “y” columns:\n",
    "    # ------------------------------------------------------------\n",
    "    # Suppose `data` is the DataFrame from your pipeline.\n",
    "\n",
    "\n",
    "    columns_to_check = [\"joint_power\", \"exhaustion_rate\"]\n",
    "    data, dist_results = check_distribution_and_zscore(data, columns_to_check)\n",
    "\n",
    "    print(\"\\before scaling z-score columns added to DataFrame:\")\n",
    "    print([col for col in data.columns if col.endswith('_zscore')])\n",
    "\n",
    "    print(\"\\nbefore scaling Distribution check summary:\")\n",
    "    print(dist_results)\n",
    "    \n",
    "    # Suppose we want to scale just these two columns for baseline LSTM:\n",
    "    columns_to_scale = [\"joint_power\", \"exhaustion_rate\"]\n",
    "\n",
    "    # 1) Use StandardScaler (z-score) as a baseline:\n",
    "    scaled_data, standard_scaler = scale_for_lstm(\n",
    "        df=data,\n",
    "        columns_to_scale=columns_to_scale,\n",
    "        method=\"standard\",  # or \"minmax\"\n",
    "        debug=True\n",
    "    )\n",
    "    \n",
    "    columns_to_check = [\"joint_power\", \"exhaustion_rate\"]\n",
    "    data, dist_results = check_distribution_and_zscore(scaled_data, columns_to_check)\n",
    "\n",
    "    print(\"\\nFinal z-score columns added to DataFrame:\")\n",
    "    print([col for col in data.columns if col.endswith('_zscore')])\n",
    "\n",
    "    print(\"\\nDistribution check summary:\")\n",
    "    print(dist_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ml/feature_selection/feature_selection.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ml/feature_selection/feature_selection.py\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import logging\n",
    "import sys\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "import shap\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "from joblib import Parallel, delayed\n",
    "from ml.load_and_prepare_data.load_data_and_analyze import (\n",
    "    load_data, prepare_joint_features, feature_engineering, summarize_data, check_and_drop_nulls\n",
    ")\n",
    "    \n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "\n",
    "\n",
    "def prepare_base_datasets(csv_path, json_path, debug=False):\n",
    "    data = load_data(csv_path, json_path, debug=debug)\n",
    "    data = prepare_joint_features(data, debug=debug)\n",
    "    data = feature_engineering(data, debug=debug)\n",
    "    data = check_and_drop_nulls(data,\n",
    "                                columns_to_drop=['energy_acceleration', 'exhaustion_rate'],\n",
    "                                df_name=\"Final Data\")\n",
    "    trial = prepare_joint_features(data, debug=debug, group_trial=True)\n",
    "    trial = feature_engineering(trial, debug=debug, group_trial=True)\n",
    "    trial_summary = summarize_data(\n",
    "        data,\n",
    "        groupby_cols=['trial_id'],\n",
    "        lag_columns=[\n",
    "            'joint_energy', 'L_ELBOW_energy', 'R_ELBOW_energy', 'L_WRIST_energy', 'R_WRIST_energy',\n",
    "            'L_KNEE_energy', 'R_KNEE_energy', 'L_HIP_energy', 'R_HIP_energy',\n",
    "            'joint_power', 'L_ELBOW_ongoing_power', 'R_ELBOW_ongoing_power', \n",
    "            'L_WRIST_ongoing_power', 'R_WRIST_ongoing_power',\n",
    "            'L_KNEE_ongoing_power', 'R_KNEE_ongoing_power', 'L_HIP_ongoing_power', 'R_HIP_ongoing_power',\n",
    "            'elbow_asymmetry', 'wrist_asymmetry', 'knee_asymmetry', 'hip_asymmetry',\n",
    "            'L_ELBOW_angle', 'R_ELBOW_angle', 'L_WRIST_angle', 'R_WRIST_angle', \n",
    "            'L_KNEE_angle', 'R_KNEE_angle',\n",
    "            'L_SHOULDER_ROM', 'R_SHOULDER_ROM', 'L_WRIST_ROM', 'R_WRIST_ROM',\n",
    "            'L_KNEE_ROM', 'R_KNEE_ROM', 'L_HIP_ROM', 'R_HIP_ROM',\n",
    "            'exhaustion_rate', 'by_trial_exhaustion_score', 'injury_risk',\n",
    "            'energy_acceleration', 'power_avg_5', 'rolling_power_std', 'rolling_hr_mean',\n",
    "            'simulated_HR',\n",
    "            'player_height_in_meters', 'player_weight__in_kg'\n",
    "        ],\n",
    "        rolling_window=3,\n",
    "        agg_columns=[\n",
    "            'joint_energy', 'L_ELBOW_energy', 'R_ELBOW_energy', 'L_WRIST_energy', 'R_WRIST_energy',\n",
    "            'L_KNEE_energy', 'R_KNEE_energy', 'L_HIP_energy', 'R_HIP_energy',\n",
    "            'joint_power', 'L_ELBOW_ongoing_power', 'R_ELBOW_ongoing_power', \n",
    "            'L_WRIST_ongoing_power', 'R_WRIST_ongoing_power',\n",
    "            'L_KNEE_ongoing_power', 'R_KNEE_ongoing_power', 'L_HIP_ongoing_power', 'R_HIP_ongoing_power',\n",
    "            'elbow_asymmetry', 'wrist_asymmetry', 'knee_asymmetry', 'hip_asymmetry',\n",
    "            'L_ELBOW_angle', 'R_ELBOW_angle', 'L_WRIST_angle', 'R_WRIST_angle', \n",
    "            'L_KNEE_angle', 'R_KNEE_angle',\n",
    "            'L_SHOULDER_ROM', 'R_SHOULDER_ROM', 'L_WRIST_ROM', 'R_WRIST_ROM',\n",
    "            'L_KNEE_ROM', 'R_KNEE_ROM', 'L_HIP_ROM', 'R_HIP_ROM',\n",
    "            'exhaustion_rate', 'by_trial_exhaustion_score', 'injury_risk',\n",
    "            'energy_acceleration', 'power_avg_5', 'rolling_power_std', 'rolling_hr_mean',\n",
    "            'simulated_HR',\n",
    "            'player_height_in_meters', 'player_weight__in_kg'\n",
    "        ],\n",
    "        global_lag=True,\n",
    "        debug=debug\n",
    "    )\n",
    "    shot = prepare_joint_features(data, debug=debug, group_trial=True, group_shot_phase=True)\n",
    "    shot = feature_engineering(shot, debug=debug, group_trial=True, group_shot_phase=True)\n",
    "    shot_summary = summarize_data(\n",
    "        shot,\n",
    "        groupby_cols=['trial_id', 'shooting_phases'],\n",
    "        lag_columns=[\n",
    "            'joint_energy', 'L_ELBOW_energy', 'R_ELBOW_energy', 'L_WRIST_energy', 'R_WRIST_energy',\n",
    "            'L_KNEE_energy', 'R_KNEE_energy', 'L_HIP_energy', 'R_HIP_energy',\n",
    "            'joint_power', 'L_ELBOW_ongoing_power', 'R_ELBOW_ongoing_power', \n",
    "            'L_WRIST_ongoing_power', 'R_WRIST_ongoing_power',\n",
    "            'L_KNEE_ongoing_power', 'R_KNEE_ongoing_power', 'L_HIP_ongoing_power', 'R_HIP_ongoing_power',\n",
    "            'elbow_asymmetry', 'wrist_asymmetry', 'knee_asymmetry', 'hip_asymmetry',\n",
    "            'L_ELBOW_angle', 'R_ELBOW_angle', 'L_WRIST_angle', 'R_WRIST_angle', \n",
    "            'L_KNEE_angle', 'R_KNEE_angle',\n",
    "            'L_SHOULDER_ROM', 'R_SHOULDER_ROM', 'L_WRIST_ROM', 'R_WRIST_ROM',\n",
    "            'L_KNEE_ROM', 'R_KNEE_ROM', 'L_HIP_ROM', 'R_HIP_ROM',\n",
    "            'exhaustion_rate', 'by_trial_exhaustion_score', 'injury_risk',\n",
    "            'energy_acceleration', 'power_avg_5', 'rolling_power_std', 'rolling_hr_mean',\n",
    "            'simulated_HR',\n",
    "            'player_height_in_meters', 'player_weight__in_kg'\n",
    "        ],\n",
    "        rolling_window=3,\n",
    "        agg_columns=[\n",
    "            'joint_energy', 'L_ELBOW_energy', 'R_ELBOW_energy', 'L_WRIST_energy', 'R_WRIST_energy',\n",
    "            'L_KNEE_energy', 'R_KNEE_energy', 'L_HIP_energy', 'R_HIP_energy',\n",
    "            'joint_power', 'L_ELBOW_ongoing_power', 'R_ELBOW_ongoing_power', \n",
    "            'L_WRIST_ongoing_power', 'R_WRIST_ongoing_power',\n",
    "            'L_KNEE_ongoing_power', 'R_KNEE_ongoing_power', 'L_HIP_ongoing_power', 'R_HIP_ongoing_power',\n",
    "            'elbow_asymmetry', 'wrist_asymmetry', 'knee_asymmetry', 'hip_asymmetry',\n",
    "            'L_ELBOW_angle', 'R_ELBOW_angle', 'L_WRIST_angle', 'R_WRIST_angle', \n",
    "            'L_KNEE_angle', 'R_KNEE_angle',\n",
    "            'L_SHOULDER_ROM', 'R_SHOULDER_ROM', 'L_WRIST_ROM', 'R_WRIST_ROM',\n",
    "            'L_KNEE_ROM', 'R_KNEE_ROM', 'L_HIP_ROM', 'R_HIP_ROM',\n",
    "            'exhaustion_rate', 'by_trial_exhaustion_score', 'injury_risk',\n",
    "            'energy_acceleration', 'power_avg_5', 'rolling_power_std', 'rolling_hr_mean',\n",
    "            'simulated_HR',\n",
    "            'player_height_in_meters', 'player_weight__in_kg'\n",
    "        ],\n",
    "        phase_list=[\"arm_cock\", \"arm_release\", \"leg_cock\", \"wrist_release\"],\n",
    "        debug=debug\n",
    "    )\n",
    "    data = check_and_drop_nulls(data, columns_to_drop=['energy_acceleration', 'exhaustion_rate'], df_name=\"Final Data\")\n",
    "    return data, trial_summary, shot_summary\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def validate_features(features, df, context):\n",
    "    missing_features = [f for f in features if f not in df.columns]\n",
    "    if missing_features:\n",
    "        logging.error(f\"{context} - Missing features: {missing_features}\")\n",
    "        raise ValueError(f\"{context} - Missing features: {missing_features}\")\n",
    "\n",
    "\n",
    "def select_top_n_features_from_df(features_df, n_top=5, sort_by=\"Consensus_Rank\", ascending=True):\n",
    "    sorted_df = features_df.sort_values(by=sort_by, ascending=ascending)\n",
    "    top_features = sorted_df.head(n_top)['Feature'].tolist()\n",
    "    return top_features\n",
    "\n",
    "\n",
    "def save_top_features(results, output_dir=\"feature_lists/base\", importance_threshold=0.5, n_top=10):\n",
    "    \"\"\"\n",
    "    Saves the top features that exceed the importance threshold to pickle files for each target.\n",
    "    If the filtering yields fewer than n_top features, falls back to selecting the top n_top based on Consensus_Rank.\n",
    "    \"\"\"\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for target, (combined, _) in results.items():\n",
    "        combined['Consensus_Rank'] = (\n",
    "            combined['Perm_Importance'].rank(ascending=False) +\n",
    "            combined['RFE_Rank'] +\n",
    "            combined['SHAP_Importance'].rank(ascending=False)\n",
    "        )\n",
    "        logging.debug(f\"[{target}] Combined shape before filtering: {combined.shape}\")\n",
    "        filtered_features = combined[\n",
    "            (combined['Perm_Importance'] > importance_threshold) &\n",
    "            (combined['SHAP_Importance'] > importance_threshold)\n",
    "        ].sort_values(\"Consensus_Rank\")\n",
    "        logging.debug(f\"[{target}] Shape after threshold filtering: {filtered_features.shape}\")\n",
    "        if len(filtered_features) < n_top:\n",
    "            logging.warning(f\"[{target}] Only {len(filtered_features)} features exceeded the thresholds; falling back to top {n_top} by Consensus_Rank.\")\n",
    "            filtered_features = combined.sort_values(\"Consensus_Rank\", ascending=True).head(n_top)\n",
    "        else:\n",
    "            filtered_features = filtered_features.head(n_top)\n",
    "        top_features = filtered_features['Feature'].tolist()\n",
    "        logging.debug(f\"[{target}] Top features to be saved: {top_features}\")\n",
    "        filename = Path(output_dir) / f\"{target}_model_feature_list.pkl\"\n",
    "        pd.to_pickle(top_features, filename)\n",
    "        logging.info(f\"✅ {target}: Saved features at {filename}. Top features: {top_features}\")\n",
    "\n",
    "\n",
    "\n",
    "def load_top_features(target, feature_dir, df, n_top=10):\n",
    "    filename = Path(feature_dir) / f\"{target}_model_feature_list.pkl\"\n",
    "    logging.debug(f\"Attempting to load feature list for target '{target}' from file: {filename}\")\n",
    "    if not filename.exists():\n",
    "        logging.error(f\"Feature list for '{target}' not found at {filename}\")\n",
    "        dir_contents = list(Path(feature_dir).iterdir())\n",
    "        logging.error(f\"Contents of {feature_dir}: {dir_contents}\")\n",
    "        sys.exit(1)\n",
    "    features = pd.read_pickle(filename)\n",
    "    logging.info(f\"Loaded feature list for '{target}': {features}\")\n",
    "    missing_features = [f for f in features if f not in df.columns]\n",
    "    if missing_features:\n",
    "        logging.error(f\"Features missing in DataFrame for {target}: {missing_features}\")\n",
    "        raise KeyError(f\"{missing_features} not in index\")\n",
    "    if n_top is not None:\n",
    "        features = features[:n_top]\n",
    "    return features\n",
    "\n",
    "\n",
    "def perform_feature_importance_analysis(data, features, target, n_features_to_select=5, debug=False):\n",
    "    filtered_features = [f for f in features if f in data.columns]\n",
    "    missing = [f for f in features if f not in data.columns]\n",
    "    if missing:\n",
    "        logging.warning(f\"The following features are missing and will be ignored: {missing}\")\n",
    "    features = filtered_features\n",
    "    X = data[features].fillna(method='ffill').fillna(method='bfill')\n",
    "    y = data[target].fillna(method='ffill').fillna(method='bfill')\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "    rf = RandomForestRegressor(n_estimators=50, random_state=42)\n",
    "    rf.fit(X_train, y_train)\n",
    "    perm_result = permutation_importance(rf, X_test, y_test, n_repeats=10, random_state=42)\n",
    "    perm_df = pd.DataFrame({\n",
    "        'Feature': features,\n",
    "        'Perm_Importance': perm_result.importances_mean\n",
    "    })\n",
    "    from sklearn.feature_selection import RFE\n",
    "    rfe_selector = RFE(estimator=rf, n_features_to_select=n_features_to_select, step=1)\n",
    "    rfe_selector.fit(X_train, y_train)\n",
    "    rfe_df = pd.DataFrame({\n",
    "        'Feature': features,\n",
    "        'RFE_Rank': rfe_selector.ranking_,\n",
    "        'RFE_Support': rfe_selector.support_\n",
    "    })\n",
    "    combined = perm_df.merge(rfe_df, on='Feature')\n",
    "    explainer = shap.TreeExplainer(rf)\n",
    "    sample_size = min(100, X_test.shape[0])\n",
    "    X_test_sampled = X_test.sample(sample_size, random_state=42)\n",
    "    shap_values = explainer.shap_values(X_test_sampled)\n",
    "    shap_abs = np.abs(shap_values).mean(axis=0)\n",
    "    shap_df = pd.DataFrame({\n",
    "        'Feature': features,\n",
    "        'SHAP_Importance': shap_abs\n",
    "    })\n",
    "    combined = combined.merge(shap_df, on='Feature')\n",
    "    if debug:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(x='Perm_Importance', y='Feature', data=combined.nlargest(20, 'Perm_Importance'))\n",
    "        plt.title('Top Permutation Importances')\n",
    "        plt.xlabel('Mean Permutation Importance')\n",
    "        plt.ylabel('Feature')\n",
    "        plt.show()\n",
    "        shap.summary_plot(shap_values, X_test_sampled, plot_type=\"bar\", max_display=20)\n",
    "    return combined, rf\n",
    "\n",
    "\n",
    "def analyze_and_display_top_features(results, n_top=5):\n",
    "    for target, (combined, _) in results.items():\n",
    "        print(f\"\\n=== Feature Analysis for Target: {target.upper()} ===\")\n",
    "        perm_top = combined.nlargest(n_top, 'Perm_Importance')['Feature'].tolist()\n",
    "        rfe_top = combined[combined['RFE_Support']]['Feature'].tolist()\n",
    "        shap_top = combined.nlargest(n_top, 'SHAP_Importance')['Feature'].tolist()\n",
    "        combined['Consensus_Rank'] = (\n",
    "            combined['Perm_Importance'].rank(ascending=False) +\n",
    "            combined['RFE_Rank'] +\n",
    "            combined['SHAP_Importance'].rank(ascending=False)\n",
    "        )\n",
    "        consensus_top = combined.nsmallest(n_top, 'Consensus_Rank')['Feature'].tolist()\n",
    "        print(f\"Permutation Top {n_top}: {perm_top}\")\n",
    "        print(f\"RFE Selected Features: {rfe_top}\")\n",
    "        print(f\"SHAP Top {n_top}: {shap_top}\")\n",
    "        print(f\"Consensus Top {n_top}: {consensus_top}\")\n",
    "\n",
    "\n",
    "def check_for_invalid_values(df):\n",
    "    numeric_df = df.select_dtypes(include=[np.number])\n",
    "    inf_mask = numeric_df.isin([np.inf, -np.inf])\n",
    "    if inf_mask.any().any():\n",
    "        logging.error(f\"Found infinite values in columns: {numeric_df.columns[inf_mask.any()].tolist()}\")\n",
    "    na_mask = numeric_df.isna()\n",
    "    if na_mask.any().any():\n",
    "        logging.error(f\"Found NA values in columns: {numeric_df.columns[na_mask.any()].tolist()}\")\n",
    "    extreme_mask = (numeric_df.abs() > 1e30).any(axis=1)\n",
    "    if extreme_mask.any():\n",
    "        logging.error(f\"Found extreme values (>1e30) in rows: {numeric_df.index[extreme_mask].tolist()}\")\n",
    "    return inf_mask.sum().sum() + na_mask.sum().sum() + extreme_mask.sum()\n",
    "\n",
    "\n",
    "def analyze_joint_injury_features(results, joint, output_dir, n_top=10, importance_threshold=0.0):\n",
    "    joint_keys = [key for key in results if f\"_{joint}_injury_risk\" in key]\n",
    "    if not joint_keys:\n",
    "        logging.warning(f\"No injury models found for joint: {joint}\")\n",
    "        return [], None\n",
    "    df_list = []\n",
    "    for key in joint_keys:\n",
    "        combined_df, _ = results[key]\n",
    "        df_list.append(combined_df.copy())\n",
    "    concat_df = pd.concat(df_list, axis=0)\n",
    "    agg_df = concat_df.groupby(\"Feature\", as_index=False).agg({\n",
    "        'Perm_Importance': 'mean',\n",
    "        'SHAP_Importance': 'mean',\n",
    "        'RFE_Rank': 'mean',\n",
    "        'RFE_Support': 'max'\n",
    "    })\n",
    "    agg_df['Consensus_Rank'] = (\n",
    "        agg_df['Perm_Importance'].rank(ascending=False) +\n",
    "        agg_df['RFE_Rank'].rank(ascending=True) +\n",
    "        agg_df['SHAP_Importance'].rank(ascending=False)\n",
    "    )\n",
    "    agg_df = agg_df.sort_values(\"Consensus_Rank\")\n",
    "    top_n = agg_df.nsmallest(n_top, \"Consensus_Rank\")\n",
    "    filtered_top = top_n[\n",
    "        (top_n['Perm_Importance'] > importance_threshold) &\n",
    "        (top_n['SHAP_Importance'] > importance_threshold)\n",
    "    ]\n",
    "    top_features = filtered_top['Feature'].tolist()\n",
    "    filename = Path(output_dir) / f\"{joint}_aggregated_top_features.pkl\"\n",
    "    pd.to_pickle(top_features, filename)\n",
    "    logging.info(f\"Aggregated and saved top features for joint {joint} at {filename}: {top_features}\")\n",
    "    return top_features, agg_df\n",
    "\n",
    "\n",
    "def run_feature_importance_analysis(dataset, features, targets, base_output_dir, output_subdir, \n",
    "                                    debug=False, dataset_label=\"Dataset\", importance_threshold=0.0, n_top=10):\n",
    "    features = [f for f in features if f in dataset.columns]\n",
    "    missing_targets = [t for t in targets if t not in dataset.columns]\n",
    "    if missing_targets:\n",
    "        logging.error(f\"{dataset_label} missing targets: {missing_targets}\")\n",
    "        sys.exit(1)\n",
    "    results = dict(zip(\n",
    "        targets,\n",
    "        Parallel(n_jobs=-1)(delayed(perform_feature_importance_analysis)(dataset, features, target, debug=debug)\n",
    "                             for target in targets)\n",
    "    ))\n",
    "    analyze_and_display_top_features(results, n_top=n_top)\n",
    "    output_path = Path(base_output_dir) / output_subdir\n",
    "    save_top_features(results, output_dir=str(output_path), importance_threshold=importance_threshold, n_top=n_top)\n",
    "\n",
    "\n",
    "def run_feature_import_and_load_top_features(dataset, features, targets, base_output_dir, output_subdir,\n",
    "                                             debug=False, dataset_label=\"Dataset\", importance_threshold=0.0, n_top=10,\n",
    "                                             run_analysis=True):\n",
    "    output_path = Path(base_output_dir) / output_subdir\n",
    "    if run_analysis:\n",
    "        logging.info(f\"Running feature importance analysis for {dataset_label}.\")\n",
    "        run_feature_importance_analysis(\n",
    "            dataset=dataset,\n",
    "            features=features,\n",
    "            targets=targets,\n",
    "            base_output_dir=base_output_dir,\n",
    "            output_subdir=output_subdir,\n",
    "            debug=debug,\n",
    "            dataset_label=dataset_label,\n",
    "            importance_threshold=importance_threshold,\n",
    "            n_top=n_top\n",
    "        )\n",
    "    else:\n",
    "        logging.info(f\"Skipping analysis for {dataset_label}; using pre-saved feature lists from {output_path}\")\n",
    "    loaded_features = {}\n",
    "    for t in targets:\n",
    "        try:\n",
    "            loaded = load_top_features(t, feature_dir=output_path, df=dataset, n_top=n_top)\n",
    "            loaded_features[t] = loaded\n",
    "            logging.info(f\"[{dataset_label}] Loaded top features for target '{t}': {loaded}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading features for target '{t}': {e}\")\n",
    "    return loaded_features\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Main Script: Running Three Separate Analyses ---\n",
    "if __name__ == \"__main__\":\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "    from ml.load_and_prepare_data.load_data_and_analyze import (\n",
    "        load_data, prepare_joint_features, feature_engineering, summarize_data, check_and_drop_nulls\n",
    "    )\n",
    "    debug = True\n",
    "    importance_threshold = 0.01\n",
    "    csv_path = \"../../data/processed/final_granular_dataset.csv\"\n",
    "    json_path = \"../../data/basketball/freethrow/participant_information.json\"\n",
    "    output_dir = \"../../data/Deep_Learning_Final\"\n",
    "    \n",
    "    base_feature_dir = os.path.join(output_dir, \"feature_lists/base\")\n",
    "    trial_feature_dir = os.path.join(output_dir, \"feature_lists/trial_summary\")\n",
    "    shot_feature_dir = os.path.join(output_dir, \"feature_lists/shot_phase_summary\")\n",
    "    \n",
    "    data, trial_df, shot_df = prepare_base_datasets(csv_path, json_path, debug=debug)\n",
    "    numeric_features = data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    summary_targets = ['exhaustion_rate', 'injury_risk']\n",
    "    trial_summary_features = [col for col in trial_df.columns if col not in summary_targets]\n",
    "    trial_summary_features = [col for col in trial_summary_features if col in numeric_features]\n",
    "    shot_summary_features = [col for col in shot_df.columns if col not in summary_targets]\n",
    "    shot_summary_features = [col for col in shot_summary_features if col in numeric_features]\n",
    "    # ========================================\n",
    "    # 1) Overall Base Dataset (including Joint-Specific Targets)\n",
    "    # ========================================\n",
    "    features = [\n",
    "        'joint_energy', 'joint_power', 'energy_acceleration',\n",
    "        'elbow_asymmetry', 'hip_asymmetry', 'ankle_asymmetry', 'wrist_asymmetry', 'knee_asymmetry', \n",
    "        '1stfinger_asymmetry', '5thfinger_asymmetry',\n",
    "        'elbow_power_ratio', 'hip_power_ratio', 'ankle_power_ratio', 'wrist_power_ratio', \n",
    "        'knee_power_ratio', '1stfinger_power_ratio', '5thfinger_power_ratio',\n",
    "        'L_KNEE_ROM', 'L_KNEE_ROM_deviation', 'L_KNEE_ROM_extreme',\n",
    "        'R_KNEE_ROM', 'R_KNEE_ROM_deviation', 'R_KNEE_ROM_extreme',\n",
    "        'L_SHOULDER_ROM', 'L_SHOULDER_ROM_deviation', 'L_SHOULDER_ROM_extreme',\n",
    "        'R_SHOULDER_ROM', 'R_SHOULDER_ROM_deviation', 'R_SHOULDER_ROM_extreme',\n",
    "        'L_HIP_ROM', 'L_HIP_ROM_deviation', 'L_HIP_ROM_extreme',\n",
    "        'R_HIP_ROM', 'R_HIP_ROM_deviation', 'R_HIP_ROM_extreme',\n",
    "        'L_ANKLE_ROM', 'L_ANKLE_ROM_deviation', 'L_ANKLE_ROM_extreme',\n",
    "        'R_ANKLE_ROM', 'R_ANKLE_ROM_deviation', 'R_ANKLE_ROM_extreme',\n",
    "        'exhaustion_lag1', 'power_avg_5', 'rolling_power_std', 'rolling_hr_mean',\n",
    "        'time_since_start', 'ema_exhaustion', 'rolling_exhaustion', 'rolling_energy_std',\n",
    "        'simulated_HR',\n",
    "        'player_height_in_meters', 'player_weight__in_kg'\n",
    "    ]\n",
    "    base_targets = ['exhaustion_rate', 'injury_risk']\n",
    "    joints = ['ANKLE', 'WRIST', 'ELBOW', 'KNEE', 'HIP']\n",
    "    joint_injury_targets = [f\"{side}_{joint}_injury_risk\" for joint in joints for side in ['L', 'R']]\n",
    "    joint_exhaustion_targets = [f\"{side}_{joint}_exhaustion_rate\" for joint in joints for side in ['L', 'R']]\n",
    "    joint_targets = joint_injury_targets + joint_exhaustion_targets\n",
    "    all_targets = base_targets + joint_targets\n",
    "\n",
    "    logging.info(\"=== Base Dataset Analysis (Overall + Joint-Specific) ===\")\n",
    "    base_loaded_features = run_feature_import_and_load_top_features(\n",
    "        dataset=data,\n",
    "        features=features,\n",
    "        targets=all_targets,\n",
    "        base_output_dir=output_dir,\n",
    "        output_subdir=\"feature_lists/base\",\n",
    "        debug=debug,\n",
    "        dataset_label=\"Base Data\",\n",
    "        importance_threshold=importance_threshold,\n",
    "        n_top=10,\n",
    "        run_analysis=False\n",
    "    )\n",
    "    print(f\"Base Loaded Features: {base_loaded_features}\")\n",
    "    \n",
    "    joint_feature_dict = {}\n",
    "    for target in joint_targets:\n",
    "        try:\n",
    "            feat_loaded = base_loaded_features.get(target, [])\n",
    "            logging.info(f\"Test Load: Features for {target}: {feat_loaded}\")\n",
    "            joint_feature_dict[target] = feat_loaded\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading features for {target}: {e}\")\n",
    "    \n",
    "    # ========================================\n",
    "    # 2) Trial Summary Dataset Analysis\n",
    "    # ========================================\n",
    "\n",
    "    \n",
    "    logging.info(\"=== Trial Summary Dataset Analysis ===\")\n",
    "    trial_loaded_features = run_feature_import_and_load_top_features(\n",
    "        dataset=trial_df,\n",
    "        features=trial_summary_features,\n",
    "        targets=summary_targets,\n",
    "        base_output_dir=output_dir,\n",
    "        output_subdir=\"feature_lists/trial_summary\",\n",
    "        debug=debug,\n",
    "        dataset_label=\"Trial Summary Data\",\n",
    "        importance_threshold=importance_threshold,\n",
    "        n_top=10,\n",
    "        run_analysis=False\n",
    "    )\n",
    "    exhaustion_rate_list = trial_loaded_features.get('exhaustion_rate', [])\n",
    "    injury_risk_list = trial_loaded_features.get('injury_risk', [])\n",
    "\n",
    "    \n",
    "    # ========================================\n",
    "    # 3) Shot Phase Summary Dataset Analysis\n",
    "    # ========================================\n",
    "\n",
    "    \n",
    "    logging.info(\"=== Shot Phase Summary Dataset Analysis ===\")\n",
    "    shot_loaded_features = run_feature_import_and_load_top_features(\n",
    "        dataset=shot_df,\n",
    "        features=shot_summary_features,\n",
    "        targets=summary_targets,\n",
    "        base_output_dir=output_dir,\n",
    "        output_subdir=\"feature_lists/shot_phase_summary\",\n",
    "        debug=debug,\n",
    "        dataset_label=\"Shot Phase Summary Data\",\n",
    "        importance_threshold=importance_threshold,\n",
    "        n_top=10,\n",
    "        run_analysis=False\n",
    "    )\n",
    "    exhaustion_rate_list = shot_loaded_features.get('exhaustion_rate', [])\n",
    "    injury_risk_list = shot_loaded_features.get('injury_risk', [])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and Split (ensure that the preprocessing is correctly occuring to the right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ml/preprocess_train_predict/base_training.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ml/preprocess_train_predict/base_training.py\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import sys\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (mean_squared_error,\n",
    "    mean_absolute_error, r2_score, accuracy_score,\n",
    "    precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import logging\n",
    "from tensorflow.keras import Input\n",
    "from ml.load_and_prepare_data.load_data_and_analyze import (\n",
    "    load_data, prepare_joint_features, feature_engineering, summarize_data, check_and_drop_nulls)\n",
    "\n",
    "from ml.feature_selection.feature_selection import (\n",
    "    load_top_features, perform_feature_importance_analysis, save_top_features,\n",
    "    analyze_joint_injury_features, check_for_invalid_values,\n",
    "    perform_feature_importance_analysis, analyze_and_display_top_features, validate_features)\n",
    "\n",
    "\n",
    "\n",
    "# ==================== UTILS ====================\n",
    "       \n",
    "def temporal_train_test_split(data, test_size=0.2):\n",
    "    \"\"\"Time-based split maintaining temporal order\"\"\"\n",
    "    split_idx = int(len(data) * (1 - test_size))\n",
    "    train_data = data.iloc[:split_idx].copy()\n",
    "    test_data = data.iloc[split_idx:].copy()\n",
    "    logging.info(f\"Performed temporal train-test split with test size = {test_size}\")\n",
    "    logging.info(f\"Training data shape: {train_data.shape}, Testing data shape: {test_data.shape}\")\n",
    "    return train_data, test_data\n",
    "\n",
    "def scale_features(X_train, X_test):\n",
    "    \"\"\"\n",
    "    Scales features using StandardScaler.\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    logging.info(\"Features have been scaled using StandardScaler.\")\n",
    "    return X_train_scaled, X_test_scaled, scaler\n",
    "\n",
    "def create_sequences(X, y, timesteps):\n",
    "    \"\"\"\n",
    "    Creates sequences of data for LSTM input.\n",
    "    \"\"\"\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(timesteps, len(X)):\n",
    "        X_seq.append(X[i-timesteps:i])\n",
    "        y_seq.append(y[i])\n",
    "    X_seq = np.array(X_seq)\n",
    "    y_seq = np.array(y_seq)\n",
    "    logging.info(f\"Created LSTM sequences: {X_seq.shape}, {y_seq.shape}\")\n",
    "    return X_seq, y_seq\n",
    "# ==================== TRAINING FUNCTIONS ====================\n",
    "\n",
    "def train_exhaustion_model(train_data, test_data, features, timesteps, \n",
    "                           epochs=50, batch_size=32, early_stop_patience=5,\n",
    "                           num_lstm_layers=1, lstm_units=64, dropout_rate=0.2,\n",
    "                           dense_units=1, dense_activation=None, target_col=\"by_trial_exhaustion_score\"):\n",
    "    \"\"\"\n",
    "    Trains an exhaustion model (regression) with a separate target scaler.\n",
    "    \n",
    "    An optional parameter 'target_col' is added so that this function can be used\n",
    "    for joint-specific exhaustion targets if needed. The default is \"by_trial_exhaustion_score\".\n",
    "    \n",
    "    Parameters:\n",
    "      - train_data (DataFrame): Training set.\n",
    "      - test_data (DataFrame): Testing set.\n",
    "      - features (list): List of feature column names for exhaustion.\n",
    "      - timesteps (int): Number of past observations to include in each sequence.\n",
    "      - epochs (int): Number of training epochs.\n",
    "      - batch_size (int): Batch size for training.\n",
    "      - early_stop_patience (int): Patience for EarlyStopping callback.\n",
    "      - num_lstm_layers (int): Number of LSTM layers in the model.\n",
    "      - lstm_units (int): Number of units in each LSTM layer.\n",
    "      - dropout_rate (float): Dropout rate applied after each LSTM layer.\n",
    "      - dense_units (int): Number of units in the final Dense layer.\n",
    "      - dense_activation (str or None): Activation function for the Dense layer.\n",
    "      - target_col (str): The name of the target column to use. Default is \"by_trial_exhaustion_score\".\n",
    "      \n",
    "    Returns:\n",
    "      - model_exhaustion: Trained Keras model.\n",
    "      - scaler_exhaustion: Fitted scaler for the features.\n",
    "      - target_scaler: Fitted scaler for the target values.\n",
    "      - X_lstm_exhaustion_val, y_lstm_exhaustion_val: Validation sequences.\n",
    "    \"\"\"\n",
    "    # --- Debug: Log the feature list and available columns ---\n",
    "    logging.info(f\"Features provided for training exhaustion model: {features}\")\n",
    "    logging.info(f\"Available train_data columns: {train_data.columns.tolist()}\")\n",
    "    missing_features = [f for f in features if f not in train_data.columns]\n",
    "    if missing_features:\n",
    "        logging.error(f\"Missing features in train_data for target {target_col}: {missing_features}\")\n",
    "        raise KeyError(f\"{missing_features} not in train_data.columns\")\n",
    "    \n",
    "    # Extract features and target from training and testing data using target_col\n",
    "    X_train = train_data[features].values\n",
    "    y_train = train_data[target_col].values\n",
    "    X_test = test_data[features].values\n",
    "    y_test = test_data[target_col].values\n",
    "\n",
    "    # Scale features\n",
    "    X_train_scaled, X_test_scaled, scaler_exhaustion = scale_features(X_train, X_test)\n",
    "    \n",
    "    # Scale target values separately\n",
    "    target_scaler = StandardScaler()\n",
    "    y_train_scaled = target_scaler.fit_transform(y_train.reshape(-1, 1))\n",
    "    y_test_scaled = target_scaler.transform(y_test.reshape(-1, 1))\n",
    "    \n",
    "    # Create sequences for LSTM input\n",
    "    X_lstm, y_lstm = create_sequences(X_train_scaled, y_train_scaled, timesteps)\n",
    "    X_lstm_val, y_lstm_val = create_sequences(X_test_scaled, y_test_scaled, timesteps)\n",
    "\n",
    "\n",
    "    model_exhaustion = Sequential()\n",
    "    model_exhaustion.add(Input(shape=(X_lstm.shape[1], X_lstm.shape[2])))\n",
    "    for i in range(num_lstm_layers):\n",
    "        return_seq = True if i < num_lstm_layers - 1 else False\n",
    "        model_exhaustion.add(LSTM(lstm_units, return_sequences=return_seq))\n",
    "        model_exhaustion.add(Dropout(dropout_rate))\n",
    "    model_exhaustion.add(Dense(dense_units, activation=dense_activation))\n",
    "    \n",
    "    model_exhaustion.compile(optimizer='adam', loss='mse')\n",
    "    from tensorflow.keras.callbacks import EarlyStopping\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=early_stop_patience)\n",
    "    \n",
    "    logging.info(\"Training exhaustion model...\")\n",
    "    model_exhaustion.fit(\n",
    "        X_lstm, y_lstm,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(X_lstm_val, y_lstm_val),\n",
    "        callbacks=[early_stop]\n",
    "    )\n",
    "    \n",
    "    return model_exhaustion, scaler_exhaustion, target_scaler, X_lstm_val, y_lstm_val\n",
    "\n",
    "\n",
    "def train_injury_model(train_data, test_data, features, timesteps,\n",
    "                       epochs=50, batch_size=32, early_stop_patience=5,\n",
    "                       num_lstm_layers=1, lstm_units=64, dropout_rate=0.2,\n",
    "                       dense_units=1, dense_activation='sigmoid', target_col=\"injury_risk\"):\n",
    "    \"\"\"\n",
    "    Trains an injury risk model (classification). An optional parameter 'target_col'\n",
    "    is added so that this function can be used for joint-specific injury targets.\n",
    "    The default is \"injury_risk\".\n",
    "    \n",
    "    Parameters:\n",
    "      - train_data (DataFrame): Training set.\n",
    "      - test_data (DataFrame): Testing set.\n",
    "      - features (list): List of feature column names for injury risk.\n",
    "      - timesteps (int): Number of past observations to include in each sequence.\n",
    "      - epochs (int): Number of training epochs.\n",
    "      - batch_size (int): Batch size for training.\n",
    "      - num_lstm_layers (int): Number of LSTM layers in the model.\n",
    "      - lstm_units (int): Number of units in each LSTM layer.\n",
    "      - dropout_rate (float): Dropout rate applied after each LSTM layer.\n",
    "      - dense_units (int): Number of units in the final Dense layer.\n",
    "      - dense_activation (str): Activation function for the Dense layer.\n",
    "      - target_col (str): The target column to use. Default is \"injury_risk\".\n",
    "      \n",
    "    Returns:\n",
    "      - model_injury: Trained Keras model.\n",
    "      - scaler_injury: Fitted scaler for the features.\n",
    "      - X_lstm_injury_val, y_lstm_injury_val: Validation sequences.\n",
    "    \"\"\"\n",
    "    # --- Debug: Log the feature list and available columns ---\n",
    "    logging.info(f\"Features provided for training injury model: {features}\")\n",
    "    logging.info(f\"Available train_data columns: {train_data.columns.tolist()}\")\n",
    "    missing_features = [f for f in features if f not in train_data.columns]\n",
    "    if missing_features:\n",
    "        logging.error(f\"Missing features in train_data for target {target_col}: {missing_features}\")\n",
    "        raise KeyError(f\"{missing_features} not in train_data.columns\")\n",
    "    \n",
    "    # Extract features and target using target_col\n",
    "    X_train = train_data[features].values\n",
    "    y_train = train_data[target_col].values\n",
    "    X_test = test_data[features].values\n",
    "    y_test = test_data[target_col].values\n",
    "\n",
    "    # Scale features\n",
    "    X_train_scaled, X_test_scaled, scaler_injury = scale_features(X_train, X_test)\n",
    "    # Create LSTM sequences\n",
    "    X_lstm, y_lstm = create_sequences(X_train_scaled, y_train, timesteps)\n",
    "    X_lstm_val, y_lstm_val = create_sequences(X_test_scaled, y_test, timesteps)\n",
    "\n",
    "\n",
    "    model_injury = Sequential()\n",
    "    model_injury.add(Input(shape=(X_lstm.shape[1], X_lstm.shape[2])))\n",
    "    for i in range(num_lstm_layers):\n",
    "        return_seq = True if i < num_lstm_layers - 1 else False\n",
    "        model_injury.add(LSTM(lstm_units, return_sequences=return_seq))\n",
    "        model_injury.add(Dropout(dropout_rate))\n",
    "    model_injury.add(Dense(dense_units, activation=dense_activation))\n",
    "    \n",
    "    model_injury.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    logging.info(\"Training injury risk model...\")\n",
    "    from tensorflow.keras.callbacks import EarlyStopping\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=early_stop_patience)\n",
    "    # Include early_stop in model.fit():\n",
    "    model_injury.fit(\n",
    "        X_lstm, y_lstm,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(X_lstm_val, y_lstm_val),\n",
    "        callbacks=[early_stop]\n",
    "    )\n",
    "    \n",
    "    return model_injury, scaler_injury, X_lstm_val, y_lstm_val\n",
    "\n",
    "\n",
    "\n",
    "# ==================== JOINT-SPECIFIC TRAINING FUNCTION ====================\n",
    "\n",
    "\n",
    "\n",
    "def train_joint_models(train_data, test_data, joints, timesteps, feature_dir,\n",
    "                       epochs=50, batch_size=32,\n",
    "                       num_lstm_layers=1, lstm_units=64, dropout_rate=0.2,\n",
    "                       dense_units=1, dense_activation='sigmoid',\n",
    "                       joint_feature_dict=None):\n",
    "    \"\"\"\n",
    "    Trains injury risk models for multiple joints.\n",
    "    \n",
    "    Parameters:\n",
    "      - train_data (DataFrame): Training set.\n",
    "      - test_data (DataFrame): Testing set.\n",
    "      - joints (list): List of joint names.\n",
    "      - timesteps (int): Number of past observations to include in each sequence.\n",
    "      - feature_dir (str): Directory containing feature lists.\n",
    "      - epochs (int): Number of training epochs.\n",
    "      - batch_size (int): Batch size for training.\n",
    "      - num_lstm_layers (int): Number of LSTM layers in the model.\n",
    "      - lstm_units (int): Number of units in each LSTM layer.\n",
    "      - dropout_rate (float): Dropout rate applied after each LSTM layer.\n",
    "      - dense_units (int): Number of units in the final Dense layer.\n",
    "      - dense_activation (str): Activation function for the Dense layer.\n",
    "      - joint_feature_dict (dict or None): (Optional) Dictionary mapping each joint target (e.g., \n",
    "            \"L_ANKLE_injury_risk\") to a preloaded feature list. If None, the function calls\n",
    "            load_top_features for each target.\n",
    "      \n",
    "    Returns:\n",
    "      - joint_models (dict): Dictionary with joint model information.\n",
    "    \"\"\"\n",
    "    joint_models = {}\n",
    "\n",
    "    for joint in joints:\n",
    "        for side in ['L', 'R']:\n",
    "            target_joint = f\"{side}_{joint}_injury_risk\"\n",
    "            logging.info(f\"Training model for {target_joint}...\")\n",
    "\n",
    "            # If a joint_feature_dict is provided and has the target, use it;\n",
    "            # otherwise, call load_top_features from the specified directory.\n",
    "            if joint_feature_dict is not None and target_joint in joint_feature_dict:\n",
    "                joint_features = joint_feature_dict[target_joint]\n",
    "                logging.info(f\"Using preloaded feature list for {target_joint}: {joint_features}\")\n",
    "            else:\n",
    "                joint_features = load_top_features(target_joint, feature_dir=feature_dir)\n",
    "            \n",
    "            # Extract joint-specific features and target values.\n",
    "            X_train_joint = train_data[joint_features].values\n",
    "            y_train_joint = train_data[target_joint].values\n",
    "            X_test_joint = test_data[joint_features].values\n",
    "            y_test_joint = test_data[target_joint].values\n",
    "\n",
    "            # Scale features for the joint-specific model.\n",
    "            X_train_scaled, X_test_scaled, scaler_joint = scale_features(X_train_joint, X_test_joint)\n",
    "            # Create sequences for LSTM input.\n",
    "            X_lstm, y_lstm = create_sequences(X_train_scaled, y_train_joint, timesteps)\n",
    "            X_lstm_val, y_lstm_val = create_sequences(X_test_scaled, y_test_joint, timesteps)\n",
    "\n",
    "            # Build the joint model.\n",
    "            model_joint = Sequential()\n",
    "            model_joint.add(Input(shape=(X_lstm.shape[1], X_lstm.shape[2])))\n",
    "            for i in range(num_lstm_layers):\n",
    "                return_seq = True if i < num_lstm_layers - 1 else False\n",
    "                model_joint.add(LSTM(lstm_units, return_sequences=return_seq))\n",
    "                model_joint.add(Dropout(dropout_rate))\n",
    "            model_joint.add(Dense(dense_units, activation=dense_activation))\n",
    "\n",
    "            model_joint.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "            model_joint.fit(\n",
    "                X_lstm, y_lstm,\n",
    "                epochs=epochs, \n",
    "                batch_size=batch_size,\n",
    "                validation_data=(X_lstm_val, y_lstm_val)\n",
    "            )\n",
    "\n",
    "            joint_models[target_joint] = {\n",
    "                'model': model_joint,\n",
    "                'features': joint_features,\n",
    "                'scaler': scaler_joint\n",
    "            }\n",
    "\n",
    "    # Save the loaded feature lists for all joint models.\n",
    "    with open(\"loaded_features.json\", \"w\") as f:\n",
    "        json.dump({target: info['features'] for target, info in joint_models.items()}, f, indent=4)\n",
    "    logging.info(\"Saved loaded features list for each joint model to 'loaded_features.json'.\")\n",
    "\n",
    "    return joint_models\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ==================== FORECASTING FUNCTION ====================\n",
    "\n",
    "def forecast_and_plot_exhaustion(model, test_data, forecast_features, scaler_exhaustion, target_scaler, timesteps, future_steps=0, title=\"Exhaustion Forecast\"):\n",
    "    \"\"\"\n",
    "    Generates predictions for the exhaustion target using multi-feature input.\n",
    "    \n",
    "    This function extracts the same features used during training (e.g. a 10-dimensional input),\n",
    "    scales them with the features scaler (scaler_exhaustion), builds forecasting sequences, makes predictions,\n",
    "    and finally inverse-transforms the predictions using the target scaler.\n",
    "    \n",
    "    Parameters:\n",
    "      - model: Trained exhaustion Keras model.\n",
    "      - test_data (DataFrame): The test DataFrame containing all features.\n",
    "      - forecast_features (list): List of feature names used for forecasting (e.g. features_exhaustion).\n",
    "      - scaler_exhaustion: Fitted StandardScaler used to scale the features.\n",
    "      - target_scaler: Fitted StandardScaler used to scale the target values.\n",
    "      - timesteps (int): Number of past observations to include in each sequence.\n",
    "      - future_steps (int): Number of future time steps to forecast.\n",
    "                          (Note: Future forecasting is approximate since it assumes constant features.)\n",
    "      - title (str): Plot title.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract multi-dimensional input from test data\n",
    "    X_forecast = test_data[forecast_features].values  # shape (n, num_features)\n",
    "    \n",
    "    # Scale the features using the features scaler\n",
    "    X_forecast_scaled = scaler_exhaustion.transform(X_forecast)\n",
    "    \n",
    "    # Create sequences for forecasting using a dummy y array (since only X is needed)\n",
    "    X_seq, _ = create_sequences(X_forecast_scaled, np.zeros(len(X_forecast_scaled)), timesteps)\n",
    "    \n",
    "    # Make predictions on the scaled sequences\n",
    "    predictions_scaled = model.predict(X_seq)\n",
    "    # Inverse-transform predictions using the target scaler\n",
    "    predictions = target_scaler.inverse_transform(predictions_scaled)\n",
    "    \n",
    "    forecast_predictions_inv = None\n",
    "    if future_steps > 0:\n",
    "        # For additional future steps, we assume the features remain constant.\n",
    "        # WARNING: This is an approximation.\n",
    "        current_sequence = X_seq[-1].copy()  # shape: (timesteps, num_features)\n",
    "        forecast_predictions = []\n",
    "        for _ in range(future_steps):\n",
    "            next_pred = model.predict(current_sequence.reshape(1, timesteps, current_sequence.shape[1]))\n",
    "            forecast_predictions.append(next_pred[0, 0])\n",
    "            # Update sequence: drop the first row and append the last row (assumed constant)\n",
    "            new_row = current_sequence[-1, :].copy()\n",
    "            current_sequence = np.vstack([current_sequence[1:], new_row])\n",
    "        forecast_predictions = np.array(forecast_predictions).reshape(-1, 1)\n",
    "        forecast_predictions_inv = target_scaler.inverse_transform(forecast_predictions)\n",
    "    \n",
    "    # Plot actual exhaustion scores versus predictions\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    actual = test_data['by_trial_exhaustion_score'].values\n",
    "    plt.plot(range(timesteps, len(actual)), actual[timesteps:], color='red', label='Actual')\n",
    "    plt.plot(range(timesteps, len(actual)), predictions, color='blue', label='Predicted')\n",
    "    if forecast_predictions_inv is not None:\n",
    "        future_x = list(range(len(actual), len(actual) + future_steps))\n",
    "        plt.plot(future_x, forecast_predictions_inv, color='green', linestyle='--', label='Forecast')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Exhaustion Score')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def forecast_and_plot_injury(model, test_data, forecast_features, scaler_injury, timesteps, future_steps=0, title=\"Injury Risk Forecast\"):\n",
    "    \"\"\"\n",
    "    Generates predictions for the injury risk model using multi-feature input.\n",
    "    \n",
    "    This function extracts the injury features from the test data, scales them using scaler_injury,\n",
    "    builds forecasting sequences, and makes predictions. Since this is a classification model,\n",
    "    it outputs probability predictions. These probabilities (or rounded binary classes) are compared\n",
    "    to the actual injury risk (assumed to be 0 or 1).\n",
    "    \n",
    "    Parameters:\n",
    "      - model: Trained injury risk Keras model.\n",
    "      - test_data (DataFrame): The test DataFrame containing all features.\n",
    "      - forecast_features (list): List of feature names used for forecasting (e.g. features_injury).\n",
    "      - scaler_injury: Fitted StandardScaler used to scale the injury features.\n",
    "      - timesteps (int): Number of past observations to include in each sequence.\n",
    "      - future_steps (int): Number of future time steps to forecast.\n",
    "                          (For classification, future forecasting is less common.)\n",
    "      - title (str): Plot title.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract multi-dimensional input for injury risk from test data\n",
    "    X_forecast = test_data[forecast_features].values  # shape (n, num_features)\n",
    "    print(f\"X_forecast shape: {X_forecast.shape}, min: {X_forecast.min()}, max: {X_forecast.max()}\")\n",
    "        \n",
    "    # Scale the features using the injury features scaler\n",
    "    X_forecast_scaled = scaler_injury.transform(X_forecast)\n",
    "    print(f\"X_forecast_scaled shape: {X_forecast_scaled.shape}, min: {X_forecast_scaled.min()}, max: {X_forecast_scaled.max()}\")\n",
    "    \n",
    "    # Create sequences for forecasting (dummy y used, since only X is needed)\n",
    "    X_seq, _ = create_sequences(X_forecast_scaled, np.zeros(len(X_forecast_scaled)), timesteps)\n",
    "    \n",
    "    # Predict probabilities on the sequences\n",
    "    predictions_prob = model.predict(X_seq)\n",
    "    print(f\"predictions_prob shape: {predictions_prob.shape}, min: {predictions_prob.min()}, max: {predictions_prob.max()}\")\n",
    "    \n",
    "    # Add clipping to ensure valid probability range\n",
    "    predictions_prob_clipped = np.clip(predictions_prob, 0, 1)\n",
    "    print(f\"After clipping - min: {predictions_prob_clipped.min()}, max: {predictions_prob_clipped.max()}\")\n",
    "    \n",
    "    # Convert probabilities to binary predictions (threshold=0.5)\n",
    "    predictions_class = (predictions_prob >= 0.5).astype(int)\n",
    "    \n",
    "    forecast_predictions = None\n",
    "    if future_steps > 0:\n",
    "        # For future steps, we assume features remain constant (approximation)\n",
    "        current_sequence = X_seq[-1].copy()  # shape: (timesteps, num_features)\n",
    "        forecast_predictions = []\n",
    "        for _ in range(future_steps):\n",
    "            next_pred = model.predict(current_sequence.reshape(1, timesteps, current_sequence.shape[1]))\n",
    "            forecast_predictions.append((next_pred[0, 0] >= 0.5).astype(int))\n",
    "            new_row = current_sequence[-1, :].copy()\n",
    "            current_sequence = np.vstack([current_sequence[1:], new_row])\n",
    "        forecast_predictions = np.array(forecast_predictions)\n",
    "    \n",
    "    # Plot the actual injury risk versus predicted probability (or binary prediction)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    actual = test_data['injury_risk'].values\n",
    "    # For plotting, we align the sequences starting at index 'timesteps'\n",
    "    plt.plot(range(timesteps, len(actual)), actual[timesteps:], color='red', label='Actual')\n",
    "    plt.plot(range(timesteps, len(actual)), predictions_prob, color='blue', label='Predicted Probability')\n",
    "    if forecast_predictions is not None:\n",
    "        future_x = list(range(len(actual), len(actual) + future_steps))\n",
    "        plt.plot(future_x, forecast_predictions, color='green', linestyle='--', label='Forecasted Class')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Injury Risk')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def forecast_and_plot_joint(joint_models, test_data, timesteps, future_steps=0):\n",
    "    \"\"\"\n",
    "    Generates forecasts for each joint model using their corresponding features and scalers.\n",
    "    \n",
    "    For each joint model in the joint_models dictionary (returned by train_joint_models),\n",
    "    this function extracts the joint-specific features from test_data, scales them using the model's scaler,\n",
    "    builds sequences, obtains predictions (probabilities), converts them to binary predictions,\n",
    "    and then plots the actual joint injury risk versus predicted values.\n",
    "    \n",
    "    Parameters:\n",
    "      - joint_models (dict): Dictionary where each key is a joint target name and each value is a dict \n",
    "                             containing 'model', 'features', and 'scaler'.\n",
    "      - test_data (DataFrame): The test DataFrame containing all features.\n",
    "      - timesteps (int): Number of past observations to include in each sequence.\n",
    "      - future_steps (int): Number of future time steps to forecast (optional).\n",
    "    \"\"\"\n",
    "    for target_joint, info in joint_models.items():\n",
    "        model_joint = info['model']\n",
    "        joint_features = info['features']\n",
    "        scaler_joint = info['scaler']\n",
    "        \n",
    "        # Extract joint-specific features from test data\n",
    "        X_forecast = test_data[joint_features].values  # shape (n, num_features)\n",
    "        # Scale the features\n",
    "        X_forecast_scaled = scaler_joint.transform(X_forecast)\n",
    "        # Create sequences for forecasting (dummy y used)\n",
    "        X_seq, _ = create_sequences(X_forecast_scaled, np.zeros(len(X_forecast_scaled)), timesteps)\n",
    "        \n",
    "        # Predict probabilities and convert to binary predictions\n",
    "        predictions_prob = model_joint.predict(X_seq)\n",
    "        predictions_class = (predictions_prob >= 0.5).astype(int)\n",
    "        \n",
    "        forecast_predictions = None\n",
    "        if future_steps > 0:\n",
    "            # Approximate forecasting by assuming constant features\n",
    "            current_sequence = X_seq[-1].copy()  # shape: (timesteps, num_features)\n",
    "            forecast_predictions = []\n",
    "            for _ in range(future_steps):\n",
    "                next_pred = model_joint.predict(current_sequence.reshape(1, timesteps, current_sequence.shape[1]))\n",
    "                forecast_predictions.append((next_pred[0, 0] >= 0.5).astype(int))\n",
    "                new_row = current_sequence[-1, :].copy()\n",
    "                current_sequence = np.vstack([current_sequence[1:], new_row])\n",
    "            forecast_predictions = np.array(forecast_predictions)\n",
    "        \n",
    "        # Plot for this joint model\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        actual = test_data[target_joint].values\n",
    "        plt.plot(range(timesteps, len(actual)), actual[timesteps:], color='red', label='Actual')\n",
    "        plt.plot(range(timesteps, len(actual)), predictions_prob, color='blue', label='Predicted Probability')\n",
    "        if forecast_predictions is not None:\n",
    "            future_x = list(range(len(actual), len(actual) + future_steps))\n",
    "            plt.plot(future_x, forecast_predictions, color='green', linestyle='--', label='Forecasted Class')\n",
    "        plt.title(f\"Forecast for {target_joint}\")\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Injury Risk')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def summarize_regression_model(model, X_val, y_val, target_scaler=None, debug=False):\n",
    "    \"\"\"\n",
    "    Evaluates regression model performance.\n",
    "    \n",
    "    Parameters:\n",
    "      - model: Trained regression Keras model.\n",
    "      - X_val: Validation feature sequences.\n",
    "      - y_val: Validation target values (scaled).\n",
    "      - target_scaler: Optional StandardScaler used to inverse-transform targets.\n",
    "      - debug (bool): If True, prints detailed debug information.\n",
    "      \n",
    "    Returns:\n",
    "      - A dictionary containing Mean Squared Error (MSE), Mean Absolute Error (MAE), and R2 Score.\n",
    "    \"\"\"\n",
    "    preds_scaled = model.predict(X_val)\n",
    "    \n",
    "    if target_scaler:\n",
    "        preds = target_scaler.inverse_transform(preds_scaled)\n",
    "        y_true = target_scaler.inverse_transform(y_val)\n",
    "    else:\n",
    "        preds = preds_scaled\n",
    "        y_true = y_val\n",
    "\n",
    "    if debug:\n",
    "        print(f\"[DEBUG] Predictions stats: min={preds.min()}, max={preds.max()}, nan_count={np.isnan(preds).sum()}\")\n",
    "        print(f\"[DEBUG] True values stats: min={y_true.min()}, max={y_true.max()}, nan_count={np.isnan(y_true).sum()}\")\n",
    "\n",
    "    mse = mean_squared_error(y_true, preds)\n",
    "    mae = mean_absolute_error(y_true, preds)\n",
    "    r2 = r2_score(y_true, preds)\n",
    "\n",
    "    return {\n",
    "        \"MSE\": mse,\n",
    "        \"MAE\": mae,\n",
    "        \"R2 Score\": r2\n",
    "    }\n",
    "\n",
    "\n",
    "def summarize_classification_model(model, X_val, y_val, debug=False):\n",
    "    \"\"\"\n",
    "    Evaluates classification model performance.\n",
    "    \n",
    "    Parameters:\n",
    "      - model: Trained classification Keras model.\n",
    "      - X_val: Validation feature sequences.\n",
    "      - y_val: True binary labels.\n",
    "      - debug (bool): If True, prints detailed debug information.\n",
    "      \n",
    "    Returns:\n",
    "      - A dictionary containing Accuracy, Precision, Recall, and F1 Score.\n",
    "    \"\"\"\n",
    "    preds_prob = model.predict(X_val)\n",
    "    preds_class = (preds_prob >= 0.5).astype(int)\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"[DEBUG] Predicted probabilities stats: min={preds_prob.min()}, max={preds_prob.max()}, nan_count={np.isnan(preds_prob).sum()}\")\n",
    "\n",
    "    accuracy = accuracy_score(y_val, preds_class)\n",
    "    precision = precision_score(y_val, preds_class, zero_division=0)\n",
    "    recall = recall_score(y_val, preds_class, zero_division=0)\n",
    "    f1 = f1_score(y_val, preds_class, zero_division=0)\n",
    "\n",
    "    return {\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1 Score\": f1\n",
    "    }\n",
    "\n",
    "def summarize_joint_models(joint_models, test_data, timesteps, debug=False):\n",
    "    \"\"\"\n",
    "    Summarizes evaluation metrics for each joint model (classification or regression).\n",
    "    \n",
    "    For every joint model, the function:\n",
    "      - Extracts joint-specific features from test_data.\n",
    "      - Scales the features.\n",
    "      - Creates sequences for LSTM input.\n",
    "      - Computes classification or regression metrics using the appropriate function.\n",
    "    \n",
    "    Parameters:\n",
    "      - joint_models (dict): Dictionary with joint model info ('model', 'features', 'scaler').\n",
    "      - test_data (DataFrame): Test dataset containing joint-specific features.\n",
    "      - timesteps (int): Number of past observations per sequence.\n",
    "      - debug (bool): If True, enables debugging outputs.\n",
    "      \n",
    "    Returns:\n",
    "      - Dictionary mapping each joint target to its evaluation metrics.\n",
    "    \"\"\"\n",
    "    summaries = {}\n",
    "    for target_joint, info in joint_models.items():\n",
    "        model_joint = info['model']\n",
    "        joint_features = info['features']\n",
    "        scaler_joint = info['scaler']\n",
    "        \n",
    "        validate_features(joint_features, test_data, context=f\"summarize_joint_models for {target_joint}\")\n",
    "        \n",
    "        X_joint = test_data[joint_features].values\n",
    "        y_joint = test_data[target_joint].values\n",
    "\n",
    "        X_joint_scaled = scaler_joint.transform(X_joint)\n",
    "        X_seq, y_seq = create_sequences(X_joint_scaled, y_joint, timesteps)\n",
    "        \n",
    "        # Determine if this is a classification or regression model\n",
    "        if 'injury_risk' in target_joint:\n",
    "            metrics = summarize_classification_model(model_joint, X_seq, y_seq, debug=debug)\n",
    "        elif 'by_trial_exhaustion_score' in target_joint:\n",
    "            # Assuming we have a target_scaler for regression models\n",
    "            target_scaler = info.get('target_scaler')\n",
    "            metrics = summarize_regression_model(model_joint, X_seq, y_seq, target_scaler, debug=debug)\n",
    "        else:\n",
    "            logging.warning(f\"Unknown model type for {target_joint}. Skipping evaluation.\")\n",
    "            continue\n",
    "        \n",
    "        summaries[target_joint] = metrics\n",
    "        \n",
    "        if debug:\n",
    "            logging.debug(f\"[DEBUG] For {target_joint}: expected features: {joint_features}\")\n",
    "            logging.debug(f\"[DEBUG] Test data columns: {test_data.columns.tolist()}\")\n",
    "\n",
    "    return summaries\n",
    "\n",
    "\n",
    "\n",
    "def summarize_all_models(model_exhaustion, X_val_exh, y_val_exh, target_scaler,\n",
    "                         model_injury, X_val_injury, y_val_injury,\n",
    "                         joint_models, test_data, timesteps, output_dir,\n",
    "                         include_joint_models=True, debug=False):\n",
    "    \"\"\"\n",
    "    Summarizes exhaustion, injury, and optionally joint models into one DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "      - include_joint_models (bool): Set False when summarizing aggregated datasets to prevent errors.\n",
    "    \"\"\"\n",
    "    summary_data = []\n",
    "\n",
    "    # Regression model summary\n",
    "    exh_metrics = summarize_regression_model(model_exhaustion, X_val_exh, y_val_exh, target_scaler, debug=debug)\n",
    "    summary_data.append({\n",
    "        \"Model\": \"Exhaustion Model\",\n",
    "        \"Type\": \"Regression\",\n",
    "        \"MSE\": exh_metrics[\"MSE\"],\n",
    "        \"MAE\": exh_metrics[\"MAE\"],\n",
    "        \"R2 Score\": exh_metrics[\"R2 Score\"],\n",
    "        \"Accuracy\": None, \"Precision\": None, \"Recall\": None, \"F1 Score\": None\n",
    "    })\n",
    "\n",
    "    # Classification model summary\n",
    "    injury_metrics = summarize_classification_model(model_injury, X_val_injury, y_val_injury, debug=debug)\n",
    "    summary_data.append({\n",
    "        \"Model\": \"Injury Model\",\n",
    "        \"Type\": \"Classification\",\n",
    "        \"MSE\": None, \"MAE\": None, \"R2 Score\": None,\n",
    "        \"Accuracy\": injury_metrics[\"Accuracy\"],\n",
    "        \"Precision\": injury_metrics[\"Precision\"],\n",
    "        \"Recall\": injury_metrics[\"Recall\"],\n",
    "        \"F1 Score\": injury_metrics[\"F1 Score\"]\n",
    "    })\n",
    "\n",
    "    # Joint models summary (if included)\n",
    "    if include_joint_models:\n",
    "        joint_summaries = summarize_joint_models(joint_models, test_data, timesteps, debug=debug)\n",
    "        for joint, metrics in joint_summaries.items():\n",
    "            # Determine the model type based on the metrics available\n",
    "            if \"MSE\" in metrics:\n",
    "                # This is a regression model\n",
    "                summary_data.append({\n",
    "                    \"Model\": joint,\n",
    "                    \"Type\": \"Regression\",\n",
    "                    \"MSE\": metrics[\"MSE\"],\n",
    "                    \"MAE\": metrics[\"MAE\"],\n",
    "                    \"R2 Score\": metrics[\"R2 Score\"],\n",
    "                    \"Accuracy\": None, \"Precision\": None, \"Recall\": None, \"F1 Score\": None\n",
    "                })\n",
    "            elif \"Accuracy\" in metrics:\n",
    "                # This is a classification model\n",
    "                summary_data.append({\n",
    "                    \"Model\": joint,\n",
    "                    \"Type\": \"Classification\",\n",
    "                    \"MSE\": None, \"MAE\": None, \"R2 Score\": None,\n",
    "                    \"Accuracy\": metrics[\"Accuracy\"],\n",
    "                    \"Precision\": metrics[\"Precision\"],\n",
    "                    \"Recall\": metrics[\"Recall\"],\n",
    "                    \"F1 Score\": metrics[\"F1 Score\"]\n",
    "                })\n",
    "            else:\n",
    "                logging.warning(f\"Unknown metrics type for {joint}: {list(metrics.keys())}\")\n",
    "\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    summary_df.to_csv(Path(output_dir) / \"model_summary_final.csv\", index=False)\n",
    "    logging.info(f\"Saved summary to {output_dir}/model_summary_final.csv\")\n",
    "    \n",
    "    return summary_df\n",
    "\n",
    "\n",
    "\n",
    "def final_model_summary(\n",
    "    regression_summaries, classification_summaries,\n",
    "    regression_names=None, classification_names=None,\n",
    "    joint_summaries=None, joint_names=None,\n",
    "    darts_metrics=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Combines multiple summary DataFrames for regression and classification models.\n",
    "    \n",
    "    Parameters:\n",
    "      - regression_summaries (list of DataFrame): Each DataFrame contains regression model summary metrics.\n",
    "      - classification_summaries (list of DataFrame): Each DataFrame contains classification model summary metrics.\n",
    "      - regression_names (list of str, optional): Names for each regression summary (e.g., \"Base\", \"Trial Aggregated\", \"Shot Aggregated\").\n",
    "      - classification_names (list of str, optional): Names for each classification summary.\n",
    "      - joint_summaries (list of DataFrame, optional): Each DataFrame contains joint model summary metrics.\n",
    "      - joint_names (list of str, optional): Names for each joint summary.\n",
    "    \n",
    "    Returns:\n",
    "      - final_reg (DataFrame): Combined regression summaries.\n",
    "      - final_class (DataFrame): Combined classification summaries.\n",
    "      - final_joint (DataFrame): Combined joint model summaries.\n",
    "      - final_all (DataFrame): Combined DataFrame of all summaries.\n",
    "    \"\"\"\n",
    "    # If names are provided, add a \"Dataset\" column to each corresponding DataFrame.\n",
    "    regression_dfs = []\n",
    "    if regression_names is not None:\n",
    "        for df, name in zip(regression_summaries, regression_names):\n",
    "            df = df.copy()\n",
    "            df[\"Dataset\"] = name\n",
    "            regression_dfs.append(df)\n",
    "    else:\n",
    "        regression_dfs = regression_summaries\n",
    "\n",
    "    classification_dfs = []\n",
    "    if classification_names is not None:\n",
    "        for df, name in zip(classification_summaries, classification_names):\n",
    "            df = df.copy()\n",
    "            df[\"Dataset\"] = name\n",
    "            classification_dfs.append(df)\n",
    "    else:\n",
    "        classification_dfs = classification_summaries\n",
    "        \n",
    "    joint_dfs = []\n",
    "    if joint_names is not None and joint_summaries is not None:\n",
    "        for df, name in zip(joint_summaries, joint_names):\n",
    "            df = df.copy()\n",
    "            df[\"Dataset\"] = name\n",
    "            joint_dfs.append(df)\n",
    "    elif joint_summaries is not None:\n",
    "        joint_dfs = joint_summaries\n",
    "\n",
    "    # Concatenate the lists into final DataFrames.\n",
    "    final_reg = pd.concat(regression_dfs, ignore_index=True)\n",
    "    final_class = pd.concat(classification_dfs, ignore_index=True)\n",
    "    \n",
    "    if joint_dfs:\n",
    "        final_joint = pd.concat(joint_dfs, ignore_index=True)\n",
    "        final_all = pd.concat([final_reg, final_class, final_joint], ignore_index=True)\n",
    "    else:\n",
    "        final_joint = pd.DataFrame()  # Empty DataFrame\n",
    "        final_all = pd.concat([final_reg, final_class], ignore_index=True)\n",
    "        \n",
    "    # AFTER building final_all:\n",
    "    if darts_metrics is not None and not darts_metrics.empty:\n",
    "        try:\n",
    "            # darts_metrics must have columns: Model, MAE, MAPE, RMSE, SMAPE, Target\n",
    "            dm = darts_metrics.copy()\n",
    "            dm['Type'] = 'Time Series Forecast'\n",
    "            dm['Dataset'] = 'Darts Models'\n",
    "            # reorder columns to match final_all if needed\n",
    "            # e.g. ['Model','Type','Dataset','MAE','MAPE','RMSE','SMAPE','Target']\n",
    "            final_all = pd.concat([final_all, dm], ignore_index=True, sort=False)\n",
    "            logging.info(\"[Summary] appended Darts metrics\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"[Summary] failed to append Darts metrics: {e}\")\n",
    "\n",
    "    return final_reg, final_class, final_joint, final_all\n",
    "\n",
    "\n",
    "def summarize_joint_exhaustion_models(joint_exh_models, test_data, timesteps, debug=False):\n",
    "    \"\"\"\n",
    "    Computes evaluation metrics for each joint-specific exhaustion model (regression).\n",
    "    \n",
    "    For each model in joint_exh_models:\n",
    "      - Extracts the joint-specific features from test_data.\n",
    "      - Scales the features.\n",
    "      - Creates sequences.\n",
    "      - Computes regression metrics using summarize_regression_model.\n",
    "    \n",
    "    Parameters:\n",
    "      - joint_exh_models (dict): Dictionary with keys as target names and values containing 'model', 'features', and 'scaler'.\n",
    "      - test_data (DataFrame): Test data.\n",
    "      - timesteps (int): Number of past observations per sequence.\n",
    "      - debug (bool): Enables debug output if True.\n",
    "      \n",
    "    Returns:\n",
    "      - summaries (dict): Dictionary mapping each joint exhaustion target to its evaluation metrics.\n",
    "    \"\"\"\n",
    "    summaries = {}\n",
    "    for target, info in joint_exh_models.items():\n",
    "        model_exh = info['model']\n",
    "        features = info['features']\n",
    "        scaler = info['scaler']\n",
    "        \n",
    "        # Validate that the required features exist in test_data.\n",
    "        validate_features(features, test_data, context=f\"summarize_joint_exhaustion_models for {target}\")\n",
    "        \n",
    "        # Extract features and target.\n",
    "        X_joint = test_data[features].values\n",
    "        y_joint = test_data[target].values\n",
    "        \n",
    "        # Scale features.\n",
    "        X_joint_scaled = scaler.transform(X_joint)\n",
    "        # Create sequences.\n",
    "        X_seq, y_seq = create_sequences(X_joint_scaled, y_joint, timesteps)\n",
    "        \n",
    "        # Use the regression summary function (no target_scaler needed if y is not scaled further).\n",
    "        metrics = summarize_regression_model(model_exh, X_seq, y_seq, target_scaler=None, debug=debug)\n",
    "        summaries[target] = metrics\n",
    "    return summaries\n",
    "\n",
    "\n",
    "# --- Main Script (Updated) ---\n",
    "if __name__ == \"__main__\":\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "    from ml.load_and_prepare_data.load_data_and_analyze import (\n",
    "        load_data, prepare_joint_features, feature_engineering, summarize_data, check_and_drop_nulls)\n",
    "    \n",
    "    from ml.feature_selection.feature_selection import (\n",
    "        load_top_features, perform_feature_importance_analysis, save_top_features,\n",
    "        analyze_joint_injury_features, check_for_invalid_values,\n",
    "        perform_feature_importance_analysis, analyze_and_display_top_features, validate_features)\n",
    "    # from ml.preprocess_train_predict.base_training.py import (\n",
    "    #     temporal_train_test_split, scale_features, create_sequences, train_exhaustion_model, \n",
    "    #     train_injury_model,  train_joint_models, forecast_and_plot_exhaustion, forecast_and_plot_injury,\n",
    "    #     forecast_and_plot_joint, summarize_regression_model, summarize_classification_model, \n",
    "    #     summarize_joint_models, summarize_all_models, final_model_summary, \n",
    "    #     summarize_joint_exhaustion_models\n",
    "    #     )\n",
    "    debug = True\n",
    "    importance_threshold = 0.05  # Set threshold as needed\n",
    "    csv_path = \"../../data/processed/final_granular_dataset.csv\"\n",
    "    json_path = \"../../data/basketball/freethrow/participant_information.json\"\n",
    "    feature_dir = \"../../data/Deep_Learning_Final\"  # Directory where feature lists were saved\n",
    "    output_dir = \"../../data/Deep_Learning_Final\"  # Base directory\n",
    "    \n",
    "    # Define directories for saving feature lists per dataset type\n",
    "    base_feature_dir = os.path.join(output_dir, \"feature_lists/base\")\n",
    "    trial_feature_dir = os.path.join(output_dir, \"feature_lists/trial_summary\")\n",
    "    shot_feature_dir = os.path.join(output_dir, \"feature_lists/shot_phase_summary\")\n",
    "    \n",
    "    # Load and process data\n",
    "    data = load_data(csv_path, json_path, debug=debug)\n",
    "    data = prepare_joint_features(data, debug=debug)\n",
    "    data = feature_engineering(data, debug=debug)\n",
    "    print(\"Base data columns:\", data.columns.tolist())\n",
    "    \n",
    "    # Create aggregated datasets for trial and shot-phase analyses\n",
    "    default_agg_columns = [\n",
    "        'joint_energy', 'L_ELBOW_energy', 'R_ELBOW_energy', 'L_WRIST_energy', 'R_WRIST_energy',\n",
    "        'L_KNEE_energy', 'R_KNEE_energy', 'L_HIP_energy', 'R_HIP_energy',\n",
    "        'joint_power', 'L_ELBOW_ongoing_power', 'R_ELBOW_ongoing_power', \n",
    "        'L_WRIST_ongoing_power', 'R_WRIST_ongoing_power',\n",
    "        'L_KNEE_ongoing_power', 'R_KNEE_ongoing_power', 'L_HIP_ongoing_power', 'R_HIP_ongoing_power',\n",
    "        'elbow_asymmetry', 'wrist_asymmetry', 'knee_asymmetry', 'hip_asymmetry',\n",
    "        'L_ELBOW_angle', 'R_ELBOW_angle', 'L_WRIST_angle', 'R_WRIST_angle', \n",
    "        'L_KNEE_angle', 'R_KNEE_angle',\n",
    "        'L_SHOULDER_ROM', 'R_SHOULDER_ROM', 'L_WRIST_ROM', 'R_WRIST_ROM',\n",
    "        'L_KNEE_ROM', 'R_KNEE_ROM', 'L_HIP_ROM', 'R_HIP_ROM',\n",
    "        'exhaustion_rate', 'by_trial_exhaustion_score', 'injury_risk',\n",
    "        'energy_acceleration', 'power_avg_5', 'rolling_power_std', 'rolling_hr_mean',\n",
    "        'simulated_HR',\n",
    "        'player_height_in_meters', 'player_weight__in_kg'\n",
    "    ]\n",
    "    \n",
    "    default_lag_columns = [\n",
    "        'joint_energy', 'joint_power',\n",
    "        'L_ELBOW_energy', 'R_ELBOW_energy', 'L_WRIST_energy', 'R_WRIST_energy',\n",
    "        'elbow_asymmetry', 'wrist_asymmetry',\n",
    "        'L_ELBOW_angle', 'R_ELBOW_angle', 'L_WRIST_angle', 'R_WRIST_angle',\n",
    "        'exhaustion_rate', 'by_trial_exhaustion_score',\n",
    "        'simulated_HR'\n",
    "    ]\n",
    "    rolling_window = 3\n",
    "    \n",
    "    trial_data = prepare_joint_features(data, debug=True, group_trial=True)\n",
    "    trial_data = feature_engineering(trial_data, debug=True, group_trial=True)\n",
    "    \n",
    "    trial_summary_data = summarize_data(data,\n",
    "                                          groupby_cols=['trial_id'],\n",
    "                                          lag_columns=default_lag_columns,\n",
    "                                          rolling_window=rolling_window,\n",
    "                                          agg_columns=default_agg_columns,\n",
    "                                          global_lag=True,\n",
    "                                          debug=True)\n",
    "    \n",
    "    shot_phase_data = prepare_joint_features(data, debug=True, group_trial=True, group_shot_phase=True)\n",
    "    shot_phase_data = feature_engineering(shot_phase_data, debug=True, group_trial=True, group_shot_phase=True)\n",
    "    shot_phase_summary_data = summarize_data(shot_phase_data,\n",
    "                                               groupby_cols=['trial_id', 'shooting_phases'],\n",
    "                                               lag_columns=default_lag_columns,\n",
    "                                               rolling_window=rolling_window,\n",
    "                                               agg_columns=default_agg_columns,\n",
    "                                               phase_list=[\"arm_cock\", \"arm_release\", \"leg_cock\", \"wrist_release\"],\n",
    "                                               debug=True)\n",
    "    print(\"Shot Phase Summary Sample:\")\n",
    "    print(shot_phase_summary_data.head())\n",
    "    print(\"Trial Summary Sample:\")\n",
    "    print(trial_summary_data.head())\n",
    "    data = check_and_drop_nulls(data, columns_to_drop=['energy_acceleration', 'exhaustion_rate'], df_name=\"Final Data\")\n",
    "    \n",
    "    # Filter base data for modeling\n",
    "    data.drop(columns=['event_idx_leg', 'event_idx_elbow', 'event_idx_release', 'event_idx_wrist'], inplace=True)\n",
    "    data = data[data['shooting_motion'] == 1]\n",
    "    invalid_count = check_for_invalid_values(data)\n",
    "    if invalid_count > 0:\n",
    "        logging.error(\"Invalid values detected in feature matrix\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # Define the base features list and target variables.\n",
    "    features = [\n",
    "        'joint_energy', 'joint_power', 'energy_acceleration',\n",
    "        'elbow_asymmetry', 'hip_asymmetry', 'ankle_asymmetry', 'wrist_asymmetry', 'knee_asymmetry', \n",
    "        '1stfinger_asymmetry', '5thfinger_asymmetry',\n",
    "        'elbow_power_ratio', 'hip_power_ratio', 'ankle_power_ratio', 'wrist_power_ratio', \n",
    "        'knee_power_ratio', '1stfinger_power_ratio', '5thfinger_power_ratio',\n",
    "        'L_KNEE_ROM', 'L_KNEE_ROM_deviation', 'L_KNEE_ROM_extreme',\n",
    "        'R_KNEE_ROM', 'R_KNEE_ROM_deviation', 'R_KNEE_ROM_extreme',\n",
    "        'L_SHOULDER_ROM', 'L_SHOULDER_ROM_deviation', 'L_SHOULDER_ROM_extreme',\n",
    "        'R_SHOULDER_ROM', 'R_SHOULDER_ROM_deviation', 'R_SHOULDER_ROM_extreme',\n",
    "        'L_HIP_ROM', 'L_HIP_ROM_deviation', 'L_HIP_ROM_extreme',\n",
    "        'R_HIP_ROM', 'R_HIP_ROM_deviation', 'R_HIP_ROM_extreme',\n",
    "        'L_ANKLE_ROM', 'L_ANKLE_ROM_deviation', 'L_ANKLE_ROM_extreme',\n",
    "        'R_ANKLE_ROM', 'R_ANKLE_ROM_deviation', 'R_ANKLE_ROM_extreme',\n",
    "        'exhaustion_lag1', 'power_avg_5', 'rolling_power_std', 'rolling_hr_mean',\n",
    "        'time_since_start', 'ema_exhaustion', 'rolling_exhaustion', 'rolling_energy_std',\n",
    "        'simulated_HR',\n",
    "        'player_height_in_meters', 'player_weight__in_kg'\n",
    "    ]\n",
    "    \n",
    "    # Build target lists without duplication\n",
    "    base_targets = ['by_trial_exhaustion_score', 'injury_risk']\n",
    "\n",
    "    # Create joint-specific targets for each joint and side\n",
    "    joints = ['ANKLE', 'WRIST', 'ELBOW', 'KNEE', 'HIP']\n",
    "    joint_injury_targets = [f\"{side}_{joint}_injury_risk\" for joint in joints for side in ['L', 'R']]\n",
    "\n",
    "    # Use the correct column naming pattern that exists in the dataset\n",
    "    joint_exhaustion_targets = [f\"{side}_{joint}_energy_by_trial_exhaustion_score\" for joint in joints for side in ['L', 'R']]\n",
    "    joint_targets = joint_injury_targets + joint_exhaustion_targets\n",
    "\n",
    "    # Combine all targets\n",
    "    all_targets = base_targets + joint_injury_targets + joint_exhaustion_targets\n",
    "\n",
    "    # # Run feature importance analysis for all targets on the base dataset.\n",
    "    # results = run_feature_importance_analysis(\n",
    "    #     dataset=data,\n",
    "    #     features=features,\n",
    "    #     targets=all_targets,\n",
    "    #     base_output_dir=output_dir,\n",
    "    #     output_subdir=\"feature_lists/base\",\n",
    "    #     debug=debug,\n",
    "    #     dataset_label=\"Base Data\",\n",
    "    #     importance_threshold=importance_threshold\n",
    "    # )\n",
    "    \n",
    "    # Now, step by step, test that each joint-specific feature list is saved and can be loaded.\n",
    "    # Build a dictionary of joint-specific feature lists for each joint target.\n",
    "    joint_feature_dict = {}\n",
    "    for target in joint_targets:\n",
    "        try:\n",
    "            features_loaded = load_top_features(target, feature_dir=base_feature_dir, df=data, n_top=10)\n",
    "            logging.info(f\"Test Load: Features for {target}: {features_loaded}\")\n",
    "            joint_feature_dict[target] = features_loaded\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading features for {target}: {e}\")\n",
    "    # --- Aggregated Datasets: Run Feature Importance Analysis for Trial and Shot-Phase ---\n",
    "    summary_features = default_agg_columns + [f\"{col}_lag1\" for col in default_lag_columns]\n",
    "    logging.info(f\"Summary features for aggregated datasets: {summary_features}\")\n",
    "    summary_targets = ['by_trial_exhaustion_score', 'injury_risk']\n",
    "    \n",
    "    # trial_results = run_feature_importance_analysis(\n",
    "    #     dataset=trial_summary_data,\n",
    "    #     features=summary_features,\n",
    "    #     targets=summary_targets,\n",
    "    #     base_output_dir=output_dir,\n",
    "    #     output_subdir=\"feature_lists/trial_summary\",\n",
    "    #     debug=debug,\n",
    "    #     dataset_label=\"Trial Summary Data\",\n",
    "    #     importance_threshold=importance_threshold\n",
    "    # )\n",
    "    \n",
    "    # shot_results = run_feature_importance_analysis(\n",
    "    #     dataset=shot_phase_summary_data,\n",
    "    #     features=summary_features,\n",
    "    #     targets=summary_targets,\n",
    "    #     base_output_dir=output_dir,\n",
    "    #     output_subdir=\"feature_lists/shot_phase_summary\",\n",
    "    #     debug=debug,\n",
    "    #     dataset_label=\"Shot Phase Summary Data\",\n",
    "    #     importance_threshold=importance_threshold\n",
    "    # )\n",
    "    \n",
    "    # Test loading a feature list for trial_summary (e.g., for \"by_trial_exhaustion_score\")\n",
    "    loaded_features_trial = load_top_features(\"by_trial_exhaustion_score\", feature_dir=trial_feature_dir, df=trial_summary_data, n_top=10)\n",
    "    logging.info(f\"Test Load: Trial Summary features for by_trial_exhaustion_score: {loaded_features_trial}\")\n",
    "    \n",
    "    # Test loading a feature list for shot_phase_summary (e.g., for \"by_trial_exhaustion_score\")\n",
    "    loaded_features_shot = load_top_features(\"by_trial_exhaustion_score\", feature_dir=shot_feature_dir, df=shot_phase_summary_data, n_top=10)\n",
    "    logging.info(f\"Test Load: Shot Phase Summary features for by_trial_exhaustion_score: {loaded_features_shot}\")\n",
    "    \n",
    "        \n",
    "    # ------------------------------\n",
    "    # 5. Split Base Data for Training Models\n",
    "    # ------------------------------\n",
    "    train_data, test_data = temporal_train_test_split(data, test_size=0.2)\n",
    "    timesteps = 5\n",
    "\n",
    "    # Hyperparameters and architecture definitions.\n",
    "    hyperparams = {\n",
    "        \"epochs\": 200,\n",
    "        \"batch_size\": 32,\n",
    "        \"early_stop_patience\": 5\n",
    "    }\n",
    "    arch_exhaustion = {\n",
    "        \"num_lstm_layers\": 1,\n",
    "        \"lstm_units\": 64,\n",
    "        \"dropout_rate\": 0.2,\n",
    "        \"dense_units\": 1,\n",
    "        \"dense_activation\": None\n",
    "    }\n",
    "    arch_injury = {\n",
    "        \"num_lstm_layers\": 1,\n",
    "        \"lstm_units\": 64,\n",
    "        \"dropout_rate\": 0.2,\n",
    "        \"dense_units\": 1,\n",
    "        \"dense_activation\": \"sigmoid\"\n",
    "    }\n",
    "\n",
    "    # For demonstration, define features/targets (you can adjust these as needed)\n",
    "    features_exhaustion = [\n",
    "        'joint_power', \n",
    "        'joint_energy', \n",
    "        'elbow_asymmetry',  \n",
    "        'L_WRIST_angle', 'R_WRIST_angle',  # Updated: removed \"wrist_angle\"\n",
    "        'exhaustion_lag1', \n",
    "        'power_avg_5',\n",
    "        'simulated_HR',\n",
    "        'player_height_in_meters',\n",
    "        'player_weight__in_kg'\n",
    "    ]\n",
    "    target_exhaustion = 'by_trial_exhaustion_score' # exhaustion_rate\n",
    "\n",
    "    features_injury = [\n",
    "        'joint_power', \n",
    "        'joint_energy', \n",
    "        'elbow_asymmetry',  \n",
    "        'knee_asymmetry', \n",
    "        'L_WRIST_angle', 'R_WRIST_angle',  # Updated: removed \"wrist_angle\"\n",
    "        'exhaustion_lag1', \n",
    "        'power_avg_5',\n",
    "        'simulated_HR',\n",
    "        'player_height_in_meters',\n",
    "        'player_weight__in_kg'\n",
    "    ]\n",
    "    target_injury = 'injury_risk'\n",
    "    # ------------------------------\n",
    "    # 6a. Train Models on Base Data for Overall Exhaustion and Injury Risk\n",
    "    # ------------------------------\n",
    "    model_exhaustion, scaler_exhaustion, target_scaler, X_val_exh, y_val_exh = train_exhaustion_model(\n",
    "        train_data, test_data, features_exhaustion, timesteps,\n",
    "        epochs=hyperparams[\"epochs\"],\n",
    "        batch_size=hyperparams[\"batch_size\"],\n",
    "        early_stop_patience=hyperparams[\"early_stop_patience\"],\n",
    "        num_lstm_layers=arch_exhaustion[\"num_lstm_layers\"],\n",
    "        lstm_units=arch_exhaustion[\"lstm_units\"],\n",
    "        dropout_rate=arch_exhaustion[\"dropout_rate\"],\n",
    "        dense_units=arch_exhaustion[\"dense_units\"],\n",
    "        dense_activation=arch_exhaustion[\"dense_activation\"]\n",
    "    )\n",
    "    model_injury, scaler_injury, X_val_injury, y_val_injury = train_injury_model(\n",
    "        train_data, test_data, features_injury, timesteps,\n",
    "        epochs=hyperparams[\"epochs\"],\n",
    "        batch_size=hyperparams[\"batch_size\"],\n",
    "        num_lstm_layers=arch_injury[\"num_lstm_layers\"],\n",
    "        lstm_units=arch_injury[\"lstm_units\"],\n",
    "        dropout_rate=arch_injury[\"dropout_rate\"],\n",
    "        dense_units=arch_injury[\"dense_units\"],\n",
    "        dense_activation=arch_injury[\"dense_activation\"]\n",
    "    )\n",
    "    # For joint models, we train using the base data and the corresponding features saved earlier.\n",
    "    joints = ['ANKLE', 'WRIST', 'ELBOW', 'KNEE', 'HIP']\n",
    "    # Now train joint models by passing the preloaded joint feature dictionary.\n",
    "    # Assume joint_feature_dict is already built (or loaded) mapping each joint target to its feature list.\n",
    "    # For example:\n",
    "    # joint_feature_dict = {\n",
    "    #    \"L_ANKLE_injury_risk\": [list of features],\n",
    "    #    \"R_ANKLE_injury_risk\": [list of features],\n",
    "    #    \"L_WRIST_injury_risk\": [list of features],\n",
    "    #    ... etc.\n",
    "    # }\n",
    "    # 6b. Train Models on Base Data for individual joint Exhaustion and Injury Risk\n",
    "    joint_models = {}\n",
    "    for joint_target, features_list in joint_feature_dict.items():\n",
    "        try:\n",
    "            logging.info(f\"Training joint-specific injury model for {joint_target} using features: {features_list}\")\n",
    "            model, scaler, X_val_joint, y_val_joint = train_injury_model(\n",
    "                train_data, test_data,\n",
    "                features=features_list,\n",
    "                timesteps=timesteps, \n",
    "                epochs=hyperparams[\"epochs\"],\n",
    "                batch_size=hyperparams[\"batch_size\"],\n",
    "                early_stop_patience=hyperparams[\"early_stop_patience\"],\n",
    "                num_lstm_layers=arch_exhaustion[\"num_lstm_layers\"],\n",
    "                lstm_units=arch_exhaustion[\"lstm_units\"],\n",
    "                dropout_rate=arch_exhaustion[\"dropout_rate\"],\n",
    "                dense_units=arch_exhaustion[\"dense_units\"],\n",
    "                dense_activation=arch_exhaustion[\"dense_activation\"],  # Use the same activation as for injury models, typically sigmoid for binary classification\n",
    "                target_col=joint_target  # use the joint-specific target\n",
    "            )\n",
    "            joint_models[joint_target] = {\n",
    "                'model': model,\n",
    "                'features': features_list,\n",
    "                'scaler': scaler\n",
    "            }\n",
    "            logging.info(f\"Successfully trained joint model for {joint_target}.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error training joint model for {joint_target}: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "    # 6c. Train Models on Base Data for individual joint Exhaustion (by_trial_exhaustion) \n",
    "    #    (in addition to the injury models)\n",
    "    joint_exhaustion_models = {}\n",
    "    for joint in joints:\n",
    "        for side in ['L', 'R']:\n",
    "            target_joint_exh = f\"{side}_{joint}_energy_by_trial_exhaustion_score\"\n",
    "            try:\n",
    "                # Try to use the preloaded feature list if available; otherwise, load it.\n",
    "                if target_joint_exh in joint_feature_dict:\n",
    "                    features_list = joint_feature_dict[target_joint_exh]\n",
    "                    logging.info(f\"Using preloaded features for {target_joint_exh}: {features_list}\")\n",
    "                else:\n",
    "                    features_list = load_top_features(target_joint_exh, feature_dir=base_feature_dir, df=data, n_top=10)\n",
    "                    logging.info(f\"Loaded features for {target_joint_exh}: {features_list}\")\n",
    "                \n",
    "                # Train a regression model for the joint-specific exhaustion score.\n",
    "                model_exh, scaler_exh, target_scaler_exh, X_val_joint_exh, y_val_joint_exh = train_exhaustion_model(\n",
    "                    train_data, test_data,\n",
    "                    features=features_list,\n",
    "                    timesteps=timesteps,\n",
    "                    epochs=hyperparams[\"epochs\"],\n",
    "                    batch_size=hyperparams[\"batch_size\"],\n",
    "                    early_stop_patience=hyperparams[\"early_stop_patience\"],\n",
    "                    num_lstm_layers=arch_exhaustion[\"num_lstm_layers\"],\n",
    "                    lstm_units=arch_exhaustion[\"lstm_units\"],\n",
    "                    dropout_rate=arch_exhaustion[\"dropout_rate\"],\n",
    "                    dense_units=arch_exhaustion[\"dense_units\"],\n",
    "                    dense_activation=arch_exhaustion[\"dense_activation\"],\n",
    "                    target_col=target_joint_exh\n",
    "                )\n",
    "                joint_exhaustion_models[target_joint_exh] = {\n",
    "                    'model': model_exh,\n",
    "                    'features': features_list,\n",
    "                    'scaler': scaler_exh\n",
    "                }\n",
    "                logging.info(f\"Successfully trained joint exhaustion model for {target_joint_exh}.\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error training joint exhaustion model for {target_joint_exh}: {e}\")\n",
    "\n",
    "    # ------------------------------\n",
    "    # 7a. Forecasting for Base Models\n",
    "    # ------------------------------\n",
    "    forecast_and_plot_exhaustion(\n",
    "        model=model_exhaustion,\n",
    "        test_data=test_data,\n",
    "        forecast_features=features_exhaustion,\n",
    "        scaler_exhaustion=scaler_exhaustion,\n",
    "        target_scaler=target_scaler,\n",
    "        timesteps=timesteps,\n",
    "        future_steps=50,\n",
    "        title=\"Overall Exhaustion Model Forecast\"\n",
    "    )\n",
    "    forecast_and_plot_injury(\n",
    "        model=model_injury,\n",
    "        test_data=test_data,\n",
    "        forecast_features=features_injury,\n",
    "        scaler_injury=scaler_injury,\n",
    "        timesteps=timesteps,\n",
    "        future_steps=50,\n",
    "        title=\"Overall Injury Risk Forecast\"\n",
    "    )\n",
    "    forecast_and_plot_joint(\n",
    "        joint_models=joint_models,\n",
    "        test_data=test_data,\n",
    "        timesteps=timesteps,\n",
    "        future_steps=50\n",
    "    )\n",
    "\n",
    "    # ------------------------------\n",
    "    # 8. Summarize Base Model Testing Results\n",
    "    # ------------------------------\n",
    "    summary_df = summarize_all_models(\n",
    "        model_exhaustion, X_val_exh, y_val_exh, target_scaler,\n",
    "        model_injury, X_val_injury, y_val_injury,\n",
    "        joint_models, test_data, timesteps, output_dir\n",
    "    )\n",
    "    print(\"=== Model Summaries (Base Data) ===\")\n",
    "    print(summary_df)\n",
    "\n",
    "    # ------------------------------\n",
    "    # 9. Train, Forecast, and Summarize Aggregated Models\n",
    "    # ------------------------------\n",
    "    # Instead of using a hard-coded summary_features list, we now load the top features\n",
    "    # specific to each aggregated dataset (which were saved using the threshold filter).\n",
    "    \n",
    "    # --- 9a. Process Trial Summary Data ---\n",
    "    trial_train_data, trial_test_data = temporal_train_test_split(trial_summary_data, test_size=0.2)\n",
    "    \n",
    "    # Load the dataset-specific features from the \"trial_summary\" folder.\n",
    "    features_exhaustion_trial = load_top_features('by_trial_exhaustion_score',\n",
    "                                                feature_dir=os.path.join(feature_dir, \"trial_summary\"),\n",
    "                                                df=trial_summary_data,\n",
    "                                                n_top=10)\n",
    "    features_injury_trial = load_top_features('injury_risk',\n",
    "                                            feature_dir=os.path.join(feature_dir, \"trial_summary\"),\n",
    "                                            df=trial_summary_data,\n",
    "                                            n_top=10)\n",
    "\n",
    "    model_exhaustion_trial, scaler_exhaustion_trial, target_scaler_trial, X_val_exh_trial, y_val_exh_trial = train_exhaustion_model(\n",
    "        trial_train_data, trial_test_data, features_exhaustion_trial, timesteps,\n",
    "        epochs=hyperparams[\"epochs\"],\n",
    "        batch_size=hyperparams[\"batch_size\"],\n",
    "        early_stop_patience=hyperparams[\"early_stop_patience\"],\n",
    "        num_lstm_layers=arch_exhaustion[\"num_lstm_layers\"],\n",
    "        lstm_units=arch_exhaustion[\"lstm_units\"],\n",
    "        dropout_rate=arch_exhaustion[\"dropout_rate\"],\n",
    "        dense_units=arch_exhaustion[\"dense_units\"],\n",
    "        dense_activation=arch_exhaustion[\"dense_activation\"]\n",
    "    )\n",
    "    model_injury_trial, scaler_injury_trial, X_val_injury_trial, y_val_injury_trial = train_injury_model(\n",
    "        trial_train_data, trial_test_data, features_injury_trial, timesteps,\n",
    "        epochs=hyperparams[\"epochs\"],\n",
    "        batch_size=hyperparams[\"batch_size\"],\n",
    "        num_lstm_layers=arch_injury[\"num_lstm_layers\"],\n",
    "        lstm_units=arch_injury[\"lstm_units\"],\n",
    "        dropout_rate=arch_injury[\"dropout_rate\"],\n",
    "        dense_units=arch_injury[\"dense_units\"],\n",
    "        dense_activation=arch_injury[\"dense_activation\"]\n",
    "    )\n",
    "    \n",
    "    forecast_and_plot_exhaustion(\n",
    "        model=model_exhaustion_trial,\n",
    "        test_data=trial_test_data,\n",
    "        forecast_features=features_exhaustion_trial,\n",
    "        scaler_exhaustion=scaler_exhaustion_trial,\n",
    "        target_scaler=target_scaler_trial,\n",
    "        timesteps=timesteps,\n",
    "        future_steps=50,\n",
    "        title=\"Trial Summary Aggregated Exhaustion Forecast\"\n",
    "    )\n",
    "    forecast_and_plot_injury(\n",
    "        model=model_injury_trial,\n",
    "        test_data=trial_test_data,\n",
    "        forecast_features=features_injury_trial,\n",
    "        scaler_injury=scaler_injury_trial,\n",
    "        timesteps=timesteps,\n",
    "        future_steps=50,\n",
    "        title=\"Trial Summary Aggregated Injury Forecast\"\n",
    "    )\n",
    "    \n",
    "    trial_summary_df = summarize_all_models(\n",
    "        model_exhaustion_trial, X_val_exh_trial, y_val_exh_trial, target_scaler_trial,\n",
    "        model_injury_trial, X_val_injury_trial, y_val_injury_trial,\n",
    "        joint_models, trial_test_data, timesteps, output_dir,\n",
    "        include_joint_models=False, debug=debug\n",
    "    )\n",
    "\n",
    "    print(\"=== Model Summaries (Trial Summary Aggregated Data) ===\")\n",
    "    print(trial_summary_df)\n",
    "    \n",
    "    # --- 9b. Process Shot Phase Summary Data ---\n",
    "    shot_train_data, shot_test_data = temporal_train_test_split(shot_phase_summary_data, test_size=0.2)\n",
    "    \n",
    "    # Load the dataset-specific features from the \"shot_phase_summary\" folder.\n",
    "    features_exhaustion_shot = load_top_features('by_trial_exhaustion_score',\n",
    "                                                feature_dir=os.path.join(feature_dir, \"shot_phase_summary\"),\n",
    "                                                df=shot_phase_summary_data,\n",
    "                                                n_top=10)\n",
    "    features_injury_shot = load_top_features('injury_risk',\n",
    "                                            feature_dir=os.path.join(feature_dir, \"shot_phase_summary\"),\n",
    "                                            df=shot_phase_summary_data,\n",
    "                                            n_top=10)\n",
    "\n",
    "    model_exhaustion_shot, scaler_exhaustion_shot, target_scaler_shot, X_val_exh_shot, y_val_exh_shot = train_exhaustion_model(\n",
    "        shot_train_data, shot_test_data, features_exhaustion_shot, timesteps,\n",
    "        epochs=hyperparams[\"epochs\"],\n",
    "        batch_size=hyperparams[\"batch_size\"],\n",
    "        early_stop_patience=hyperparams[\"early_stop_patience\"],\n",
    "        num_lstm_layers=arch_exhaustion[\"num_lstm_layers\"],\n",
    "        lstm_units=arch_exhaustion[\"lstm_units\"],\n",
    "        dropout_rate=arch_exhaustion[\"dropout_rate\"],\n",
    "        dense_units=arch_exhaustion[\"dense_units\"],\n",
    "        dense_activation=arch_exhaustion[\"dense_activation\"]\n",
    "    )\n",
    "    model_injury_shot, scaler_injury_shot, X_val_injury_shot, y_val_injury_shot = train_injury_model(\n",
    "        shot_train_data, shot_test_data, features_injury_shot, timesteps,\n",
    "        epochs=hyperparams[\"epochs\"],\n",
    "        batch_size=hyperparams[\"batch_size\"],\n",
    "        num_lstm_layers=arch_injury[\"num_lstm_layers\"],\n",
    "        lstm_units=arch_injury[\"lstm_units\"],\n",
    "        dropout_rate=arch_injury[\"dropout_rate\"],\n",
    "        dense_units=arch_injury[\"dense_units\"],\n",
    "        dense_activation=arch_injury[\"dense_activation\"]\n",
    "    )\n",
    "    \n",
    "    forecast_and_plot_exhaustion(\n",
    "        model=model_exhaustion_shot,\n",
    "        test_data=shot_test_data,\n",
    "        forecast_features=features_exhaustion_shot,\n",
    "        scaler_exhaustion=scaler_exhaustion_shot,\n",
    "        target_scaler=target_scaler_shot,\n",
    "        timesteps=timesteps,\n",
    "        future_steps=50,\n",
    "        title=\"Shot Phase Summary Aggregated Exhaustion Forecast\"\n",
    "    )\n",
    "    forecast_and_plot_injury(\n",
    "        model=model_injury_shot,\n",
    "        test_data=shot_test_data,\n",
    "        forecast_features=features_injury_shot,\n",
    "        scaler_injury=scaler_injury_shot,\n",
    "        timesteps=timesteps,\n",
    "        future_steps=50,\n",
    "        title=\"Shot Phase Summary Aggregated Injury Forecast\"\n",
    "    )\n",
    "    \n",
    "    shot_summary_df = summarize_all_models(\n",
    "        model_exhaustion_shot, X_val_exh_shot, y_val_exh_shot, target_scaler_shot,\n",
    "        model_injury_shot, X_val_injury_shot, y_val_injury_shot,\n",
    "        joint_models, shot_test_data, timesteps, output_dir,\n",
    "        include_joint_models=False, debug=debug\n",
    "    )\n",
    "\n",
    "    print(\"=== Model Summaries (Shot Phase Summary Aggregated Data) ===\")\n",
    "    print(shot_summary_df)\n",
    "\n",
    "\n",
    "    # ------------------------------\n",
    "    # Final Step: Group and Compare Summaries Across Datasets\n",
    "    # ------------------------------\n",
    "\n",
    "    # Separate base summary into regression and classification parts.\n",
    "    base_reg = summary_df[summary_df[\"Type\"] == \"Regression\"]\n",
    "    base_class = summary_df[summary_df[\"Type\"] == \"Classification\"]\n",
    "\n",
    "    trial_reg = trial_summary_df[trial_summary_df[\"Type\"] == \"Regression\"]\n",
    "    trial_class = trial_summary_df[trial_summary_df[\"Type\"] == \"Classification\"]\n",
    "\n",
    "    shot_reg = shot_summary_df[shot_summary_df[\"Type\"] == \"Regression\"]\n",
    "    shot_class = shot_summary_df[shot_summary_df[\"Type\"] == \"Classification\"]\n",
    "\n",
    "    # Generate joint injury summary from the base test data.\n",
    "    joint_injury_dict = summarize_joint_models(joint_models, test_data, timesteps, debug=debug)\n",
    "    joint_injury_df = pd.DataFrame.from_dict(joint_injury_dict, orient='index').reset_index().rename(columns={'index': 'Model'})\n",
    "    joint_injury_df[\"Type\"] = \"Classification\"\n",
    "\n",
    "    # Generate joint exhaustion summary from the base test data.\n",
    "    joint_exh_dict = summarize_joint_exhaustion_models(joint_exhaustion_models, test_data, timesteps, debug=debug)\n",
    "    joint_exh_df = pd.DataFrame.from_dict(joint_exh_dict, orient='index').reset_index().rename(columns={'index': 'Model'})\n",
    "    joint_exh_df[\"Type\"] = \"Regression\"\n",
    "\n",
    "    # Build lists for each group.\n",
    "    regression_summaries = [base_reg, trial_reg, shot_reg]\n",
    "    classification_summaries = [base_class, trial_class, shot_class]\n",
    "    # Combine both joint summaries into one list.\n",
    "    joint_summaries = [joint_injury_df, joint_exh_df]\n",
    "\n",
    "    # Provide names for each dataset.\n",
    "    dataset_names = [\"Base\", \"Trial Aggregated\", \"Shot Aggregated\"]\n",
    "    # For joint models, you may label them as \"Joint Injury\" and \"Joint Exhaustion\".\n",
    "    joint_names = [\"Joint Injury Models\", \"Joint Exhaustion Models\"]\n",
    "\n",
    "    # Get the final combined summaries (including joint summaries).\n",
    "    final_reg, final_class, final_joint, final_all = final_model_summary(\n",
    "        regression_summaries, classification_summaries, \n",
    "        regression_names=dataset_names, classification_names=dataset_names,\n",
    "        joint_summaries=joint_summaries, joint_names=joint_names\n",
    "    )\n",
    "\n",
    "    print(\"=== Final Regression Summary ===\")\n",
    "    print(final_reg)\n",
    "    print(\"\\n=== Final Classification Summary ===\")\n",
    "    print(final_class)\n",
    "    print(\"\\n=== Final Joint Summary ===\")\n",
    "    print(final_joint)\n",
    "    print(\"\\n=== Final Combined Summary ===\")\n",
    "    print(final_all)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add in Conformal Tights\n",
    "https://github.com/superlinear-ai/conformal-tights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ml/preprocess_train_predict/conformal_tights.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ml/preprocess_train_predict/conformal_tights.py\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import sys\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error, r2_score, accuracy_score,\n",
    "    precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from ml.load_and_prepare_data.load_data_and_analyze import (\n",
    "    load_data, prepare_joint_features, feature_engineering, summarize_data, check_and_drop_nulls,\n",
    "    prepare_base_datasets)\n",
    "\n",
    "from ml.feature_selection.feature_selection import (\n",
    "    load_top_features, perform_feature_importance_analysis, save_top_features,\n",
    "    analyze_joint_injury_features, check_for_invalid_values,\n",
    "    perform_feature_importance_analysis, analyze_and_display_top_features,\n",
    "    run_feature_import_and_load_top_features)\n",
    "\n",
    "from ml.preprocess_train_predict.base_training import (\n",
    "    temporal_train_test_split, scale_features, create_sequences, train_exhaustion_model, \n",
    "    train_injury_model,  train_joint_models, forecast_and_plot_exhaustion, forecast_and_plot_injury,\n",
    "    forecast_and_plot_joint, summarize_regression_model, summarize_classification_model, \n",
    "    summarize_joint_models, summarize_all_models, final_model_summary, \n",
    "    summarize_joint_exhaustion_models\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "# ==================== CONFORMAL UNCERTAINTY FUNCTIONS ====================\n",
    "def train_conformal_model(train_data, test_data, features, target_col=\"by_trial_exhaustion_score\", \n",
    "                         estimator=None):\n",
    "    \"\"\"\n",
    "    Trains a scikit-learn model with Conformal Tights for uncertainty quantification.\n",
    "    \n",
    "    Parameters:\n",
    "      - train_data (DataFrame): Training data.\n",
    "      - test_data (DataFrame): Testing data.\n",
    "      - features (list): List of feature column names.\n",
    "      - target_col (str): Target variable name (default \"by_trial_exhaustion_score\").\n",
    "      - estimator: A scikit-learn estimator. If None, uses XGBRegressor by default.\n",
    "      \n",
    "    Returns:\n",
    "      - conformal_predictor: Fitted ConformalCoherentQuantileRegressor.\n",
    "      - X_test (numpy array): Test features.\n",
    "      - y_test (numpy array): Test target values.\n",
    "    \"\"\"\n",
    "    from conformal_tights import ConformalCoherentQuantileRegressor\n",
    "    from xgboost import XGBRegressor\n",
    "    \n",
    "    # Use XGBRegressor as the default estimator if none is provided.\n",
    "    if estimator is None:\n",
    "        estimator = XGBRegressor(objective=\"reg:squarederror\")\n",
    "    \n",
    "    # Extract features and target from train and test data.\n",
    "    X_train = train_data[features].values\n",
    "    y_train = train_data[target_col].values\n",
    "    X_test = test_data[features].values\n",
    "    y_test = test_data[target_col].values\n",
    "    \n",
    "    # Create and fit the conformal predictor.\n",
    "    conformal_predictor = ConformalCoherentQuantileRegressor(estimator=estimator)\n",
    "    conformal_predictor.fit(X_train, y_train)\n",
    "    \n",
    "    logging.info(f\"Trained conformal model for target '{target_col}' using {len(features)} features\")\n",
    "    \n",
    "    return conformal_predictor, X_test, y_test\n",
    "\n",
    "def predict_with_uncertainty(conformal_predictor, X_test, y_test, \n",
    "                           quantiles=(0.025, 0.05, 0.1, 0.5, 0.9, 0.95, 0.975)):\n",
    "    \"\"\"\n",
    "    Makes predictions with uncertainty estimates.\n",
    "    \n",
    "    Parameters:\n",
    "      - conformal_predictor: Fitted conformal predictor.\n",
    "      - X_test (numpy array): Test features.\n",
    "      - y_test (numpy array): True target values.\n",
    "      - quantiles (tuple): Quantiles to compute.\n",
    "      \n",
    "    Returns:\n",
    "      - y_pred: Point predictions.\n",
    "      - y_quantiles: DataFrame of quantile predictions.\n",
    "      - y_interval: Prediction interval for 95% coverage.\n",
    "    \"\"\"\n",
    "    # Make point predictions.\n",
    "    y_pred = conformal_predictor.predict(X_test)\n",
    "    \n",
    "    # Get quantile predictions (as a DataFrame).\n",
    "    y_quantiles = conformal_predictor.predict_quantiles(X_test, quantiles=quantiles)\n",
    "    \n",
    "    # Get interval predictions (95% coverage).\n",
    "    y_interval = conformal_predictor.predict_interval(X_test, coverage=0.95)\n",
    "    \n",
    "    # Calculate empirical coverage.\n",
    "    lower_bound = y_interval[:, 0]\n",
    "    upper_bound = y_interval[:, 1]\n",
    "    empirical_coverage = np.mean((lower_bound <= y_test) & (y_test <= upper_bound))\n",
    "    print(f\"Empirical coverage: {empirical_coverage:.4f}\")\n",
    "    \n",
    "    return y_pred, y_quantiles, y_interval\n",
    "\n",
    "def plot_conformal_results(X_test, y_test, y_pred, y_quantiles, y_interval, sample_size=50):\n",
    "    \"\"\"\n",
    "    Plots predictions with uncertainty for a subset of test points.\n",
    "    \n",
    "    Parameters:\n",
    "      - X_test: Test features (not used directly in plot).\n",
    "      - y_test: True target values.\n",
    "      - y_pred: Point predictions.\n",
    "      - y_quantiles: DataFrame of quantile predictions.\n",
    "      - y_interval: Array of prediction intervals.\n",
    "      - sample_size: Number of sample points to plot.\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    \n",
    "    # Select sample indices.\n",
    "    if len(y_test) > sample_size:\n",
    "        idx = np.sort(np.random.choice(len(y_test), sample_size, replace=False))\n",
    "    else:\n",
    "        idx = np.arange(len(y_test))\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot actual values.\n",
    "    plt.plot(range(len(idx)), y_test[idx], 'ro', markersize=5, label='Actual')\n",
    "    \n",
    "    # Plot predicted values.\n",
    "    plt.plot(range(len(idx)), y_pred[idx], 'bo', markersize=3, label='Predicted')\n",
    "    \n",
    "    # Plot a 90% prediction interval (using 0.05 and 0.95 quantiles).\n",
    "    try:\n",
    "        lower_q = np.where(y_quantiles.columns == 0.05)[0][0]\n",
    "        upper_q = np.where(y_quantiles.columns == 0.95)[0][0]\n",
    "        plt.fill_between(\n",
    "            range(len(idx)),\n",
    "            y_quantiles.iloc[idx, lower_q],\n",
    "            y_quantiles.iloc[idx, upper_q],\n",
    "            alpha=0.3,\n",
    "            color='skyblue',\n",
    "            label='90% Prediction Interval'\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(\"Error plotting quantile intervals:\", e)\n",
    "    \n",
    "    plt.title('Predictions with Conformal Uncertainty')\n",
    "    plt.xlabel('Sample Index')\n",
    "    plt.ylabel('Target Value')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "def add_conformal_to_exhaustion_model(train_data, test_data, target_col=\"by_trial_exhaustion_score\"):\n",
    "    \"\"\"\n",
    "    Example integration to add conformal prediction to the exhaustion model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_data : pandas.DataFrame\n",
    "        The training dataset, must include all features listed below plus the target column.\n",
    "    test_data : pandas.DataFrame\n",
    "        The test dataset, must include all features listed below plus the target column.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    conformal_model : object\n",
    "        The fitted conformal predictor.\n",
    "    X_test : pandas.DataFrame\n",
    "        The test‐set features used for evaluation.\n",
    "    y_test : pandas.Series\n",
    "        The true target values for the test set.\n",
    "    y_pred : numpy.ndarray\n",
    "        The point predictions on the test set.\n",
    "    y_quantiles : numpy.ndarray\n",
    "        The predicted lower and upper quantiles for each test instance.\n",
    "    y_interval : numpy.ndarray\n",
    "        The full prediction intervals for each test instance.\n",
    "    \"\"\"\n",
    "    # Define features for the exhaustion model (can be adjusted as needed).\n",
    "    features_exhaustion = [\n",
    "        'joint_energy', 'joint_power', 'energy_acceleration',\n",
    "        'elbow_asymmetry', 'hip_asymmetry', 'wrist_asymmetry', 'knee_asymmetry',\n",
    "        'L_KNEE_ROM', 'R_KNEE_ROM', 'L_SHOULDER_ROM', 'R_SHOULDER_ROM',\n",
    "        'exhaustion_lag1', 'power_avg_5', 'rolling_hr_mean'\n",
    "    ]\n",
    "\n",
    "    # Split into X/y for train and test\n",
    "    X_train = train_data[features_exhaustion]\n",
    "    y_train = train_data[target_col]\n",
    "\n",
    "    # Train the conformal model using the provided train/test data.\n",
    "    conformal_model, X_test, y_test = train_conformal_model(\n",
    "        train_data, test_data,\n",
    "        features=features_exhaustion,\n",
    "        target_col=target_col\n",
    "    )\n",
    "\n",
    "    # Get predictions with uncertainty.\n",
    "    y_pred, y_quantiles, y_interval = predict_with_uncertainty(\n",
    "        conformal_model, X_test, y_test\n",
    "    )\n",
    "\n",
    "    # Visualize the results.\n",
    "    plot_conformal_results(X_test, y_test, y_pred, y_quantiles, y_interval)\n",
    "\n",
    "    return conformal_model\n",
    "\n",
    "\n",
    "def add_time_series_forecasting(\n",
    "    data: pd.DataFrame,\n",
    "    time_col: str = 'timestamp',\n",
    "    value_cols: list = ['by_trial_exhaustion_score']\n",
    "):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pandas.DataFrame\n",
    "        The full DataFrame with your time series data.\n",
    "    time_col : str\n",
    "        Column name for timestamps.\n",
    "    value_cols : list of str\n",
    "        Column(s) for target series.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    from darts import TimeSeries\n",
    "    from conformal_tights import ConformalCoherentQuantileRegressor, DartsForecaster\n",
    "    from xgboost import XGBRegressor\n",
    "    \n",
    "    features_exhaustion = [\n",
    "        'joint_energy', 'joint_power', 'energy_acceleration',\n",
    "        'elbow_asymmetry', 'hip_asymmetry', 'wrist_asymmetry', 'knee_asymmetry',\n",
    "        'L_KNEE_ROM', 'R_KNEE_ROM', 'L_SHOULDER_ROM', 'R_SHOULDER_ROM',\n",
    "        'exhaustion_lag1', 'power_avg_5', 'rolling_hr_mean'\n",
    "    ]\n",
    "    # Ensure timestamp column exists\n",
    "    if time_col not in data.columns:\n",
    "        raise ValueError(f\"Timestamp column '{time_col}' not found in DataFrame.\")\n",
    "\n",
    "    # Instantiate predictor\n",
    "    xgb_regressor = XGBRegressor(objective=\"reg:squarederror\")\n",
    "    conformal_predictor = ConformalCoherentQuantileRegressor(estimator=xgb_regressor)\n",
    "\n",
    "    # Build TimeSeries objects\n",
    "    target_series = TimeSeries.from_dataframe(data, time_col=time_col, value_cols=value_cols)\n",
    "    covariates    = TimeSeries.from_dataframe(data, time_col=time_col, value_cols=features_exhaustion)\n",
    "\n",
    "    # Split 80/20\n",
    "    train_cutoff   = int(0.8 * len(data))\n",
    "    train_target   = target_series[:train_cutoff]\n",
    "    test_target    = target_series[train_cutoff:]\n",
    "    train_covariates = covariates[:train_cutoff]\n",
    "    test_covariates  = covariates[train_cutoff:]\n",
    "\n",
    "    # Fit forecaster\n",
    "    forecaster = DartsForecaster(model=conformal_predictor, lags=24, lags_future_covariates=[0])\n",
    "    forecaster.fit(train_target, future_covariates=train_covariates)\n",
    "\n",
    "    # Predict\n",
    "    forecast = forecaster.predict(\n",
    "        n=len(test_target),\n",
    "        future_covariates=test_covariates,\n",
    "        num_samples=500,\n",
    "        quantiles=(0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95)\n",
    "    )\n",
    "\n",
    "    return forecaster, forecast, test_target\n",
    "\n",
    "\n",
    "# ==================== MODEL SUMMARY FUNCTIONS ====================\n",
    "\n",
    "# --- Main Script: Running Three Separate Analyses ---\n",
    "if __name__ == \"__main__\":\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "    from ml.load_and_prepare_data.load_data_and_analyze import (\n",
    "        load_data, prepare_joint_features, feature_engineering, summarize_data, check_and_drop_nulls\n",
    "    )\n",
    "    debug = True\n",
    "    importance_threshold = 0.01\n",
    "    csv_path = \"../../data/processed/final_granular_dataset.csv\"\n",
    "    json_path = \"../../data/basketball/freethrow/participant_information.json\"\n",
    "    output_dir = \"../../data/Deep_Learning_Final\"\n",
    "    \n",
    "    base_feature_dir = os.path.join(output_dir, \"feature_lists/base\")\n",
    "    trial_feature_dir = os.path.join(output_dir, \"feature_lists/trial_summary\")\n",
    "    shot_feature_dir = os.path.join(output_dir, \"feature_lists/shot_phase_summary\")\n",
    "    \n",
    "    data, trial_df, shot_df = prepare_base_datasets(csv_path, json_path, debug=debug)\n",
    "    numeric_features = data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    summary_targets = ['exhaustion_rate', 'injury_risk']\n",
    "    trial_summary_features = [col for col in trial_df.columns if col not in summary_targets]\n",
    "    trial_summary_features = [col for col in trial_summary_features if col in numeric_features]\n",
    "    shot_summary_features = [col for col in shot_df.columns if col not in summary_targets]\n",
    "    shot_summary_features = [col for col in shot_summary_features if col in numeric_features]\n",
    "    # ========================================\n",
    "    # 1) Overall Base Dataset (including Joint-Specific Targets)\n",
    "    # ========================================\n",
    "    features = [\n",
    "        'joint_energy', 'joint_power', 'energy_acceleration',\n",
    "        'elbow_asymmetry', 'hip_asymmetry', 'ankle_asymmetry', 'wrist_asymmetry', 'knee_asymmetry', \n",
    "        '1stfinger_asymmetry', '5thfinger_asymmetry',\n",
    "        'elbow_power_ratio', 'hip_power_ratio', 'ankle_power_ratio', 'wrist_power_ratio', \n",
    "        'knee_power_ratio', '1stfinger_power_ratio', '5thfinger_power_ratio',\n",
    "        'L_KNEE_ROM', 'L_KNEE_ROM_deviation', 'L_KNEE_ROM_extreme',\n",
    "        'R_KNEE_ROM', 'R_KNEE_ROM_deviation', 'R_KNEE_ROM_extreme',\n",
    "        'L_SHOULDER_ROM', 'L_SHOULDER_ROM_deviation', 'L_SHOULDER_ROM_extreme',\n",
    "        'R_SHOULDER_ROM', 'R_SHOULDER_ROM_deviation', 'R_SHOULDER_ROM_extreme',\n",
    "        'L_HIP_ROM', 'L_HIP_ROM_deviation', 'L_HIP_ROM_extreme',\n",
    "        'R_HIP_ROM', 'R_HIP_ROM_deviation', 'R_HIP_ROM_extreme',\n",
    "        'L_ANKLE_ROM', 'L_ANKLE_ROM_deviation', 'L_ANKLE_ROM_extreme',\n",
    "        'R_ANKLE_ROM', 'R_ANKLE_ROM_deviation', 'R_ANKLE_ROM_extreme',\n",
    "        'exhaustion_lag1', 'power_avg_5', 'rolling_power_std', 'rolling_hr_mean',\n",
    "        'time_since_start', 'ema_exhaustion', 'rolling_exhaustion', 'rolling_energy_std',\n",
    "        'simulated_HR',\n",
    "        'player_height_in_meters', 'player_weight__in_kg'\n",
    "    ]\n",
    "    base_targets = ['exhaustion_rate', 'injury_risk']\n",
    "    joints = ['ANKLE', 'WRIST', 'ELBOW', 'KNEE', 'HIP']\n",
    "    joint_injury_targets = [f\"{side}_{joint}_injury_risk\" for joint in joints for side in ['L', 'R']]\n",
    "    joint_exhaustion_targets = [f\"{side}_{joint}_exhaustion_rate\" for joint in joints for side in ['L', 'R']]\n",
    "    joint_targets = joint_injury_targets + joint_exhaustion_targets\n",
    "    all_targets = base_targets + joint_targets\n",
    "\n",
    "    logging.info(\"=== Base Dataset Analysis (Overall + Joint-Specific) ===\")\n",
    "    base_loaded_features = run_feature_import_and_load_top_features(\n",
    "        dataset=data,\n",
    "        features=features,\n",
    "        targets=all_targets,\n",
    "        base_output_dir=output_dir,\n",
    "        output_subdir=\"feature_lists/base\",\n",
    "        debug=debug,\n",
    "        dataset_label=\"Base Data\",\n",
    "        importance_threshold=importance_threshold,\n",
    "        n_top=10,\n",
    "        run_analysis=False\n",
    "    )\n",
    "    print(f\"Base Loaded Features: {base_loaded_features}\")\n",
    "    \n",
    "    joint_feature_dict = {}\n",
    "    for target in joint_targets:\n",
    "        try:\n",
    "            feat_loaded = base_loaded_features.get(target, [])\n",
    "            logging.info(f\"Test Load: Features for {target}: {feat_loaded}\")\n",
    "            joint_feature_dict[target] = feat_loaded\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading features for {target}: {e}\")\n",
    "    \n",
    "    # ========================================\n",
    "    # 2) Trial Summary Dataset Analysis\n",
    "    # ========================================\n",
    "\n",
    "    \n",
    "    logging.info(\"=== Trial Summary Dataset Analysis ===\")\n",
    "    trial_loaded_features = run_feature_import_and_load_top_features(\n",
    "        dataset=trial_df,\n",
    "        features=trial_summary_features,\n",
    "        targets=summary_targets,\n",
    "        base_output_dir=output_dir,\n",
    "        output_subdir=\"feature_lists/trial_summary\",\n",
    "        debug=debug,\n",
    "        dataset_label=\"Trial Summary Data\",\n",
    "        importance_threshold=importance_threshold,\n",
    "        n_top=10,\n",
    "        run_analysis=False\n",
    "    )\n",
    "    features_exhaustion_trial = trial_loaded_features.get('exhaustion_rate', [])\n",
    "    features_injury_trial = trial_loaded_features.get('injury_risk', [])\n",
    "    trial_summary_data = trial_df.copy()\n",
    "    \n",
    "    # ========================================\n",
    "    # 3) Shot Phase Summary Dataset Analysis\n",
    "    # ========================================\n",
    "\n",
    "    \n",
    "    logging.info(\"=== Shot Phase Summary Dataset Analysis ===\")\n",
    "    shot_loaded_features = run_feature_import_and_load_top_features(\n",
    "        dataset=shot_df,\n",
    "        features=shot_summary_features,\n",
    "        targets=summary_targets,\n",
    "        base_output_dir=output_dir,\n",
    "        output_subdir=\"feature_lists/shot_phase_summary\",\n",
    "        debug=debug,\n",
    "        dataset_label=\"Shot Phase Summary Data\",\n",
    "        importance_threshold=importance_threshold,\n",
    "        n_top=10,\n",
    "        run_analysis=False\n",
    "    )\n",
    "    features_exhaustion_shot = shot_loaded_features.get('exhaustion_rate', [])\n",
    "    features_injury_shot = shot_loaded_features.get('injury_risk', [])\n",
    "    shot_phase_summary_data = shot_df.copy()\n",
    "    \n",
    "    # ------------------------------\n",
    "    # 5. Split Base Data for Training Models\n",
    "    # ------------------------------\n",
    "    train_data, test_data = temporal_train_test_split(data, test_size=0.2)\n",
    "    timesteps = 5\n",
    "\n",
    "    # Hyperparameters and architecture definitions.\n",
    "    hyperparams = {\n",
    "        \"epochs\": 10,\n",
    "        \"batch_size\": 32,\n",
    "        \"early_stop_patience\": 5\n",
    "    }\n",
    "    arch_exhaustion = {\n",
    "        \"num_lstm_layers\": 1,\n",
    "        \"lstm_units\": 64,\n",
    "        \"dropout_rate\": 0.2,\n",
    "        \"dense_units\": 1,\n",
    "        \"dense_activation\": None\n",
    "    }\n",
    "    arch_injury = {\n",
    "        \"num_lstm_layers\": 1,\n",
    "        \"lstm_units\": 64,\n",
    "        \"dropout_rate\": 0.2,\n",
    "        \"dense_units\": 1,\n",
    "        \"dense_activation\": \"sigmoid\"\n",
    "    }\n",
    "    \n",
    "    # For demonstration, define features/targets (you can adjust these as needed)\n",
    "    features_exhaustion = [\n",
    "        'joint_power', \n",
    "        'joint_energy', \n",
    "        'elbow_asymmetry',  \n",
    "        'L_WRIST_angle', 'R_WRIST_angle',  # Updated: removed \"wrist_angle\"\n",
    "        'exhaustion_lag1', \n",
    "        'power_avg_5',\n",
    "        'simulated_HR',\n",
    "        'player_height_in_meters',\n",
    "        'player_weight__in_kg'\n",
    "    ]\n",
    "    target_exhaustion = 'by_trial_exhaustion_score'\n",
    "\n",
    "    features_injury = [\n",
    "        'joint_power', \n",
    "        'joint_energy', \n",
    "        'elbow_asymmetry',  \n",
    "        'knee_asymmetry', \n",
    "        'L_WRIST_angle', 'R_WRIST_angle',  # Updated: removed \"wrist_angle\"\n",
    "        'exhaustion_lag1', \n",
    "        'power_avg_5',\n",
    "        'simulated_HR',\n",
    "        'player_height_in_meters',\n",
    "        'player_weight__in_kg'\n",
    "    ]\n",
    "    target_injury = 'injury_risk'\n",
    "    # ------------------------------\n",
    "    # 6a. Train Models on Base Data for Overall Exhaustion and Injury Risk\n",
    "    # ------------------------------\n",
    "    model_exhaustion, scaler_exhaustion, target_scaler, X_val_exh, y_val_exh = train_exhaustion_model(\n",
    "        train_data, test_data, features_exhaustion, timesteps,\n",
    "        epochs=hyperparams[\"epochs\"],\n",
    "        batch_size=hyperparams[\"batch_size\"],\n",
    "        early_stop_patience=hyperparams[\"early_stop_patience\"],\n",
    "        num_lstm_layers=arch_exhaustion[\"num_lstm_layers\"],\n",
    "        lstm_units=arch_exhaustion[\"lstm_units\"],\n",
    "        dropout_rate=arch_exhaustion[\"dropout_rate\"],\n",
    "        dense_units=arch_exhaustion[\"dense_units\"],\n",
    "        dense_activation=arch_exhaustion[\"dense_activation\"]\n",
    "    )\n",
    "    model_injury, scaler_injury, X_val_injury, y_val_injury = train_injury_model(\n",
    "        train_data, test_data, features_injury, timesteps,\n",
    "        epochs=hyperparams[\"epochs\"],\n",
    "        batch_size=hyperparams[\"batch_size\"],\n",
    "        num_lstm_layers=arch_injury[\"num_lstm_layers\"],\n",
    "        lstm_units=arch_injury[\"lstm_units\"],\n",
    "        dropout_rate=arch_injury[\"dropout_rate\"],\n",
    "        dense_units=arch_injury[\"dense_units\"],\n",
    "        dense_activation=arch_injury[\"dense_activation\"]\n",
    "    )\n",
    "    # For joint models, train using the base data and the corresponding feature lists.\n",
    "    joint_models = {}\n",
    "    for joint_target, features_list in joint_feature_dict.items():\n",
    "        try:\n",
    "            logging.info(f\"Training joint-specific injury model for {joint_target} using features: {features_list}\")\n",
    "            model, scaler, X_val_joint, y_val_joint = train_injury_model(\n",
    "                train_data, test_data,\n",
    "                features=features_list,\n",
    "                timesteps=timesteps, \n",
    "                epochs=hyperparams[\"epochs\"],\n",
    "                batch_size=hyperparams[\"batch_size\"],\n",
    "                early_stop_patience=hyperparams[\"early_stop_patience\"],\n",
    "                num_lstm_layers=arch_exhaustion[\"num_lstm_layers\"],\n",
    "                lstm_units=arch_exhaustion[\"lstm_units\"],\n",
    "                dropout_rate=arch_exhaustion[\"dropout_rate\"],\n",
    "                dense_units=arch_exhaustion[\"dense_units\"],\n",
    "                dense_activation=arch_exhaustion[\"dense_activation\"],\n",
    "                target_col=joint_target\n",
    "            )\n",
    "            joint_models[joint_target] = {\n",
    "                'model': model,\n",
    "                'features': features_list,\n",
    "                'scaler': scaler\n",
    "            }\n",
    "            logging.info(f\"Successfully trained joint model for {joint_target}.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error training joint model for {joint_target}: {e}\")\n",
    "\n",
    "    # 6c. Train joint exhaustion models (by_trial_exhaustion) for each joint.\n",
    "    joint_exhaustion_models = {}\n",
    "    for joint in joints:\n",
    "        for side in ['L', 'R']:\n",
    "            target_joint_exh = f\"{side}_{joint}_energy_by_trial_exhaustion_score\"\n",
    "            try:\n",
    "                if target_joint_exh in joint_feature_dict:\n",
    "                    features_list = joint_feature_dict[target_joint_exh]\n",
    "                    logging.info(f\"Using preloaded features for {target_joint_exh}: {features_list}\")\n",
    "                else:\n",
    "                    features_list = load_top_features(target_joint_exh, feature_dir=base_feature_dir, df=data, n_top=10)\n",
    "                    logging.info(f\"Loaded features for {target_joint_exh}: {features_list}\")\n",
    "                \n",
    "                model_exh, scaler_exh, target_scaler_exh, X_val_joint_exh, y_val_joint_exh = train_exhaustion_model(\n",
    "                    train_data, test_data,\n",
    "                    features=features_list,\n",
    "                    timesteps=timesteps,\n",
    "                    epochs=hyperparams[\"epochs\"],\n",
    "                    batch_size=hyperparams[\"batch_size\"],\n",
    "                    early_stop_patience=hyperparams[\"early_stop_patience\"],\n",
    "                    num_lstm_layers=arch_exhaustion[\"num_lstm_layers\"],\n",
    "                    lstm_units=arch_exhaustion[\"lstm_units\"],\n",
    "                    dropout_rate=arch_exhaustion[\"dropout_rate\"],\n",
    "                    dense_units=arch_exhaustion[\"dense_units\"],\n",
    "                    dense_activation=arch_exhaustion[\"dense_activation\"],\n",
    "                    target_col=target_joint_exh\n",
    "                )\n",
    "                joint_exhaustion_models[target_joint_exh] = {\n",
    "                    'model': model_exh,\n",
    "                    'features': features_list,\n",
    "                    'scaler': scaler_exh\n",
    "                }\n",
    "                logging.info(f\"Successfully trained joint exhaustion model for {target_joint_exh}.\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error training joint exhaustion model for {target_joint_exh}: {e}\")\n",
    "\n",
    "    # ------------------------------\n",
    "    # 7a. Forecasting for Base Models\n",
    "    # ------------------------------\n",
    "    forecast_and_plot_exhaustion(\n",
    "        model=model_exhaustion,\n",
    "        test_data=test_data,\n",
    "        forecast_features=features_exhaustion,\n",
    "        scaler_exhaustion=scaler_exhaustion,\n",
    "        target_scaler=target_scaler,\n",
    "        timesteps=timesteps,\n",
    "        future_steps=50,\n",
    "        title=\"Overall Exhaustion Model Forecast\"\n",
    "    )\n",
    "    forecast_and_plot_injury(\n",
    "        model=model_injury,\n",
    "        test_data=test_data,\n",
    "        forecast_features=features_injury,\n",
    "        scaler_injury=scaler_injury,\n",
    "        timesteps=timesteps,\n",
    "        future_steps=50,\n",
    "        title=\"Overall Injury Risk Forecast\"\n",
    "    )\n",
    "    forecast_and_plot_joint(\n",
    "        joint_models=joint_models,\n",
    "        test_data=test_data,\n",
    "        timesteps=timesteps,\n",
    "        future_steps=50\n",
    "    )\n",
    "\n",
    "    # ------------------------------\n",
    "    # 8. Summarize Base Model Testing Results\n",
    "    # ------------------------------\n",
    "    summary_df = summarize_all_models(\n",
    "        model_exhaustion, X_val_exh, y_val_exh, target_scaler,\n",
    "        model_injury, X_val_injury, y_val_injury,\n",
    "        joint_models, test_data, timesteps, output_dir\n",
    "    )\n",
    "    print(\"=== Model Summaries (Base Data) ===\")\n",
    "    print(summary_df)\n",
    "\n",
    "\n",
    "    # ------------------------------\n",
    "    # Additional: Conformal Uncertainty Integration\n",
    "    # ------------------------------\n",
    "    logging.info(\"Running conformal uncertainty integration for exhaustion model...\")\n",
    "    conformal_model = add_conformal_to_exhaustion_model(train_data, test_data, target_col='by_trial_exhaustion_score',)\n",
    "    \n",
    "    try:\n",
    "        # Time series forecasting with uncertainty using Darts.\n",
    "        logging.info(\"Running time series forecasting with conformal uncertainty...\")\n",
    "        forecaster, forecast, test_target = add_time_series_forecasting(data\n",
    "                                                                        , time_col='timestamp'\n",
    "                                                                        , value_cols=['by_trial_exhaustion_score'])\n",
    "        # Plot forecast vs. actual (using Darts built-in plotting).\n",
    "        forecast.plot(label=\"Forecast\")\n",
    "        test_target.plot(label=\"Actual\")\n",
    "        plt.title(\"Time Series Forecast with Conformal Uncertainty\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        logging.error(\"An error occurred in time series forecasting: \" + str(e))\n",
    "        sys.exit(1)\n",
    "\n",
    "    # ------------------------------\n",
    "    # 9. Train, Forecast, and Summarize Aggregated Models\n",
    "    # ------------------------------\n",
    "    # Instead of using a hard-coded summary_features list, we now load the top features\n",
    "    # specific to each aggregated dataset (which were saved using the threshold filter).\n",
    "    \n",
    "    # --- 9a. Process Trial Summary Data ---\n",
    "    trial_train_data, trial_test_data = temporal_train_test_split(trial_summary_data, test_size=0.2)\n",
    "    \n",
    "\n",
    "    model_exhaustion_trial, scaler_exhaustion_trial, target_scaler_trial, X_val_exh_trial, y_val_exh_trial = train_exhaustion_model(\n",
    "        trial_train_data, trial_test_data, features_exhaustion_trial, timesteps,\n",
    "        epochs=hyperparams[\"epochs\"],\n",
    "        batch_size=hyperparams[\"batch_size\"],\n",
    "        early_stop_patience=hyperparams[\"early_stop_patience\"],\n",
    "        num_lstm_layers=arch_exhaustion[\"num_lstm_layers\"],\n",
    "        lstm_units=arch_exhaustion[\"lstm_units\"],\n",
    "        dropout_rate=arch_exhaustion[\"dropout_rate\"],\n",
    "        dense_units=arch_exhaustion[\"dense_units\"],\n",
    "        dense_activation=arch_exhaustion[\"dense_activation\"]\n",
    "    )\n",
    "    model_injury_trial, scaler_injury_trial, X_val_injury_trial, y_val_injury_trial = train_injury_model(\n",
    "        trial_train_data, trial_test_data, features_injury_trial, timesteps,\n",
    "        epochs=hyperparams[\"epochs\"],\n",
    "        batch_size=hyperparams[\"batch_size\"],\n",
    "        num_lstm_layers=arch_injury[\"num_lstm_layers\"],\n",
    "        lstm_units=arch_injury[\"lstm_units\"],\n",
    "        dropout_rate=arch_injury[\"dropout_rate\"],\n",
    "        dense_units=arch_injury[\"dense_units\"],\n",
    "        dense_activation=arch_injury[\"dense_activation\"]\n",
    "    )\n",
    "    \n",
    "    forecast_and_plot_exhaustion(\n",
    "        model=model_exhaustion_trial,\n",
    "        test_data=trial_test_data,\n",
    "        forecast_features=features_exhaustion_trial,\n",
    "        scaler_exhaustion=scaler_exhaustion_trial,\n",
    "        target_scaler=target_scaler_trial,\n",
    "        timesteps=timesteps,\n",
    "        future_steps=50,\n",
    "        title=\"Trial Summary Aggregated Exhaustion Forecast\"\n",
    "    )\n",
    "    forecast_and_plot_injury(\n",
    "        model=model_injury_trial,\n",
    "        test_data=trial_test_data,\n",
    "        forecast_features=features_injury_trial,\n",
    "        scaler_injury=scaler_injury_trial,\n",
    "        timesteps=timesteps,\n",
    "        future_steps=50,\n",
    "        title=\"Trial Summary Aggregated Injury Forecast\"\n",
    "    )\n",
    "    \n",
    "    trial_summary_df = summarize_all_models(\n",
    "        model_exhaustion_trial, X_val_exh_trial, y_val_exh_trial, target_scaler_trial,\n",
    "        model_injury_trial, X_val_injury_trial, y_val_injury_trial,\n",
    "        joint_models, trial_test_data, timesteps, output_dir,\n",
    "        include_joint_models=False, debug=debug\n",
    "    )\n",
    "\n",
    "    print(\"=== Model Summaries (Trial Summary Aggregated Data) ===\")\n",
    "    print(trial_summary_df)\n",
    "    \n",
    "    # --- 9b. Process Shot Phase Summary Data ---\n",
    "    shot_train_data, shot_test_data = temporal_train_test_split(shot_phase_summary_data, test_size=0.2)\n",
    "\n",
    "\n",
    "    model_exhaustion_shot, scaler_exhaustion_shot, target_scaler_shot, X_val_exh_shot, y_val_exh_shot = train_exhaustion_model(\n",
    "        shot_train_data, shot_test_data, features_exhaustion_shot, timesteps,\n",
    "        epochs=hyperparams[\"epochs\"],\n",
    "        batch_size=hyperparams[\"batch_size\"],\n",
    "        early_stop_patience=hyperparams[\"early_stop_patience\"],\n",
    "        num_lstm_layers=arch_exhaustion[\"num_lstm_layers\"],\n",
    "        lstm_units=arch_exhaustion[\"lstm_units\"],\n",
    "        dropout_rate=arch_exhaustion[\"dropout_rate\"],\n",
    "        dense_units=arch_exhaustion[\"dense_units\"],\n",
    "        dense_activation=arch_exhaustion[\"dense_activation\"]\n",
    "    )\n",
    "    model_injury_shot, scaler_injury_shot, X_val_injury_shot, y_val_injury_shot = train_injury_model(\n",
    "        shot_train_data, shot_test_data, features_injury_shot, timesteps,\n",
    "        epochs=hyperparams[\"epochs\"],\n",
    "        batch_size=hyperparams[\"batch_size\"],\n",
    "        num_lstm_layers=arch_injury[\"num_lstm_layers\"],\n",
    "        lstm_units=arch_injury[\"lstm_units\"],\n",
    "        dropout_rate=arch_injury[\"dropout_rate\"],\n",
    "        dense_units=arch_injury[\"dense_units\"],\n",
    "        dense_activation=arch_injury[\"dense_activation\"]\n",
    "    )\n",
    "    \n",
    "    forecast_and_plot_exhaustion(\n",
    "        model=model_exhaustion_shot,\n",
    "        test_data=shot_test_data,\n",
    "        forecast_features=features_exhaustion_shot,\n",
    "        scaler_exhaustion=scaler_exhaustion_shot,\n",
    "        target_scaler=target_scaler_shot,\n",
    "        timesteps=timesteps,\n",
    "        future_steps=50,\n",
    "        title=\"Shot Phase Summary Aggregated Exhaustion Forecast\"\n",
    "    )\n",
    "    forecast_and_plot_injury(\n",
    "        model=model_injury_shot,\n",
    "        test_data=shot_test_data,\n",
    "        forecast_features=features_injury_shot,\n",
    "        scaler_injury=scaler_injury_shot,\n",
    "        timesteps=timesteps,\n",
    "        future_steps=50,\n",
    "        title=\"Shot Phase Summary Aggregated Injury Forecast\"\n",
    "    )\n",
    "    \n",
    "    shot_summary_df = summarize_all_models(\n",
    "        model_exhaustion_shot, X_val_exh_shot, y_val_exh_shot, target_scaler_shot,\n",
    "        model_injury_shot, X_val_injury_shot, y_val_injury_shot,\n",
    "        joint_models, shot_test_data, timesteps, output_dir,\n",
    "        include_joint_models=False, debug=debug\n",
    "    )\n",
    "\n",
    "    print(\"=== Model Summaries (Shot Phase Summary Aggregated Data) ===\")\n",
    "    print(shot_summary_df)\n",
    "\n",
    "\n",
    "    # ------------------------------\n",
    "    # Final Step: Group and Compare Summaries Across Datasets\n",
    "    # ------------------------------\n",
    "\n",
    "    # Separate base summary into regression and classification parts.\n",
    "    base_reg = summary_df[summary_df[\"Type\"] == \"Regression\"]\n",
    "    base_class = summary_df[summary_df[\"Type\"] == \"Classification\"]\n",
    "\n",
    "    trial_reg = trial_summary_df[trial_summary_df[\"Type\"] == \"Regression\"]\n",
    "    trial_class = trial_summary_df[trial_summary_df[\"Type\"] == \"Classification\"]\n",
    "\n",
    "    shot_reg = shot_summary_df[shot_summary_df[\"Type\"] == \"Regression\"]\n",
    "    shot_class = shot_summary_df[shot_summary_df[\"Type\"] == \"Classification\"]\n",
    "\n",
    "    # Generate joint injury summary from the base test data.\n",
    "    joint_injury_dict = summarize_joint_models(joint_models, test_data, timesteps, debug=debug)\n",
    "    joint_injury_df = pd.DataFrame.from_dict(joint_injury_dict, orient='index').reset_index().rename(columns={'index': 'Model'})\n",
    "    joint_injury_df[\"Type\"] = \"Classification\"\n",
    "\n",
    "    # Generate joint exhaustion summary from the base test data.\n",
    "    joint_exh_dict = summarize_joint_exhaustion_models(joint_exhaustion_models, test_data, timesteps, debug=debug)\n",
    "    joint_exh_df = pd.DataFrame.from_dict(joint_exh_dict, orient='index').reset_index().rename(columns={'index': 'Model'})\n",
    "    joint_exh_df[\"Type\"] = \"Regression\"\n",
    "    \n",
    "\n",
    "    # Build lists for each group.\n",
    "    regression_summaries = [base_reg, trial_reg, shot_reg]\n",
    "    classification_summaries = [base_class, trial_class, shot_class]\n",
    "    # Combine both joint summaries into one list.\n",
    "    joint_summaries = [joint_injury_df, joint_exh_df]\n",
    "\n",
    "    # Provide names for each dataset.\n",
    "    dataset_names = [\"Base\", \"Trial Aggregated\", \"Shot Aggregated\"]\n",
    "    # For joint models, you may label them as \"Joint Injury\" and \"Joint Exhaustion\".\n",
    "    joint_names = [\"Joint Injury Models\", \"Joint Exhaustion Models\"]\n",
    "\n",
    "    # Get the final combined summaries (including joint summaries).\n",
    "    final_reg, final_class, final_joint, final_all = final_model_summary(\n",
    "        regression_summaries, classification_summaries, \n",
    "        regression_names=dataset_names, classification_names=dataset_names,\n",
    "        joint_summaries=joint_summaries, joint_names=joint_names\n",
    "    )\n",
    "\n",
    "    print(\"=== Final Regression Summary ===\")\n",
    "    print(final_reg)\n",
    "    print(\"\\n=== Final Classification Summary ===\")\n",
    "    print(final_class)\n",
    "    print(\"\\n=== Final Joint Summary ===\")\n",
    "    print(final_joint)\n",
    "    print(\"\\n=== Final Combined Summary ===\")\n",
    "    print(final_all) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add in Darts to compare to other regressors like XGBoost\n",
    "https://github.com/unit8co/darts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Loaded data from ../../data/processed/final_granular_dataset.csv with shape (16047, 228)\n",
      "INFO: Added 'participant_id' column with value 'P0001'\n",
      "INFO: Loaded participant information from ../../data/basketball/freethrow/participant_information.json\n",
      "INFO: Merged participant data. New shape: (16047, 231)\n",
      "INFO: Step [load_data]: DataFrame shape = (16047, 231)\n",
      "INFO: Calculated L_SHOULDER_angle with mean: 105.13°\n",
      "INFO: Calculated R_SHOULDER_angle with mean: 114.71°\n",
      "INFO: Calculated L_HIP_angle with mean: 156.78°\n",
      "INFO: Calculated R_HIP_angle with mean: 159.84°\n",
      "INFO: Calculated L_KNEE_angle with mean: 150.54°\n",
      "INFO: Calculated R_KNEE_angle with mean: 146.17°\n",
      "INFO: Calculated L_ANKLE_angle with mean: 119.13°\n",
      "INFO: Calculated R_ANKLE_angle with mean: 118.34°\n",
      "INFO: Step [calculate_joint_angles]: DataFrame shape = (2958, 233)\n",
      "INFO: New columns added: ['L_SHOULDER_angle', 'R_SHOULDER_angle', 'L_HIP_angle', 'R_HIP_angle', 'L_KNEE_angle', 'R_KNEE_angle', 'L_ANKLE_angle', 'R_ANKLE_angle']\n",
      "INFO:  - L_SHOULDER_angle: dtype=float64, sample values=[67.7599255  73.45068109 79.43065512 85.81173982 92.16039774]\n",
      "INFO:  - R_SHOULDER_angle: dtype=float64, sample values=[63.47467081 70.58256269 77.61355791 84.25189069 90.42032843]\n",
      "INFO:  - L_HIP_angle: dtype=float64, sample values=[129.98600909 130.26576012 131.26589143 132.97719442 135.59436666]\n",
      "INFO:  - R_HIP_angle: dtype=float64, sample values=[127.91693652 128.58897622 129.68513614 131.45738564 133.93337483]\n",
      "INFO:  - L_KNEE_angle: dtype=float64, sample values=[124.65379485 123.01557197 121.86645657 121.3458327  121.87955413]\n",
      "INFO:  - R_KNEE_angle: dtype=float64, sample values=[120.38247641 118.76931833 117.27120232 116.45899301 116.64025318]\n",
      "INFO:  - L_ANKLE_angle: dtype=float64, sample values=[102.01098373 101.2180925  100.72639333 100.41915703 100.67716742]\n",
      "INFO:  - R_ANKLE_angle: dtype=float64, sample values=[94.65854452 93.72844337 92.84398138 92.62063176 92.95167131]\n",
      "INFO: Renamed participant anthropometrics.\n",
      "INFO: Identified 15 joint energy and 14 joint power columns.\n",
      "INFO: Created aggregated 'joint_energy' and 'joint_power'.\n",
      "INFO: Created 'energy_acceleration' as derivative of joint_energy over time.\n",
      "INFO: Created 'ankle_power_ratio' feature comparing left to right ankle ongoing power.\n",
      "INFO: Created asymmetry feature: hip_asymmetry\n",
      "INFO: Created asymmetry feature: ankle_asymmetry\n",
      "INFO: Created asymmetry feature: wrist_asymmetry\n",
      "INFO: Created asymmetry feature: elbow_asymmetry\n",
      "INFO: Created asymmetry feature: knee_asymmetry\n",
      "INFO: Created asymmetry feature: 1stfinger_asymmetry\n",
      "INFO: Created asymmetry feature: 5thfinger_asymmetry\n",
      "INFO: Created power ratio feature: hip_power_ratio using columns L_HIP_ongoing_power and R_HIP_ongoing_power\n",
      "INFO: Created power ratio feature: ankle_power_ratio using columns L_ANKLE_ongoing_power and R_ANKLE_ongoing_power\n",
      "INFO: Created power ratio feature: wrist_power_ratio using columns L_WRIST_ongoing_power and R_WRIST_ongoing_power\n",
      "INFO: Created power ratio feature: elbow_power_ratio using columns L_ELBOW_ongoing_power and R_ELBOW_ongoing_power\n",
      "INFO: Created power ratio feature: knee_power_ratio using columns L_KNEE_ongoing_power and R_KNEE_ongoing_power\n",
      "INFO: Created power ratio feature: 1stfinger_power_ratio using columns L_1STFINGER_ongoing_power and R_1STFINGER_ongoing_power\n",
      "INFO: Created power ratio feature: 5thfinger_power_ratio using columns L_5THFINGER_ongoing_power and R_5THFINGER_ongoing_power\n",
      "INFO: Computed ROM for L KNEE as L_KNEE_ROM\n",
      "INFO: Computed ROM deviation for L KNEE as L_KNEE_ROM_deviation\n",
      "INFO: Created binary flag for L KNEE ROM extremes: L_KNEE_ROM_extreme\n",
      "INFO: Computed ROM for R KNEE as R_KNEE_ROM\n",
      "INFO: Computed ROM deviation for R KNEE as R_KNEE_ROM_deviation\n",
      "INFO: Created binary flag for R KNEE ROM extremes: R_KNEE_ROM_extreme\n",
      "INFO: Computed ROM for L SHOULDER as L_SHOULDER_ROM\n",
      "INFO: Computed ROM deviation for L SHOULDER as L_SHOULDER_ROM_deviation\n",
      "INFO: Created binary flag for L SHOULDER ROM extremes: L_SHOULDER_ROM_extreme\n",
      "INFO: Computed ROM for R SHOULDER as R_SHOULDER_ROM\n",
      "INFO: Computed ROM deviation for R SHOULDER as R_SHOULDER_ROM_deviation\n",
      "INFO: Created binary flag for R SHOULDER ROM extremes: R_SHOULDER_ROM_extreme\n",
      "INFO: Computed ROM for L HIP as L_HIP_ROM\n",
      "INFO: Computed ROM deviation for L HIP as L_HIP_ROM_deviation\n",
      "INFO: Created binary flag for L HIP ROM extremes: L_HIP_ROM_extreme\n",
      "INFO: Computed ROM for R HIP as R_HIP_ROM\n",
      "INFO: Computed ROM deviation for R HIP as R_HIP_ROM_deviation\n",
      "INFO: Created binary flag for R HIP ROM extremes: R_HIP_ROM_extreme\n",
      "INFO: Computed ROM for L ANKLE as L_ANKLE_ROM\n",
      "INFO: Computed ROM deviation for L ANKLE as L_ANKLE_ROM_deviation\n",
      "INFO: Created binary flag for L ANKLE ROM extremes: L_ANKLE_ROM_extreme\n",
      "INFO: Computed ROM for R ANKLE as R_ANKLE_ROM\n",
      "INFO: Computed ROM deviation for R ANKLE as R_ANKLE_ROM_deviation\n",
      "INFO: Created binary flag for R ANKLE ROM extremes: R_ANKLE_ROM_extreme\n",
      "INFO: Computed ROM for L WRIST as L_WRIST_ROM\n",
      "INFO: Computed ROM deviation for L WRIST as L_WRIST_ROM_deviation\n",
      "INFO: Created binary flag for L WRIST ROM extremes: L_WRIST_ROM_extreme\n",
      "INFO: Computed ROM for R WRIST as R_WRIST_ROM\n",
      "INFO: Computed ROM deviation for R WRIST as R_WRIST_ROM_deviation\n",
      "INFO: Created binary flag for R WRIST ROM extremes: R_WRIST_ROM_extreme\n",
      "INFO: Sorted data by 'participant_id' and 'continuous_frame_time'.\n",
      "INFO: Created 'exhaustion_rate' feature.\n",
      "INFO: Created 'simulated_HR' feature.\n",
      "INFO: Step [prepare_joint_features]: DataFrame shape = (2958, 282)\n",
      "INFO: New columns added: ['player_height_in_meters', 'player_weight__in_kg', 'joint_energy', 'joint_power', 'energy_acceleration', 'ankle_power_ratio', 'hip_asymmetry', 'ankle_asymmetry', 'wrist_asymmetry', 'elbow_asymmetry', 'knee_asymmetry', '1stfinger_asymmetry', '5thfinger_asymmetry', 'hip_power_ratio', 'ankle_power_ratio', 'wrist_power_ratio', 'elbow_power_ratio', 'knee_power_ratio', '1stfinger_power_ratio', '5thfinger_power_ratio', 'L_KNEE_ROM', 'L_KNEE_ROM_deviation', 'L_KNEE_ROM_extreme', 'R_KNEE_ROM', 'R_KNEE_ROM_deviation', 'R_KNEE_ROM_extreme', 'L_SHOULDER_ROM', 'L_SHOULDER_ROM_deviation', 'L_SHOULDER_ROM_extreme', 'R_SHOULDER_ROM', 'R_SHOULDER_ROM_deviation', 'R_SHOULDER_ROM_extreme', 'L_HIP_ROM', 'L_HIP_ROM_deviation', 'L_HIP_ROM_extreme', 'R_HIP_ROM', 'R_HIP_ROM_deviation', 'R_HIP_ROM_extreme', 'L_ANKLE_ROM', 'L_ANKLE_ROM_deviation', 'L_ANKLE_ROM_extreme', 'R_ANKLE_ROM', 'R_ANKLE_ROM_deviation', 'R_ANKLE_ROM_extreme', 'L_WRIST_ROM', 'L_WRIST_ROM_deviation', 'L_WRIST_ROM_extreme', 'R_WRIST_ROM', 'R_WRIST_ROM_deviation', 'R_WRIST_ROM_extreme', 'exhaustion_rate', 'simulated_HR']\n",
      "INFO:  - player_height_in_meters: dtype=float64, sample values=[1.91]\n",
      "INFO:  - player_weight__in_kg: dtype=float64, sample values=[90.7]\n",
      "INFO:  - joint_energy: dtype=float64, sample values=[2.8822437  3.2430416  3.48739073 3.58848085 3.59444513]\n",
      "INFO:  - joint_power: dtype=float64, sample values=[43.67035913 47.69178819 52.83925347 54.37092192 52.85948725]\n",
      "INFO:  - energy_acceleration: dtype=float64, sample values=[1.06117028e-02 7.40451917e-03 3.06333689e-03 1.75420184e-04\n",
      " 9.91668866e-05]\n",
      "INFO:  - ankle_power_ratio: dtype=float64, sample values=[0.999989   0.72760088 1.01904274 0.73720619 0.64757333]\n",
      "INFO:  - hip_asymmetry: dtype=float64, sample values=[0.00163023 0.00523449 0.00622917 0.00687023 0.00457927]\n",
      "INFO:  - ankle_asymmetry: dtype=float64, sample values=[0.00000000e+00 1.12310563e-03 9.71329091e-05 1.78232998e-03\n",
      " 2.77498836e-03]\n",
      "INFO:  - wrist_asymmetry: dtype=float64, sample values=[0.01851108 0.02550815 0.03312197 0.03329263 0.02598216]\n",
      "INFO:  - elbow_asymmetry: dtype=float64, sample values=[0.03478465 0.03811244 0.03162486 0.02243636 0.01536964]\n",
      "INFO:  - knee_asymmetry: dtype=float64, sample values=[0.00292639 0.00433634 0.00717197 0.00687313 0.00816541]\n",
      "INFO:  - 1stfinger_asymmetry: dtype=float64, sample values=[0.00883756 0.0226187  0.0408745  0.04942299 0.04134683]\n",
      "INFO:  - 5thfinger_asymmetry: dtype=float64, sample values=[0.02580722 0.03603709 0.04348361 0.04887027 0.04612895]\n",
      "INFO:  - hip_power_ratio: dtype=float64, sample values=[0.93242342 0.82056318 0.80637473 0.8000839  0.87877647]\n",
      "INFO:  - ankle_power_ratio: dtype=float64, sample values=[0.999989   0.72760088 1.01904274 0.73720619 0.64757333]\n",
      "INFO:  - wrist_power_ratio: dtype=float64, sample values=[1.12396468 1.15111823 1.18023156 1.17195659 1.1294117 ]\n",
      "INFO:  - elbow_power_ratio: dtype=float64, sample values=[0.70966359 0.72417731 0.7859565  0.85657133 0.90817653]\n",
      "INFO:  - knee_power_ratio: dtype=float64, sample values=[0.89528465 0.86841355 0.79235586 0.76452974 0.5967381 ]\n",
      "INFO:  - 1stfinger_power_ratio: dtype=float64, sample values=[1.04899916 1.11843528 1.21095949 1.25377493 1.20770464]\n",
      "INFO:  - 5thfinger_power_ratio: dtype=float64, sample values=[1.1234271  1.15744961 1.18233426 1.20779857 1.20574002]\n",
      "INFO:  - L_KNEE_ROM: dtype=float64, sample values=[52.00092712 53.51096764 53.65900592 52.45838556 49.12814203]\n",
      "INFO:  - L_KNEE_ROM_deviation: dtype=float64, sample values=[67.99907288 66.48903236 66.34099408 67.54161444 70.87185797]\n",
      "INFO:  - L_KNEE_ROM_extreme: dtype=int32, sample values=[1]\n",
      "INFO:  - R_KNEE_ROM: dtype=float64, sample values=[52.19842097 55.56237009 55.28397109 56.00313843 52.32530605]\n",
      "INFO:  - R_KNEE_ROM_deviation: dtype=float64, sample values=[67.80157903 64.43762991 64.71602891 63.99686157 67.67469395]\n",
      "INFO:  - R_KNEE_ROM_extreme: dtype=int32, sample values=[1]\n",
      "INFO:  - L_SHOULDER_ROM: dtype=float64, sample values=[51.20705589 51.7884663  49.83456004 51.11853246 50.20623207]\n",
      "INFO:  - L_SHOULDER_ROM_deviation: dtype=float64, sample values=[0.]\n",
      "INFO:  - L_SHOULDER_ROM_extreme: dtype=int32, sample values=[0]\n",
      "INFO:  - R_SHOULDER_ROM: dtype=float64, sample values=[83.00079595 79.93982326 80.64680686 72.81436519 73.63295774]\n",
      "INFO:  - R_SHOULDER_ROM_deviation: dtype=float64, sample values=[0.]\n",
      "INFO:  - R_SHOULDER_ROM_extreme: dtype=int32, sample values=[0]\n",
      "INFO:  - L_HIP_ROM: dtype=float64, sample values=[38.65006486 36.65101834 40.67778275 35.13016974 35.02083926]\n",
      "INFO:  - L_HIP_ROM_deviation: dtype=float64, sample values=[0.]\n",
      "INFO:  - L_HIP_ROM_extreme: dtype=int32, sample values=[0]\n",
      "INFO:  - R_HIP_ROM: dtype=float64, sample values=[50.5703134  53.29735427 55.01726113 50.74228775 50.23341005]\n",
      "INFO:  - R_HIP_ROM_deviation: dtype=float64, sample values=[0.]\n",
      "INFO:  - R_HIP_ROM_extreme: dtype=int32, sample values=[0]\n",
      "INFO:  - L_ANKLE_ROM: dtype=float64, sample values=[32.32505371 32.99123382 36.39764221 35.45367686 32.1162011 ]\n",
      "INFO:  - L_ANKLE_ROM_deviation: dtype=float64, sample values=[12.32505371 12.99123382 16.39764221 15.45367686 12.1162011 ]\n",
      "INFO:  - L_ANKLE_ROM_extreme: dtype=int32, sample values=[1]\n",
      "INFO:  - R_ANKLE_ROM: dtype=float64, sample values=[39.81071151 45.54250042 45.70572465 43.1338381  39.83072086]\n",
      "INFO:  - R_ANKLE_ROM_deviation: dtype=float64, sample values=[19.81071151 25.54250042 25.70572465 23.1338381  19.83072086]\n",
      "INFO:  - R_ANKLE_ROM_extreme: dtype=int32, sample values=[1]\n",
      "INFO:  - L_WRIST_ROM: dtype=float64, sample values=[20.51159253 24.85503336 27.6434808  26.09213451 19.99680555]\n",
      "INFO:  - L_WRIST_ROM_deviation: dtype=float64, sample values=[0.]\n",
      "INFO:  - L_WRIST_ROM_extreme: dtype=int32, sample values=[0]\n",
      "INFO:  - R_WRIST_ROM: dtype=float64, sample values=[26.94867994 30.72422249 27.0633164  35.1551556  30.2743699 ]\n",
      "INFO:  - R_WRIST_ROM_deviation: dtype=float64, sample values=[0.]\n",
      "INFO:  - R_WRIST_ROM_extreme: dtype=int32, sample values=[0]\n",
      "INFO:  - exhaustion_rate: dtype=float64, sample values=[0.00064171 0.00071098 0.00073159 0.00071125 0.00073347]\n",
      "INFO:  - simulated_HR: dtype=float64, sample values=[61.82696779 61.9679346  62.07643265 62.14297316 62.18103611]\n",
      "INFO: Created 'rolling_energy_std' with sample: [0.0, 0.180398946872653, 0.24857025439306304, 0.27207569408657545, 0.27030460398811107, 0.1359849697907839, 0.05712817700778464, 0.05510293737012356, 0.06401417244507163, 0.05305103813969808]\n",
      "INFO: Created 'rolling_energy_std' with window 5.\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:654: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{joint_name}_rolling_exhaustion'] = data[score_col].rolling(rolling_window, min_periods=1).sum()\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:656: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{joint_name}_injury_risk'] = (rolling_series > safe_expanding_quantile(rolling_series)).astype(int)\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:653: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{joint_name}_exhaustion_rate'] = data[score_col].diff() / dt\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:654: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{joint_name}_rolling_exhaustion'] = data[score_col].rolling(rolling_window, min_periods=1).sum()\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:656: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{joint_name}_injury_risk'] = (rolling_series > safe_expanding_quantile(rolling_series)).astype(int)\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:653: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{joint_name}_exhaustion_rate'] = data[score_col].diff() / dt\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:654: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{joint_name}_rolling_exhaustion'] = data[score_col].rolling(rolling_window, min_periods=1).sum()\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:656: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{joint_name}_injury_risk'] = (rolling_series > safe_expanding_quantile(rolling_series)).astype(int)\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:653: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{joint_name}_exhaustion_rate'] = data[score_col].diff() / dt\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:654: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{joint_name}_rolling_exhaustion'] = data[score_col].rolling(rolling_window, min_periods=1).sum()\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:656: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{joint_name}_injury_risk'] = (rolling_series > safe_expanding_quantile(rolling_series)).astype(int)\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:653: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{joint_name}_exhaustion_rate'] = data[score_col].diff() / dt\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:654: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{joint_name}_rolling_exhaustion'] = data[score_col].rolling(rolling_window, min_periods=1).sum()\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:656: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{joint_name}_injury_risk'] = (rolling_series > safe_expanding_quantile(rolling_series)).astype(int)\n",
      "INFO: Step [feature_engineering]: DataFrame shape = (2957, 321)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joint energy columns:  ['L_ANKLE_energy', 'R_ANKLE_energy', 'L_KNEE_energy', 'R_KNEE_energy', 'L_HIP_energy', 'R_HIP_energy', 'L_ELBOW_energy', 'R_ELBOW_energy', 'L_WRIST_energy', 'R_WRIST_energy', 'L_1STFINGER_energy', 'R_1STFINGER_energy', 'L_5THFINGER_energy', 'R_5THFINGER_energy', 'total_energy']\n",
      "Joint power columns:  ['L_ANKLE_ongoing_power', 'R_ANKLE_ongoing_power', 'L_KNEE_ongoing_power', 'R_KNEE_ongoing_power', 'L_HIP_ongoing_power', 'R_HIP_ongoing_power', 'L_ELBOW_ongoing_power', 'R_ELBOW_ongoing_power', 'L_WRIST_ongoing_power', 'R_WRIST_ongoing_power', 'L_1STFINGER_ongoing_power', 'L_5THFINGER_ongoing_power', 'R_1STFINGER_ongoing_power', 'R_5THFINGER_ongoing_power']\n",
      "All angle columns:  ['entry_angle', 'L_ELBOW_angle', 'L_WRIST_angle', 'L_KNEE_angle', 'L_ELBOW_ongoing_angle', 'L_WRIST_ongoing_angle', 'L_KNEE_ongoing_angle', 'R_ELBOW_angle', 'R_WRIST_angle', 'R_KNEE_angle', 'R_ELBOW_ongoing_angle', 'R_WRIST_ongoing_angle', 'R_KNEE_ongoing_angle', 'initial_release_angle', 'calculated_release_angle', 'angle_difference', 'optimal_release_angle', 'L_SHOULDER_angle', 'R_SHOULDER_angle', 'L_HIP_angle', 'R_HIP_angle', 'L_ANKLE_angle', 'R_ANKLE_angle']\n",
      "print all the columns with by_trial_exhaustion_score:  ['by_trial_exhaustion_score', 'L_ANKLE_energy_by_trial_exhaustion_score', 'R_ANKLE_energy_by_trial_exhaustion_score', 'L_KNEE_energy_by_trial_exhaustion_score', 'R_KNEE_energy_by_trial_exhaustion_score', 'L_HIP_energy_by_trial_exhaustion_score', 'R_HIP_energy_by_trial_exhaustion_score', 'L_ELBOW_energy_by_trial_exhaustion_score', 'R_ELBOW_energy_by_trial_exhaustion_score', 'L_WRIST_energy_by_trial_exhaustion_score', 'R_WRIST_energy_by_trial_exhaustion_score', 'L_1STFINGER_energy_by_trial_exhaustion_score', 'R_1STFINGER_energy_by_trial_exhaustion_score', 'L_5THFINGER_energy_by_trial_exhaustion_score', 'R_5THFINGER_energy_by_trial_exhaustion_score']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: New columns added: ['time_since_start', 'rolling_energy_std', 'exhaustion_lag1', 'ema_exhaustion', 'rolling_exhaustion', 'injury_risk', 'L_ANKLE_exhaustion_rate', 'L_ANKLE_rolling_exhaustion', 'L_ANKLE_injury_risk', 'R_ANKLE_exhaustion_rate', 'R_ANKLE_rolling_exhaustion', 'R_ANKLE_injury_risk', 'L_WRIST_exhaustion_rate', 'L_WRIST_rolling_exhaustion', 'L_WRIST_injury_risk', 'R_WRIST_exhaustion_rate', 'R_WRIST_rolling_exhaustion', 'R_WRIST_injury_risk', 'L_ELBOW_exhaustion_rate', 'L_ELBOW_rolling_exhaustion', 'L_ELBOW_injury_risk', 'R_ELBOW_exhaustion_rate', 'R_ELBOW_rolling_exhaustion', 'R_ELBOW_injury_risk', 'L_KNEE_exhaustion_rate', 'L_KNEE_rolling_exhaustion', 'L_KNEE_injury_risk', 'R_KNEE_exhaustion_rate', 'R_KNEE_rolling_exhaustion', 'R_KNEE_injury_risk', 'L_HIP_exhaustion_rate', 'L_HIP_rolling_exhaustion', 'L_HIP_injury_risk', 'R_HIP_exhaustion_rate', 'R_HIP_rolling_exhaustion', 'R_HIP_injury_risk']\n",
      "INFO:  - time_since_start: dtype=int64, sample values=[ 34  67 100 134 167]\n",
      "INFO:  - rolling_energy_std: dtype=float64, sample values=[0.18039895 0.24857025 0.27207569 0.2703046  0.13598497]\n",
      "INFO:  - exhaustion_lag1: dtype=float64, sample values=[0.64152978 0.66334808 0.68681029 0.7109526  0.73513505]\n",
      "INFO:  - ema_exhaustion: dtype=float64, sample values=[0.64549675 0.6530083  0.66354363 0.67656025 0.69161102]\n",
      "INFO:  - rolling_exhaustion: dtype=float64, sample values=[1.30487786 1.99168815 2.70264076 3.4377758  4.19711531]\n",
      "INFO:  - injury_risk: dtype=int32, sample values=[1 0]\n",
      "INFO:  - L_ANKLE_exhaustion_rate: dtype=float64, sample values=[7.35203613e-05 1.31199819e-04 1.26247085e-04 1.24960586e-04\n",
      " 1.63634924e-04]\n",
      "INFO:  - L_ANKLE_rolling_exhaustion: dtype=float64, sample values=[1.38389373 2.08142003 2.78311249 3.4890536  4.20039467]\n",
      "INFO:  - L_ANKLE_injury_risk: dtype=int32, sample values=[1 0]\n",
      "INFO:  - R_ANKLE_exhaustion_rate: dtype=float64, sample values=[5.73294637e-05 7.30474516e-05 9.71621936e-05 1.09483649e-04\n",
      " 1.24064928e-04]\n",
      "INFO:  - R_ANKLE_rolling_exhaustion: dtype=float64, sample values=[1.65561522 2.486808   3.32120713 4.1593287  5.00154441]\n",
      "INFO:  - R_ANKLE_injury_risk: dtype=int32, sample values=[1 0]\n",
      "INFO:  - L_WRIST_exhaustion_rate: dtype=float64, sample values=[0.0008175  0.00094021 0.00098359 0.00095403 0.0009489 ]\n",
      "INFO:  - L_WRIST_rolling_exhaustion: dtype=float64, sample values=[1.24625794 1.91431144 2.61482338 3.3477723  4.11203508]\n",
      "INFO:  - L_WRIST_injury_risk: dtype=int32, sample values=[1 0]\n",
      "INFO:  - R_WRIST_exhaustion_rate: dtype=float64, sample values=[0.00062388 0.00069982 0.00073728 0.00074206 0.00078717]\n",
      "INFO:  - R_WRIST_rolling_exhaustion: dtype=float64, sample values=[1.30224046 1.98706061 2.69621087 3.43059105 4.1909478 ]\n",
      "INFO:  - R_WRIST_injury_risk: dtype=int32, sample values=[1 0]\n",
      "INFO:  - L_ELBOW_exhaustion_rate: dtype=float64, sample values=[0.0005854  0.00069994 0.00080763 0.0008893  0.00101637]\n",
      "INFO:  - L_ELBOW_rolling_exhaustion: dtype=float64, sample values=[1.14187402 1.74586067 2.37649919 3.03737398 3.73178899]\n",
      "INFO:  - L_ELBOW_injury_risk: dtype=int32, sample values=[1 0]\n",
      "INFO:  - R_ELBOW_exhaustion_rate: dtype=float64, sample values=[0.00066485 0.00073245 0.00077548 0.00080537 0.00089206]\n",
      "INFO:  - R_ELBOW_rolling_exhaustion: dtype=float64, sample values=[1.16006294 1.77556768 2.41666311 3.0851412  3.78305741]\n",
      "INFO:  - R_ELBOW_injury_risk: dtype=int32, sample values=[1 0]\n",
      "INFO:  - L_KNEE_exhaustion_rate: dtype=float64, sample values=[0.00034893 0.0003438  0.00028033 0.00014732 0.00015179]\n",
      "INFO:  - L_KNEE_rolling_exhaustion: dtype=float64, sample values=[1.34758643 2.03865667 2.73897788 3.44430808 4.15464725]\n",
      "INFO:  - L_KNEE_injury_risk: dtype=int32, sample values=[1 0]\n",
      "INFO:  - R_KNEE_exhaustion_rate: dtype=float64, sample values=[0.00035779 0.00038636 0.00032651 0.00021984 0.00013516]\n",
      "INFO:  - R_KNEE_rolling_exhaustion: dtype=float64, sample values=[1.39815239 2.11606098 2.84474439 3.58090229 4.32152052]\n",
      "INFO:  - R_KNEE_injury_risk: dtype=int32, sample values=[1 0]\n",
      "INFO:  - L_HIP_exhaustion_rate: dtype=float64, sample values=[0.00027266 0.00030445 0.00032267 0.00037812 0.00051342]\n",
      "INFO:  - L_HIP_rolling_exhaustion: dtype=float64, sample values=[1.28379964 1.9403813  2.60761117 3.28769708 3.98472584]\n",
      "INFO:  - L_HIP_injury_risk: dtype=int32, sample values=[1 0]\n",
      "INFO:  - R_HIP_exhaustion_rate: dtype=float64, sample values=[0.00031144 0.00035387 0.000378   0.00040329 0.00052453]\n",
      "INFO:  - R_HIP_rolling_exhaustion: dtype=float64, sample values=[1.28251862 1.94075002 2.61145554 3.29587294 3.99759968]\n",
      "INFO:  - R_HIP_injury_risk: dtype=int32, sample values=[1 0]\n",
      "INFO: Calculated L_SHOULDER_angle with mean: 105.15°\n",
      "INFO: Calculated R_SHOULDER_angle with mean: 114.73°\n",
      "INFO: Calculated L_HIP_angle with mean: 156.79°\n",
      "INFO: Calculated R_HIP_angle with mean: 159.85°\n",
      "INFO: Calculated L_KNEE_angle with mean: 150.55°\n",
      "INFO: Calculated R_KNEE_angle with mean: 146.18°\n",
      "INFO: Calculated L_ANKLE_angle with mean: 119.14°\n",
      "INFO: Calculated R_ANKLE_angle with mean: 118.35°\n",
      "INFO: Step [calculate_joint_angles]: DataFrame shape = (2957, 321)\n",
      "INFO: New columns added: ['L_SHOULDER_angle', 'R_SHOULDER_angle', 'L_HIP_angle', 'R_HIP_angle', 'L_KNEE_angle', 'R_KNEE_angle', 'L_ANKLE_angle', 'R_ANKLE_angle']\n",
      "INFO:  - L_SHOULDER_angle: dtype=float64, sample values=[73.45068109 79.43065512 85.81173982 92.16039774 98.24138364]\n",
      "INFO:  - R_SHOULDER_angle: dtype=float64, sample values=[70.58256269 77.61355791 84.25189069 90.42032843 96.1295666 ]\n",
      "INFO:  - L_HIP_angle: dtype=float64, sample values=[130.26576012 131.26589143 132.97719442 135.59436666 139.11360281]\n",
      "INFO:  - R_HIP_angle: dtype=float64, sample values=[128.58897622 129.68513614 131.45738564 133.93337483 137.30094714]\n",
      "INFO:  - L_KNEE_angle: dtype=float64, sample values=[123.01557197 121.86645657 121.3458327  121.87955413 123.97636026]\n",
      "INFO:  - R_KNEE_angle: dtype=float64, sample values=[118.76931833 117.27120232 116.45899301 116.64025318 118.36309984]\n",
      "INFO:  - L_ANKLE_angle: dtype=float64, sample values=[101.2180925  100.72639333 100.41915703 100.67716742 102.2248724 ]\n",
      "INFO:  - R_ANKLE_angle: dtype=float64, sample values=[93.72844337 92.84398138 92.62063176 92.95167131 94.37553481]\n",
      "WARNING: Participant anthropometric columns not found during renaming.\n",
      "INFO: Identified 17 joint energy and 14 joint power columns.\n",
      "INFO: Created aggregated 'joint_energy' and 'joint_power'.\n",
      "INFO: Created 'energy_acceleration' as derivative of joint_energy over time.\n",
      "INFO: Created 'ankle_power_ratio' feature comparing left to right ankle ongoing power.\n",
      "INFO: Created asymmetry feature: hip_asymmetry\n",
      "INFO: Created asymmetry feature: ankle_asymmetry\n",
      "INFO: Created asymmetry feature: wrist_asymmetry\n",
      "INFO: Created asymmetry feature: elbow_asymmetry\n",
      "INFO: Created asymmetry feature: knee_asymmetry\n",
      "INFO: Created asymmetry feature: 1stfinger_asymmetry\n",
      "INFO: Created asymmetry feature: 5thfinger_asymmetry\n",
      "INFO: Created power ratio feature: hip_power_ratio using columns L_HIP_ongoing_power and R_HIP_ongoing_power\n",
      "INFO: Created power ratio feature: ankle_power_ratio using columns L_ANKLE_ongoing_power and R_ANKLE_ongoing_power\n",
      "INFO: Created power ratio feature: wrist_power_ratio using columns L_WRIST_ongoing_power and R_WRIST_ongoing_power\n",
      "INFO: Created power ratio feature: elbow_power_ratio using columns L_ELBOW_ongoing_power and R_ELBOW_ongoing_power\n",
      "INFO: Created power ratio feature: knee_power_ratio using columns L_KNEE_ongoing_power and R_KNEE_ongoing_power\n",
      "INFO: Created power ratio feature: 1stfinger_power_ratio using columns L_1STFINGER_ongoing_power and R_1STFINGER_ongoing_power\n",
      "INFO: Created power ratio feature: 5thfinger_power_ratio using columns L_5THFINGER_ongoing_power and R_5THFINGER_ongoing_power\n",
      "INFO: Computed ROM for L KNEE as L_KNEE_ROM\n",
      "INFO: Computed ROM deviation for L KNEE as L_KNEE_ROM_deviation\n",
      "INFO: Created binary flag for L KNEE ROM extremes: L_KNEE_ROM_extreme\n",
      "INFO: Computed ROM for R KNEE as R_KNEE_ROM\n",
      "INFO: Computed ROM deviation for R KNEE as R_KNEE_ROM_deviation\n",
      "INFO: Created binary flag for R KNEE ROM extremes: R_KNEE_ROM_extreme\n",
      "INFO: Computed ROM for L SHOULDER as L_SHOULDER_ROM\n",
      "INFO: Computed ROM deviation for L SHOULDER as L_SHOULDER_ROM_deviation\n",
      "INFO: Created binary flag for L SHOULDER ROM extremes: L_SHOULDER_ROM_extreme\n",
      "INFO: Computed ROM for R SHOULDER as R_SHOULDER_ROM\n",
      "INFO: Computed ROM deviation for R SHOULDER as R_SHOULDER_ROM_deviation\n",
      "INFO: Created binary flag for R SHOULDER ROM extremes: R_SHOULDER_ROM_extreme\n",
      "INFO: Computed ROM for L HIP as L_HIP_ROM\n",
      "INFO: Computed ROM deviation for L HIP as L_HIP_ROM_deviation\n",
      "INFO: Created binary flag for L HIP ROM extremes: L_HIP_ROM_extreme\n",
      "INFO: Computed ROM for R HIP as R_HIP_ROM\n",
      "INFO: Computed ROM deviation for R HIP as R_HIP_ROM_deviation\n",
      "INFO: Created binary flag for R HIP ROM extremes: R_HIP_ROM_extreme\n",
      "INFO: Computed ROM for L ANKLE as L_ANKLE_ROM\n",
      "INFO: Computed ROM deviation for L ANKLE as L_ANKLE_ROM_deviation\n",
      "INFO: Created binary flag for L ANKLE ROM extremes: L_ANKLE_ROM_extreme\n",
      "INFO: Computed ROM for R ANKLE as R_ANKLE_ROM\n",
      "INFO: Computed ROM deviation for R ANKLE as R_ANKLE_ROM_deviation\n",
      "INFO: Created binary flag for R ANKLE ROM extremes: R_ANKLE_ROM_extreme\n",
      "INFO: Computed ROM for L WRIST as L_WRIST_ROM\n",
      "INFO: Computed ROM deviation for L WRIST as L_WRIST_ROM_deviation\n",
      "INFO: Created binary flag for L WRIST ROM extremes: L_WRIST_ROM_extreme\n",
      "INFO: Computed ROM for R WRIST as R_WRIST_ROM\n",
      "INFO: Computed ROM deviation for R WRIST as R_WRIST_ROM_deviation\n",
      "INFO: Created binary flag for R WRIST ROM extremes: R_WRIST_ROM_extreme\n",
      "INFO: Sorted data by 'participant_id' and 'continuous_frame_time'.\n",
      "INFO: Created 'exhaustion_rate' feature.\n",
      "INFO: Created 'simulated_HR' feature.\n",
      "INFO: Added trial-level aggregated features: trial_mean_exhaustion, trial_total_joint_energy.\n",
      "INFO: Step [prepare_joint_features]: DataFrame shape = (2957, 323)\n",
      "INFO: New columns added: ['joint_energy', 'joint_power', 'energy_acceleration', 'ankle_power_ratio', 'hip_asymmetry', 'ankle_asymmetry', 'wrist_asymmetry', 'elbow_asymmetry', 'knee_asymmetry', '1stfinger_asymmetry', '5thfinger_asymmetry', 'hip_power_ratio', 'ankle_power_ratio', 'wrist_power_ratio', 'elbow_power_ratio', 'knee_power_ratio', '1stfinger_power_ratio', '5thfinger_power_ratio', 'L_KNEE_ROM', 'L_KNEE_ROM_deviation', 'L_KNEE_ROM_extreme', 'R_KNEE_ROM', 'R_KNEE_ROM_deviation', 'R_KNEE_ROM_extreme', 'L_SHOULDER_ROM', 'L_SHOULDER_ROM_deviation', 'L_SHOULDER_ROM_extreme', 'R_SHOULDER_ROM', 'R_SHOULDER_ROM_deviation', 'R_SHOULDER_ROM_extreme', 'L_HIP_ROM', 'L_HIP_ROM_deviation', 'L_HIP_ROM_extreme', 'R_HIP_ROM', 'R_HIP_ROM_deviation', 'R_HIP_ROM_extreme', 'L_ANKLE_ROM', 'L_ANKLE_ROM_deviation', 'L_ANKLE_ROM_extreme', 'R_ANKLE_ROM', 'R_ANKLE_ROM_deviation', 'R_ANKLE_ROM_extreme', 'L_WRIST_ROM', 'L_WRIST_ROM_deviation', 'L_WRIST_ROM_extreme', 'R_WRIST_ROM', 'R_WRIST_ROM_deviation', 'R_WRIST_ROM_extreme', 'exhaustion_rate', 'simulated_HR', 'trial_mean_exhaustion', 'trial_total_joint_energy']\n",
      "INFO:  - joint_energy: dtype=float64, sample values=[6.66648214 7.22335171 7.44903739 7.45919487 7.33142025]\n",
      "INFO:  - joint_power: dtype=float64, sample values=[47.69178819 52.83925347 54.37092192 52.85948725 54.51087334]\n",
      "INFO:  - energy_acceleration: dtype=float64, sample values=[ 0.01687484  0.00683896  0.00029875 -0.00387196  0.00174215]\n",
      "INFO:  - ankle_power_ratio: dtype=float64, sample values=[0.72760088 1.01904274 0.73720619 0.64757333 0.74832863]\n",
      "INFO:  - hip_asymmetry: dtype=float64, sample values=[0.00523449 0.00622917 0.00687023 0.00457927 0.00393719]\n",
      "INFO:  - ankle_asymmetry: dtype=float64, sample values=[1.12310563e-03 9.71329091e-05 1.78232998e-03 2.77498836e-03\n",
      " 2.17951334e-03]\n",
      "INFO:  - wrist_asymmetry: dtype=float64, sample values=[0.02550815 0.03312197 0.03329263 0.02598216 0.01218944]\n",
      "INFO:  - elbow_asymmetry: dtype=float64, sample values=[0.03811244 0.03162486 0.02243636 0.01536964 0.01132337]\n",
      "INFO:  - knee_asymmetry: dtype=float64, sample values=[4.33633806e-03 7.17196771e-03 6.87312543e-03 8.16541076e-03\n",
      " 3.98986399e-16]\n",
      "INFO:  - 1stfinger_asymmetry: dtype=float64, sample values=[0.0226187  0.0408745  0.04942299 0.04134683 0.02343194]\n",
      "INFO:  - 5thfinger_asymmetry: dtype=float64, sample values=[0.03603709 0.04348361 0.04887027 0.04612895 0.02932847]\n",
      "INFO:  - hip_power_ratio: dtype=float64, sample values=[0.82056318 0.80637473 0.8000839  0.87877647 0.91743528]\n",
      "INFO:  - ankle_power_ratio: dtype=float64, sample values=[0.72760088 1.01904274 0.73720619 0.64757333 0.74832863]\n",
      "INFO:  - wrist_power_ratio: dtype=float64, sample values=[1.15111823 1.18023156 1.17195659 1.1294117  1.05896797]\n",
      "INFO:  - elbow_power_ratio: dtype=float64, sample values=[0.72417731 0.7859565  0.85657133 0.90817653 0.93707375]\n",
      "INFO:  - knee_power_ratio: dtype=float64, sample values=[0.86841355 0.79235586 0.76452974 0.5967381  0.99999727]\n",
      "INFO:  - 1stfinger_power_ratio: dtype=float64, sample values=[1.11843528 1.21095949 1.25377493 1.20770464 1.11492578]\n",
      "INFO:  - 5thfinger_power_ratio: dtype=float64, sample values=[1.15744961 1.18233426 1.20779857 1.20574002 1.13534248]\n",
      "INFO:  - L_KNEE_ROM: dtype=float64, sample values=[52.00092712 53.51096764 53.65900592 52.45838556 49.12814203]\n",
      "INFO:  - L_KNEE_ROM_deviation: dtype=float64, sample values=[67.99907288 66.48903236 66.34099408 67.54161444 70.87185797]\n",
      "INFO:  - L_KNEE_ROM_extreme: dtype=int32, sample values=[1]\n",
      "INFO:  - R_KNEE_ROM: dtype=float64, sample values=[52.19842097 55.56237009 55.28397109 56.00313843 52.32530605]\n",
      "INFO:  - R_KNEE_ROM_deviation: dtype=float64, sample values=[67.80157903 64.43762991 64.71602891 63.99686157 67.67469395]\n",
      "INFO:  - R_KNEE_ROM_extreme: dtype=int32, sample values=[1]\n",
      "INFO:  - L_SHOULDER_ROM: dtype=float64, sample values=[45.5163003  51.7884663  49.83456004 51.11853246 50.20623207]\n",
      "INFO:  - L_SHOULDER_ROM_deviation: dtype=float64, sample values=[0.]\n",
      "INFO:  - L_SHOULDER_ROM_extreme: dtype=int32, sample values=[0]\n",
      "INFO:  - R_SHOULDER_ROM: dtype=float64, sample values=[75.89290407 79.93982326 80.64680686 72.81436519 73.63295774]\n",
      "INFO:  - R_SHOULDER_ROM_deviation: dtype=float64, sample values=[0.]\n",
      "INFO:  - R_SHOULDER_ROM_extreme: dtype=int32, sample values=[0]\n",
      "INFO:  - L_HIP_ROM: dtype=float64, sample values=[38.37031383 36.65101834 40.67778275 35.13016974 35.02083926]\n",
      "INFO:  - L_HIP_ROM_deviation: dtype=float64, sample values=[0.]\n",
      "INFO:  - L_HIP_ROM_extreme: dtype=int32, sample values=[0]\n",
      "INFO:  - R_HIP_ROM: dtype=float64, sample values=[49.89827371 53.29735427 55.01726113 50.74228775 50.23341005]\n",
      "INFO:  - R_HIP_ROM_deviation: dtype=float64, sample values=[0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Null summary for Final Data: Total Rows = 2957\n",
      "\n",
      "After dropping rows with nulls in columns: ['energy_acceleration', 'exhaustion_rate']\n",
      "Total Rows = 2957\n",
      "trial_id: 0 nulls, 0.00% null\n",
      "result: 0 nulls, 0.00% null\n",
      "landing_x: 0 nulls, 0.00% null\n",
      "landing_y: 0 nulls, 0.00% null\n",
      "entry_angle: 0 nulls, 0.00% null\n",
      "frame_time: 0 nulls, 0.00% null\n",
      "ball_x: 0 nulls, 0.00% null\n",
      "ball_y: 0 nulls, 0.00% null\n",
      "ball_z: 0 nulls, 0.00% null\n",
      "R_EYE_x: 0 nulls, 0.00% null\n",
      "R_EYE_y: 0 nulls, 0.00% null\n",
      "R_EYE_z: 0 nulls, 0.00% null\n",
      "L_EYE_x: 0 nulls, 0.00% null\n",
      "L_EYE_y: 0 nulls, 0.00% null\n",
      "L_EYE_z: 0 nulls, 0.00% null\n",
      "NOSE_x: 0 nulls, 0.00% null\n",
      "NOSE_y: 0 nulls, 0.00% null\n",
      "NOSE_z: 0 nulls, 0.00% null\n",
      "R_EAR_x: 0 nulls, 0.00% null\n",
      "R_EAR_y: 0 nulls, 0.00% null\n",
      "R_EAR_z: 0 nulls, 0.00% null\n",
      "L_EAR_x: 0 nulls, 0.00% null\n",
      "L_EAR_y: 0 nulls, 0.00% null\n",
      "L_EAR_z: 0 nulls, 0.00% null\n",
      "R_SHOULDER_x: 0 nulls, 0.00% null\n",
      "R_SHOULDER_y: 0 nulls, 0.00% null\n",
      "R_SHOULDER_z: 0 nulls, 0.00% null\n",
      "L_SHOULDER_x: 0 nulls, 0.00% null\n",
      "L_SHOULDER_y: 0 nulls, 0.00% null\n",
      "L_SHOULDER_z: 0 nulls, 0.00% null\n",
      "R_ELBOW_x: 0 nulls, 0.00% null\n",
      "R_ELBOW_y: 0 nulls, 0.00% null\n",
      "R_ELBOW_z: 0 nulls, 0.00% null\n",
      "L_ELBOW_x: 0 nulls, 0.00% null\n",
      "L_ELBOW_y: 0 nulls, 0.00% null\n",
      "L_ELBOW_z: 0 nulls, 0.00% null\n",
      "R_WRIST_x: 0 nulls, 0.00% null\n",
      "R_WRIST_y: 0 nulls, 0.00% null\n",
      "R_WRIST_z: 0 nulls, 0.00% null\n",
      "L_WRIST_x: 0 nulls, 0.00% null\n",
      "L_WRIST_y: 0 nulls, 0.00% null\n",
      "L_WRIST_z: 0 nulls, 0.00% null\n",
      "R_HIP_x: 0 nulls, 0.00% null\n",
      "R_HIP_y: 0 nulls, 0.00% null\n",
      "R_HIP_z: 0 nulls, 0.00% null\n",
      "L_HIP_x: 0 nulls, 0.00% null\n",
      "L_HIP_y: 0 nulls, 0.00% null\n",
      "L_HIP_z: 0 nulls, 0.00% null\n",
      "R_KNEE_x: 0 nulls, 0.00% null\n",
      "R_KNEE_y: 0 nulls, 0.00% null\n",
      "R_KNEE_z: 0 nulls, 0.00% null\n",
      "L_KNEE_x: 0 nulls, 0.00% null\n",
      "L_KNEE_y: 0 nulls, 0.00% null\n",
      "L_KNEE_z: 0 nulls, 0.00% null\n",
      "R_ANKLE_x: 0 nulls, 0.00% null\n",
      "R_ANKLE_y: 0 nulls, 0.00% null\n",
      "R_ANKLE_z: 0 nulls, 0.00% null\n",
      "L_ANKLE_x: 0 nulls, 0.00% null\n",
      "L_ANKLE_y: 0 nulls, 0.00% null\n",
      "L_ANKLE_z: 0 nulls, 0.00% null\n",
      "R_1STFINGER_x: 0 nulls, 0.00% null\n",
      "R_1STFINGER_y: 0 nulls, 0.00% null\n",
      "R_1STFINGER_z: 0 nulls, 0.00% null\n",
      "R_5THFINGER_x: 0 nulls, 0.00% null\n",
      "R_5THFINGER_y: 0 nulls, 0.00% null\n",
      "R_5THFINGER_z: 0 nulls, 0.00% null\n",
      "L_1STFINGER_x: 0 nulls, 0.00% null\n",
      "L_1STFINGER_y: 0 nulls, 0.00% null\n",
      "L_1STFINGER_z: 0 nulls, 0.00% null\n",
      "L_5THFINGER_x: 0 nulls, 0.00% null\n",
      "L_5THFINGER_y: 0 nulls, 0.00% null\n",
      "L_5THFINGER_z: 0 nulls, 0.00% null\n",
      "R_1STTOE_x: 0 nulls, 0.00% null\n",
      "R_1STTOE_y: 0 nulls, 0.00% null\n",
      "R_1STTOE_z: 0 nulls, 0.00% null\n",
      "R_5THTOE_x: 0 nulls, 0.00% null\n",
      "R_5THTOE_y: 0 nulls, 0.00% null\n",
      "R_5THTOE_z: 0 nulls, 0.00% null\n",
      "L_1STTOE_x: 0 nulls, 0.00% null\n",
      "L_1STTOE_y: 0 nulls, 0.00% null\n",
      "L_1STTOE_z: 0 nulls, 0.00% null\n",
      "L_5THTOE_x: 0 nulls, 0.00% null\n",
      "L_5THTOE_y: 0 nulls, 0.00% null\n",
      "L_5THTOE_z: 0 nulls, 0.00% null\n",
      "R_CALC_x: 0 nulls, 0.00% null\n",
      "R_CALC_y: 0 nulls, 0.00% null\n",
      "R_CALC_z: 0 nulls, 0.00% null\n",
      "L_CALC_x: 0 nulls, 0.00% null\n",
      "L_CALC_y: 0 nulls, 0.00% null\n",
      "L_CALC_z: 0 nulls, 0.00% null\n",
      "ball_speed: 0 nulls, 0.00% null\n",
      "ball_velocity_x: 0 nulls, 0.00% null\n",
      "ball_velocity_y: 0 nulls, 0.00% null\n",
      "ball_velocity_z: 0 nulls, 0.00% null\n",
      "overall_ball_velocity: 0 nulls, 0.00% null\n",
      "ball_direction_x: 0 nulls, 0.00% null\n",
      "ball_direction_y: 0 nulls, 0.00% null\n",
      "ball_direction_z: 0 nulls, 0.00% null\n",
      "computed_ball_velocity_x: 0 nulls, 0.00% null\n",
      "computed_ball_velocity_y: 0 nulls, 0.00% null\n",
      "computed_ball_velocity_z: 0 nulls, 0.00% null\n",
      "dist_ball_R_1STFINGER: 0 nulls, 0.00% null\n",
      "dist_ball_R_5THFINGER: 0 nulls, 0.00% null\n",
      "dist_ball_L_1STFINGER: 0 nulls, 0.00% null\n",
      "dist_ball_L_5THFINGER: 0 nulls, 0.00% null\n",
      "ball_in_hands: 0 nulls, 0.00% null\n",
      "shooting_motion: 0 nulls, 0.00% null\n",
      "avg_shoulder_height: 0 nulls, 0.00% null\n",
      "release_point_filter: 0 nulls, 0.00% null\n",
      "dt: 0 nulls, 0.00% null\n",
      "dx: 0 nulls, 0.00% null\n",
      "dy: 0 nulls, 0.00% null\n",
      "dz: 0 nulls, 0.00% null\n",
      "L_ANKLE_ongoing_power: 0 nulls, 0.00% null\n",
      "R_ANKLE_ongoing_power: 0 nulls, 0.00% null\n",
      "L_KNEE_ongoing_power: 0 nulls, 0.00% null\n",
      "R_KNEE_ongoing_power: 0 nulls, 0.00% null\n",
      "L_HIP_ongoing_power: 0 nulls, 0.00% null\n",
      "R_HIP_ongoing_power: 0 nulls, 0.00% null\n",
      "L_ELBOW_ongoing_power: 0 nulls, 0.00% null\n",
      "R_ELBOW_ongoing_power: 0 nulls, 0.00% null\n",
      "L_WRIST_ongoing_power: 0 nulls, 0.00% null\n",
      "R_WRIST_ongoing_power: 0 nulls, 0.00% null\n",
      "L_1STFINGER_ongoing_power: 0 nulls, 0.00% null\n",
      "L_5THFINGER_ongoing_power: 0 nulls, 0.00% null\n",
      "R_1STFINGER_ongoing_power: 0 nulls, 0.00% null\n",
      "R_5THFINGER_ongoing_power: 0 nulls, 0.00% null\n",
      "L_ELBOW_angle: 0 nulls, 0.00% null\n",
      "L_WRIST_angle: 0 nulls, 0.00% null\n",
      "L_KNEE_angle: 0 nulls, 0.00% null\n",
      "L_ELBOW_ongoing_angle: 0 nulls, 0.00% null\n",
      "L_WRIST_ongoing_angle: 0 nulls, 0.00% null\n",
      "L_KNEE_ongoing_angle: 0 nulls, 0.00% null\n",
      "R_ELBOW_angle: 0 nulls, 0.00% null\n",
      "R_WRIST_angle: 0 nulls, 0.00% null\n",
      "R_KNEE_angle: 0 nulls, 0.00% null\n",
      "R_ELBOW_ongoing_angle: 0 nulls, 0.00% null\n",
      "R_WRIST_ongoing_angle: 0 nulls, 0.00% null\n",
      "R_KNEE_ongoing_angle: 0 nulls, 0.00% null\n",
      "shooting_phases: 0 nulls, 0.00% null\n",
      "player_height_in_meters: 0 nulls, 0.00% null\n",
      "player_height_ft: 0 nulls, 0.00% null\n",
      "initial_release_angle: 0 nulls, 0.00% null\n",
      "calculated_release_angle: 0 nulls, 0.00% null\n",
      "angle_difference: 0 nulls, 0.00% null\n",
      "distance_to_basket: 0 nulls, 0.00% null\n",
      "optimal_release_angle: 0 nulls, 0.00% null\n",
      "by_trial_time: 0 nulls, 0.00% null\n",
      "continuous_frame_time: 0 nulls, 0.00% null\n",
      "L_ANKLE_energy: 0 nulls, 0.00% null\n",
      "R_ANKLE_energy: 0 nulls, 0.00% null\n",
      "L_KNEE_energy: 0 nulls, 0.00% null\n",
      "R_KNEE_energy: 0 nulls, 0.00% null\n",
      "L_HIP_energy: 0 nulls, 0.00% null\n",
      "R_HIP_energy: 0 nulls, 0.00% null\n",
      "L_ELBOW_energy: 0 nulls, 0.00% null\n",
      "R_ELBOW_energy: 0 nulls, 0.00% null\n",
      "L_WRIST_energy: 0 nulls, 0.00% null\n",
      "R_WRIST_energy: 0 nulls, 0.00% null\n",
      "L_1STFINGER_energy: 0 nulls, 0.00% null\n",
      "R_1STFINGER_energy: 0 nulls, 0.00% null\n",
      "L_5THFINGER_energy: 0 nulls, 0.00% null\n",
      "R_5THFINGER_energy: 0 nulls, 0.00% null\n",
      "total_energy: 0 nulls, 0.00% null\n",
      "by_trial_energy: 0 nulls, 0.00% null\n",
      "by_trial_exhaustion_score: 0 nulls, 0.00% null\n",
      "overall_cumulative_energy: 0 nulls, 0.00% null\n",
      "overall_exhaustion_score: 0 nulls, 0.00% null\n",
      "L_ANKLE_energy_by_trial: 0 nulls, 0.00% null\n",
      "L_ANKLE_energy_by_trial_exhaustion_score: 0 nulls, 0.00% null\n",
      "L_ANKLE_energy_overall_cumulative: 0 nulls, 0.00% null\n",
      "L_ANKLE_energy_overall_exhaustion_score: 0 nulls, 0.00% null\n",
      "R_ANKLE_energy_by_trial: 0 nulls, 0.00% null\n",
      "R_ANKLE_energy_by_trial_exhaustion_score: 0 nulls, 0.00% null\n",
      "R_ANKLE_energy_overall_cumulative: 0 nulls, 0.00% null\n",
      "R_ANKLE_energy_overall_exhaustion_score: 0 nulls, 0.00% null\n",
      "L_KNEE_energy_by_trial: 0 nulls, 0.00% null\n",
      "L_KNEE_energy_by_trial_exhaustion_score: 0 nulls, 0.00% null\n",
      "L_KNEE_energy_overall_cumulative: 0 nulls, 0.00% null\n",
      "L_KNEE_energy_overall_exhaustion_score: 0 nulls, 0.00% null\n",
      "R_KNEE_energy_by_trial: 0 nulls, 0.00% null\n",
      "R_KNEE_energy_by_trial_exhaustion_score: 0 nulls, 0.00% null\n",
      "R_KNEE_energy_overall_cumulative: 0 nulls, 0.00% null\n",
      "R_KNEE_energy_overall_exhaustion_score: 0 nulls, 0.00% null\n",
      "L_HIP_energy_by_trial: 0 nulls, 0.00% null\n",
      "L_HIP_energy_by_trial_exhaustion_score: 0 nulls, 0.00% null\n",
      "L_HIP_energy_overall_cumulative: 0 nulls, 0.00% null\n",
      "L_HIP_energy_overall_exhaustion_score: 0 nulls, 0.00% null\n",
      "R_HIP_energy_by_trial: 0 nulls, 0.00% null\n",
      "R_HIP_energy_by_trial_exhaustion_score: 0 nulls, 0.00% null\n",
      "R_HIP_energy_overall_cumulative: 0 nulls, 0.00% null\n",
      "R_HIP_energy_overall_exhaustion_score: 0 nulls, 0.00% null\n",
      "L_ELBOW_energy_by_trial: 0 nulls, 0.00% null\n",
      "L_ELBOW_energy_by_trial_exhaustion_score: 0 nulls, 0.00% null\n",
      "L_ELBOW_energy_overall_cumulative: 0 nulls, 0.00% null\n",
      "L_ELBOW_energy_overall_exhaustion_score: 0 nulls, 0.00% null\n",
      "R_ELBOW_energy_by_trial: 0 nulls, 0.00% null\n",
      "R_ELBOW_energy_by_trial_exhaustion_score: 0 nulls, 0.00% null\n",
      "R_ELBOW_energy_overall_cumulative: 0 nulls, 0.00% null\n",
      "R_ELBOW_energy_overall_exhaustion_score: 0 nulls, 0.00% null\n",
      "L_WRIST_energy_by_trial: 0 nulls, 0.00% null\n",
      "L_WRIST_energy_by_trial_exhaustion_score: 0 nulls, 0.00% null\n",
      "L_WRIST_energy_overall_cumulative: 0 nulls, 0.00% null\n",
      "L_WRIST_energy_overall_exhaustion_score: 0 nulls, 0.00% null\n",
      "R_WRIST_energy_by_trial: 0 nulls, 0.00% null\n",
      "R_WRIST_energy_by_trial_exhaustion_score: 0 nulls, 0.00% null\n",
      "R_WRIST_energy_overall_cumulative: 0 nulls, 0.00% null\n",
      "R_WRIST_energy_overall_exhaustion_score: 0 nulls, 0.00% null\n",
      "L_1STFINGER_energy_by_trial: 0 nulls, 0.00% null\n",
      "L_1STFINGER_energy_by_trial_exhaustion_score: 0 nulls, 0.00% null\n",
      "L_1STFINGER_energy_overall_cumulative: 0 nulls, 0.00% null\n",
      "L_1STFINGER_energy_overall_exhaustion_score: 0 nulls, 0.00% null\n",
      "R_1STFINGER_energy_by_trial: 0 nulls, 0.00% null\n",
      "R_1STFINGER_energy_by_trial_exhaustion_score: 0 nulls, 0.00% null\n",
      "R_1STFINGER_energy_overall_cumulative: 0 nulls, 0.00% null\n",
      "R_1STFINGER_energy_overall_exhaustion_score: 0 nulls, 0.00% null\n",
      "L_5THFINGER_energy_by_trial: 0 nulls, 0.00% null\n",
      "L_5THFINGER_energy_by_trial_exhaustion_score: 0 nulls, 0.00% null\n",
      "L_5THFINGER_energy_overall_cumulative: 0 nulls, 0.00% null\n",
      "L_5THFINGER_energy_overall_exhaustion_score: 0 nulls, 0.00% null\n",
      "R_5THFINGER_energy_by_trial: 0 nulls, 0.00% null\n",
      "R_5THFINGER_energy_by_trial_exhaustion_score: 0 nulls, 0.00% null\n",
      "R_5THFINGER_energy_overall_cumulative: 0 nulls, 0.00% null\n",
      "R_5THFINGER_energy_overall_exhaustion_score: 0 nulls, 0.00% null\n",
      "participant_id: 0 nulls, 0.00% null\n",
      "L_SHOULDER_angle: 0 nulls, 0.00% null\n",
      "R_SHOULDER_angle: 0 nulls, 0.00% null\n",
      "L_HIP_angle: 0 nulls, 0.00% null\n",
      "R_HIP_angle: 0 nulls, 0.00% null\n",
      "L_ANKLE_angle: 0 nulls, 0.00% null\n",
      "R_ANKLE_angle: 0 nulls, 0.00% null\n",
      "datetime: 0 nulls, 0.00% null\n",
      "player_weight__in_kg: 0 nulls, 0.00% null\n",
      "joint_energy: 0 nulls, 0.00% null\n",
      "joint_power: 0 nulls, 0.00% null\n",
      "energy_acceleration: 0 nulls, 0.00% null\n",
      "ankle_power_ratio: 0 nulls, 0.00% null\n",
      "hip_asymmetry: 0 nulls, 0.00% null\n",
      "ankle_asymmetry: 0 nulls, 0.00% null\n",
      "wrist_asymmetry: 0 nulls, 0.00% null\n",
      "elbow_asymmetry: 0 nulls, 0.00% null\n",
      "knee_asymmetry: 0 nulls, 0.00% null\n",
      "1stfinger_asymmetry: 0 nulls, 0.00% null\n",
      "5thfinger_asymmetry: 0 nulls, 0.00% null\n",
      "hip_power_ratio: 0 nulls, 0.00% null\n",
      "wrist_power_ratio: 0 nulls, 0.00% null\n",
      "elbow_power_ratio: 0 nulls, 0.00% null\n",
      "knee_power_ratio: 0 nulls, 0.00% null\n",
      "1stfinger_power_ratio: 0 nulls, 0.00% null\n",
      "5thfinger_power_ratio: 0 nulls, 0.00% null\n",
      "L_KNEE_ROM: 0 nulls, 0.00% null\n",
      "L_KNEE_ROM_deviation: 0 nulls, 0.00% null\n",
      "L_KNEE_ROM_extreme: 0 nulls, 0.00% null\n",
      "R_KNEE_ROM: 0 nulls, 0.00% null\n",
      "R_KNEE_ROM_deviation: 0 nulls, 0.00% null\n",
      "R_KNEE_ROM_extreme: 0 nulls, 0.00% null\n",
      "L_SHOULDER_ROM: 0 nulls, 0.00% null\n",
      "L_SHOULDER_ROM_deviation: 0 nulls, 0.00% null\n",
      "L_SHOULDER_ROM_extreme: 0 nulls, 0.00% null\n",
      "R_SHOULDER_ROM: 0 nulls, 0.00% null\n",
      "R_SHOULDER_ROM_deviation: 0 nulls, 0.00% null\n",
      "R_SHOULDER_ROM_extreme: 0 nulls, 0.00% null\n",
      "L_HIP_ROM: 0 nulls, 0.00% null\n",
      "L_HIP_ROM_deviation: 0 nulls, 0.00% null\n",
      "L_HIP_ROM_extreme: 0 nulls, 0.00% null\n",
      "R_HIP_ROM: 0 nulls, 0.00% null\n",
      "R_HIP_ROM_deviation: 0 nulls, 0.00% null\n",
      "R_HIP_ROM_extreme: 0 nulls, 0.00% null\n",
      "L_ANKLE_ROM: 0 nulls, 0.00% null\n",
      "L_ANKLE_ROM_deviation: 0 nulls, 0.00% null\n",
      "L_ANKLE_ROM_extreme: 0 nulls, 0.00% null\n",
      "R_ANKLE_ROM: 0 nulls, 0.00% null\n",
      "R_ANKLE_ROM_deviation: 0 nulls, 0.00% null\n",
      "R_ANKLE_ROM_extreme: 0 nulls, 0.00% null\n",
      "L_WRIST_ROM: 0 nulls, 0.00% null\n",
      "L_WRIST_ROM_deviation: 0 nulls, 0.00% null\n",
      "L_WRIST_ROM_extreme: 0 nulls, 0.00% null\n",
      "R_WRIST_ROM: 0 nulls, 0.00% null\n",
      "R_WRIST_ROM_deviation: 0 nulls, 0.00% null\n",
      "R_WRIST_ROM_extreme: 0 nulls, 0.00% null\n",
      "exhaustion_rate: 0 nulls, 0.00% null\n",
      "simulated_HR: 0 nulls, 0.00% null\n",
      "time_since_start: 0 nulls, 0.00% null\n",
      "power_avg_5: 0 nulls, 0.00% null\n",
      "rolling_power_std: 0 nulls, 0.00% null\n",
      "rolling_hr_mean: 0 nulls, 0.00% null\n",
      "rolling_energy_std: 0 nulls, 0.00% null\n",
      "exhaustion_lag1: 0 nulls, 0.00% null\n",
      "ema_exhaustion: 0 nulls, 0.00% null\n",
      "rolling_exhaustion: 0 nulls, 0.00% null\n",
      "injury_risk: 0 nulls, 0.00% null\n",
      "L_ANKLE_exhaustion_rate: 0 nulls, 0.00% null\n",
      "L_ANKLE_rolling_exhaustion: 0 nulls, 0.00% null\n",
      "L_ANKLE_injury_risk: 0 nulls, 0.00% null\n",
      "R_ANKLE_exhaustion_rate: 0 nulls, 0.00% null\n",
      "R_ANKLE_rolling_exhaustion: 0 nulls, 0.00% null\n",
      "R_ANKLE_injury_risk: 0 nulls, 0.00% null\n",
      "L_WRIST_exhaustion_rate: 0 nulls, 0.00% null\n",
      "L_WRIST_rolling_exhaustion: 0 nulls, 0.00% null\n",
      "L_WRIST_injury_risk: 0 nulls, 0.00% null\n",
      "R_WRIST_exhaustion_rate: 0 nulls, 0.00% null\n",
      "R_WRIST_rolling_exhaustion: 0 nulls, 0.00% null\n",
      "R_WRIST_injury_risk: 0 nulls, 0.00% null\n",
      "L_ELBOW_exhaustion_rate: 0 nulls, 0.00% null\n",
      "L_ELBOW_rolling_exhaustion: 0 nulls, 0.00% null\n",
      "L_ELBOW_injury_risk: 0 nulls, 0.00% null\n",
      "R_ELBOW_exhaustion_rate: 0 nulls, 0.00% null\n",
      "R_ELBOW_rolling_exhaustion: 0 nulls, 0.00% null\n",
      "R_ELBOW_injury_risk: 0 nulls, 0.00% null\n",
      "L_KNEE_exhaustion_rate: 0 nulls, 0.00% null\n",
      "L_KNEE_rolling_exhaustion: 0 nulls, 0.00% null\n",
      "L_KNEE_injury_risk: 0 nulls, 0.00% null\n",
      "R_KNEE_exhaustion_rate: 0 nulls, 0.00% null\n",
      "R_KNEE_rolling_exhaustion: 0 nulls, 0.00% null\n",
      "R_KNEE_injury_risk: 0 nulls, 0.00% null\n",
      "L_HIP_exhaustion_rate: 0 nulls, 0.00% null\n",
      "L_HIP_rolling_exhaustion: 0 nulls, 0.00% null\n",
      "L_HIP_injury_risk: 0 nulls, 0.00% null\n",
      "R_HIP_exhaustion_rate: 0 nulls, 0.00% null\n",
      "R_HIP_rolling_exhaustion: 0 nulls, 0.00% null\n",
      "R_HIP_injury_risk: 0 nulls, 0.00% null\n",
      "Joint energy columns:  ['L_ANKLE_energy', 'R_ANKLE_energy', 'L_KNEE_energy', 'R_KNEE_energy', 'L_HIP_energy', 'R_HIP_energy', 'L_ELBOW_energy', 'R_ELBOW_energy', 'L_WRIST_energy', 'R_WRIST_energy', 'L_1STFINGER_energy', 'R_1STFINGER_energy', 'L_5THFINGER_energy', 'R_5THFINGER_energy', 'total_energy', 'joint_energy', 'rolling_energy_std']\n",
      "Joint power columns:  ['L_ANKLE_ongoing_power', 'R_ANKLE_ongoing_power', 'L_KNEE_ongoing_power', 'R_KNEE_ongoing_power', 'L_HIP_ongoing_power', 'R_HIP_ongoing_power', 'L_ELBOW_ongoing_power', 'R_ELBOW_ongoing_power', 'L_WRIST_ongoing_power', 'R_WRIST_ongoing_power', 'L_1STFINGER_ongoing_power', 'L_5THFINGER_ongoing_power', 'R_1STFINGER_ongoing_power', 'R_5THFINGER_ongoing_power']\n",
      "All angle columns:  ['entry_angle', 'L_ELBOW_angle', 'L_WRIST_angle', 'L_KNEE_angle', 'L_ELBOW_ongoing_angle', 'L_WRIST_ongoing_angle', 'L_KNEE_ongoing_angle', 'R_ELBOW_angle', 'R_WRIST_angle', 'R_KNEE_angle', 'R_ELBOW_ongoing_angle', 'R_WRIST_ongoing_angle', 'R_KNEE_ongoing_angle', 'initial_release_angle', 'calculated_release_angle', 'angle_difference', 'optimal_release_angle', 'L_SHOULDER_angle', 'R_SHOULDER_angle', 'L_HIP_angle', 'R_HIP_angle', 'L_ANKLE_angle', 'R_ANKLE_angle']\n",
      "print all the columns with by_trial_exhaustion_score:  ['by_trial_exhaustion_score', 'L_ANKLE_energy_by_trial_exhaustion_score', 'R_ANKLE_energy_by_trial_exhaustion_score', 'L_KNEE_energy_by_trial_exhaustion_score', 'R_KNEE_energy_by_trial_exhaustion_score', 'L_HIP_energy_by_trial_exhaustion_score', 'R_HIP_energy_by_trial_exhaustion_score', 'L_ELBOW_energy_by_trial_exhaustion_score', 'R_ELBOW_energy_by_trial_exhaustion_score', 'L_WRIST_energy_by_trial_exhaustion_score', 'R_WRIST_energy_by_trial_exhaustion_score', 'L_1STFINGER_energy_by_trial_exhaustion_score', 'R_1STFINGER_energy_by_trial_exhaustion_score', 'L_5THFINGER_energy_by_trial_exhaustion_score', 'R_5THFINGER_energy_by_trial_exhaustion_score']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:  - R_HIP_ROM_extreme: dtype=int32, sample values=[0]\n",
      "INFO:  - L_ANKLE_ROM: dtype=float64, sample values=[32.32505371 32.99123382 36.39764221 35.45367686 32.1162011 ]\n",
      "INFO:  - L_ANKLE_ROM_deviation: dtype=float64, sample values=[12.32505371 12.99123382 16.39764221 15.45367686 12.1162011 ]\n",
      "INFO:  - L_ANKLE_ROM_extreme: dtype=int32, sample values=[1]\n",
      "INFO:  - R_ANKLE_ROM: dtype=float64, sample values=[39.81071151 45.54250042 45.70572465 43.1338381  39.83072086]\n",
      "INFO:  - R_ANKLE_ROM_deviation: dtype=float64, sample values=[19.81071151 25.54250042 25.70572465 23.1338381  19.83072086]\n",
      "INFO:  - R_ANKLE_ROM_extreme: dtype=int32, sample values=[1]\n",
      "INFO:  - L_WRIST_ROM: dtype=float64, sample values=[19.60628366 24.85503336 27.6434808  26.09213451 19.99680555]\n",
      "INFO:  - L_WRIST_ROM_deviation: dtype=float64, sample values=[0.]\n",
      "INFO:  - L_WRIST_ROM_extreme: dtype=int32, sample values=[0]\n",
      "INFO:  - R_WRIST_ROM: dtype=float64, sample values=[26.94867994 30.72422249 27.0633164  35.1551556  30.2743699 ]\n",
      "INFO:  - R_WRIST_ROM_deviation: dtype=float64, sample values=[0.]\n",
      "INFO:  - R_WRIST_ROM_extreme: dtype=int32, sample values=[0]\n",
      "INFO:  - exhaustion_rate: dtype=float64, sample values=[0.00071098 0.00073159 0.00071125 0.00073347 0.00074737]\n",
      "INFO:  - simulated_HR: dtype=float64, sample values=[62.99496676 63.19722095 63.30114012 63.34046103 63.33843533]\n",
      "INFO:  - trial_mean_exhaustion: dtype=float64, sample values=[0.87051117 0.84010503 0.85347528 0.90039509 0.8677941 ]\n",
      "INFO:  - trial_total_joint_energy: dtype=float64, sample values=[112.17925789 136.85983511 128.7307413  123.02011784 125.10100657]\n",
      "INFO: Created 'rolling_energy_std' with sample: [0.0, 0.27843478641511976, 0.328875265121227, 0.32186459703499126, 0.2926793976104387, 0.0866644713904257, 0.0630286767478577, 0.08237311275552762, 0.08228513832412952, 0.11555788289496277]\n",
      "INFO: Created 'rolling_energy_std' with window 5.\n",
      "INFO: Added trial-level aggregated features in feature_engineering: trial_mean_exhaustion_fe, trial_injury_rate_fe.\n",
      "INFO: Step [feature_engineering]: DataFrame shape = (2956, 325)\n",
      "INFO: New columns added: ['time_since_start', 'rolling_energy_std', 'exhaustion_lag1', 'ema_exhaustion', 'rolling_exhaustion', 'injury_risk', 'L_ANKLE_exhaustion_rate', 'L_ANKLE_rolling_exhaustion', 'L_ANKLE_injury_risk', 'R_ANKLE_exhaustion_rate', 'R_ANKLE_rolling_exhaustion', 'R_ANKLE_injury_risk', 'L_WRIST_exhaustion_rate', 'L_WRIST_rolling_exhaustion', 'L_WRIST_injury_risk', 'R_WRIST_exhaustion_rate', 'R_WRIST_rolling_exhaustion', 'R_WRIST_injury_risk', 'L_ELBOW_exhaustion_rate', 'L_ELBOW_rolling_exhaustion', 'L_ELBOW_injury_risk', 'R_ELBOW_exhaustion_rate', 'R_ELBOW_rolling_exhaustion', 'R_ELBOW_injury_risk', 'L_KNEE_exhaustion_rate', 'L_KNEE_rolling_exhaustion', 'L_KNEE_injury_risk', 'R_KNEE_exhaustion_rate', 'R_KNEE_rolling_exhaustion', 'R_KNEE_injury_risk', 'L_HIP_exhaustion_rate', 'L_HIP_rolling_exhaustion', 'L_HIP_injury_risk', 'R_HIP_exhaustion_rate', 'R_HIP_rolling_exhaustion', 'R_HIP_injury_risk', 'trial_mean_exhaustion_fe', 'trial_injury_rate_fe']\n",
      "INFO:  - time_since_start: dtype=int64, sample values=[ 33  66 100 133 166]\n",
      "INFO:  - rolling_energy_std: dtype=float64, sample values=[0.27843479 0.32887527 0.3218646  0.2926794  0.08666447]\n",
      "INFO:  - exhaustion_lag1: dtype=float64, sample values=[0.66334808 0.68681029 0.7109526  0.73513505 0.75933951]\n",
      "INFO:  - ema_exhaustion: dtype=float64, sample values=[0.66761394 0.67549369 0.68633758 0.69961065 0.71495465]\n",
      "INFO:  - rolling_exhaustion: dtype=float64, sample values=[1.35015837 2.06111097 2.79624602 3.55558552 4.33958814]\n",
      "INFO:  - injury_risk: dtype=int32, sample values=[1 0]\n",
      "INFO:  - L_ANKLE_exhaustion_rate: dtype=float64, sample values=[0.0001312  0.00012625 0.00012496 0.00016363 0.00021867]\n",
      "INFO:  - L_ANKLE_rolling_exhaustion: dtype=float64, sample values=[1.39072301 2.09241547 2.79835659 3.50969766 4.22825472]\n",
      "INFO:  - L_ANKLE_injury_risk: dtype=int32, sample values=[1 0]\n",
      "INFO:  - R_ANKLE_exhaustion_rate: dtype=float64, sample values=[7.30474516e-05 9.71621936e-05 1.09483649e-04 1.24064928e-04\n",
      " 1.63966091e-04]\n",
      "INFO:  - R_ANKLE_rolling_exhaustion: dtype=float64, sample values=[1.65997499 2.49437412 3.33249569 4.1747114  5.022338  ]\n",
      "INFO:  - R_ANKLE_injury_risk: dtype=int32, sample values=[1 0]\n",
      "INFO:  - L_WRIST_exhaustion_rate: dtype=float64, sample values=[0.00094021 0.00098359 0.00095403 0.0009489  0.00089518]\n",
      "INFO:  - L_WRIST_rolling_exhaustion: dtype=float64, sample values=[1.30508004 2.00559197 2.73854089 3.50280367 4.29660753]\n",
      "INFO:  - L_WRIST_injury_risk: dtype=int32, sample values=[1 0]\n",
      "INFO:  - R_WRIST_exhaustion_rate: dtype=float64, sample values=[0.00069982 0.00073728 0.00074206 0.00078717 0.00080418]\n",
      "INFO:  - R_WRIST_rolling_exhaustion: dtype=float64, sample values=[1.34654626 2.05569651 2.7900767  3.55043345 4.33732818]\n",
      "INFO:  - R_WRIST_injury_risk: dtype=int32, sample values=[1 0]\n",
      "INFO:  - L_ELBOW_exhaustion_rate: dtype=float64, sample values=[0.00069994 0.00080763 0.0008893  0.00101637 0.00107346]\n",
      "INFO:  - L_ELBOW_rolling_exhaustion: dtype=float64, sample values=[1.1848754  1.81551392 2.47638871 3.17080372 3.90064293]\n",
      "INFO:  - L_ELBOW_injury_risk: dtype=int32, sample values=[1 0]\n",
      "INFO:  - R_ELBOW_exhaustion_rate: dtype=float64, sample values=[0.00073245 0.00077548 0.00080537 0.00089206 0.00093756]\n",
      "INFO:  - R_ELBOW_rolling_exhaustion: dtype=float64, sample values=[1.20683865 1.84793407 2.51641216 3.21432837 3.943184  ]\n",
      "INFO:  - R_ELBOW_injury_risk: dtype=int32, sample values=[1 0]\n",
      "INFO:  - L_KNEE_exhaustion_rate: dtype=float64, sample values=[0.0003438  0.00028033 0.00014732 0.00015179 0.00038617]\n",
      "INFO:  - L_KNEE_rolling_exhaustion: dtype=float64, sample values=[1.37079523 2.07111644 2.77644664 3.48678581 4.20986849]\n",
      "INFO:  - L_KNEE_injury_risk: dtype=int32, sample values=[1 0]\n",
      "INFO:  - R_KNEE_exhaustion_rate: dtype=float64, sample values=[0.00038636 0.00032651 0.00021984 0.00013516 0.00031836]\n",
      "INFO:  - R_KNEE_rolling_exhaustion: dtype=float64, sample values=[1.42306719 2.15175059 2.8879085  3.62852672 4.37965083]\n",
      "INFO:  - R_KNEE_injury_risk: dtype=int32, sample values=[1 0]\n",
      "INFO:  - L_HIP_exhaustion_rate: dtype=float64, sample values=[0.00030445 0.00032267 0.00037812 0.00051342 0.00067221]\n",
      "INFO:  - L_HIP_rolling_exhaustion: dtype=float64, sample values=[1.30311662 1.9703465  2.6504324  3.34746117 4.06667287]\n",
      "INFO:  - L_HIP_injury_risk: dtype=int32, sample values=[1 0]\n",
      "INFO:  - R_HIP_exhaustion_rate: dtype=float64, sample values=[0.00035387 0.000378   0.00040329 0.00052453 0.0006885 ]\n",
      "INFO:  - R_HIP_rolling_exhaustion: dtype=float64, sample values=[1.30478515 1.97549067 2.65990807 3.36163481 4.08608201]\n",
      "INFO:  - R_HIP_injury_risk: dtype=int32, sample values=[1 0]\n",
      "INFO:  - trial_mean_exhaustion_fe: dtype=float64, sample values=[0.88086932 0.84010503 0.85347528 0.90039509 0.8677941 ]\n",
      "INFO:  - trial_injury_rate_fe: dtype=float64, sample values=[1.         0.38461538 0.34782609 0.30434783 0.04347826]\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:515: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  summary[f\"{col}_lag1\"] = summary[col].shift(1)\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:516: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  summary[f\"{col}_rolling_avg\"] = summary[col].rolling(window=rolling_window, min_periods=1).mean()\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:526: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  summary[f\"{col}_delta\"] = summary[col] - summary[f\"{col}_lag1\"]\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:515: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  summary[f\"{col}_lag1\"] = summary[col].shift(1)\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:516: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  summary[f\"{col}_rolling_avg\"] = summary[col].rolling(window=rolling_window, min_periods=1).mean()\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:526: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  summary[f\"{col}_delta\"] = summary[col] - summary[f\"{col}_lag1\"]\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:515: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  summary[f\"{col}_lag1\"] = summary[col].shift(1)\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:516: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  summary[f\"{col}_rolling_avg\"] = summary[col].rolling(window=rolling_window, min_periods=1).mean()\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:526: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  summary[f\"{col}_delta\"] = summary[col] - summary[f\"{col}_lag1\"]\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:515: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  summary[f\"{col}_lag1\"] = summary[col].shift(1)\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:516: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  summary[f\"{col}_rolling_avg\"] = summary[col].rolling(window=rolling_window, min_periods=1).mean()\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:526: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  summary[f\"{col}_delta\"] = summary[col] - summary[f\"{col}_lag1\"]\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:515: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  summary[f\"{col}_lag1\"] = summary[col].shift(1)\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:516: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  summary[f\"{col}_rolling_avg\"] = summary[col].rolling(window=rolling_window, min_periods=1).mean()\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:526: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  summary[f\"{col}_delta\"] = summary[col] - summary[f\"{col}_lag1\"]\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:515: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  summary[f\"{col}_lag1\"] = summary[col].shift(1)\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:516: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  summary[f\"{col}_rolling_avg\"] = summary[col].rolling(window=rolling_window, min_periods=1).mean()\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:526: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  summary[f\"{col}_delta\"] = summary[col] - summary[f\"{col}_lag1\"]\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:515: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  summary[f\"{col}_lag1\"] = summary[col].shift(1)\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:516: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  summary[f\"{col}_rolling_avg\"] = summary[col].rolling(window=rolling_window, min_periods=1).mean()\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:526: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  summary[f\"{col}_delta\"] = summary[col] - summary[f\"{col}_lag1\"]\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:515: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  summary[f\"{col}_lag1\"] = summary[col].shift(1)\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:516: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  summary[f\"{col}_rolling_avg\"] = summary[col].rolling(window=rolling_window, min_periods=1).mean()\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:526: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  summary[f\"{col}_delta\"] = summary[col] - summary[f\"{col}_lag1\"]\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:515: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  summary[f\"{col}_lag1\"] = summary[col].shift(1)\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:516: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  summary[f\"{col}_rolling_avg\"] = summary[col].rolling(window=rolling_window, min_periods=1).mean()\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:526: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  summary[f\"{col}_delta\"] = summary[col] - summary[f\"{col}_lag1\"]\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:515: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  summary[f\"{col}_lag1\"] = summary[col].shift(1)\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:516: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  summary[f\"{col}_rolling_avg\"] = summary[col].rolling(window=rolling_window, min_periods=1).mean()\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:526: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  summary[f\"{col}_delta\"] = summary[col] - summary[f\"{col}_lag1\"]\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:515: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  summary[f\"{col}_lag1\"] = summary[col].shift(1)\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:516: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  summary[f\"{col}_rolling_avg\"] = summary[col].rolling(window=rolling_window, min_periods=1).mean()\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:526: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  summary[f\"{col}_delta\"] = summary[col] - summary[f\"{col}_lag1\"]\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:515: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  summary[f\"{col}_lag1\"] = summary[col].shift(1)\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:516: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  summary[f\"{col}_rolling_avg\"] = summary[col].rolling(window=rolling_window, min_periods=1).mean()\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:526: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  summary[f\"{col}_delta\"] = summary[col] - summary[f\"{col}_lag1\"]\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:515: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  summary[f\"{col}_lag1\"] = summary[col].shift(1)\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:516: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  summary[f\"{col}_rolling_avg\"] = summary[col].rolling(window=rolling_window, min_periods=1).mean()\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:526: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  summary[f\"{col}_delta\"] = summary[col] - summary[f\"{col}_lag1\"]\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:515: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  summary[f\"{col}_lag1\"] = summary[col].shift(1)\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:516: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  summary[f\"{col}_rolling_avg\"] = summary[col].rolling(window=rolling_window, min_periods=1).mean()\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:526: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  summary[f\"{col}_delta\"] = summary[col] - summary[f\"{col}_lag1\"]\n",
      "INFO: Calculated L_SHOULDER_angle with mean: 105.15°\n",
      "INFO: Calculated R_SHOULDER_angle with mean: 114.73°\n",
      "INFO: Calculated L_HIP_angle with mean: 156.79°\n",
      "INFO: Calculated R_HIP_angle with mean: 159.85°\n",
      "INFO: Calculated L_KNEE_angle with mean: 150.55°\n",
      "INFO: Calculated R_KNEE_angle with mean: 146.18°\n",
      "INFO: Calculated L_ANKLE_angle with mean: 119.14°\n",
      "INFO: Calculated R_ANKLE_angle with mean: 118.35°\n",
      "INFO: Step [calculate_joint_angles]: DataFrame shape = (2957, 321)\n",
      "INFO: New columns added: ['L_SHOULDER_angle', 'R_SHOULDER_angle', 'L_HIP_angle', 'R_HIP_angle', 'L_KNEE_angle', 'R_KNEE_angle', 'L_ANKLE_angle', 'R_ANKLE_angle']\n",
      "INFO:  - L_SHOULDER_angle: dtype=float64, sample values=[73.45068109 79.43065512 85.81173982 92.16039774 98.24138364]\n",
      "INFO:  - R_SHOULDER_angle: dtype=float64, sample values=[70.58256269 77.61355791 84.25189069 90.42032843 96.1295666 ]\n",
      "INFO:  - L_HIP_angle: dtype=float64, sample values=[130.26576012 131.26589143 132.97719442 135.59436666 139.11360281]\n",
      "INFO:  - R_HIP_angle: dtype=float64, sample values=[128.58897622 129.68513614 131.45738564 133.93337483 137.30094714]\n",
      "INFO:  - L_KNEE_angle: dtype=float64, sample values=[123.01557197 121.86645657 121.3458327  121.87955413 123.97636026]\n",
      "INFO:  - R_KNEE_angle: dtype=float64, sample values=[118.76931833 117.27120232 116.45899301 116.64025318 118.36309984]\n",
      "INFO:  - L_ANKLE_angle: dtype=float64, sample values=[101.2180925  100.72639333 100.41915703 100.67716742 102.2248724 ]\n",
      "INFO:  - R_ANKLE_angle: dtype=float64, sample values=[93.72844337 92.84398138 92.62063176 92.95167131 94.37553481]\n",
      "WARNING: Participant anthropometric columns not found during renaming.\n",
      "INFO: Identified 17 joint energy and 14 joint power columns.\n",
      "INFO: Created aggregated 'joint_energy' and 'joint_power'.\n",
      "INFO: Created 'energy_acceleration' as derivative of joint_energy over time.\n",
      "INFO: Created 'ankle_power_ratio' feature comparing left to right ankle ongoing power.\n",
      "INFO: Created asymmetry feature: hip_asymmetry\n",
      "INFO: Created asymmetry feature: ankle_asymmetry\n",
      "INFO: Created asymmetry feature: wrist_asymmetry\n",
      "INFO: Created asymmetry feature: elbow_asymmetry\n",
      "INFO: Created asymmetry feature: knee_asymmetry\n",
      "INFO: Created asymmetry feature: 1stfinger_asymmetry\n",
      "INFO: Created asymmetry feature: 5thfinger_asymmetry\n",
      "INFO: Created power ratio feature: hip_power_ratio using columns L_HIP_ongoing_power and R_HIP_ongoing_power\n",
      "INFO: Created power ratio feature: ankle_power_ratio using columns L_ANKLE_ongoing_power and R_ANKLE_ongoing_power\n",
      "INFO: Created power ratio feature: wrist_power_ratio using columns L_WRIST_ongoing_power and R_WRIST_ongoing_power\n",
      "INFO: Created power ratio feature: elbow_power_ratio using columns L_ELBOW_ongoing_power and R_ELBOW_ongoing_power\n",
      "INFO: Created power ratio feature: knee_power_ratio using columns L_KNEE_ongoing_power and R_KNEE_ongoing_power\n",
      "INFO: Created power ratio feature: 1stfinger_power_ratio using columns L_1STFINGER_ongoing_power and R_1STFINGER_ongoing_power\n",
      "INFO: Created power ratio feature: 5thfinger_power_ratio using columns L_5THFINGER_ongoing_power and R_5THFINGER_ongoing_power\n",
      "INFO: Computed ROM for L KNEE as L_KNEE_ROM\n",
      "INFO: Computed ROM deviation for L KNEE as L_KNEE_ROM_deviation\n",
      "INFO: Created binary flag for L KNEE ROM extremes: L_KNEE_ROM_extreme\n",
      "INFO: Computed ROM for R KNEE as R_KNEE_ROM\n",
      "INFO: Computed ROM deviation for R KNEE as R_KNEE_ROM_deviation\n",
      "INFO: Created binary flag for R KNEE ROM extremes: R_KNEE_ROM_extreme\n",
      "INFO: Computed ROM for L SHOULDER as L_SHOULDER_ROM\n",
      "INFO: Computed ROM deviation for L SHOULDER as L_SHOULDER_ROM_deviation\n",
      "INFO: Created binary flag for L SHOULDER ROM extremes: L_SHOULDER_ROM_extreme\n",
      "INFO: Computed ROM for R SHOULDER as R_SHOULDER_ROM\n",
      "INFO: Computed ROM deviation for R SHOULDER as R_SHOULDER_ROM_deviation\n",
      "INFO: Created binary flag for R SHOULDER ROM extremes: R_SHOULDER_ROM_extreme\n",
      "INFO: Computed ROM for L HIP as L_HIP_ROM\n",
      "INFO: Computed ROM deviation for L HIP as L_HIP_ROM_deviation\n",
      "INFO: Created binary flag for L HIP ROM extremes: L_HIP_ROM_extreme\n",
      "INFO: Computed ROM for R HIP as R_HIP_ROM\n",
      "INFO: Computed ROM deviation for R HIP as R_HIP_ROM_deviation\n",
      "INFO: Created binary flag for R HIP ROM extremes: R_HIP_ROM_extreme\n",
      "INFO: Computed ROM for L ANKLE as L_ANKLE_ROM\n",
      "INFO: Computed ROM deviation for L ANKLE as L_ANKLE_ROM_deviation\n",
      "INFO: Created binary flag for L ANKLE ROM extremes: L_ANKLE_ROM_extreme\n",
      "INFO: Computed ROM for R ANKLE as R_ANKLE_ROM\n",
      "INFO: Computed ROM deviation for R ANKLE as R_ANKLE_ROM_deviation\n",
      "INFO: Created binary flag for R ANKLE ROM extremes: R_ANKLE_ROM_extreme\n",
      "INFO: Computed ROM for L WRIST as L_WRIST_ROM\n",
      "INFO: Computed ROM deviation for L WRIST as L_WRIST_ROM_deviation\n",
      "INFO: Created binary flag for L WRIST ROM extremes: L_WRIST_ROM_extreme\n",
      "INFO: Computed ROM for R WRIST as R_WRIST_ROM\n",
      "INFO: Computed ROM deviation for R WRIST as R_WRIST_ROM_deviation\n",
      "INFO: Created binary flag for R WRIST ROM extremes: R_WRIST_ROM_extreme\n",
      "INFO: Sorted data by 'participant_id' and 'continuous_frame_time'.\n",
      "INFO: Created 'exhaustion_rate' feature.\n",
      "INFO: Created 'simulated_HR' feature.\n",
      "INFO: Added trial-level aggregated features: trial_mean_exhaustion, trial_total_joint_energy.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- DEBUG: summarize_data START ---\n",
      "Initial data shape: (2957, 321)\n",
      "Grouping by: ['trial_id']\n",
      "Aggregation columns: ['joint_energy', 'L_ELBOW_energy', 'R_ELBOW_energy', 'L_WRIST_energy', 'R_WRIST_energy', 'L_KNEE_energy', 'R_KNEE_energy', 'L_HIP_energy', 'R_HIP_energy', 'joint_power', 'L_ELBOW_ongoing_power', 'R_ELBOW_ongoing_power', 'L_WRIST_ongoing_power', 'R_WRIST_ongoing_power', 'L_KNEE_ongoing_power', 'R_KNEE_ongoing_power', 'L_HIP_ongoing_power', 'R_HIP_ongoing_power', 'elbow_asymmetry', 'wrist_asymmetry', 'knee_asymmetry', 'hip_asymmetry', 'L_ELBOW_angle', 'R_ELBOW_angle', 'L_WRIST_angle', 'R_WRIST_angle', 'L_KNEE_angle', 'R_KNEE_angle', 'L_SHOULDER_ROM', 'R_SHOULDER_ROM', 'L_WRIST_ROM', 'R_WRIST_ROM', 'L_KNEE_ROM', 'R_KNEE_ROM', 'L_HIP_ROM', 'R_HIP_ROM', 'exhaustion_rate', 'by_trial_exhaustion_score', 'injury_risk', 'energy_acceleration', 'power_avg_5', 'rolling_power_std', 'rolling_hr_mean', 'simulated_HR', 'player_height_in_meters', 'player_weight__in_kg']\n",
      "Lag columns: ['joint_energy', 'L_ELBOW_energy', 'R_ELBOW_energy', 'L_WRIST_energy', 'R_WRIST_energy', 'L_KNEE_energy', 'R_KNEE_energy', 'L_HIP_energy', 'R_HIP_energy', 'joint_power', 'L_ELBOW_ongoing_power', 'R_ELBOW_ongoing_power', 'L_WRIST_ongoing_power', 'R_WRIST_ongoing_power', 'L_KNEE_ongoing_power', 'R_KNEE_ongoing_power', 'L_HIP_ongoing_power', 'R_HIP_ongoing_power', 'elbow_asymmetry', 'wrist_asymmetry', 'knee_asymmetry', 'hip_asymmetry', 'L_ELBOW_angle', 'R_ELBOW_angle', 'L_WRIST_angle', 'R_WRIST_angle', 'L_KNEE_angle', 'R_KNEE_angle', 'L_SHOULDER_ROM', 'R_SHOULDER_ROM', 'L_WRIST_ROM', 'R_WRIST_ROM', 'L_KNEE_ROM', 'R_KNEE_ROM', 'L_HIP_ROM', 'R_HIP_ROM', 'exhaustion_rate', 'by_trial_exhaustion_score', 'injury_risk', 'energy_acceleration', 'power_avg_5', 'rolling_power_std', 'rolling_hr_mean', 'simulated_HR', 'player_height_in_meters', 'player_weight__in_kg']\n",
      "Rolling window: 3\n",
      "Global lag: True\n",
      "No forced phase list provided.\n",
      "\n",
      "Computed lag features for 'joint_energy' with global_lag=True\n",
      "\n",
      "Computed lag features for 'L_ELBOW_energy' with global_lag=True\n",
      "\n",
      "Computed lag features for 'R_ELBOW_energy' with global_lag=True\n",
      "\n",
      "Computed lag features for 'L_WRIST_energy' with global_lag=True\n",
      "\n",
      "Computed lag features for 'R_WRIST_energy' with global_lag=True\n",
      "\n",
      "Computed lag features for 'L_KNEE_energy' with global_lag=True\n",
      "\n",
      "Computed lag features for 'R_KNEE_energy' with global_lag=True\n",
      "\n",
      "Computed lag features for 'L_HIP_energy' with global_lag=True\n",
      "\n",
      "Computed lag features for 'R_HIP_energy' with global_lag=True\n",
      "\n",
      "Computed lag features for 'joint_power' with global_lag=True\n",
      "\n",
      "Computed lag features for 'L_ELBOW_ongoing_power' with global_lag=True\n",
      "\n",
      "Computed lag features for 'R_ELBOW_ongoing_power' with global_lag=True\n",
      "\n",
      "Computed lag features for 'L_WRIST_ongoing_power' with global_lag=True\n",
      "\n",
      "Computed lag features for 'R_WRIST_ongoing_power' with global_lag=True\n",
      "\n",
      "Computed lag features for 'L_KNEE_ongoing_power' with global_lag=True\n",
      "\n",
      "Computed lag features for 'R_KNEE_ongoing_power' with global_lag=True\n",
      "\n",
      "Computed lag features for 'L_HIP_ongoing_power' with global_lag=True\n",
      "\n",
      "Computed lag features for 'R_HIP_ongoing_power' with global_lag=True\n",
      "\n",
      "Computed lag features for 'elbow_asymmetry' with global_lag=True\n",
      "\n",
      "Computed lag features for 'wrist_asymmetry' with global_lag=True\n",
      "\n",
      "Computed lag features for 'knee_asymmetry' with global_lag=True\n",
      "\n",
      "Computed lag features for 'hip_asymmetry' with global_lag=True\n",
      "\n",
      "Computed lag features for 'L_ELBOW_angle' with global_lag=True\n",
      "\n",
      "Computed lag features for 'R_ELBOW_angle' with global_lag=True\n",
      "\n",
      "Computed lag features for 'L_WRIST_angle' with global_lag=True\n",
      "\n",
      "Computed lag features for 'R_WRIST_angle' with global_lag=True\n",
      "\n",
      "Computed lag features for 'L_KNEE_angle' with global_lag=True\n",
      "\n",
      "Computed lag features for 'R_KNEE_angle' with global_lag=True\n",
      "\n",
      "Computed lag features for 'L_SHOULDER_ROM' with global_lag=True\n",
      "\n",
      "Computed lag features for 'R_SHOULDER_ROM' with global_lag=True\n",
      "\n",
      "Computed lag features for 'L_WRIST_ROM' with global_lag=True\n",
      "\n",
      "Computed lag features for 'R_WRIST_ROM' with global_lag=True\n",
      "\n",
      "Computed lag features for 'L_KNEE_ROM' with global_lag=True\n",
      "\n",
      "Computed lag features for 'R_KNEE_ROM' with global_lag=True\n",
      "\n",
      "Computed lag features for 'L_HIP_ROM' with global_lag=True\n",
      "\n",
      "Computed lag features for 'R_HIP_ROM' with global_lag=True\n",
      "\n",
      "Computed lag features for 'exhaustion_rate' with global_lag=True\n",
      "\n",
      "Computed lag features for 'by_trial_exhaustion_score' with global_lag=True\n",
      "\n",
      "Computed lag features for 'injury_risk' with global_lag=True\n",
      "\n",
      "Computed lag features for 'energy_acceleration' with global_lag=True\n",
      "\n",
      "Computed lag features for 'power_avg_5' with global_lag=True\n",
      "\n",
      "Computed lag features for 'rolling_power_std' with global_lag=True\n",
      "\n",
      "Computed lag features for 'rolling_hr_mean' with global_lag=True\n",
      "\n",
      "Computed lag features for 'simulated_HR' with global_lag=True\n",
      "\n",
      "Computed lag features for 'player_height_in_meters' with global_lag=True\n",
      "\n",
      "Computed lag features for 'player_weight__in_kg' with global_lag=True\n",
      "Imputed 0 NaN(s) in column 'joint_energy_lag1' with overall mean 5.3950\n",
      "Imputed 0 NaN(s) in column 'joint_energy_delta' with overall mean -0.0014\n",
      "Imputed 0 NaN(s) in column 'L_ELBOW_energy_lag1' with overall mean 0.1053\n",
      "Imputed 0 NaN(s) in column 'L_ELBOW_energy_delta' with overall mean -0.0001\n",
      "Imputed 0 NaN(s) in column 'R_ELBOW_energy_lag1' with overall mean 0.1158\n",
      "Imputed 0 NaN(s) in column 'R_ELBOW_energy_delta' with overall mean -0.0001\n",
      "Imputed 0 NaN(s) in column 'L_WRIST_energy_lag1' with overall mean 0.1311\n",
      "Imputed 0 NaN(s) in column 'L_WRIST_energy_delta' with overall mean -0.0001\n",
      "Imputed 0 NaN(s) in column 'R_WRIST_energy_lag1' with overall mean 0.1297\n",
      "Imputed 0 NaN(s) in column 'R_WRIST_energy_delta' with overall mean -0.0001\n",
      "Imputed 0 NaN(s) in column 'L_KNEE_energy_lag1' with overall mean 0.0394\n",
      "Imputed 0 NaN(s) in column 'L_KNEE_energy_delta' with overall mean 0.0000\n",
      "Imputed 0 NaN(s) in column 'R_KNEE_energy_lag1' with overall mean 0.0399\n",
      "Imputed 0 NaN(s) in column 'R_KNEE_energy_delta' with overall mean 0.0000\n",
      "Imputed 0 NaN(s) in column 'L_HIP_energy_lag1' with overall mean 0.0448\n",
      "Imputed 0 NaN(s) in column 'L_HIP_energy_delta' with overall mean 0.0000\n",
      "Imputed 0 NaN(s) in column 'R_HIP_energy_lag1' with overall mean 0.0488\n",
      "Imputed 0 NaN(s) in column 'R_HIP_energy_delta' with overall mean 0.0000\n",
      "Imputed 0 NaN(s) in column 'joint_power_lag1' with overall mean 37.7862\n",
      "Imputed 0 NaN(s) in column 'joint_power_delta' with overall mean -0.0172\n",
      "Imputed 0 NaN(s) in column 'L_ELBOW_ongoing_power_lag1' with overall mean 3.1595\n",
      "Imputed 0 NaN(s) in column 'L_ELBOW_ongoing_power_delta' with overall mean -0.0018\n",
      "Imputed 0 NaN(s) in column 'R_ELBOW_ongoing_power_lag1' with overall mean 3.4757\n",
      "Imputed 0 NaN(s) in column 'R_ELBOW_ongoing_power_delta' with overall mean -0.0033\n",
      "Imputed 0 NaN(s) in column 'L_WRIST_ongoing_power_lag1' with overall mean 3.9327\n",
      "Imputed 0 NaN(s) in column 'L_WRIST_ongoing_power_delta' with overall mean -0.0024\n",
      "Imputed 0 NaN(s) in column 'R_WRIST_ongoing_power_lag1' with overall mean 3.8925\n",
      "Imputed 0 NaN(s) in column 'R_WRIST_ongoing_power_delta' with overall mean -0.0031\n",
      "Imputed 0 NaN(s) in column 'L_KNEE_ongoing_power_lag1' with overall mean 1.1827\n",
      "Imputed 0 NaN(s) in column 'L_KNEE_ongoing_power_delta' with overall mean 0.0006\n",
      "Imputed 0 NaN(s) in column 'R_KNEE_ongoing_power_lag1' with overall mean 1.1986\n",
      "Imputed 0 NaN(s) in column 'R_KNEE_ongoing_power_delta' with overall mean 0.0006\n",
      "Imputed 0 NaN(s) in column 'L_HIP_ongoing_power_lag1' with overall mean 1.3447\n",
      "Imputed 0 NaN(s) in column 'L_HIP_ongoing_power_delta' with overall mean 0.0001\n",
      "Imputed 0 NaN(s) in column 'R_HIP_ongoing_power_lag1' with overall mean 1.4656\n",
      "Imputed 0 NaN(s) in column 'R_HIP_ongoing_power_delta' with overall mean 0.0004\n",
      "Imputed 0 NaN(s) in column 'elbow_asymmetry_lag1' with overall mean 0.0352\n",
      "Imputed 0 NaN(s) in column 'elbow_asymmetry_delta' with overall mean -0.0001\n",
      "Imputed 0 NaN(s) in column 'wrist_asymmetry_lag1' with overall mean 0.0385\n",
      "Imputed 0 NaN(s) in column 'wrist_asymmetry_delta' with overall mean -0.0000\n",
      "Imputed 0 NaN(s) in column 'knee_asymmetry_lag1' with overall mean 0.0032\n",
      "Imputed 0 NaN(s) in column 'knee_asymmetry_delta' with overall mean -0.0000\n",
      "Imputed 0 NaN(s) in column 'hip_asymmetry_lag1' with overall mean 0.0044\n",
      "Imputed 0 NaN(s) in column 'hip_asymmetry_delta' with overall mean 0.0000\n",
      "Imputed 0 NaN(s) in column 'L_ELBOW_angle_lag1' with overall mean 78.8959\n",
      "Imputed 0 NaN(s) in column 'L_ELBOW_angle_delta' with overall mean -0.0114\n",
      "Imputed 0 NaN(s) in column 'R_ELBOW_angle_lag1' with overall mean 59.6911\n",
      "Imputed 0 NaN(s) in column 'R_ELBOW_angle_delta' with overall mean -0.0202\n",
      "Imputed 0 NaN(s) in column 'L_WRIST_angle_lag1' with overall mean 18.2266\n",
      "Imputed 0 NaN(s) in column 'L_WRIST_angle_delta' with overall mean 0.0052\n",
      "Imputed 0 NaN(s) in column 'R_WRIST_angle_lag1' with overall mean 26.3611\n",
      "Imputed 0 NaN(s) in column 'R_WRIST_angle_delta' with overall mean 0.0029\n",
      "Imputed 0 NaN(s) in column 'L_KNEE_angle_lag1' with overall mean 150.0362\n",
      "Imputed 0 NaN(s) in column 'L_KNEE_angle_delta' with overall mean 0.0024\n",
      "Imputed 0 NaN(s) in column 'R_KNEE_angle_lag1' with overall mean 145.6723\n",
      "Imputed 0 NaN(s) in column 'R_KNEE_angle_delta' with overall mean 0.0100\n",
      "Imputed 0 NaN(s) in column 'L_SHOULDER_ROM_lag1' with overall mean 55.9528\n",
      "Imputed 0 NaN(s) in column 'L_SHOULDER_ROM_delta' with overall mean 0.0965\n",
      "Imputed 0 NaN(s) in column 'R_SHOULDER_ROM_lag1' with overall mean 80.3533\n",
      "Imputed 0 NaN(s) in column 'R_SHOULDER_ROM_delta' with overall mean 0.0507\n",
      "Imputed 0 NaN(s) in column 'L_WRIST_ROM_lag1' with overall mean 21.7208\n",
      "Imputed 0 NaN(s) in column 'L_WRIST_ROM_delta' with overall mean 0.0202\n",
      "Imputed 0 NaN(s) in column 'R_WRIST_ROM_lag1' with overall mean 32.8124\n",
      "Imputed 0 NaN(s) in column 'R_WRIST_ROM_delta' with overall mean 0.1024\n",
      "Imputed 0 NaN(s) in column 'L_KNEE_ROM_lag1' with overall mean 48.8062\n",
      "Imputed 0 NaN(s) in column 'L_KNEE_ROM_delta' with overall mean -0.0200\n",
      "Imputed 0 NaN(s) in column 'R_KNEE_ROM_lag1' with overall mean 50.7584\n",
      "Imputed 0 NaN(s) in column 'R_KNEE_ROM_delta' with overall mean -0.0044\n",
      "Imputed 0 NaN(s) in column 'L_HIP_ROM_lag1' with overall mean 32.8269\n",
      "Imputed 0 NaN(s) in column 'L_HIP_ROM_delta' with overall mean -0.0527\n",
      "Imputed 0 NaN(s) in column 'R_HIP_ROM_lag1' with overall mean 46.3797\n",
      "Imputed 0 NaN(s) in column 'R_HIP_ROM_delta' with overall mean -0.0531\n",
      "Imputed 0 NaN(s) in column 'exhaustion_rate_lag1' with overall mean 0.0004\n",
      "Imputed 0 NaN(s) in column 'exhaustion_rate_delta' with overall mean -0.0000\n",
      "Imputed 0 NaN(s) in column 'by_trial_exhaustion_score_lag1' with overall mean 0.8625\n",
      "Imputed 0 NaN(s) in column 'by_trial_exhaustion_score_delta' with overall mean 0.0001\n",
      "Imputed 0 NaN(s) in column 'injury_risk_lag1' with overall mean 0.9274\n",
      "Imputed 0 NaN(s) in column 'injury_risk_delta' with overall mean 0.0000\n",
      "Imputed 0 NaN(s) in column 'energy_acceleration_lag1' with overall mean -0.0037\n",
      "Imputed 0 NaN(s) in column 'energy_acceleration_delta' with overall mean 0.0000\n",
      "Imputed 0 NaN(s) in column 'power_avg_5_lag1' with overall mean 37.7435\n",
      "Imputed 0 NaN(s) in column 'power_avg_5_delta' with overall mean -0.0402\n",
      "Imputed 0 NaN(s) in column 'rolling_power_std_lag1' with overall mean 6.0845\n",
      "Imputed 0 NaN(s) in column 'rolling_power_std_delta' with overall mean 0.0156\n",
      "Imputed 0 NaN(s) in column 'rolling_hr_mean_lag1' with overall mean 62.0484\n",
      "Imputed 0 NaN(s) in column 'rolling_hr_mean_delta' with overall mean -0.0003\n",
      "Imputed 0 NaN(s) in column 'simulated_HR_lag1' with overall mean 62.9122\n",
      "Imputed 0 NaN(s) in column 'simulated_HR_delta' with overall mean -0.0003\n",
      "Imputed 0 NaN(s) in column 'player_height_in_meters_lag1' with overall mean 1.9100\n",
      "Imputed 0 NaN(s) in column 'player_height_in_meters_delta' with overall mean 0.0000\n",
      "Imputed 0 NaN(s) in column 'player_weight__in_kg_lag1' with overall mean 90.7000\n",
      "Imputed 0 NaN(s) in column 'player_weight__in_kg_delta' with overall mean 0.0000\n",
      "\n",
      "--- Final debug: summary at end of function ---\n",
      "Final summary shape: (125, 233)\n",
      "Final summary columns: ['trial_id', 'joint_energy', 'L_ELBOW_energy', 'R_ELBOW_energy', 'L_WRIST_energy', 'R_WRIST_energy', 'L_KNEE_energy', 'R_KNEE_energy', 'L_HIP_energy', 'R_HIP_energy', 'joint_power', 'L_ELBOW_ongoing_power', 'R_ELBOW_ongoing_power', 'L_WRIST_ongoing_power', 'R_WRIST_ongoing_power', 'L_KNEE_ongoing_power', 'R_KNEE_ongoing_power', 'L_HIP_ongoing_power', 'R_HIP_ongoing_power', 'elbow_asymmetry', 'wrist_asymmetry', 'knee_asymmetry', 'hip_asymmetry', 'L_ELBOW_angle', 'R_ELBOW_angle', 'L_WRIST_angle', 'R_WRIST_angle', 'L_KNEE_angle', 'R_KNEE_angle', 'L_SHOULDER_ROM', 'R_SHOULDER_ROM', 'L_WRIST_ROM', 'R_WRIST_ROM', 'L_KNEE_ROM', 'R_KNEE_ROM', 'L_HIP_ROM', 'R_HIP_ROM', 'exhaustion_rate', 'by_trial_exhaustion_score', 'injury_risk', 'energy_acceleration', 'power_avg_5', 'rolling_power_std', 'rolling_hr_mean', 'simulated_HR', 'player_height_in_meters', 'player_weight__in_kg', 'joint_energy_std', 'L_ELBOW_energy_std', 'R_ELBOW_energy_std', 'L_WRIST_energy_std', 'R_WRIST_energy_std', 'L_KNEE_energy_std', 'R_KNEE_energy_std', 'L_HIP_energy_std', 'R_HIP_energy_std', 'joint_power_std', 'L_ELBOW_ongoing_power_std', 'R_ELBOW_ongoing_power_std', 'L_WRIST_ongoing_power_std', 'R_WRIST_ongoing_power_std', 'L_KNEE_ongoing_power_std', 'R_KNEE_ongoing_power_std', 'L_HIP_ongoing_power_std', 'R_HIP_ongoing_power_std', 'elbow_asymmetry_std', 'wrist_asymmetry_std', 'knee_asymmetry_std', 'hip_asymmetry_std', 'L_ELBOW_angle_std', 'R_ELBOW_angle_std', 'L_WRIST_angle_std', 'R_WRIST_angle_std', 'L_KNEE_angle_std', 'R_KNEE_angle_std', 'L_SHOULDER_ROM_std', 'R_SHOULDER_ROM_std', 'L_WRIST_ROM_std', 'R_WRIST_ROM_std', 'L_KNEE_ROM_std', 'R_KNEE_ROM_std', 'L_HIP_ROM_std', 'R_HIP_ROM_std', 'exhaustion_rate_std', 'by_trial_exhaustion_score_std', 'injury_risk_std', 'energy_acceleration_std', 'power_avg_5_std', 'rolling_power_std_std', 'rolling_hr_mean_std', 'simulated_HR_std', 'player_height_in_meters_std', 'player_weight__in_kg_std', 'frame_count', 'phase_duration', 'joint_energy_lag1', 'joint_energy_rolling_avg', 'joint_energy_delta', 'L_ELBOW_energy_lag1', 'L_ELBOW_energy_rolling_avg', 'L_ELBOW_energy_delta', 'R_ELBOW_energy_lag1', 'R_ELBOW_energy_rolling_avg', 'R_ELBOW_energy_delta', 'L_WRIST_energy_lag1', 'L_WRIST_energy_rolling_avg', 'L_WRIST_energy_delta', 'R_WRIST_energy_lag1', 'R_WRIST_energy_rolling_avg', 'R_WRIST_energy_delta', 'L_KNEE_energy_lag1', 'L_KNEE_energy_rolling_avg', 'L_KNEE_energy_delta', 'R_KNEE_energy_lag1', 'R_KNEE_energy_rolling_avg', 'R_KNEE_energy_delta', 'L_HIP_energy_lag1', 'L_HIP_energy_rolling_avg', 'L_HIP_energy_delta', 'R_HIP_energy_lag1', 'R_HIP_energy_rolling_avg', 'R_HIP_energy_delta', 'joint_power_lag1', 'joint_power_rolling_avg', 'joint_power_delta', 'L_ELBOW_ongoing_power_lag1', 'L_ELBOW_ongoing_power_rolling_avg', 'L_ELBOW_ongoing_power_delta', 'R_ELBOW_ongoing_power_lag1', 'R_ELBOW_ongoing_power_rolling_avg', 'R_ELBOW_ongoing_power_delta', 'L_WRIST_ongoing_power_lag1', 'L_WRIST_ongoing_power_rolling_avg', 'L_WRIST_ongoing_power_delta', 'R_WRIST_ongoing_power_lag1', 'R_WRIST_ongoing_power_rolling_avg', 'R_WRIST_ongoing_power_delta', 'L_KNEE_ongoing_power_lag1', 'L_KNEE_ongoing_power_rolling_avg', 'L_KNEE_ongoing_power_delta', 'R_KNEE_ongoing_power_lag1', 'R_KNEE_ongoing_power_rolling_avg', 'R_KNEE_ongoing_power_delta', 'L_HIP_ongoing_power_lag1', 'L_HIP_ongoing_power_rolling_avg', 'L_HIP_ongoing_power_delta', 'R_HIP_ongoing_power_lag1', 'R_HIP_ongoing_power_rolling_avg', 'R_HIP_ongoing_power_delta', 'elbow_asymmetry_lag1', 'elbow_asymmetry_rolling_avg', 'elbow_asymmetry_delta', 'wrist_asymmetry_lag1', 'wrist_asymmetry_rolling_avg', 'wrist_asymmetry_delta', 'knee_asymmetry_lag1', 'knee_asymmetry_rolling_avg', 'knee_asymmetry_delta', 'hip_asymmetry_lag1', 'hip_asymmetry_rolling_avg', 'hip_asymmetry_delta', 'L_ELBOW_angle_lag1', 'L_ELBOW_angle_rolling_avg', 'L_ELBOW_angle_delta', 'R_ELBOW_angle_lag1', 'R_ELBOW_angle_rolling_avg', 'R_ELBOW_angle_delta', 'L_WRIST_angle_lag1', 'L_WRIST_angle_rolling_avg', 'L_WRIST_angle_delta', 'R_WRIST_angle_lag1', 'R_WRIST_angle_rolling_avg', 'R_WRIST_angle_delta', 'L_KNEE_angle_lag1', 'L_KNEE_angle_rolling_avg', 'L_KNEE_angle_delta', 'R_KNEE_angle_lag1', 'R_KNEE_angle_rolling_avg', 'R_KNEE_angle_delta', 'L_SHOULDER_ROM_lag1', 'L_SHOULDER_ROM_rolling_avg', 'L_SHOULDER_ROM_delta', 'R_SHOULDER_ROM_lag1', 'R_SHOULDER_ROM_rolling_avg', 'R_SHOULDER_ROM_delta', 'L_WRIST_ROM_lag1', 'L_WRIST_ROM_rolling_avg', 'L_WRIST_ROM_delta', 'R_WRIST_ROM_lag1', 'R_WRIST_ROM_rolling_avg', 'R_WRIST_ROM_delta', 'L_KNEE_ROM_lag1', 'L_KNEE_ROM_rolling_avg', 'L_KNEE_ROM_delta', 'R_KNEE_ROM_lag1', 'R_KNEE_ROM_rolling_avg', 'R_KNEE_ROM_delta', 'L_HIP_ROM_lag1', 'L_HIP_ROM_rolling_avg', 'L_HIP_ROM_delta', 'R_HIP_ROM_lag1', 'R_HIP_ROM_rolling_avg', 'R_HIP_ROM_delta', 'exhaustion_rate_lag1', 'exhaustion_rate_rolling_avg', 'exhaustion_rate_delta', 'by_trial_exhaustion_score_lag1', 'by_trial_exhaustion_score_rolling_avg', 'by_trial_exhaustion_score_delta', 'injury_risk_lag1', 'injury_risk_rolling_avg', 'injury_risk_delta', 'energy_acceleration_lag1', 'energy_acceleration_rolling_avg', 'energy_acceleration_delta', 'power_avg_5_lag1', 'power_avg_5_rolling_avg', 'power_avg_5_delta', 'rolling_power_std_lag1', 'rolling_power_std_rolling_avg', 'rolling_power_std_delta', 'rolling_hr_mean_lag1', 'rolling_hr_mean_rolling_avg', 'rolling_hr_mean_delta', 'simulated_HR_lag1', 'simulated_HR_rolling_avg', 'simulated_HR_delta', 'player_height_in_meters_lag1', 'player_height_in_meters_rolling_avg', 'player_height_in_meters_delta', 'player_weight__in_kg_lag1', 'player_weight__in_kg_rolling_avg', 'player_weight__in_kg_delta']\n",
      "Sample final summary rows:\n",
      "   trial_id  joint_energy  L_ELBOW_energy  R_ELBOW_energy  L_WRIST_energy  \\\n",
      "0    T0001      5.341869        0.105102        0.125535        0.130081   \n",
      "1    T0002      5.263840        0.103878        0.110386        0.129912   \n",
      "2    T0003      5.596989        0.101178        0.124936        0.131504   \n",
      "3    T0004      5.348701        0.104602        0.112017        0.129291   \n",
      "4    T0005      5.439174        0.106606        0.115325        0.131957   \n",
      "5    T0006      5.455351        0.101439        0.123728        0.133370   \n",
      "6    T0007      5.487271        0.107464        0.114087        0.134256   \n",
      "7    T0008      5.243526        0.093924        0.116931        0.119810   \n",
      "8    T0009      5.694075        0.104512        0.120290        0.134745   \n",
      "9    T0010      5.280447        0.102296        0.119196        0.129231   \n",
      "\n",
      "   R_WRIST_energy  L_KNEE_energy  R_KNEE_energy  L_HIP_energy  R_HIP_energy  \\\n",
      "0        0.136222       0.038153       0.039604      0.044602      0.047757   \n",
      "1        0.122849       0.040647       0.039783      0.043791      0.046801   \n",
      "2        0.142632       0.039833       0.040594      0.047543      0.053032   \n",
      "3        0.128318       0.039384       0.040105      0.045022      0.049800   \n",
      "4        0.127199       0.040484       0.040081      0.045247      0.050183   \n",
      "5        0.136529       0.038523       0.038588      0.042542      0.047046   \n",
      "6        0.127609       0.039623       0.040943      0.047514      0.051892   \n",
      "7        0.134414       0.038938       0.039539      0.044936      0.049904   \n",
      "8        0.135239       0.041029       0.042029      0.047296      0.051660   \n",
      "9        0.128929       0.036153       0.037201      0.041754      0.046329   \n",
      "\n",
      "   ...  rolling_hr_mean_delta  simulated_HR_lag1  simulated_HR_rolling_avg  \\\n",
      "0  ...              -0.000250          62.912189                 62.908328   \n",
      "1  ...              -0.086236          62.908328                 62.873819   \n",
      "2  ...               0.088210          62.839309                 62.902316   \n",
      "3  ...               0.030100          62.959310                 62.917941   \n",
      "4  ...              -0.056297          62.955203                 62.949319   \n",
      "5  ...               0.007493          62.933443                 62.933977   \n",
      "6  ...              -0.018010          62.913286                 62.924030   \n",
      "7  ...               0.010164          62.925361                 62.903651   \n",
      "8  ...               0.009179          62.872308                 62.930027   \n",
      "9  ...              -0.018864          62.992413                 62.911220   \n",
      "\n",
      "   simulated_HR_delta  player_height_in_meters_lag1  \\\n",
      "0           -0.000251                          1.91   \n",
      "1           -0.069018                          1.91   \n",
      "2            0.120000                          1.91   \n",
      "3           -0.004107                          1.91   \n",
      "4           -0.021759                          1.91   \n",
      "5           -0.020158                          1.91   \n",
      "6            0.012075                          1.91   \n",
      "7           -0.053053                          1.91   \n",
      "8            0.120105                          1.91   \n",
      "9           -0.123473                          1.91   \n",
      "\n",
      "   player_height_in_meters_rolling_avg  player_height_in_meters_delta  \\\n",
      "0                                 1.91                            0.0   \n",
      "1                                 1.91                            0.0   \n",
      "2                                 1.91                            0.0   \n",
      "3                                 1.91                            0.0   \n",
      "4                                 1.91                            0.0   \n",
      "5                                 1.91                            0.0   \n",
      "6                                 1.91                            0.0   \n",
      "7                                 1.91                            0.0   \n",
      "8                                 1.91                            0.0   \n",
      "9                                 1.91                            0.0   \n",
      "\n",
      "   player_weight__in_kg_lag1  player_weight__in_kg_rolling_avg  \\\n",
      "0                       90.7                              90.7   \n",
      "1                       90.7                              90.7   \n",
      "2                       90.7                              90.7   \n",
      "3                       90.7                              90.7   \n",
      "4                       90.7                              90.7   \n",
      "5                       90.7                              90.7   \n",
      "6                       90.7                              90.7   \n",
      "7                       90.7                              90.7   \n",
      "8                       90.7                              90.7   \n",
      "9                       90.7                              90.7   \n",
      "\n",
      "   player_weight__in_kg_delta  \n",
      "0                0.000000e+00  \n",
      "1                1.421085e-14  \n",
      "2               -1.421085e-14  \n",
      "3                0.000000e+00  \n",
      "4                0.000000e+00  \n",
      "5                0.000000e+00  \n",
      "6                0.000000e+00  \n",
      "7                0.000000e+00  \n",
      "8                0.000000e+00  \n",
      "9                0.000000e+00  \n",
      "\n",
      "[10 rows x 233 columns]\n",
      "--- DEBUG: summarize_data END ---\n",
      "\n",
      "Joint energy columns:  ['L_ANKLE_energy', 'R_ANKLE_energy', 'L_KNEE_energy', 'R_KNEE_energy', 'L_HIP_energy', 'R_HIP_energy', 'L_ELBOW_energy', 'R_ELBOW_energy', 'L_WRIST_energy', 'R_WRIST_energy', 'L_1STFINGER_energy', 'R_1STFINGER_energy', 'L_5THFINGER_energy', 'R_5THFINGER_energy', 'total_energy', 'joint_energy', 'rolling_energy_std']\n",
      "Joint power columns:  ['L_ANKLE_ongoing_power', 'R_ANKLE_ongoing_power', 'L_KNEE_ongoing_power', 'R_KNEE_ongoing_power', 'L_HIP_ongoing_power', 'R_HIP_ongoing_power', 'L_ELBOW_ongoing_power', 'R_ELBOW_ongoing_power', 'L_WRIST_ongoing_power', 'R_WRIST_ongoing_power', 'L_1STFINGER_ongoing_power', 'L_5THFINGER_ongoing_power', 'R_1STFINGER_ongoing_power', 'R_5THFINGER_ongoing_power']\n",
      "All angle columns:  ['entry_angle', 'L_ELBOW_angle', 'L_WRIST_angle', 'L_KNEE_angle', 'L_ELBOW_ongoing_angle', 'L_WRIST_ongoing_angle', 'L_KNEE_ongoing_angle', 'R_ELBOW_angle', 'R_WRIST_angle', 'R_KNEE_angle', 'R_ELBOW_ongoing_angle', 'R_WRIST_ongoing_angle', 'R_KNEE_ongoing_angle', 'initial_release_angle', 'calculated_release_angle', 'angle_difference', 'optimal_release_angle', 'L_SHOULDER_angle', 'R_SHOULDER_angle', 'L_HIP_angle', 'R_HIP_angle', 'L_ANKLE_angle', 'R_ANKLE_angle']\n",
      "print all the columns with by_trial_exhaustion_score:  ['by_trial_exhaustion_score', 'L_ANKLE_energy_by_trial_exhaustion_score', 'R_ANKLE_energy_by_trial_exhaustion_score', 'L_KNEE_energy_by_trial_exhaustion_score', 'R_KNEE_energy_by_trial_exhaustion_score', 'L_HIP_energy_by_trial_exhaustion_score', 'R_HIP_energy_by_trial_exhaustion_score', 'L_ELBOW_energy_by_trial_exhaustion_score', 'R_ELBOW_energy_by_trial_exhaustion_score', 'L_WRIST_energy_by_trial_exhaustion_score', 'R_WRIST_energy_by_trial_exhaustion_score', 'L_1STFINGER_energy_by_trial_exhaustion_score', 'R_1STFINGER_energy_by_trial_exhaustion_score', 'L_5THFINGER_energy_by_trial_exhaustion_score', 'R_5THFINGER_energy_by_trial_exhaustion_score']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Added shot-phase-level aggregated features: shot_phase_mean_exhaustion, shot_phase_total_joint_energy.\n",
      "INFO: Step [prepare_joint_features]: DataFrame shape = (2957, 325)\n",
      "INFO: New columns added: ['joint_energy', 'joint_power', 'energy_acceleration', 'ankle_power_ratio', 'hip_asymmetry', 'ankle_asymmetry', 'wrist_asymmetry', 'elbow_asymmetry', 'knee_asymmetry', '1stfinger_asymmetry', '5thfinger_asymmetry', 'hip_power_ratio', 'ankle_power_ratio', 'wrist_power_ratio', 'elbow_power_ratio', 'knee_power_ratio', '1stfinger_power_ratio', '5thfinger_power_ratio', 'L_KNEE_ROM', 'L_KNEE_ROM_deviation', 'L_KNEE_ROM_extreme', 'R_KNEE_ROM', 'R_KNEE_ROM_deviation', 'R_KNEE_ROM_extreme', 'L_SHOULDER_ROM', 'L_SHOULDER_ROM_deviation', 'L_SHOULDER_ROM_extreme', 'R_SHOULDER_ROM', 'R_SHOULDER_ROM_deviation', 'R_SHOULDER_ROM_extreme', 'L_HIP_ROM', 'L_HIP_ROM_deviation', 'L_HIP_ROM_extreme', 'R_HIP_ROM', 'R_HIP_ROM_deviation', 'R_HIP_ROM_extreme', 'L_ANKLE_ROM', 'L_ANKLE_ROM_deviation', 'L_ANKLE_ROM_extreme', 'R_ANKLE_ROM', 'R_ANKLE_ROM_deviation', 'R_ANKLE_ROM_extreme', 'L_WRIST_ROM', 'L_WRIST_ROM_deviation', 'L_WRIST_ROM_extreme', 'R_WRIST_ROM', 'R_WRIST_ROM_deviation', 'R_WRIST_ROM_extreme', 'exhaustion_rate', 'simulated_HR', 'trial_mean_exhaustion', 'trial_total_joint_energy', 'shot_phase_mean_exhaustion', 'shot_phase_total_joint_energy']\n",
      "INFO:  - joint_energy: dtype=float64, sample values=[10.08992268 10.9593127  11.30959393 11.32394461 11.06512286]\n",
      "INFO:  - joint_power: dtype=float64, sample values=[47.69178819 52.83925347 54.37092192 52.85948725 54.51087334]\n",
      "INFO:  - energy_acceleration: dtype=float64, sample values=[ 0.02634515  0.01061458  0.00042208 -0.00784308  0.00141843]\n",
      "INFO:  - ankle_power_ratio: dtype=float64, sample values=[0.72760088 1.01904274 0.73720619 0.64757333 0.74832863]\n",
      "INFO:  - hip_asymmetry: dtype=float64, sample values=[0.00523449 0.00622917 0.00687023 0.00457927 0.00393719]\n",
      "INFO:  - ankle_asymmetry: dtype=float64, sample values=[1.12310563e-03 9.71329091e-05 1.78232998e-03 2.77498836e-03\n",
      " 2.17951334e-03]\n",
      "INFO:  - wrist_asymmetry: dtype=float64, sample values=[0.02550815 0.03312197 0.03329263 0.02598216 0.01218944]\n",
      "INFO:  - elbow_asymmetry: dtype=float64, sample values=[0.03811244 0.03162486 0.02243636 0.01536964 0.01132337]\n",
      "INFO:  - knee_asymmetry: dtype=float64, sample values=[4.33633806e-03 7.17196771e-03 6.87312543e-03 8.16541076e-03\n",
      " 3.98986399e-16]\n",
      "INFO:  - 1stfinger_asymmetry: dtype=float64, sample values=[0.0226187  0.0408745  0.04942299 0.04134683 0.02343194]\n",
      "INFO:  - 5thfinger_asymmetry: dtype=float64, sample values=[0.03603709 0.04348361 0.04887027 0.04612895 0.02932847]\n",
      "INFO:  - hip_power_ratio: dtype=float64, sample values=[0.82056318 0.80637473 0.8000839  0.87877647 0.91743528]\n",
      "INFO:  - ankle_power_ratio: dtype=float64, sample values=[0.72760088 1.01904274 0.73720619 0.64757333 0.74832863]\n",
      "INFO:  - wrist_power_ratio: dtype=float64, sample values=[1.15111823 1.18023156 1.17195659 1.1294117  1.05896797]\n",
      "INFO:  - elbow_power_ratio: dtype=float64, sample values=[0.72417731 0.7859565  0.85657133 0.90817653 0.93707375]\n",
      "INFO:  - knee_power_ratio: dtype=float64, sample values=[0.86841355 0.79235586 0.76452974 0.5967381  0.99999727]\n",
      "INFO:  - 1stfinger_power_ratio: dtype=float64, sample values=[1.11843528 1.21095949 1.25377493 1.20770464 1.11492578]\n",
      "INFO:  - 5thfinger_power_ratio: dtype=float64, sample values=[1.15744961 1.18233426 1.20779857 1.20574002 1.13534248]\n",
      "INFO:  - L_KNEE_ROM: dtype=float64, sample values=[52.00092712 53.51096764 53.65900592 52.45838556 49.12814203]\n",
      "INFO:  - L_KNEE_ROM_deviation: dtype=float64, sample values=[67.99907288 66.48903236 66.34099408 67.54161444 70.87185797]\n",
      "INFO:  - L_KNEE_ROM_extreme: dtype=int32, sample values=[1]\n",
      "INFO:  - R_KNEE_ROM: dtype=float64, sample values=[52.19842097 55.56237009 55.28397109 56.00313843 52.32530605]\n",
      "INFO:  - R_KNEE_ROM_deviation: dtype=float64, sample values=[67.80157903 64.43762991 64.71602891 63.99686157 67.67469395]\n",
      "INFO:  - R_KNEE_ROM_extreme: dtype=int32, sample values=[1]\n",
      "INFO:  - L_SHOULDER_ROM: dtype=float64, sample values=[45.5163003  51.7884663  49.83456004 51.11853246 50.20623207]\n",
      "INFO:  - L_SHOULDER_ROM_deviation: dtype=float64, sample values=[0.]\n",
      "INFO:  - L_SHOULDER_ROM_extreme: dtype=int32, sample values=[0]\n",
      "INFO:  - R_SHOULDER_ROM: dtype=float64, sample values=[75.89290407 79.93982326 80.64680686 72.81436519 73.63295774]\n",
      "INFO:  - R_SHOULDER_ROM_deviation: dtype=float64, sample values=[0.]\n",
      "INFO:  - R_SHOULDER_ROM_extreme: dtype=int32, sample values=[0]\n",
      "INFO:  - L_HIP_ROM: dtype=float64, sample values=[38.37031383 36.65101834 40.67778275 35.13016974 35.02083926]\n",
      "INFO:  - L_HIP_ROM_deviation: dtype=float64, sample values=[0.]\n",
      "INFO:  - L_HIP_ROM_extreme: dtype=int32, sample values=[0]\n",
      "INFO:  - R_HIP_ROM: dtype=float64, sample values=[49.89827371 53.29735427 55.01726113 50.74228775 50.23341005]\n",
      "INFO:  - R_HIP_ROM_deviation: dtype=float64, sample values=[0.]\n",
      "INFO:  - R_HIP_ROM_extreme: dtype=int32, sample values=[0]\n",
      "INFO:  - L_ANKLE_ROM: dtype=float64, sample values=[32.32505371 32.99123382 36.39764221 35.45367686 32.1162011 ]\n",
      "INFO:  - L_ANKLE_ROM_deviation: dtype=float64, sample values=[12.32505371 12.99123382 16.39764221 15.45367686 12.1162011 ]\n",
      "INFO:  - L_ANKLE_ROM_extreme: dtype=int32, sample values=[1]\n",
      "INFO:  - R_ANKLE_ROM: dtype=float64, sample values=[39.81071151 45.54250042 45.70572465 43.1338381  39.83072086]\n",
      "INFO:  - R_ANKLE_ROM_deviation: dtype=float64, sample values=[19.81071151 25.54250042 25.70572465 23.1338381  19.83072086]\n",
      "INFO:  - R_ANKLE_ROM_extreme: dtype=int32, sample values=[1]\n",
      "INFO:  - L_WRIST_ROM: dtype=float64, sample values=[19.60628366 24.85503336 27.6434808  26.09213451 19.99680555]\n",
      "INFO:  - L_WRIST_ROM_deviation: dtype=float64, sample values=[0.]\n",
      "INFO:  - L_WRIST_ROM_extreme: dtype=int32, sample values=[0]\n",
      "INFO:  - R_WRIST_ROM: dtype=float64, sample values=[26.94867994 30.72422249 27.0633164  35.1551556  30.2743699 ]\n",
      "INFO:  - R_WRIST_ROM_deviation: dtype=float64, sample values=[0.]\n",
      "INFO:  - R_WRIST_ROM_extreme: dtype=int32, sample values=[0]\n",
      "INFO:  - exhaustion_rate: dtype=float64, sample values=[0.00071098 0.00073159 0.00071125 0.00073347 0.00074737]\n",
      "INFO:  - simulated_HR: dtype=float64, sample values=[64.02199892 64.31800924 64.45930709 64.49988595 64.45854612]\n",
      "INFO:  - trial_mean_exhaustion: dtype=float64, sample values=[0.87051117 0.84010503 0.85347528 0.90039509 0.8677941 ]\n",
      "INFO:  - trial_total_joint_energy: dtype=float64, sample values=[171.07599289 209.72466733 196.98082251 188.60814653 192.18649936]\n",
      "INFO:  - shot_phase_mean_exhaustion: dtype=float64, sample values=[0.68703699 0.73513505 0.82154545 0.95956508 0.6638939 ]\n",
      "INFO:  - shot_phase_total_joint_energy: dtype=float64, sample values=[32.35882931 11.32394461 66.92475658 60.4684624  59.15439675]\n",
      "INFO: Created 'rolling_energy_std' with sample: [0.0, 0.43469500650278237, 0.5127414207685912, 0.5013797438165062, 0.4521536170384085, 0.14188920416340065, 0.11037872479530932, 0.12198165947376093, 0.11345879120283103, 0.17385184126468062]\n",
      "INFO: Created 'rolling_energy_std' with window 5.\n",
      "INFO: Added trial-level aggregated features in feature_engineering: trial_mean_exhaustion_fe, trial_injury_rate_fe.\n",
      "INFO: Added shot-phase-level aggregated features in feature_engineering: shot_phase_mean_exhaustion_fe, shot_phase_injury_rate_fe.\n",
      "INFO: Step [feature_engineering]: DataFrame shape = (2956, 329)\n",
      "INFO: New columns added: ['time_since_start', 'rolling_energy_std', 'exhaustion_lag1', 'ema_exhaustion', 'rolling_exhaustion', 'injury_risk', 'L_ANKLE_exhaustion_rate', 'L_ANKLE_rolling_exhaustion', 'L_ANKLE_injury_risk', 'R_ANKLE_exhaustion_rate', 'R_ANKLE_rolling_exhaustion', 'R_ANKLE_injury_risk', 'L_WRIST_exhaustion_rate', 'L_WRIST_rolling_exhaustion', 'L_WRIST_injury_risk', 'R_WRIST_exhaustion_rate', 'R_WRIST_rolling_exhaustion', 'R_WRIST_injury_risk', 'L_ELBOW_exhaustion_rate', 'L_ELBOW_rolling_exhaustion', 'L_ELBOW_injury_risk', 'R_ELBOW_exhaustion_rate', 'R_ELBOW_rolling_exhaustion', 'R_ELBOW_injury_risk', 'L_KNEE_exhaustion_rate', 'L_KNEE_rolling_exhaustion', 'L_KNEE_injury_risk', 'R_KNEE_exhaustion_rate', 'R_KNEE_rolling_exhaustion', 'R_KNEE_injury_risk', 'L_HIP_exhaustion_rate', 'L_HIP_rolling_exhaustion', 'L_HIP_injury_risk', 'R_HIP_exhaustion_rate', 'R_HIP_rolling_exhaustion', 'R_HIP_injury_risk', 'trial_mean_exhaustion_fe', 'trial_injury_rate_fe', 'shot_phase_mean_exhaustion_fe', 'shot_phase_injury_rate_fe']\n",
      "INFO:  - time_since_start: dtype=int64, sample values=[ 33  66 100 133 166]\n",
      "INFO:  - rolling_energy_std: dtype=float64, sample values=[0.43469501 0.51274142 0.50137974 0.45215362 0.1418892 ]\n",
      "INFO:  - exhaustion_lag1: dtype=float64, sample values=[0.66334808 0.68681029 0.7109526  0.73513505 0.75933951]\n",
      "INFO:  - ema_exhaustion: dtype=float64, sample values=[0.66761394 0.67549369 0.68633758 0.69961065 0.71495465]\n",
      "INFO:  - rolling_exhaustion: dtype=float64, sample values=[1.35015837 2.06111097 2.79624602 3.55558552 4.33958814]\n",
      "INFO:  - injury_risk: dtype=int32, sample values=[1 0]\n",
      "INFO:  - L_ANKLE_exhaustion_rate: dtype=float64, sample values=[0.0001312  0.00012625 0.00012496 0.00016363 0.00021867]\n",
      "INFO:  - L_ANKLE_rolling_exhaustion: dtype=float64, sample values=[1.39072301 2.09241547 2.79835659 3.50969766 4.22825472]\n",
      "INFO:  - L_ANKLE_injury_risk: dtype=int32, sample values=[1 0]\n",
      "INFO:  - R_ANKLE_exhaustion_rate: dtype=float64, sample values=[7.30474516e-05 9.71621936e-05 1.09483649e-04 1.24064928e-04\n",
      " 1.63966091e-04]\n",
      "INFO:  - R_ANKLE_rolling_exhaustion: dtype=float64, sample values=[1.65997499 2.49437412 3.33249569 4.1747114  5.022338  ]\n",
      "INFO:  - R_ANKLE_injury_risk: dtype=int32, sample values=[1 0]\n",
      "INFO:  - L_WRIST_exhaustion_rate: dtype=float64, sample values=[0.00094021 0.00098359 0.00095403 0.0009489  0.00089518]\n",
      "INFO:  - L_WRIST_rolling_exhaustion: dtype=float64, sample values=[1.30508004 2.00559197 2.73854089 3.50280367 4.29660753]\n",
      "INFO:  - L_WRIST_injury_risk: dtype=int32, sample values=[1 0]\n",
      "INFO:  - R_WRIST_exhaustion_rate: dtype=float64, sample values=[0.00069982 0.00073728 0.00074206 0.00078717 0.00080418]\n",
      "INFO:  - R_WRIST_rolling_exhaustion: dtype=float64, sample values=[1.34654626 2.05569651 2.7900767  3.55043345 4.33732818]\n",
      "INFO:  - R_WRIST_injury_risk: dtype=int32, sample values=[1 0]\n",
      "INFO:  - L_ELBOW_exhaustion_rate: dtype=float64, sample values=[0.00069994 0.00080763 0.0008893  0.00101637 0.00107346]\n",
      "INFO:  - L_ELBOW_rolling_exhaustion: dtype=float64, sample values=[1.1848754  1.81551392 2.47638871 3.17080372 3.90064293]\n",
      "INFO:  - L_ELBOW_injury_risk: dtype=int32, sample values=[1 0]\n",
      "INFO:  - R_ELBOW_exhaustion_rate: dtype=float64, sample values=[0.00073245 0.00077548 0.00080537 0.00089206 0.00093756]\n",
      "INFO:  - R_ELBOW_rolling_exhaustion: dtype=float64, sample values=[1.20683865 1.84793407 2.51641216 3.21432837 3.943184  ]\n",
      "INFO:  - R_ELBOW_injury_risk: dtype=int32, sample values=[1 0]\n",
      "INFO:  - L_KNEE_exhaustion_rate: dtype=float64, sample values=[0.0003438  0.00028033 0.00014732 0.00015179 0.00038617]\n",
      "INFO:  - L_KNEE_rolling_exhaustion: dtype=float64, sample values=[1.37079523 2.07111644 2.77644664 3.48678581 4.20986849]\n",
      "INFO:  - L_KNEE_injury_risk: dtype=int32, sample values=[1 0]\n",
      "INFO:  - R_KNEE_exhaustion_rate: dtype=float64, sample values=[0.00038636 0.00032651 0.00021984 0.00013516 0.00031836]\n",
      "INFO:  - R_KNEE_rolling_exhaustion: dtype=float64, sample values=[1.42306719 2.15175059 2.8879085  3.62852672 4.37965083]\n",
      "INFO:  - R_KNEE_injury_risk: dtype=int32, sample values=[1 0]\n",
      "INFO:  - L_HIP_exhaustion_rate: dtype=float64, sample values=[0.00030445 0.00032267 0.00037812 0.00051342 0.00067221]\n",
      "INFO:  - L_HIP_rolling_exhaustion: dtype=float64, sample values=[1.30311662 1.9703465  2.6504324  3.34746117 4.06667287]\n",
      "INFO:  - L_HIP_injury_risk: dtype=int32, sample values=[1 0]\n",
      "INFO:  - R_HIP_exhaustion_rate: dtype=float64, sample values=[0.00035387 0.000378   0.00040329 0.00052453 0.0006885 ]\n",
      "INFO:  - R_HIP_rolling_exhaustion: dtype=float64, sample values=[1.30478515 1.97549067 2.65990807 3.36163481 4.08608201]\n",
      "INFO:  - R_HIP_injury_risk: dtype=int32, sample values=[1 0]\n",
      "INFO:  - trial_mean_exhaustion_fe: dtype=float64, sample values=[0.88086932 0.84010503 0.85347528 0.90039509 0.8677941 ]\n",
      "INFO:  - trial_injury_rate_fe: dtype=float64, sample values=[1.         0.38461538 0.34782609 0.30434783 0.04347826]\n",
      "INFO:  - shot_phase_mean_exhaustion_fe: dtype=float64, sample values=[0.69888145 0.73513505 0.82154545 0.95956508 0.6638939 ]\n",
      "INFO:  - shot_phase_injury_rate_fe: dtype=float64, sample values=[1.         0.         0.30769231 0.18181818 0.09090909]\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:519: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  summary[f\"{col}_lag1\"] = summary.groupby(group_key)[col].shift(1)\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:520: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  summary[f\"{col}_rolling_avg\"] = (\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:526: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  summary[f\"{col}_delta\"] = summary[col] - summary[f\"{col}_lag1\"]\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:519: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  summary[f\"{col}_lag1\"] = summary.groupby(group_key)[col].shift(1)\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:520: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  summary[f\"{col}_rolling_avg\"] = (\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:526: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  summary[f\"{col}_delta\"] = summary[col] - summary[f\"{col}_lag1\"]\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:519: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  summary[f\"{col}_lag1\"] = summary.groupby(group_key)[col].shift(1)\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:520: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  summary[f\"{col}_rolling_avg\"] = (\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:526: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  summary[f\"{col}_delta\"] = summary[col] - summary[f\"{col}_lag1\"]\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:519: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  summary[f\"{col}_lag1\"] = summary.groupby(group_key)[col].shift(1)\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:520: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  summary[f\"{col}_rolling_avg\"] = (\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:526: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  summary[f\"{col}_delta\"] = summary[col] - summary[f\"{col}_lag1\"]\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:519: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  summary[f\"{col}_lag1\"] = summary.groupby(group_key)[col].shift(1)\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:520: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  summary[f\"{col}_rolling_avg\"] = (\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:526: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  summary[f\"{col}_delta\"] = summary[col] - summary[f\"{col}_lag1\"]\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:519: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  summary[f\"{col}_lag1\"] = summary.groupby(group_key)[col].shift(1)\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:520: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  summary[f\"{col}_rolling_avg\"] = (\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:526: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  summary[f\"{col}_delta\"] = summary[col] - summary[f\"{col}_lag1\"]\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:519: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  summary[f\"{col}_lag1\"] = summary.groupby(group_key)[col].shift(1)\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:520: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  summary[f\"{col}_rolling_avg\"] = (\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:526: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  summary[f\"{col}_delta\"] = summary[col] - summary[f\"{col}_lag1\"]\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:519: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  summary[f\"{col}_lag1\"] = summary.groupby(group_key)[col].shift(1)\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:520: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  summary[f\"{col}_rolling_avg\"] = (\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:526: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  summary[f\"{col}_delta\"] = summary[col] - summary[f\"{col}_lag1\"]\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:519: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  summary[f\"{col}_lag1\"] = summary.groupby(group_key)[col].shift(1)\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:520: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  summary[f\"{col}_rolling_avg\"] = (\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:526: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  summary[f\"{col}_delta\"] = summary[col] - summary[f\"{col}_lag1\"]\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:519: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  summary[f\"{col}_lag1\"] = summary.groupby(group_key)[col].shift(1)\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:520: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  summary[f\"{col}_rolling_avg\"] = (\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:526: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  summary[f\"{col}_delta\"] = summary[col] - summary[f\"{col}_lag1\"]\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:519: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  summary[f\"{col}_lag1\"] = summary.groupby(group_key)[col].shift(1)\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:520: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  summary[f\"{col}_rolling_avg\"] = (\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:526: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  summary[f\"{col}_delta\"] = summary[col] - summary[f\"{col}_lag1\"]\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:519: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  summary[f\"{col}_lag1\"] = summary.groupby(group_key)[col].shift(1)\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:520: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  summary[f\"{col}_rolling_avg\"] = (\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:526: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  summary[f\"{col}_delta\"] = summary[col] - summary[f\"{col}_lag1\"]\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:519: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  summary[f\"{col}_lag1\"] = summary.groupby(group_key)[col].shift(1)\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:520: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  summary[f\"{col}_rolling_avg\"] = (\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:526: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  summary[f\"{col}_delta\"] = summary[col] - summary[f\"{col}_lag1\"]\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:519: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  summary[f\"{col}_lag1\"] = summary.groupby(group_key)[col].shift(1)\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:520: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  summary[f\"{col}_rolling_avg\"] = (\n",
      "c:\\docker_projects\\spl_freethrow_biomechanics_analysis_ml_prediction\\notebooks\\Deep_Learning_Final\\ml\\load_and_prepare_data\\load_data_and_analyze.py:526: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  summary[f\"{col}_delta\"] = summary[col] - summary[f\"{col}_lag1\"]\n",
      "INFO: === Base Dataset Analysis (Overall + Joint-Specific) ===\n",
      "INFO: Running feature importance analysis for Base Data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- DEBUG: summarize_data START ---\n",
      "Initial data shape: (2956, 329)\n",
      "Grouping by: ['trial_id', 'shooting_phases']\n",
      "Aggregation columns: ['joint_energy', 'L_ELBOW_energy', 'R_ELBOW_energy', 'L_WRIST_energy', 'R_WRIST_energy', 'L_KNEE_energy', 'R_KNEE_energy', 'L_HIP_energy', 'R_HIP_energy', 'joint_power', 'L_ELBOW_ongoing_power', 'R_ELBOW_ongoing_power', 'L_WRIST_ongoing_power', 'R_WRIST_ongoing_power', 'L_KNEE_ongoing_power', 'R_KNEE_ongoing_power', 'L_HIP_ongoing_power', 'R_HIP_ongoing_power', 'elbow_asymmetry', 'wrist_asymmetry', 'knee_asymmetry', 'hip_asymmetry', 'L_ELBOW_angle', 'R_ELBOW_angle', 'L_WRIST_angle', 'R_WRIST_angle', 'L_KNEE_angle', 'R_KNEE_angle', 'L_SHOULDER_ROM', 'R_SHOULDER_ROM', 'L_WRIST_ROM', 'R_WRIST_ROM', 'L_KNEE_ROM', 'R_KNEE_ROM', 'L_HIP_ROM', 'R_HIP_ROM', 'exhaustion_rate', 'by_trial_exhaustion_score', 'injury_risk', 'energy_acceleration', 'power_avg_5', 'rolling_power_std', 'rolling_hr_mean', 'simulated_HR', 'player_height_in_meters', 'player_weight__in_kg']\n",
      "Lag columns: ['joint_energy', 'L_ELBOW_energy', 'R_ELBOW_energy', 'L_WRIST_energy', 'R_WRIST_energy', 'L_KNEE_energy', 'R_KNEE_energy', 'L_HIP_energy', 'R_HIP_energy', 'joint_power', 'L_ELBOW_ongoing_power', 'R_ELBOW_ongoing_power', 'L_WRIST_ongoing_power', 'R_WRIST_ongoing_power', 'L_KNEE_ongoing_power', 'R_KNEE_ongoing_power', 'L_HIP_ongoing_power', 'R_HIP_ongoing_power', 'elbow_asymmetry', 'wrist_asymmetry', 'knee_asymmetry', 'hip_asymmetry', 'L_ELBOW_angle', 'R_ELBOW_angle', 'L_WRIST_angle', 'R_WRIST_angle', 'L_KNEE_angle', 'R_KNEE_angle', 'L_SHOULDER_ROM', 'R_SHOULDER_ROM', 'L_WRIST_ROM', 'R_WRIST_ROM', 'L_KNEE_ROM', 'R_KNEE_ROM', 'L_HIP_ROM', 'R_HIP_ROM', 'exhaustion_rate', 'by_trial_exhaustion_score', 'injury_risk', 'energy_acceleration', 'power_avg_5', 'rolling_power_std', 'rolling_hr_mean', 'simulated_HR', 'player_height_in_meters', 'player_weight__in_kg']\n",
      "Rolling window: 3\n",
      "Global lag: False\n",
      "Forced phase list: ['arm_cock', 'arm_release', 'leg_cock', 'wrist_release']\n",
      "\n",
      "Computed lag features for 'joint_energy' with global_lag=False\n",
      "\n",
      "Computed lag features for 'L_ELBOW_energy' with global_lag=False\n",
      "\n",
      "Computed lag features for 'R_ELBOW_energy' with global_lag=False\n",
      "\n",
      "Computed lag features for 'L_WRIST_energy' with global_lag=False\n",
      "\n",
      "Computed lag features for 'R_WRIST_energy' with global_lag=False\n",
      "\n",
      "Computed lag features for 'L_KNEE_energy' with global_lag=False\n",
      "\n",
      "Computed lag features for 'R_KNEE_energy' with global_lag=False\n",
      "\n",
      "Computed lag features for 'L_HIP_energy' with global_lag=False\n",
      "\n",
      "Computed lag features for 'R_HIP_energy' with global_lag=False\n",
      "\n",
      "Computed lag features for 'joint_power' with global_lag=False\n",
      "\n",
      "Computed lag features for 'L_ELBOW_ongoing_power' with global_lag=False\n",
      "\n",
      "Computed lag features for 'R_ELBOW_ongoing_power' with global_lag=False\n",
      "\n",
      "Computed lag features for 'L_WRIST_ongoing_power' with global_lag=False\n",
      "\n",
      "Computed lag features for 'R_WRIST_ongoing_power' with global_lag=False\n",
      "\n",
      "Computed lag features for 'L_KNEE_ongoing_power' with global_lag=False\n",
      "\n",
      "Computed lag features for 'R_KNEE_ongoing_power' with global_lag=False\n",
      "\n",
      "Computed lag features for 'L_HIP_ongoing_power' with global_lag=False\n",
      "\n",
      "Computed lag features for 'R_HIP_ongoing_power' with global_lag=False\n",
      "\n",
      "Computed lag features for 'elbow_asymmetry' with global_lag=False\n",
      "\n",
      "Computed lag features for 'wrist_asymmetry' with global_lag=False\n",
      "\n",
      "Computed lag features for 'knee_asymmetry' with global_lag=False\n",
      "\n",
      "Computed lag features for 'hip_asymmetry' with global_lag=False\n",
      "\n",
      "Computed lag features for 'L_ELBOW_angle' with global_lag=False\n",
      "\n",
      "Computed lag features for 'R_ELBOW_angle' with global_lag=False\n",
      "\n",
      "Computed lag features for 'L_WRIST_angle' with global_lag=False\n",
      "\n",
      "Computed lag features for 'R_WRIST_angle' with global_lag=False\n",
      "\n",
      "Computed lag features for 'L_KNEE_angle' with global_lag=False\n",
      "\n",
      "Computed lag features for 'R_KNEE_angle' with global_lag=False\n",
      "\n",
      "Computed lag features for 'L_SHOULDER_ROM' with global_lag=False\n",
      "\n",
      "Computed lag features for 'R_SHOULDER_ROM' with global_lag=False\n",
      "\n",
      "Computed lag features for 'L_WRIST_ROM' with global_lag=False\n",
      "\n",
      "Computed lag features for 'R_WRIST_ROM' with global_lag=False\n",
      "\n",
      "Computed lag features for 'L_KNEE_ROM' with global_lag=False\n",
      "\n",
      "Computed lag features for 'R_KNEE_ROM' with global_lag=False\n",
      "\n",
      "Computed lag features for 'L_HIP_ROM' with global_lag=False\n",
      "\n",
      "Computed lag features for 'R_HIP_ROM' with global_lag=False\n",
      "\n",
      "Computed lag features for 'exhaustion_rate' with global_lag=False\n",
      "\n",
      "Computed lag features for 'by_trial_exhaustion_score' with global_lag=False\n",
      "\n",
      "Computed lag features for 'injury_risk' with global_lag=False\n",
      "\n",
      "Computed lag features for 'energy_acceleration' with global_lag=False\n",
      "\n",
      "Computed lag features for 'power_avg_5' with global_lag=False\n",
      "\n",
      "Computed lag features for 'rolling_power_std' with global_lag=False\n",
      "\n",
      "Computed lag features for 'rolling_hr_mean' with global_lag=False\n",
      "\n",
      "Computed lag features for 'simulated_HR' with global_lag=False\n",
      "\n",
      "Computed lag features for 'player_height_in_meters' with global_lag=False\n",
      "\n",
      "Computed lag features for 'player_weight__in_kg' with global_lag=False\n",
      "Imputed 0 NaN(s) in column 'joint_energy_lag1' with overall mean 9.4837\n",
      "Imputed 0 NaN(s) in column 'joint_energy_delta' with overall mean -0.0038\n",
      "Imputed 0 NaN(s) in column 'L_ELBOW_energy_lag1' with overall mean 0.1244\n",
      "Imputed 0 NaN(s) in column 'L_ELBOW_energy_delta' with overall mean -0.0000\n",
      "Imputed 0 NaN(s) in column 'R_ELBOW_energy_lag1' with overall mean 0.1360\n",
      "Imputed 0 NaN(s) in column 'R_ELBOW_energy_delta' with overall mean -0.0001\n",
      "Imputed 0 NaN(s) in column 'L_WRIST_energy_lag1' with overall mean 0.1615\n",
      "Imputed 0 NaN(s) in column 'L_WRIST_energy_delta' with overall mean -0.0002\n",
      "Imputed 0 NaN(s) in column 'R_WRIST_energy_lag1' with overall mean 0.1568\n",
      "Imputed 0 NaN(s) in column 'R_WRIST_energy_delta' with overall mean -0.0001\n",
      "Imputed 0 NaN(s) in column 'L_KNEE_energy_lag1' with overall mean 0.0359\n",
      "Imputed 0 NaN(s) in column 'L_KNEE_energy_delta' with overall mean 0.0000\n",
      "Imputed 0 NaN(s) in column 'R_KNEE_energy_lag1' with overall mean 0.0373\n",
      "Imputed 0 NaN(s) in column 'R_KNEE_energy_delta' with overall mean 0.0000\n",
      "Imputed 0 NaN(s) in column 'L_HIP_energy_lag1' with overall mean 0.0460\n",
      "Imputed 0 NaN(s) in column 'L_HIP_energy_delta' with overall mean 0.0000\n",
      "Imputed 0 NaN(s) in column 'R_HIP_energy_lag1' with overall mean 0.0509\n",
      "Imputed 0 NaN(s) in column 'R_HIP_energy_delta' with overall mean 0.0000\n",
      "Imputed 0 NaN(s) in column 'joint_power_lag1' with overall mean 44.2079\n",
      "Imputed 0 NaN(s) in column 'joint_power_delta' with overall mean -0.0272\n",
      "Imputed 0 NaN(s) in column 'L_ELBOW_ongoing_power_lag1' with overall mean 3.7329\n",
      "Imputed 0 NaN(s) in column 'L_ELBOW_ongoing_power_delta' with overall mean -0.0014\n",
      "Imputed 0 NaN(s) in column 'R_ELBOW_ongoing_power_lag1' with overall mean 4.0810\n",
      "Imputed 0 NaN(s) in column 'R_ELBOW_ongoing_power_delta' with overall mean -0.0032\n",
      "Imputed 0 NaN(s) in column 'L_WRIST_ongoing_power_lag1' with overall mean 4.8446\n",
      "Imputed 0 NaN(s) in column 'L_WRIST_ongoing_power_delta' with overall mean -0.0052\n",
      "Imputed 0 NaN(s) in column 'R_WRIST_ongoing_power_lag1' with overall mean 4.7025\n",
      "Imputed 0 NaN(s) in column 'R_WRIST_ongoing_power_delta' with overall mean -0.0035\n",
      "Imputed 0 NaN(s) in column 'L_KNEE_ongoing_power_lag1' with overall mean 1.0763\n",
      "Imputed 0 NaN(s) in column 'L_KNEE_ongoing_power_delta' with overall mean 0.0010\n",
      "Imputed 0 NaN(s) in column 'R_KNEE_ongoing_power_lag1' with overall mean 1.1197\n",
      "Imputed 0 NaN(s) in column 'R_KNEE_ongoing_power_delta' with overall mean 0.0007\n",
      "Imputed 0 NaN(s) in column 'L_HIP_ongoing_power_lag1' with overall mean 1.3791\n",
      "Imputed 0 NaN(s) in column 'L_HIP_ongoing_power_delta' with overall mean 0.0011\n",
      "Imputed 0 NaN(s) in column 'R_HIP_ongoing_power_lag1' with overall mean 1.5275\n",
      "Imputed 0 NaN(s) in column 'R_HIP_ongoing_power_delta' with overall mean 0.0014\n",
      "Imputed 0 NaN(s) in column 'elbow_asymmetry_lag1' with overall mean 0.0241\n",
      "Imputed 0 NaN(s) in column 'elbow_asymmetry_delta' with overall mean -0.0001\n",
      "Imputed 0 NaN(s) in column 'wrist_asymmetry_lag1' with overall mean 0.0300\n",
      "Imputed 0 NaN(s) in column 'wrist_asymmetry_delta' with overall mean -0.0001\n",
      "Imputed 0 NaN(s) in column 'knee_asymmetry_lag1' with overall mean 0.0032\n",
      "Imputed 0 NaN(s) in column 'knee_asymmetry_delta' with overall mean -0.0000\n",
      "Imputed 0 NaN(s) in column 'hip_asymmetry_lag1' with overall mean 0.0052\n",
      "Imputed 0 NaN(s) in column 'hip_asymmetry_delta' with overall mean 0.0000\n",
      "Imputed 0 NaN(s) in column 'L_ELBOW_angle_lag1' with overall mean 82.9675\n",
      "Imputed 0 NaN(s) in column 'L_ELBOW_angle_delta' with overall mean -0.0178\n",
      "Imputed 0 NaN(s) in column 'R_ELBOW_angle_lag1' with overall mean 77.5659\n",
      "Imputed 0 NaN(s) in column 'R_ELBOW_angle_delta' with overall mean -0.0413\n",
      "Imputed 0 NaN(s) in column 'L_WRIST_angle_lag1' with overall mean 21.7915\n",
      "Imputed 0 NaN(s) in column 'L_WRIST_angle_delta' with overall mean 0.0043\n",
      "Imputed 0 NaN(s) in column 'R_WRIST_angle_lag1' with overall mean 30.3174\n",
      "Imputed 0 NaN(s) in column 'R_WRIST_angle_delta' with overall mean 0.0139\n",
      "Imputed 0 NaN(s) in column 'L_KNEE_angle_lag1' with overall mean 139.7027\n",
      "Imputed 0 NaN(s) in column 'L_KNEE_angle_delta' with overall mean 0.0225\n",
      "Imputed 0 NaN(s) in column 'R_KNEE_angle_lag1' with overall mean 134.7460\n",
      "Imputed 0 NaN(s) in column 'R_KNEE_angle_delta' with overall mean 0.0256\n",
      "Imputed 0 NaN(s) in column 'L_SHOULDER_ROM_lag1' with overall mean 55.9528\n",
      "Imputed 0 NaN(s) in column 'L_SHOULDER_ROM_delta' with overall mean 0.0965\n",
      "Imputed 0 NaN(s) in column 'R_SHOULDER_ROM_lag1' with overall mean 80.3533\n",
      "Imputed 0 NaN(s) in column 'R_SHOULDER_ROM_delta' with overall mean 0.0507\n",
      "Imputed 0 NaN(s) in column 'L_WRIST_ROM_lag1' with overall mean 21.7208\n",
      "Imputed 0 NaN(s) in column 'L_WRIST_ROM_delta' with overall mean 0.0202\n",
      "Imputed 0 NaN(s) in column 'R_WRIST_ROM_lag1' with overall mean 32.8124\n",
      "Imputed 0 NaN(s) in column 'R_WRIST_ROM_delta' with overall mean 0.1024\n",
      "Imputed 0 NaN(s) in column 'L_KNEE_ROM_lag1' with overall mean 48.8062\n",
      "Imputed 0 NaN(s) in column 'L_KNEE_ROM_delta' with overall mean -0.0200\n",
      "Imputed 0 NaN(s) in column 'R_KNEE_ROM_lag1' with overall mean 50.7584\n",
      "Imputed 0 NaN(s) in column 'R_KNEE_ROM_delta' with overall mean -0.0044\n",
      "Imputed 0 NaN(s) in column 'L_HIP_ROM_lag1' with overall mean 32.8269\n",
      "Imputed 0 NaN(s) in column 'L_HIP_ROM_delta' with overall mean -0.0527\n",
      "Imputed 0 NaN(s) in column 'R_HIP_ROM_lag1' with overall mean 46.3797\n",
      "Imputed 0 NaN(s) in column 'R_HIP_ROM_delta' with overall mean -0.0531\n",
      "Imputed 0 NaN(s) in column 'exhaustion_rate_lag1' with overall mean 0.0005\n",
      "Imputed 0 NaN(s) in column 'exhaustion_rate_delta' with overall mean -0.0000\n",
      "Imputed 0 NaN(s) in column 'by_trial_exhaustion_score_lag1' with overall mean 0.8162\n",
      "Imputed 0 NaN(s) in column 'by_trial_exhaustion_score_delta' with overall mean 0.0003\n",
      "Imputed 0 NaN(s) in column 'injury_risk_lag1' with overall mean 0.5585\n",
      "Imputed 0 NaN(s) in column 'injury_risk_delta' with overall mean -0.0020\n",
      "Imputed 0 NaN(s) in column 'energy_acceleration_lag1' with overall mean -0.0052\n",
      "Imputed 0 NaN(s) in column 'energy_acceleration_delta' with overall mean -0.0000\n",
      "Imputed 0 NaN(s) in column 'power_avg_5_lag1' with overall mean 42.1922\n",
      "Imputed 0 NaN(s) in column 'power_avg_5_delta' with overall mean -0.0503\n",
      "Imputed 0 NaN(s) in column 'rolling_power_std_lag1' with overall mean 5.4747\n",
      "Imputed 0 NaN(s) in column 'rolling_power_std_delta' with overall mean 0.0175\n",
      "Imputed 0 NaN(s) in column 'rolling_hr_mean_lag1' with overall mean 63.9903\n",
      "Imputed 0 NaN(s) in column 'rolling_hr_mean_delta' with overall mean -0.0013\n",
      "Imputed 0 NaN(s) in column 'simulated_HR_lag1' with overall mean 64.0695\n",
      "Imputed 0 NaN(s) in column 'simulated_HR_delta' with overall mean -0.0007\n",
      "Imputed 0 NaN(s) in column 'player_height_in_meters_lag1' with overall mean 1.9100\n",
      "Imputed 0 NaN(s) in column 'player_height_in_meters_delta' with overall mean -0.0000\n",
      "Imputed 0 NaN(s) in column 'player_weight__in_kg_lag1' with overall mean 90.7000\n",
      "Imputed 0 NaN(s) in column 'player_weight__in_kg_delta' with overall mean 0.0000\n",
      "\n",
      "--- Final debug: summary at end of function ---\n",
      "Final summary shape: (500, 234)\n",
      "Final summary columns: ['trial_id', 'shooting_phases', 'joint_energy', 'L_ELBOW_energy', 'R_ELBOW_energy', 'L_WRIST_energy', 'R_WRIST_energy', 'L_KNEE_energy', 'R_KNEE_energy', 'L_HIP_energy', 'R_HIP_energy', 'joint_power', 'L_ELBOW_ongoing_power', 'R_ELBOW_ongoing_power', 'L_WRIST_ongoing_power', 'R_WRIST_ongoing_power', 'L_KNEE_ongoing_power', 'R_KNEE_ongoing_power', 'L_HIP_ongoing_power', 'R_HIP_ongoing_power', 'elbow_asymmetry', 'wrist_asymmetry', 'knee_asymmetry', 'hip_asymmetry', 'L_ELBOW_angle', 'R_ELBOW_angle', 'L_WRIST_angle', 'R_WRIST_angle', 'L_KNEE_angle', 'R_KNEE_angle', 'L_SHOULDER_ROM', 'R_SHOULDER_ROM', 'L_WRIST_ROM', 'R_WRIST_ROM', 'L_KNEE_ROM', 'R_KNEE_ROM', 'L_HIP_ROM', 'R_HIP_ROM', 'exhaustion_rate', 'by_trial_exhaustion_score', 'injury_risk', 'energy_acceleration', 'power_avg_5', 'rolling_power_std', 'rolling_hr_mean', 'simulated_HR', 'player_height_in_meters', 'player_weight__in_kg', 'joint_energy_std', 'L_ELBOW_energy_std', 'R_ELBOW_energy_std', 'L_WRIST_energy_std', 'R_WRIST_energy_std', 'L_KNEE_energy_std', 'R_KNEE_energy_std', 'L_HIP_energy_std', 'R_HIP_energy_std', 'joint_power_std', 'L_ELBOW_ongoing_power_std', 'R_ELBOW_ongoing_power_std', 'L_WRIST_ongoing_power_std', 'R_WRIST_ongoing_power_std', 'L_KNEE_ongoing_power_std', 'R_KNEE_ongoing_power_std', 'L_HIP_ongoing_power_std', 'R_HIP_ongoing_power_std', 'elbow_asymmetry_std', 'wrist_asymmetry_std', 'knee_asymmetry_std', 'hip_asymmetry_std', 'L_ELBOW_angle_std', 'R_ELBOW_angle_std', 'L_WRIST_angle_std', 'R_WRIST_angle_std', 'L_KNEE_angle_std', 'R_KNEE_angle_std', 'L_SHOULDER_ROM_std', 'R_SHOULDER_ROM_std', 'L_WRIST_ROM_std', 'R_WRIST_ROM_std', 'L_KNEE_ROM_std', 'R_KNEE_ROM_std', 'L_HIP_ROM_std', 'R_HIP_ROM_std', 'exhaustion_rate_std', 'by_trial_exhaustion_score_std', 'injury_risk_std', 'energy_acceleration_std', 'power_avg_5_std', 'rolling_power_std_std', 'rolling_hr_mean_std', 'simulated_HR_std', 'player_height_in_meters_std', 'player_weight__in_kg_std', 'frame_count', 'phase_duration', 'joint_energy_lag1', 'joint_energy_rolling_avg', 'joint_energy_delta', 'L_ELBOW_energy_lag1', 'L_ELBOW_energy_rolling_avg', 'L_ELBOW_energy_delta', 'R_ELBOW_energy_lag1', 'R_ELBOW_energy_rolling_avg', 'R_ELBOW_energy_delta', 'L_WRIST_energy_lag1', 'L_WRIST_energy_rolling_avg', 'L_WRIST_energy_delta', 'R_WRIST_energy_lag1', 'R_WRIST_energy_rolling_avg', 'R_WRIST_energy_delta', 'L_KNEE_energy_lag1', 'L_KNEE_energy_rolling_avg', 'L_KNEE_energy_delta', 'R_KNEE_energy_lag1', 'R_KNEE_energy_rolling_avg', 'R_KNEE_energy_delta', 'L_HIP_energy_lag1', 'L_HIP_energy_rolling_avg', 'L_HIP_energy_delta', 'R_HIP_energy_lag1', 'R_HIP_energy_rolling_avg', 'R_HIP_energy_delta', 'joint_power_lag1', 'joint_power_rolling_avg', 'joint_power_delta', 'L_ELBOW_ongoing_power_lag1', 'L_ELBOW_ongoing_power_rolling_avg', 'L_ELBOW_ongoing_power_delta', 'R_ELBOW_ongoing_power_lag1', 'R_ELBOW_ongoing_power_rolling_avg', 'R_ELBOW_ongoing_power_delta', 'L_WRIST_ongoing_power_lag1', 'L_WRIST_ongoing_power_rolling_avg', 'L_WRIST_ongoing_power_delta', 'R_WRIST_ongoing_power_lag1', 'R_WRIST_ongoing_power_rolling_avg', 'R_WRIST_ongoing_power_delta', 'L_KNEE_ongoing_power_lag1', 'L_KNEE_ongoing_power_rolling_avg', 'L_KNEE_ongoing_power_delta', 'R_KNEE_ongoing_power_lag1', 'R_KNEE_ongoing_power_rolling_avg', 'R_KNEE_ongoing_power_delta', 'L_HIP_ongoing_power_lag1', 'L_HIP_ongoing_power_rolling_avg', 'L_HIP_ongoing_power_delta', 'R_HIP_ongoing_power_lag1', 'R_HIP_ongoing_power_rolling_avg', 'R_HIP_ongoing_power_delta', 'elbow_asymmetry_lag1', 'elbow_asymmetry_rolling_avg', 'elbow_asymmetry_delta', 'wrist_asymmetry_lag1', 'wrist_asymmetry_rolling_avg', 'wrist_asymmetry_delta', 'knee_asymmetry_lag1', 'knee_asymmetry_rolling_avg', 'knee_asymmetry_delta', 'hip_asymmetry_lag1', 'hip_asymmetry_rolling_avg', 'hip_asymmetry_delta', 'L_ELBOW_angle_lag1', 'L_ELBOW_angle_rolling_avg', 'L_ELBOW_angle_delta', 'R_ELBOW_angle_lag1', 'R_ELBOW_angle_rolling_avg', 'R_ELBOW_angle_delta', 'L_WRIST_angle_lag1', 'L_WRIST_angle_rolling_avg', 'L_WRIST_angle_delta', 'R_WRIST_angle_lag1', 'R_WRIST_angle_rolling_avg', 'R_WRIST_angle_delta', 'L_KNEE_angle_lag1', 'L_KNEE_angle_rolling_avg', 'L_KNEE_angle_delta', 'R_KNEE_angle_lag1', 'R_KNEE_angle_rolling_avg', 'R_KNEE_angle_delta', 'L_SHOULDER_ROM_lag1', 'L_SHOULDER_ROM_rolling_avg', 'L_SHOULDER_ROM_delta', 'R_SHOULDER_ROM_lag1', 'R_SHOULDER_ROM_rolling_avg', 'R_SHOULDER_ROM_delta', 'L_WRIST_ROM_lag1', 'L_WRIST_ROM_rolling_avg', 'L_WRIST_ROM_delta', 'R_WRIST_ROM_lag1', 'R_WRIST_ROM_rolling_avg', 'R_WRIST_ROM_delta', 'L_KNEE_ROM_lag1', 'L_KNEE_ROM_rolling_avg', 'L_KNEE_ROM_delta', 'R_KNEE_ROM_lag1', 'R_KNEE_ROM_rolling_avg', 'R_KNEE_ROM_delta', 'L_HIP_ROM_lag1', 'L_HIP_ROM_rolling_avg', 'L_HIP_ROM_delta', 'R_HIP_ROM_lag1', 'R_HIP_ROM_rolling_avg', 'R_HIP_ROM_delta', 'exhaustion_rate_lag1', 'exhaustion_rate_rolling_avg', 'exhaustion_rate_delta', 'by_trial_exhaustion_score_lag1', 'by_trial_exhaustion_score_rolling_avg', 'by_trial_exhaustion_score_delta', 'injury_risk_lag1', 'injury_risk_rolling_avg', 'injury_risk_delta', 'energy_acceleration_lag1', 'energy_acceleration_rolling_avg', 'energy_acceleration_delta', 'power_avg_5_lag1', 'power_avg_5_rolling_avg', 'power_avg_5_delta', 'rolling_power_std_lag1', 'rolling_power_std_rolling_avg', 'rolling_power_std_delta', 'rolling_hr_mean_lag1', 'rolling_hr_mean_rolling_avg', 'rolling_hr_mean_delta', 'simulated_HR_lag1', 'simulated_HR_rolling_avg', 'simulated_HR_delta', 'player_height_in_meters_lag1', 'player_height_in_meters_rolling_avg', 'player_height_in_meters_delta', 'player_weight__in_kg_lag1', 'player_weight__in_kg_rolling_avg', 'player_weight__in_kg_delta']\n",
      "Sample final summary rows:\n",
      "   trial_id shooting_phases  joint_energy  L_ELBOW_energy  R_ELBOW_energy  \\\n",
      "0    T0001        arm_cock     11.323945        0.152013        0.167383   \n",
      "1    T0001     arm_release     11.154126        0.168149        0.185725   \n",
      "2    T0001        leg_cock     11.134453        0.125059        0.152089   \n",
      "3    T0001   wrist_release      5.497133        0.063278        0.082923   \n",
      "4    T0002        arm_cock     10.586764        0.147492        0.163037   \n",
      "5    T0002     arm_release     10.784496        0.152401        0.175041   \n",
      "6    T0002        leg_cock      9.859066        0.100461        0.117822   \n",
      "7    T0002   wrist_release      5.790502        0.079704        0.073063   \n",
      "8    T0003        arm_cock     11.193878        0.158764        0.167356   \n",
      "9    T0003     arm_release     10.886252        0.160316        0.181977   \n",
      "\n",
      "   L_WRIST_energy  R_WRIST_energy  L_KNEE_energy  R_KNEE_energy  L_HIP_energy  \\\n",
      "0        0.226753        0.200771       0.012083       0.020248      0.033196   \n",
      "1        0.183681        0.202609       0.055116       0.056183      0.074682   \n",
      "2        0.221900        0.188692       0.024842       0.031864      0.026719   \n",
      "3        0.069523        0.081641       0.034557       0.034332      0.034362   \n",
      "4        0.200102        0.188322       0.022405       0.020833      0.041737   \n",
      "5        0.170453        0.192235       0.057900       0.057580      0.083917   \n",
      "6        0.178312        0.152088       0.033677       0.030866      0.028170   \n",
      "7        0.083463        0.072294       0.037303       0.037142      0.032638   \n",
      "8        0.216134        0.201529       0.022204       0.029086      0.043612   \n",
      "9        0.174939        0.195750       0.052735       0.053100      0.081888   \n",
      "\n",
      "   ...  rolling_hr_mean_delta  simulated_HR_lag1  simulated_HR_rolling_avg  \\\n",
      "0  ...              -0.001340          64.069455                 64.499886   \n",
      "1  ...              -0.001340          64.069455                 64.578556   \n",
      "2  ...              -0.001340          64.069455                 64.388658   \n",
      "3  ...              -0.001340          64.069455                 63.088487   \n",
      "4  ...              -0.104546          64.499886                 64.388685   \n",
      "5  ...              -0.160495          64.578556                 64.514100   \n",
      "6  ...              -0.827547          64.388658                 64.171109   \n",
      "7  ...              -0.074332          63.088487                 63.120582   \n",
      "8  ...               0.181945          64.277484                 64.418908   \n",
      "9  ...               0.127080          64.449644                 64.510310   \n",
      "\n",
      "   simulated_HR_delta  player_height_in_meters_lag1  \\\n",
      "0           -0.000733                          1.91   \n",
      "1           -0.000733                          1.91   \n",
      "2           -0.000733                          1.91   \n",
      "3           -0.000733                          1.91   \n",
      "4           -0.222402                          1.91   \n",
      "5           -0.128912                          1.91   \n",
      "6           -0.435097                          1.91   \n",
      "7            0.064188                          1.91   \n",
      "8            0.201870                          1.91   \n",
      "9            0.053085                          1.91   \n",
      "\n",
      "   player_height_in_meters_rolling_avg  player_height_in_meters_delta  \\\n",
      "0                                 1.91                  -4.476706e-19   \n",
      "1                                 1.91                  -4.476706e-19   \n",
      "2                                 1.91                  -4.476706e-19   \n",
      "3                                 1.91                  -4.476706e-19   \n",
      "4                                 1.91                   0.000000e+00   \n",
      "5                                 1.91                   0.000000e+00   \n",
      "6                                 1.91                   0.000000e+00   \n",
      "7                                 1.91                   0.000000e+00   \n",
      "8                                 1.91                   0.000000e+00   \n",
      "9                                 1.91                   0.000000e+00   \n",
      "\n",
      "   player_weight__in_kg_lag1  player_weight__in_kg_rolling_avg  \\\n",
      "0                       90.7                              90.7   \n",
      "1                       90.7                              90.7   \n",
      "2                       90.7                              90.7   \n",
      "3                       90.7                              90.7   \n",
      "4                       90.7                              90.7   \n",
      "5                       90.7                              90.7   \n",
      "6                       90.7                              90.7   \n",
      "7                       90.7                              90.7   \n",
      "8                       90.7                              90.7   \n",
      "9                       90.7                              90.7   \n",
      "\n",
      "   player_weight__in_kg_delta  \n",
      "0                0.000000e+00  \n",
      "1                0.000000e+00  \n",
      "2                0.000000e+00  \n",
      "3                0.000000e+00  \n",
      "4                0.000000e+00  \n",
      "5                0.000000e+00  \n",
      "6                0.000000e+00  \n",
      "7                1.421085e-14  \n",
      "8                0.000000e+00  \n",
      "9                0.000000e+00  \n",
      "\n",
      "[10 rows x 234 columns]\n",
      "--- DEBUG: summarize_data END ---\n",
      "\n",
      "\n",
      "Null summary for Final Data: Total Rows = 2957\n",
      "\n",
      "After dropping rows with nulls in columns: ['energy_acceleration', 'exhaustion_rate']\n",
      "Total Rows = 2956\n",
      "trial_id: 0 nulls, 0.00% null\n",
      "result: 0 nulls, 0.00% null\n",
      "landing_x: 0 nulls, 0.00% null\n",
      "landing_y: 0 nulls, 0.00% null\n",
      "entry_angle: 0 nulls, 0.00% null\n",
      "frame_time: 0 nulls, 0.00% null\n",
      "ball_x: 0 nulls, 0.00% null\n",
      "ball_y: 0 nulls, 0.00% null\n",
      "ball_z: 0 nulls, 0.00% null\n",
      "R_EYE_x: 0 nulls, 0.00% null\n",
      "R_EYE_y: 0 nulls, 0.00% null\n",
      "R_EYE_z: 0 nulls, 0.00% null\n",
      "L_EYE_x: 0 nulls, 0.00% null\n",
      "L_EYE_y: 0 nulls, 0.00% null\n",
      "L_EYE_z: 0 nulls, 0.00% null\n",
      "NOSE_x: 0 nulls, 0.00% null\n",
      "NOSE_y: 0 nulls, 0.00% null\n",
      "NOSE_z: 0 nulls, 0.00% null\n",
      "R_EAR_x: 0 nulls, 0.00% null\n",
      "R_EAR_y: 0 nulls, 0.00% null\n",
      "R_EAR_z: 0 nulls, 0.00% null\n",
      "L_EAR_x: 0 nulls, 0.00% null\n",
      "L_EAR_y: 0 nulls, 0.00% null\n",
      "L_EAR_z: 0 nulls, 0.00% null\n",
      "R_SHOULDER_x: 0 nulls, 0.00% null\n",
      "R_SHOULDER_y: 0 nulls, 0.00% null\n",
      "R_SHOULDER_z: 0 nulls, 0.00% null\n",
      "L_SHOULDER_x: 0 nulls, 0.00% null\n",
      "L_SHOULDER_y: 0 nulls, 0.00% null\n",
      "L_SHOULDER_z: 0 nulls, 0.00% null\n",
      "R_ELBOW_x: 0 nulls, 0.00% null\n",
      "R_ELBOW_y: 0 nulls, 0.00% null\n",
      "R_ELBOW_z: 0 nulls, 0.00% null\n",
      "L_ELBOW_x: 0 nulls, 0.00% null\n",
      "L_ELBOW_y: 0 nulls, 0.00% null\n",
      "L_ELBOW_z: 0 nulls, 0.00% null\n",
      "R_WRIST_x: 0 nulls, 0.00% null\n",
      "R_WRIST_y: 0 nulls, 0.00% null\n",
      "R_WRIST_z: 0 nulls, 0.00% null\n",
      "L_WRIST_x: 0 nulls, 0.00% null\n",
      "L_WRIST_y: 0 nulls, 0.00% null\n",
      "L_WRIST_z: 0 nulls, 0.00% null\n",
      "R_HIP_x: 0 nulls, 0.00% null\n",
      "R_HIP_y: 0 nulls, 0.00% null\n",
      "R_HIP_z: 0 nulls, 0.00% null\n",
      "L_HIP_x: 0 nulls, 0.00% null\n",
      "L_HIP_y: 0 nulls, 0.00% null\n",
      "L_HIP_z: 0 nulls, 0.00% null\n",
      "R_KNEE_x: 0 nulls, 0.00% null\n",
      "R_KNEE_y: 0 nulls, 0.00% null\n",
      "R_KNEE_z: 0 nulls, 0.00% null\n",
      "L_KNEE_x: 0 nulls, 0.00% null\n",
      "L_KNEE_y: 0 nulls, 0.00% null\n",
      "L_KNEE_z: 0 nulls, 0.00% null\n",
      "R_ANKLE_x: 0 nulls, 0.00% null\n",
      "R_ANKLE_y: 0 nulls, 0.00% null\n",
      "R_ANKLE_z: 0 nulls, 0.00% null\n",
      "L_ANKLE_x: 0 nulls, 0.00% null\n",
      "L_ANKLE_y: 0 nulls, 0.00% null\n",
      "L_ANKLE_z: 0 nulls, 0.00% null\n",
      "R_1STFINGER_x: 0 nulls, 0.00% null\n",
      "R_1STFINGER_y: 0 nulls, 0.00% null\n",
      "R_1STFINGER_z: 0 nulls, 0.00% null\n",
      "R_5THFINGER_x: 0 nulls, 0.00% null\n",
      "R_5THFINGER_y: 0 nulls, 0.00% null\n",
      "R_5THFINGER_z: 0 nulls, 0.00% null\n",
      "L_1STFINGER_x: 0 nulls, 0.00% null\n",
      "L_1STFINGER_y: 0 nulls, 0.00% null\n",
      "L_1STFINGER_z: 0 nulls, 0.00% null\n",
      "L_5THFINGER_x: 0 nulls, 0.00% null\n",
      "L_5THFINGER_y: 0 nulls, 0.00% null\n",
      "L_5THFINGER_z: 0 nulls, 0.00% null\n",
      "R_1STTOE_x: 0 nulls, 0.00% null\n",
      "R_1STTOE_y: 0 nulls, 0.00% null\n",
      "R_1STTOE_z: 0 nulls, 0.00% null\n",
      "R_5THTOE_x: 0 nulls, 0.00% null\n",
      "R_5THTOE_y: 0 nulls, 0.00% null\n",
      "R_5THTOE_z: 0 nulls, 0.00% null\n",
      "L_1STTOE_x: 0 nulls, 0.00% null\n",
      "L_1STTOE_y: 0 nulls, 0.00% null\n",
      "L_1STTOE_z: 0 nulls, 0.00% null\n",
      "L_5THTOE_x: 0 nulls, 0.00% null\n",
      "L_5THTOE_y: 0 nulls, 0.00% null\n",
      "L_5THTOE_z: 0 nulls, 0.00% null\n",
      "R_CALC_x: 0 nulls, 0.00% null\n",
      "R_CALC_y: 0 nulls, 0.00% null\n",
      "R_CALC_z: 0 nulls, 0.00% null\n",
      "L_CALC_x: 0 nulls, 0.00% null\n",
      "L_CALC_y: 0 nulls, 0.00% null\n",
      "L_CALC_z: 0 nulls, 0.00% null\n",
      "ball_speed: 0 nulls, 0.00% null\n",
      "ball_velocity_x: 0 nulls, 0.00% null\n",
      "ball_velocity_y: 0 nulls, 0.00% null\n",
      "ball_velocity_z: 0 nulls, 0.00% null\n",
      "overall_ball_velocity: 0 nulls, 0.00% null\n",
      "ball_direction_x: 0 nulls, 0.00% null\n",
      "ball_direction_y: 0 nulls, 0.00% null\n",
      "ball_direction_z: 0 nulls, 0.00% null\n",
      "computed_ball_velocity_x: 0 nulls, 0.00% null\n",
      "computed_ball_velocity_y: 0 nulls, 0.00% null\n",
      "computed_ball_velocity_z: 0 nulls, 0.00% null\n",
      "dist_ball_R_1STFINGER: 0 nulls, 0.00% null\n",
      "dist_ball_R_5THFINGER: 0 nulls, 0.00% null\n",
      "dist_ball_L_1STFINGER: 0 nulls, 0.00% null\n",
      "dist_ball_L_5THFINGER: 0 nulls, 0.00% null\n",
      "ball_in_hands: 0 nulls, 0.00% null\n",
      "shooting_motion: 0 nulls, 0.00% null\n",
      "avg_shoulder_height: 0 nulls, 0.00% null\n",
      "release_point_filter: 0 nulls, 0.00% null\n",
      "dt: 0 nulls, 0.00% null\n",
      "dx: 0 nulls, 0.00% null\n",
      "dy: 0 nulls, 0.00% null\n",
      "dz: 0 nulls, 0.00% null\n",
      "L_ANKLE_ongoing_power: 0 nulls, 0.00% null\n",
      "R_ANKLE_ongoing_power: 0 nulls, 0.00% null\n",
      "L_KNEE_ongoing_power: 0 nulls, 0.00% null\n",
      "R_KNEE_ongoing_power: 0 nulls, 0.00% null\n",
      "L_HIP_ongoing_power: 0 nulls, 0.00% null\n",
      "R_HIP_ongoing_power: 0 nulls, 0.00% null\n",
      "L_ELBOW_ongoing_power: 0 nulls, 0.00% null\n",
      "R_ELBOW_ongoing_power: 0 nulls, 0.00% null\n",
      "L_WRIST_ongoing_power: 0 nulls, 0.00% null\n",
      "R_WRIST_ongoing_power: 0 nulls, 0.00% null\n",
      "L_1STFINGER_ongoing_power: 0 nulls, 0.00% null\n",
      "L_5THFINGER_ongoing_power: 0 nulls, 0.00% null\n",
      "R_1STFINGER_ongoing_power: 0 nulls, 0.00% null\n",
      "R_5THFINGER_ongoing_power: 0 nulls, 0.00% null\n",
      "L_ELBOW_angle: 0 nulls, 0.00% null\n",
      "L_WRIST_angle: 0 nulls, 0.00% null\n",
      "L_KNEE_angle: 0 nulls, 0.00% null\n",
      "L_ELBOW_ongoing_angle: 0 nulls, 0.00% null\n",
      "L_WRIST_ongoing_angle: 0 nulls, 0.00% null\n",
      "L_KNEE_ongoing_angle: 0 nulls, 0.00% null\n",
      "R_ELBOW_angle: 0 nulls, 0.00% null\n",
      "R_WRIST_angle: 0 nulls, 0.00% null\n",
      "R_KNEE_angle: 0 nulls, 0.00% null\n",
      "R_ELBOW_ongoing_angle: 0 nulls, 0.00% null\n",
      "R_WRIST_ongoing_angle: 0 nulls, 0.00% null\n",
      "R_KNEE_ongoing_angle: 0 nulls, 0.00% null\n",
      "shooting_phases: 0 nulls, 0.00% null\n",
      "player_height_in_meters: 0 nulls, 0.00% null\n",
      "player_height_ft: 0 nulls, 0.00% null\n",
      "initial_release_angle: 0 nulls, 0.00% null\n",
      "calculated_release_angle: 0 nulls, 0.00% null\n",
      "angle_difference: 0 nulls, 0.00% null\n",
      "distance_to_basket: 0 nulls, 0.00% null\n",
      "optimal_release_angle: 0 nulls, 0.00% null\n",
      "by_trial_time: 0 nulls, 0.00% null\n",
      "continuous_frame_time: 0 nulls, 0.00% null\n",
      "L_ANKLE_energy: 0 nulls, 0.00% null\n",
      "R_ANKLE_energy: 0 nulls, 0.00% null\n",
      "L_KNEE_energy: 0 nulls, 0.00% null\n",
      "R_KNEE_energy: 0 nulls, 0.00% null\n",
      "L_HIP_energy: 0 nulls, 0.00% null\n",
      "R_HIP_energy: 0 nulls, 0.00% null\n",
      "L_ELBOW_energy: 0 nulls, 0.00% null\n",
      "R_ELBOW_energy: 0 nulls, 0.00% null\n",
      "L_WRIST_energy: 0 nulls, 0.00% null\n",
      "R_WRIST_energy: 0 nulls, 0.00% null\n",
      "L_1STFINGER_energy: 0 nulls, 0.00% null\n",
      "R_1STFINGER_energy: 0 nulls, 0.00% null\n",
      "L_5THFINGER_energy: 0 nulls, 0.00% null\n",
      "R_5THFINGER_energy: 0 nulls, 0.00% null\n",
      "total_energy: 0 nulls, 0.00% null\n",
      "by_trial_energy: 0 nulls, 0.00% null\n",
      "by_trial_exhaustion_score: 0 nulls, 0.00% null\n",
      "overall_cumulative_energy: 0 nulls, 0.00% null\n",
      "overall_exhaustion_score: 0 nulls, 0.00% null\n",
      "L_ANKLE_energy_by_trial: 0 nulls, 0.00% null\n",
      "L_ANKLE_energy_by_trial_exhaustion_score: 0 nulls, 0.00% null\n",
      "L_ANKLE_energy_overall_cumulative: 0 nulls, 0.00% null\n",
      "L_ANKLE_energy_overall_exhaustion_score: 0 nulls, 0.00% null\n",
      "R_ANKLE_energy_by_trial: 0 nulls, 0.00% null\n",
      "R_ANKLE_energy_by_trial_exhaustion_score: 0 nulls, 0.00% null\n",
      "R_ANKLE_energy_overall_cumulative: 0 nulls, 0.00% null\n",
      "R_ANKLE_energy_overall_exhaustion_score: 0 nulls, 0.00% null\n",
      "L_KNEE_energy_by_trial: 0 nulls, 0.00% null\n",
      "L_KNEE_energy_by_trial_exhaustion_score: 0 nulls, 0.00% null\n",
      "L_KNEE_energy_overall_cumulative: 0 nulls, 0.00% null\n",
      "L_KNEE_energy_overall_exhaustion_score: 0 nulls, 0.00% null\n",
      "R_KNEE_energy_by_trial: 0 nulls, 0.00% null\n",
      "R_KNEE_energy_by_trial_exhaustion_score: 0 nulls, 0.00% null\n",
      "R_KNEE_energy_overall_cumulative: 0 nulls, 0.00% null\n",
      "R_KNEE_energy_overall_exhaustion_score: 0 nulls, 0.00% null\n",
      "L_HIP_energy_by_trial: 0 nulls, 0.00% null\n",
      "L_HIP_energy_by_trial_exhaustion_score: 0 nulls, 0.00% null\n",
      "L_HIP_energy_overall_cumulative: 0 nulls, 0.00% null\n",
      "L_HIP_energy_overall_exhaustion_score: 0 nulls, 0.00% null\n",
      "R_HIP_energy_by_trial: 0 nulls, 0.00% null\n",
      "R_HIP_energy_by_trial_exhaustion_score: 0 nulls, 0.00% null\n",
      "R_HIP_energy_overall_cumulative: 0 nulls, 0.00% null\n",
      "R_HIP_energy_overall_exhaustion_score: 0 nulls, 0.00% null\n",
      "L_ELBOW_energy_by_trial: 0 nulls, 0.00% null\n",
      "L_ELBOW_energy_by_trial_exhaustion_score: 0 nulls, 0.00% null\n",
      "L_ELBOW_energy_overall_cumulative: 0 nulls, 0.00% null\n",
      "L_ELBOW_energy_overall_exhaustion_score: 0 nulls, 0.00% null\n",
      "R_ELBOW_energy_by_trial: 0 nulls, 0.00% null\n",
      "R_ELBOW_energy_by_trial_exhaustion_score: 0 nulls, 0.00% null\n",
      "R_ELBOW_energy_overall_cumulative: 0 nulls, 0.00% null\n",
      "R_ELBOW_energy_overall_exhaustion_score: 0 nulls, 0.00% null\n",
      "L_WRIST_energy_by_trial: 0 nulls, 0.00% null\n",
      "L_WRIST_energy_by_trial_exhaustion_score: 0 nulls, 0.00% null\n",
      "L_WRIST_energy_overall_cumulative: 0 nulls, 0.00% null\n",
      "L_WRIST_energy_overall_exhaustion_score: 0 nulls, 0.00% null\n",
      "R_WRIST_energy_by_trial: 0 nulls, 0.00% null\n",
      "R_WRIST_energy_by_trial_exhaustion_score: 0 nulls, 0.00% null\n",
      "R_WRIST_energy_overall_cumulative: 0 nulls, 0.00% null\n",
      "R_WRIST_energy_overall_exhaustion_score: 0 nulls, 0.00% null\n",
      "L_1STFINGER_energy_by_trial: 0 nulls, 0.00% null\n",
      "L_1STFINGER_energy_by_trial_exhaustion_score: 0 nulls, 0.00% null\n",
      "L_1STFINGER_energy_overall_cumulative: 0 nulls, 0.00% null\n",
      "L_1STFINGER_energy_overall_exhaustion_score: 0 nulls, 0.00% null\n",
      "R_1STFINGER_energy_by_trial: 0 nulls, 0.00% null\n",
      "R_1STFINGER_energy_by_trial_exhaustion_score: 0 nulls, 0.00% null\n",
      "R_1STFINGER_energy_overall_cumulative: 0 nulls, 0.00% null\n",
      "R_1STFINGER_energy_overall_exhaustion_score: 0 nulls, 0.00% null\n",
      "L_5THFINGER_energy_by_trial: 0 nulls, 0.00% null\n",
      "L_5THFINGER_energy_by_trial_exhaustion_score: 0 nulls, 0.00% null\n",
      "L_5THFINGER_energy_overall_cumulative: 0 nulls, 0.00% null\n",
      "L_5THFINGER_energy_overall_exhaustion_score: 0 nulls, 0.00% null\n",
      "R_5THFINGER_energy_by_trial: 0 nulls, 0.00% null\n",
      "R_5THFINGER_energy_by_trial_exhaustion_score: 0 nulls, 0.00% null\n",
      "R_5THFINGER_energy_overall_cumulative: 0 nulls, 0.00% null\n",
      "R_5THFINGER_energy_overall_exhaustion_score: 0 nulls, 0.00% null\n",
      "participant_id: 0 nulls, 0.00% null\n",
      "L_SHOULDER_angle: 0 nulls, 0.00% null\n",
      "R_SHOULDER_angle: 0 nulls, 0.00% null\n",
      "L_HIP_angle: 0 nulls, 0.00% null\n",
      "R_HIP_angle: 0 nulls, 0.00% null\n",
      "L_ANKLE_angle: 0 nulls, 0.00% null\n",
      "R_ANKLE_angle: 0 nulls, 0.00% null\n",
      "datetime: 0 nulls, 0.00% null\n",
      "player_weight__in_kg: 0 nulls, 0.00% null\n",
      "joint_energy: 0 nulls, 0.00% null\n",
      "joint_power: 0 nulls, 0.00% null\n",
      "energy_acceleration: 0 nulls, 0.00% null\n",
      "ankle_power_ratio: 0 nulls, 0.00% null\n",
      "hip_asymmetry: 0 nulls, 0.00% null\n",
      "ankle_asymmetry: 0 nulls, 0.00% null\n",
      "wrist_asymmetry: 0 nulls, 0.00% null\n",
      "elbow_asymmetry: 0 nulls, 0.00% null\n",
      "knee_asymmetry: 0 nulls, 0.00% null\n",
      "1stfinger_asymmetry: 0 nulls, 0.00% null\n",
      "5thfinger_asymmetry: 0 nulls, 0.00% null\n",
      "hip_power_ratio: 0 nulls, 0.00% null\n",
      "wrist_power_ratio: 0 nulls, 0.00% null\n",
      "elbow_power_ratio: 0 nulls, 0.00% null\n",
      "knee_power_ratio: 0 nulls, 0.00% null\n",
      "1stfinger_power_ratio: 0 nulls, 0.00% null\n",
      "5thfinger_power_ratio: 0 nulls, 0.00% null\n",
      "L_KNEE_ROM: 0 nulls, 0.00% null\n",
      "L_KNEE_ROM_deviation: 0 nulls, 0.00% null\n",
      "L_KNEE_ROM_extreme: 0 nulls, 0.00% null\n",
      "R_KNEE_ROM: 0 nulls, 0.00% null\n",
      "R_KNEE_ROM_deviation: 0 nulls, 0.00% null\n",
      "R_KNEE_ROM_extreme: 0 nulls, 0.00% null\n",
      "L_SHOULDER_ROM: 0 nulls, 0.00% null\n",
      "L_SHOULDER_ROM_deviation: 0 nulls, 0.00% null\n",
      "L_SHOULDER_ROM_extreme: 0 nulls, 0.00% null\n",
      "R_SHOULDER_ROM: 0 nulls, 0.00% null\n",
      "R_SHOULDER_ROM_deviation: 0 nulls, 0.00% null\n",
      "R_SHOULDER_ROM_extreme: 0 nulls, 0.00% null\n",
      "L_HIP_ROM: 0 nulls, 0.00% null\n",
      "L_HIP_ROM_deviation: 0 nulls, 0.00% null\n",
      "L_HIP_ROM_extreme: 0 nulls, 0.00% null\n",
      "R_HIP_ROM: 0 nulls, 0.00% null\n",
      "R_HIP_ROM_deviation: 0 nulls, 0.00% null\n",
      "R_HIP_ROM_extreme: 0 nulls, 0.00% null\n",
      "L_ANKLE_ROM: 0 nulls, 0.00% null\n",
      "L_ANKLE_ROM_deviation: 0 nulls, 0.00% null\n",
      "L_ANKLE_ROM_extreme: 0 nulls, 0.00% null\n",
      "R_ANKLE_ROM: 0 nulls, 0.00% null\n",
      "R_ANKLE_ROM_deviation: 0 nulls, 0.00% null\n",
      "R_ANKLE_ROM_extreme: 0 nulls, 0.00% null\n",
      "L_WRIST_ROM: 0 nulls, 0.00% null\n",
      "L_WRIST_ROM_deviation: 0 nulls, 0.00% null\n",
      "L_WRIST_ROM_extreme: 0 nulls, 0.00% null\n",
      "R_WRIST_ROM: 0 nulls, 0.00% null\n",
      "R_WRIST_ROM_deviation: 0 nulls, 0.00% null\n",
      "R_WRIST_ROM_extreme: 0 nulls, 0.00% null\n",
      "exhaustion_rate: 0 nulls, 0.00% null\n",
      "simulated_HR: 0 nulls, 0.00% null\n",
      "time_since_start: 0 nulls, 0.00% null\n",
      "power_avg_5: 0 nulls, 0.00% null\n",
      "rolling_power_std: 0 nulls, 0.00% null\n",
      "rolling_hr_mean: 0 nulls, 0.00% null\n",
      "rolling_energy_std: 0 nulls, 0.00% null\n",
      "exhaustion_lag1: 0 nulls, 0.00% null\n",
      "ema_exhaustion: 0 nulls, 0.00% null\n",
      "rolling_exhaustion: 0 nulls, 0.00% null\n",
      "injury_risk: 0 nulls, 0.00% null\n",
      "L_ANKLE_exhaustion_rate: 0 nulls, 0.00% null\n",
      "L_ANKLE_rolling_exhaustion: 0 nulls, 0.00% null\n",
      "L_ANKLE_injury_risk: 0 nulls, 0.00% null\n",
      "R_ANKLE_exhaustion_rate: 0 nulls, 0.00% null\n",
      "R_ANKLE_rolling_exhaustion: 0 nulls, 0.00% null\n",
      "R_ANKLE_injury_risk: 0 nulls, 0.00% null\n",
      "L_WRIST_exhaustion_rate: 0 nulls, 0.00% null\n",
      "L_WRIST_rolling_exhaustion: 0 nulls, 0.00% null\n",
      "L_WRIST_injury_risk: 0 nulls, 0.00% null\n",
      "R_WRIST_exhaustion_rate: 0 nulls, 0.00% null\n",
      "R_WRIST_rolling_exhaustion: 0 nulls, 0.00% null\n",
      "R_WRIST_injury_risk: 0 nulls, 0.00% null\n",
      "L_ELBOW_exhaustion_rate: 0 nulls, 0.00% null\n",
      "L_ELBOW_rolling_exhaustion: 0 nulls, 0.00% null\n",
      "L_ELBOW_injury_risk: 0 nulls, 0.00% null\n",
      "R_ELBOW_exhaustion_rate: 0 nulls, 0.00% null\n",
      "R_ELBOW_rolling_exhaustion: 0 nulls, 0.00% null\n",
      "R_ELBOW_injury_risk: 0 nulls, 0.00% null\n",
      "L_KNEE_exhaustion_rate: 0 nulls, 0.00% null\n",
      "L_KNEE_rolling_exhaustion: 0 nulls, 0.00% null\n",
      "L_KNEE_injury_risk: 0 nulls, 0.00% null\n",
      "R_KNEE_exhaustion_rate: 0 nulls, 0.00% null\n",
      "R_KNEE_rolling_exhaustion: 0 nulls, 0.00% null\n",
      "R_KNEE_injury_risk: 0 nulls, 0.00% null\n",
      "L_HIP_exhaustion_rate: 0 nulls, 0.00% null\n",
      "L_HIP_rolling_exhaustion: 0 nulls, 0.00% null\n",
      "L_HIP_injury_risk: 0 nulls, 0.00% null\n",
      "R_HIP_exhaustion_rate: 0 nulls, 0.00% null\n",
      "R_HIP_rolling_exhaustion: 0 nulls, 0.00% null\n",
      "R_HIP_injury_risk: 0 nulls, 0.00% null\n",
      "Available target columns: ['by_trial_exhaustion_score', 'overall_exhaustion_score', 'L_ANKLE_energy_by_trial_exhaustion_score', 'L_ANKLE_energy_overall_exhaustion_score', 'R_ANKLE_energy_by_trial_exhaustion_score', 'R_ANKLE_energy_overall_exhaustion_score', 'L_KNEE_energy_by_trial_exhaustion_score', 'L_KNEE_energy_overall_exhaustion_score', 'R_KNEE_energy_by_trial_exhaustion_score', 'R_KNEE_energy_overall_exhaustion_score', 'L_HIP_energy_by_trial_exhaustion_score', 'L_HIP_energy_overall_exhaustion_score', 'R_HIP_energy_by_trial_exhaustion_score', 'R_HIP_energy_overall_exhaustion_score', 'L_ELBOW_energy_by_trial_exhaustion_score', 'L_ELBOW_energy_overall_exhaustion_score', 'R_ELBOW_energy_by_trial_exhaustion_score', 'R_ELBOW_energy_overall_exhaustion_score', 'L_WRIST_energy_by_trial_exhaustion_score', 'L_WRIST_energy_overall_exhaustion_score', 'R_WRIST_energy_by_trial_exhaustion_score', 'R_WRIST_energy_overall_exhaustion_score', 'L_1STFINGER_energy_by_trial_exhaustion_score', 'L_1STFINGER_energy_overall_exhaustion_score', 'R_1STFINGER_energy_by_trial_exhaustion_score', 'R_1STFINGER_energy_overall_exhaustion_score', 'L_5THFINGER_energy_by_trial_exhaustion_score', 'L_5THFINGER_energy_overall_exhaustion_score', 'R_5THFINGER_energy_by_trial_exhaustion_score', 'R_5THFINGER_energy_overall_exhaustion_score', 'exhaustion_rate', 'exhaustion_lag1', 'ema_exhaustion', 'rolling_exhaustion', 'L_ANKLE_exhaustion_rate', 'L_ANKLE_rolling_exhaustion', 'R_ANKLE_exhaustion_rate', 'R_ANKLE_rolling_exhaustion', 'L_WRIST_exhaustion_rate', 'L_WRIST_rolling_exhaustion', 'R_WRIST_exhaustion_rate', 'R_WRIST_rolling_exhaustion', 'L_ELBOW_exhaustion_rate', 'L_ELBOW_rolling_exhaustion', 'R_ELBOW_exhaustion_rate', 'R_ELBOW_rolling_exhaustion', 'L_KNEE_exhaustion_rate', 'L_KNEE_rolling_exhaustion', 'R_KNEE_exhaustion_rate', 'R_KNEE_rolling_exhaustion', 'L_HIP_exhaustion_rate', 'L_HIP_rolling_exhaustion', 'R_HIP_exhaustion_rate', 'R_HIP_rolling_exhaustion']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: [injury_risk] Only 1 features exceeded the thresholds; falling back to top 10 by Consensus_Rank.\n",
      "INFO: ✅ injury_risk: Saved features at ..\\..\\data\\Deep_Learning_Final\\feature_lists\\base\\injury_risk_model_feature_list.pkl. Top features: ['rolling_exhaustion', 'ema_exhaustion', 'knee_power_ratio', 'exhaustion_lag1', 'hip_asymmetry', 'power_avg_5', 'time_since_start', 'knee_asymmetry', 'L_SHOULDER_ROM', 'elbow_power_ratio']\n",
      "WARNING: [by_trial_exhaustion_score] Only 4 features exceeded the thresholds; falling back to top 10 by Consensus_Rank.\n",
      "INFO: ✅ by_trial_exhaustion_score: Saved features at ..\\..\\data\\Deep_Learning_Final\\feature_lists\\base\\by_trial_exhaustion_score_model_feature_list.pkl. Top features: ['exhaustion_lag1', 'joint_power', 'rolling_hr_mean', 'ema_exhaustion', 'simulated_HR', '5thfinger_power_ratio', 'wrist_power_ratio', '1stfinger_power_ratio', 'power_avg_5', 'hip_power_ratio']\n",
      "WARNING: [L_ANKLE_injury_risk] Only 4 features exceeded the thresholds; falling back to top 10 by Consensus_Rank.\n",
      "INFO: ✅ L_ANKLE_injury_risk: Saved features at ..\\..\\data\\Deep_Learning_Final\\feature_lists\\base\\L_ANKLE_injury_risk_model_feature_list.pkl. Top features: ['rolling_exhaustion', 'R_SHOULDER_ROM', 'L_HIP_ROM', 'L_ANKLE_ROM', 'time_since_start', 'wrist_power_ratio', 'L_ANKLE_ROM_deviation', '5thfinger_power_ratio', 'joint_power', 'R_HIP_ROM']\n",
      "WARNING: [R_ANKLE_injury_risk] Only 3 features exceeded the thresholds; falling back to top 10 by Consensus_Rank.\n",
      "INFO: ✅ R_ANKLE_injury_risk: Saved features at ..\\..\\data\\Deep_Learning_Final\\feature_lists\\base\\R_ANKLE_injury_risk_model_feature_list.pkl. Top features: ['rolling_exhaustion', 'exhaustion_lag1', 'L_SHOULDER_ROM', 'joint_energy', 'R_SHOULDER_ROM', 'L_KNEE_ROM_deviation', 'joint_power', 'L_HIP_ROM', 'L_KNEE_ROM', 'rolling_hr_mean']\n",
      "WARNING: [L_WRIST_injury_risk] Only 3 features exceeded the thresholds; falling back to top 10 by Consensus_Rank.\n",
      "INFO: ✅ L_WRIST_injury_risk: Saved features at ..\\..\\data\\Deep_Learning_Final\\feature_lists\\base\\L_WRIST_injury_risk_model_feature_list.pkl. Top features: ['rolling_exhaustion', 'exhaustion_lag1', 'ema_exhaustion', 'power_avg_5', 'L_ANKLE_ROM_deviation', 'rolling_hr_mean', 'L_ANKLE_ROM', 'joint_energy', 'energy_acceleration', 'R_SHOULDER_ROM']\n",
      "WARNING: [R_WRIST_injury_risk] Only 3 features exceeded the thresholds; falling back to top 10 by Consensus_Rank.\n",
      "INFO: ✅ R_WRIST_injury_risk: Saved features at ..\\..\\data\\Deep_Learning_Final\\feature_lists\\base\\R_WRIST_injury_risk_model_feature_list.pkl. Top features: ['rolling_exhaustion', 'wrist_power_ratio', '5thfinger_power_ratio', 'elbow_power_ratio', 'exhaustion_lag1', 'joint_power', 'R_HIP_ROM', '1stfinger_power_ratio', 'rolling_hr_mean', '5thfinger_asymmetry']\n",
      "WARNING: [L_ELBOW_injury_risk] Only 3 features exceeded the thresholds; falling back to top 10 by Consensus_Rank.\n",
      "INFO: ✅ L_ELBOW_injury_risk: Saved features at ..\\..\\data\\Deep_Learning_Final\\feature_lists\\base\\L_ELBOW_injury_risk_model_feature_list.pkl. Top features: ['rolling_exhaustion', 'exhaustion_lag1', 'ema_exhaustion', '5thfinger_asymmetry', 'rolling_hr_mean', 'energy_acceleration', 'elbow_power_ratio', 'rolling_energy_std', 'R_ANKLE_ROM', 'time_since_start']\n",
      "WARNING: [R_ELBOW_injury_risk] Only 3 features exceeded the thresholds; falling back to top 10 by Consensus_Rank.\n",
      "INFO: ✅ R_ELBOW_injury_risk: Saved features at ..\\..\\data\\Deep_Learning_Final\\feature_lists\\base\\R_ELBOW_injury_risk_model_feature_list.pkl. Top features: ['rolling_exhaustion', 'wrist_power_ratio', 'joint_power', 'elbow_power_ratio', '5thfinger_power_ratio', 'energy_acceleration', 'R_SHOULDER_ROM', 'joint_energy', 'exhaustion_lag1', 'knee_power_ratio']\n",
      "WARNING: [L_KNEE_injury_risk] Only 3 features exceeded the thresholds; falling back to top 10 by Consensus_Rank.\n",
      "INFO: ✅ L_KNEE_injury_risk: Saved features at ..\\..\\data\\Deep_Learning_Final\\feature_lists\\base\\L_KNEE_injury_risk_model_feature_list.pkl. Top features: ['rolling_exhaustion', 'L_ANKLE_ROM', 'R_HIP_ROM', 'ema_exhaustion', 'L_HIP_ROM', 'L_ANKLE_ROM_deviation', 'rolling_hr_mean', 'R_KNEE_ROM_deviation', 'time_since_start', 'L_KNEE_ROM_deviation']\n",
      "WARNING: [R_KNEE_injury_risk] Only 1 features exceeded the thresholds; falling back to top 10 by Consensus_Rank.\n",
      "INFO: ✅ R_KNEE_injury_risk: Saved features at ..\\..\\data\\Deep_Learning_Final\\feature_lists\\base\\R_KNEE_injury_risk_model_feature_list.pkl. Top features: ['rolling_exhaustion', 'exhaustion_lag1', 'R_HIP_ROM', 'knee_power_ratio', 'R_KNEE_ROM', 'rolling_power_std', '5thfinger_power_ratio', 'rolling_hr_mean', 'R_KNEE_ROM_deviation', 'wrist_power_ratio']\n",
      "WARNING: [L_HIP_injury_risk] Only 2 features exceeded the thresholds; falling back to top 10 by Consensus_Rank.\n",
      "INFO: ✅ L_HIP_injury_risk: Saved features at ..\\..\\data\\Deep_Learning_Final\\feature_lists\\base\\L_HIP_injury_risk_model_feature_list.pkl. Top features: ['rolling_exhaustion', 'wrist_power_ratio', 'ema_exhaustion', 'L_ANKLE_ROM', 'L_KNEE_ROM_deviation', 'exhaustion_lag1', 'hip_asymmetry', 'time_since_start', 'L_KNEE_ROM', 'ankle_asymmetry']\n",
      "WARNING: [R_HIP_injury_risk] Only 2 features exceeded the thresholds; falling back to top 10 by Consensus_Rank.\n",
      "INFO: ✅ R_HIP_injury_risk: Saved features at ..\\..\\data\\Deep_Learning_Final\\feature_lists\\base\\R_HIP_injury_risk_model_feature_list.pkl. Top features: ['rolling_exhaustion', 'exhaustion_lag1', 'ema_exhaustion', 'wrist_power_ratio', 'L_KNEE_ROM_deviation', 'R_SHOULDER_ROM', 'time_since_start', 'rolling_hr_mean', 'L_SHOULDER_ROM', 'hip_asymmetry']\n",
      "WARNING: [L_ANKLE_energy_by_trial_exhaustion_score] Only 4 features exceeded the thresholds; falling back to top 10 by Consensus_Rank.\n",
      "INFO: ✅ L_ANKLE_energy_by_trial_exhaustion_score: Saved features at ..\\..\\data\\Deep_Learning_Final\\feature_lists\\base\\L_ANKLE_energy_by_trial_exhaustion_score_model_feature_list.pkl. Top features: ['joint_power', 'ema_exhaustion', 'exhaustion_lag1', 'rolling_hr_mean', 'rolling_exhaustion', 'R_SHOULDER_ROM', 'simulated_HR', '1stfinger_asymmetry', '5thfinger_power_ratio', 'L_SHOULDER_ROM']\n",
      "WARNING: [R_ANKLE_energy_by_trial_exhaustion_score] Only 2 features exceeded the thresholds; falling back to top 10 by Consensus_Rank.\n",
      "INFO: ✅ R_ANKLE_energy_by_trial_exhaustion_score: Saved features at ..\\..\\data\\Deep_Learning_Final\\feature_lists\\base\\R_ANKLE_energy_by_trial_exhaustion_score_model_feature_list.pkl. Top features: ['ema_exhaustion', 'exhaustion_lag1', 'joint_power', 'rolling_exhaustion', 'rolling_hr_mean', 'R_KNEE_ROM_deviation', 'L_ANKLE_ROM_deviation', 'wrist_asymmetry', 'R_ANKLE_ROM_deviation', 'time_since_start']\n",
      "WARNING: [L_WRIST_energy_by_trial_exhaustion_score] Only 3 features exceeded the thresholds; falling back to top 10 by Consensus_Rank.\n",
      "INFO: ✅ L_WRIST_energy_by_trial_exhaustion_score: Saved features at ..\\..\\data\\Deep_Learning_Final\\feature_lists\\base\\L_WRIST_energy_by_trial_exhaustion_score_model_feature_list.pkl. Top features: ['exhaustion_lag1', 'ema_exhaustion', 'rolling_hr_mean', 'L_SHOULDER_ROM', '1stfinger_power_ratio', 'simulated_HR', 'energy_acceleration', 'rolling_exhaustion', 'wrist_power_ratio', 'L_HIP_ROM']\n",
      "WARNING: [R_WRIST_energy_by_trial_exhaustion_score] Only 3 features exceeded the thresholds; falling back to top 10 by Consensus_Rank.\n",
      "INFO: ✅ R_WRIST_energy_by_trial_exhaustion_score: Saved features at ..\\..\\data\\Deep_Learning_Final\\feature_lists\\base\\R_WRIST_energy_by_trial_exhaustion_score_model_feature_list.pkl. Top features: ['rolling_hr_mean', 'joint_power', 'exhaustion_lag1', 'ema_exhaustion', 'wrist_power_ratio', 'elbow_asymmetry', '5thfinger_power_ratio', '1stfinger_power_ratio', 'simulated_HR', 'energy_acceleration']\n",
      "WARNING: [L_ELBOW_energy_by_trial_exhaustion_score] Only 4 features exceeded the thresholds; falling back to top 10 by Consensus_Rank.\n",
      "INFO: ✅ L_ELBOW_energy_by_trial_exhaustion_score: Saved features at ..\\..\\data\\Deep_Learning_Final\\feature_lists\\base\\L_ELBOW_energy_by_trial_exhaustion_score_model_feature_list.pkl. Top features: ['exhaustion_lag1', 'ema_exhaustion', 'joint_power', 'rolling_hr_mean', 'simulated_HR', '5thfinger_power_ratio', 'rolling_exhaustion', 'L_SHOULDER_ROM', 'wrist_power_ratio', '1stfinger_power_ratio']\n",
      "WARNING: [R_ELBOW_energy_by_trial_exhaustion_score] Only 3 features exceeded the thresholds; falling back to top 10 by Consensus_Rank.\n",
      "INFO: ✅ R_ELBOW_energy_by_trial_exhaustion_score: Saved features at ..\\..\\data\\Deep_Learning_Final\\feature_lists\\base\\R_ELBOW_energy_by_trial_exhaustion_score_model_feature_list.pkl. Top features: ['rolling_hr_mean', 'joint_power', 'exhaustion_lag1', 'ema_exhaustion', '5thfinger_power_ratio', 'elbow_power_ratio', '1stfinger_power_ratio', 'energy_acceleration', 'simulated_HR', 'wrist_power_ratio']\n",
      "WARNING: [L_KNEE_energy_by_trial_exhaustion_score] Only 4 features exceeded the thresholds; falling back to top 10 by Consensus_Rank.\n",
      "INFO: ✅ L_KNEE_energy_by_trial_exhaustion_score: Saved features at ..\\..\\data\\Deep_Learning_Final\\feature_lists\\base\\L_KNEE_energy_by_trial_exhaustion_score_model_feature_list.pkl. Top features: ['ema_exhaustion', 'joint_power', 'rolling_hr_mean', 'exhaustion_lag1', 'simulated_HR', 'energy_acceleration', 'L_SHOULDER_ROM', '5thfinger_power_ratio', 'elbow_power_ratio', 'time_since_start']\n",
      "WARNING: [R_KNEE_energy_by_trial_exhaustion_score] Only 3 features exceeded the thresholds; falling back to top 10 by Consensus_Rank.\n",
      "INFO: ✅ R_KNEE_energy_by_trial_exhaustion_score: Saved features at ..\\..\\data\\Deep_Learning_Final\\feature_lists\\base\\R_KNEE_energy_by_trial_exhaustion_score_model_feature_list.pkl. Top features: ['ema_exhaustion', 'rolling_hr_mean', 'exhaustion_lag1', 'simulated_HR', 'energy_acceleration', 'wrist_power_ratio', 'L_SHOULDER_ROM', 'elbow_power_ratio', 'power_avg_5', 'L_KNEE_ROM_deviation']\n",
      "WARNING: [L_HIP_energy_by_trial_exhaustion_score] Only 3 features exceeded the thresholds; falling back to top 10 by Consensus_Rank.\n",
      "INFO: ✅ L_HIP_energy_by_trial_exhaustion_score: Saved features at ..\\..\\data\\Deep_Learning_Final\\feature_lists\\base\\L_HIP_energy_by_trial_exhaustion_score_model_feature_list.pkl. Top features: ['exhaustion_lag1', 'ema_exhaustion', 'rolling_hr_mean', 'simulated_HR', 'energy_acceleration', 'R_SHOULDER_ROM', 'elbow_power_ratio', '1stfinger_power_ratio', 'L_SHOULDER_ROM', 'time_since_start']\n",
      "WARNING: [R_HIP_energy_by_trial_exhaustion_score] Only 3 features exceeded the thresholds; falling back to top 10 by Consensus_Rank.\n",
      "INFO: ✅ R_HIP_energy_by_trial_exhaustion_score: Saved features at ..\\..\\data\\Deep_Learning_Final\\feature_lists\\base\\R_HIP_energy_by_trial_exhaustion_score_model_feature_list.pkl. Top features: ['exhaustion_lag1', 'ema_exhaustion', 'joint_power', 'rolling_hr_mean', 'simulated_HR', 'energy_acceleration', '1stfinger_power_ratio', '5thfinger_power_ratio', 'hip_asymmetry', 'wrist_power_ratio']\n",
      "INFO: Loaded feature list for 'injury_risk': ['rolling_exhaustion', 'ema_exhaustion', 'knee_power_ratio', 'exhaustion_lag1', 'hip_asymmetry', 'power_avg_5', 'time_since_start', 'knee_asymmetry', 'L_SHOULDER_ROM', 'elbow_power_ratio']\n",
      "INFO: [Base Data] Loaded top features for target 'injury_risk': ['rolling_exhaustion', 'ema_exhaustion', 'knee_power_ratio', 'exhaustion_lag1', 'hip_asymmetry', 'power_avg_5', 'time_since_start', 'knee_asymmetry', 'L_SHOULDER_ROM', 'elbow_power_ratio']\n",
      "INFO: Loaded feature list for 'by_trial_exhaustion_score': ['exhaustion_lag1', 'joint_power', 'rolling_hr_mean', 'ema_exhaustion', 'simulated_HR', '5thfinger_power_ratio', 'wrist_power_ratio', '1stfinger_power_ratio', 'power_avg_5', 'hip_power_ratio']\n",
      "INFO: [Base Data] Loaded top features for target 'by_trial_exhaustion_score': ['exhaustion_lag1', 'joint_power', 'rolling_hr_mean', 'ema_exhaustion', 'simulated_HR', '5thfinger_power_ratio', 'wrist_power_ratio', '1stfinger_power_ratio', 'power_avg_5', 'hip_power_ratio']\n",
      "INFO: Loaded feature list for 'L_ANKLE_injury_risk': ['rolling_exhaustion', 'R_SHOULDER_ROM', 'L_HIP_ROM', 'L_ANKLE_ROM', 'time_since_start', 'wrist_power_ratio', 'L_ANKLE_ROM_deviation', '5thfinger_power_ratio', 'joint_power', 'R_HIP_ROM']\n",
      "INFO: [Base Data] Loaded top features for target 'L_ANKLE_injury_risk': ['rolling_exhaustion', 'R_SHOULDER_ROM', 'L_HIP_ROM', 'L_ANKLE_ROM', 'time_since_start', 'wrist_power_ratio', 'L_ANKLE_ROM_deviation', '5thfinger_power_ratio', 'joint_power', 'R_HIP_ROM']\n",
      "INFO: Loaded feature list for 'R_ANKLE_injury_risk': ['rolling_exhaustion', 'exhaustion_lag1', 'L_SHOULDER_ROM', 'joint_energy', 'R_SHOULDER_ROM', 'L_KNEE_ROM_deviation', 'joint_power', 'L_HIP_ROM', 'L_KNEE_ROM', 'rolling_hr_mean']\n",
      "INFO: [Base Data] Loaded top features for target 'R_ANKLE_injury_risk': ['rolling_exhaustion', 'exhaustion_lag1', 'L_SHOULDER_ROM', 'joint_energy', 'R_SHOULDER_ROM', 'L_KNEE_ROM_deviation', 'joint_power', 'L_HIP_ROM', 'L_KNEE_ROM', 'rolling_hr_mean']\n",
      "INFO: Loaded feature list for 'L_WRIST_injury_risk': ['rolling_exhaustion', 'exhaustion_lag1', 'ema_exhaustion', 'power_avg_5', 'L_ANKLE_ROM_deviation', 'rolling_hr_mean', 'L_ANKLE_ROM', 'joint_energy', 'energy_acceleration', 'R_SHOULDER_ROM']\n",
      "INFO: [Base Data] Loaded top features for target 'L_WRIST_injury_risk': ['rolling_exhaustion', 'exhaustion_lag1', 'ema_exhaustion', 'power_avg_5', 'L_ANKLE_ROM_deviation', 'rolling_hr_mean', 'L_ANKLE_ROM', 'joint_energy', 'energy_acceleration', 'R_SHOULDER_ROM']\n",
      "INFO: Loaded feature list for 'R_WRIST_injury_risk': ['rolling_exhaustion', 'wrist_power_ratio', '5thfinger_power_ratio', 'elbow_power_ratio', 'exhaustion_lag1', 'joint_power', 'R_HIP_ROM', '1stfinger_power_ratio', 'rolling_hr_mean', '5thfinger_asymmetry']\n",
      "INFO: [Base Data] Loaded top features for target 'R_WRIST_injury_risk': ['rolling_exhaustion', 'wrist_power_ratio', '5thfinger_power_ratio', 'elbow_power_ratio', 'exhaustion_lag1', 'joint_power', 'R_HIP_ROM', '1stfinger_power_ratio', 'rolling_hr_mean', '5thfinger_asymmetry']\n",
      "INFO: Loaded feature list for 'L_ELBOW_injury_risk': ['rolling_exhaustion', 'exhaustion_lag1', 'ema_exhaustion', '5thfinger_asymmetry', 'rolling_hr_mean', 'energy_acceleration', 'elbow_power_ratio', 'rolling_energy_std', 'R_ANKLE_ROM', 'time_since_start']\n",
      "INFO: [Base Data] Loaded top features for target 'L_ELBOW_injury_risk': ['rolling_exhaustion', 'exhaustion_lag1', 'ema_exhaustion', '5thfinger_asymmetry', 'rolling_hr_mean', 'energy_acceleration', 'elbow_power_ratio', 'rolling_energy_std', 'R_ANKLE_ROM', 'time_since_start']\n",
      "INFO: Loaded feature list for 'R_ELBOW_injury_risk': ['rolling_exhaustion', 'wrist_power_ratio', 'joint_power', 'elbow_power_ratio', '5thfinger_power_ratio', 'energy_acceleration', 'R_SHOULDER_ROM', 'joint_energy', 'exhaustion_lag1', 'knee_power_ratio']\n",
      "INFO: [Base Data] Loaded top features for target 'R_ELBOW_injury_risk': ['rolling_exhaustion', 'wrist_power_ratio', 'joint_power', 'elbow_power_ratio', '5thfinger_power_ratio', 'energy_acceleration', 'R_SHOULDER_ROM', 'joint_energy', 'exhaustion_lag1', 'knee_power_ratio']\n",
      "INFO: Loaded feature list for 'L_KNEE_injury_risk': ['rolling_exhaustion', 'L_ANKLE_ROM', 'R_HIP_ROM', 'ema_exhaustion', 'L_HIP_ROM', 'L_ANKLE_ROM_deviation', 'rolling_hr_mean', 'R_KNEE_ROM_deviation', 'time_since_start', 'L_KNEE_ROM_deviation']\n",
      "INFO: [Base Data] Loaded top features for target 'L_KNEE_injury_risk': ['rolling_exhaustion', 'L_ANKLE_ROM', 'R_HIP_ROM', 'ema_exhaustion', 'L_HIP_ROM', 'L_ANKLE_ROM_deviation', 'rolling_hr_mean', 'R_KNEE_ROM_deviation', 'time_since_start', 'L_KNEE_ROM_deviation']\n",
      "INFO: Loaded feature list for 'R_KNEE_injury_risk': ['rolling_exhaustion', 'exhaustion_lag1', 'R_HIP_ROM', 'knee_power_ratio', 'R_KNEE_ROM', 'rolling_power_std', '5thfinger_power_ratio', 'rolling_hr_mean', 'R_KNEE_ROM_deviation', 'wrist_power_ratio']\n",
      "INFO: [Base Data] Loaded top features for target 'R_KNEE_injury_risk': ['rolling_exhaustion', 'exhaustion_lag1', 'R_HIP_ROM', 'knee_power_ratio', 'R_KNEE_ROM', 'rolling_power_std', '5thfinger_power_ratio', 'rolling_hr_mean', 'R_KNEE_ROM_deviation', 'wrist_power_ratio']\n",
      "INFO: Loaded feature list for 'L_HIP_injury_risk': ['rolling_exhaustion', 'wrist_power_ratio', 'ema_exhaustion', 'L_ANKLE_ROM', 'L_KNEE_ROM_deviation', 'exhaustion_lag1', 'hip_asymmetry', 'time_since_start', 'L_KNEE_ROM', 'ankle_asymmetry']\n",
      "INFO: [Base Data] Loaded top features for target 'L_HIP_injury_risk': ['rolling_exhaustion', 'wrist_power_ratio', 'ema_exhaustion', 'L_ANKLE_ROM', 'L_KNEE_ROM_deviation', 'exhaustion_lag1', 'hip_asymmetry', 'time_since_start', 'L_KNEE_ROM', 'ankle_asymmetry']\n",
      "INFO: Loaded feature list for 'R_HIP_injury_risk': ['rolling_exhaustion', 'exhaustion_lag1', 'ema_exhaustion', 'wrist_power_ratio', 'L_KNEE_ROM_deviation', 'R_SHOULDER_ROM', 'time_since_start', 'rolling_hr_mean', 'L_SHOULDER_ROM', 'hip_asymmetry']\n",
      "INFO: [Base Data] Loaded top features for target 'R_HIP_injury_risk': ['rolling_exhaustion', 'exhaustion_lag1', 'ema_exhaustion', 'wrist_power_ratio', 'L_KNEE_ROM_deviation', 'R_SHOULDER_ROM', 'time_since_start', 'rolling_hr_mean', 'L_SHOULDER_ROM', 'hip_asymmetry']\n",
      "INFO: Loaded feature list for 'L_ANKLE_energy_by_trial_exhaustion_score': ['joint_power', 'ema_exhaustion', 'exhaustion_lag1', 'rolling_hr_mean', 'rolling_exhaustion', 'R_SHOULDER_ROM', 'simulated_HR', '1stfinger_asymmetry', '5thfinger_power_ratio', 'L_SHOULDER_ROM']\n",
      "INFO: [Base Data] Loaded top features for target 'L_ANKLE_energy_by_trial_exhaustion_score': ['joint_power', 'ema_exhaustion', 'exhaustion_lag1', 'rolling_hr_mean', 'rolling_exhaustion', 'R_SHOULDER_ROM', 'simulated_HR', '1stfinger_asymmetry', '5thfinger_power_ratio', 'L_SHOULDER_ROM']\n",
      "INFO: Loaded feature list for 'R_ANKLE_energy_by_trial_exhaustion_score': ['ema_exhaustion', 'exhaustion_lag1', 'joint_power', 'rolling_exhaustion', 'rolling_hr_mean', 'R_KNEE_ROM_deviation', 'L_ANKLE_ROM_deviation', 'wrist_asymmetry', 'R_ANKLE_ROM_deviation', 'time_since_start']\n",
      "INFO: [Base Data] Loaded top features for target 'R_ANKLE_energy_by_trial_exhaustion_score': ['ema_exhaustion', 'exhaustion_lag1', 'joint_power', 'rolling_exhaustion', 'rolling_hr_mean', 'R_KNEE_ROM_deviation', 'L_ANKLE_ROM_deviation', 'wrist_asymmetry', 'R_ANKLE_ROM_deviation', 'time_since_start']\n",
      "INFO: Loaded feature list for 'L_WRIST_energy_by_trial_exhaustion_score': ['exhaustion_lag1', 'ema_exhaustion', 'rolling_hr_mean', 'L_SHOULDER_ROM', '1stfinger_power_ratio', 'simulated_HR', 'energy_acceleration', 'rolling_exhaustion', 'wrist_power_ratio', 'L_HIP_ROM']\n",
      "INFO: [Base Data] Loaded top features for target 'L_WRIST_energy_by_trial_exhaustion_score': ['exhaustion_lag1', 'ema_exhaustion', 'rolling_hr_mean', 'L_SHOULDER_ROM', '1stfinger_power_ratio', 'simulated_HR', 'energy_acceleration', 'rolling_exhaustion', 'wrist_power_ratio', 'L_HIP_ROM']\n",
      "INFO: Loaded feature list for 'R_WRIST_energy_by_trial_exhaustion_score': ['rolling_hr_mean', 'joint_power', 'exhaustion_lag1', 'ema_exhaustion', 'wrist_power_ratio', 'elbow_asymmetry', '5thfinger_power_ratio', '1stfinger_power_ratio', 'simulated_HR', 'energy_acceleration']\n",
      "INFO: [Base Data] Loaded top features for target 'R_WRIST_energy_by_trial_exhaustion_score': ['rolling_hr_mean', 'joint_power', 'exhaustion_lag1', 'ema_exhaustion', 'wrist_power_ratio', 'elbow_asymmetry', '5thfinger_power_ratio', '1stfinger_power_ratio', 'simulated_HR', 'energy_acceleration']\n",
      "INFO: Loaded feature list for 'L_ELBOW_energy_by_trial_exhaustion_score': ['exhaustion_lag1', 'ema_exhaustion', 'joint_power', 'rolling_hr_mean', 'simulated_HR', '5thfinger_power_ratio', 'rolling_exhaustion', 'L_SHOULDER_ROM', 'wrist_power_ratio', '1stfinger_power_ratio']\n",
      "INFO: [Base Data] Loaded top features for target 'L_ELBOW_energy_by_trial_exhaustion_score': ['exhaustion_lag1', 'ema_exhaustion', 'joint_power', 'rolling_hr_mean', 'simulated_HR', '5thfinger_power_ratio', 'rolling_exhaustion', 'L_SHOULDER_ROM', 'wrist_power_ratio', '1stfinger_power_ratio']\n",
      "INFO: Loaded feature list for 'R_ELBOW_energy_by_trial_exhaustion_score': ['rolling_hr_mean', 'joint_power', 'exhaustion_lag1', 'ema_exhaustion', '5thfinger_power_ratio', 'elbow_power_ratio', '1stfinger_power_ratio', 'energy_acceleration', 'simulated_HR', 'wrist_power_ratio']\n",
      "INFO: [Base Data] Loaded top features for target 'R_ELBOW_energy_by_trial_exhaustion_score': ['rolling_hr_mean', 'joint_power', 'exhaustion_lag1', 'ema_exhaustion', '5thfinger_power_ratio', 'elbow_power_ratio', '1stfinger_power_ratio', 'energy_acceleration', 'simulated_HR', 'wrist_power_ratio']\n",
      "INFO: Loaded feature list for 'L_KNEE_energy_by_trial_exhaustion_score': ['ema_exhaustion', 'joint_power', 'rolling_hr_mean', 'exhaustion_lag1', 'simulated_HR', 'energy_acceleration', 'L_SHOULDER_ROM', '5thfinger_power_ratio', 'elbow_power_ratio', 'time_since_start']\n",
      "INFO: [Base Data] Loaded top features for target 'L_KNEE_energy_by_trial_exhaustion_score': ['ema_exhaustion', 'joint_power', 'rolling_hr_mean', 'exhaustion_lag1', 'simulated_HR', 'energy_acceleration', 'L_SHOULDER_ROM', '5thfinger_power_ratio', 'elbow_power_ratio', 'time_since_start']\n",
      "INFO: Loaded feature list for 'R_KNEE_energy_by_trial_exhaustion_score': ['ema_exhaustion', 'rolling_hr_mean', 'exhaustion_lag1', 'simulated_HR', 'energy_acceleration', 'wrist_power_ratio', 'L_SHOULDER_ROM', 'elbow_power_ratio', 'power_avg_5', 'L_KNEE_ROM_deviation']\n",
      "INFO: [Base Data] Loaded top features for target 'R_KNEE_energy_by_trial_exhaustion_score': ['ema_exhaustion', 'rolling_hr_mean', 'exhaustion_lag1', 'simulated_HR', 'energy_acceleration', 'wrist_power_ratio', 'L_SHOULDER_ROM', 'elbow_power_ratio', 'power_avg_5', 'L_KNEE_ROM_deviation']\n",
      "INFO: Loaded feature list for 'L_HIP_energy_by_trial_exhaustion_score': ['exhaustion_lag1', 'ema_exhaustion', 'rolling_hr_mean', 'simulated_HR', 'energy_acceleration', 'R_SHOULDER_ROM', 'elbow_power_ratio', '1stfinger_power_ratio', 'L_SHOULDER_ROM', 'time_since_start']\n",
      "INFO: [Base Data] Loaded top features for target 'L_HIP_energy_by_trial_exhaustion_score': ['exhaustion_lag1', 'ema_exhaustion', 'rolling_hr_mean', 'simulated_HR', 'energy_acceleration', 'R_SHOULDER_ROM', 'elbow_power_ratio', '1stfinger_power_ratio', 'L_SHOULDER_ROM', 'time_since_start']\n",
      "INFO: Loaded feature list for 'R_HIP_energy_by_trial_exhaustion_score': ['exhaustion_lag1', 'ema_exhaustion', 'joint_power', 'rolling_hr_mean', 'simulated_HR', 'energy_acceleration', '1stfinger_power_ratio', '5thfinger_power_ratio', 'hip_asymmetry', 'wrist_power_ratio']\n",
      "INFO: [Base Data] Loaded top features for target 'R_HIP_energy_by_trial_exhaustion_score': ['exhaustion_lag1', 'ema_exhaustion', 'joint_power', 'rolling_hr_mean', 'simulated_HR', 'energy_acceleration', '1stfinger_power_ratio', '5thfinger_power_ratio', 'hip_asymmetry', 'wrist_power_ratio']\n",
      "INFO: Test Load: Features for L_ANKLE_injury_risk: ['rolling_exhaustion', 'R_SHOULDER_ROM', 'L_HIP_ROM', 'L_ANKLE_ROM', 'time_since_start', 'wrist_power_ratio', 'L_ANKLE_ROM_deviation', '5thfinger_power_ratio', 'joint_power', 'R_HIP_ROM']\n",
      "INFO: Test Load: Features for R_ANKLE_injury_risk: ['rolling_exhaustion', 'exhaustion_lag1', 'L_SHOULDER_ROM', 'joint_energy', 'R_SHOULDER_ROM', 'L_KNEE_ROM_deviation', 'joint_power', 'L_HIP_ROM', 'L_KNEE_ROM', 'rolling_hr_mean']\n",
      "INFO: Test Load: Features for L_WRIST_injury_risk: ['rolling_exhaustion', 'exhaustion_lag1', 'ema_exhaustion', 'power_avg_5', 'L_ANKLE_ROM_deviation', 'rolling_hr_mean', 'L_ANKLE_ROM', 'joint_energy', 'energy_acceleration', 'R_SHOULDER_ROM']\n",
      "INFO: Test Load: Features for R_WRIST_injury_risk: ['rolling_exhaustion', 'wrist_power_ratio', '5thfinger_power_ratio', 'elbow_power_ratio', 'exhaustion_lag1', 'joint_power', 'R_HIP_ROM', '1stfinger_power_ratio', 'rolling_hr_mean', '5thfinger_asymmetry']\n",
      "INFO: Test Load: Features for L_ELBOW_injury_risk: ['rolling_exhaustion', 'exhaustion_lag1', 'ema_exhaustion', '5thfinger_asymmetry', 'rolling_hr_mean', 'energy_acceleration', 'elbow_power_ratio', 'rolling_energy_std', 'R_ANKLE_ROM', 'time_since_start']\n",
      "INFO: Test Load: Features for R_ELBOW_injury_risk: ['rolling_exhaustion', 'wrist_power_ratio', 'joint_power', 'elbow_power_ratio', '5thfinger_power_ratio', 'energy_acceleration', 'R_SHOULDER_ROM', 'joint_energy', 'exhaustion_lag1', 'knee_power_ratio']\n",
      "INFO: Test Load: Features for L_KNEE_injury_risk: ['rolling_exhaustion', 'L_ANKLE_ROM', 'R_HIP_ROM', 'ema_exhaustion', 'L_HIP_ROM', 'L_ANKLE_ROM_deviation', 'rolling_hr_mean', 'R_KNEE_ROM_deviation', 'time_since_start', 'L_KNEE_ROM_deviation']\n",
      "INFO: Test Load: Features for R_KNEE_injury_risk: ['rolling_exhaustion', 'exhaustion_lag1', 'R_HIP_ROM', 'knee_power_ratio', 'R_KNEE_ROM', 'rolling_power_std', '5thfinger_power_ratio', 'rolling_hr_mean', 'R_KNEE_ROM_deviation', 'wrist_power_ratio']\n",
      "INFO: Test Load: Features for L_HIP_injury_risk: ['rolling_exhaustion', 'wrist_power_ratio', 'ema_exhaustion', 'L_ANKLE_ROM', 'L_KNEE_ROM_deviation', 'exhaustion_lag1', 'hip_asymmetry', 'time_since_start', 'L_KNEE_ROM', 'ankle_asymmetry']\n",
      "INFO: Test Load: Features for R_HIP_injury_risk: ['rolling_exhaustion', 'exhaustion_lag1', 'ema_exhaustion', 'wrist_power_ratio', 'L_KNEE_ROM_deviation', 'R_SHOULDER_ROM', 'time_since_start', 'rolling_hr_mean', 'L_SHOULDER_ROM', 'hip_asymmetry']\n",
      "INFO: Test Load: Features for L_ANKLE_energy_by_trial_exhaustion_score: ['joint_power', 'ema_exhaustion', 'exhaustion_lag1', 'rolling_hr_mean', 'rolling_exhaustion', 'R_SHOULDER_ROM', 'simulated_HR', '1stfinger_asymmetry', '5thfinger_power_ratio', 'L_SHOULDER_ROM']\n",
      "INFO: Test Load: Features for R_ANKLE_energy_by_trial_exhaustion_score: ['ema_exhaustion', 'exhaustion_lag1', 'joint_power', 'rolling_exhaustion', 'rolling_hr_mean', 'R_KNEE_ROM_deviation', 'L_ANKLE_ROM_deviation', 'wrist_asymmetry', 'R_ANKLE_ROM_deviation', 'time_since_start']\n",
      "INFO: Test Load: Features for L_WRIST_energy_by_trial_exhaustion_score: ['exhaustion_lag1', 'ema_exhaustion', 'rolling_hr_mean', 'L_SHOULDER_ROM', '1stfinger_power_ratio', 'simulated_HR', 'energy_acceleration', 'rolling_exhaustion', 'wrist_power_ratio', 'L_HIP_ROM']\n",
      "INFO: Test Load: Features for R_WRIST_energy_by_trial_exhaustion_score: ['rolling_hr_mean', 'joint_power', 'exhaustion_lag1', 'ema_exhaustion', 'wrist_power_ratio', 'elbow_asymmetry', '5thfinger_power_ratio', '1stfinger_power_ratio', 'simulated_HR', 'energy_acceleration']\n",
      "INFO: Test Load: Features for L_ELBOW_energy_by_trial_exhaustion_score: ['exhaustion_lag1', 'ema_exhaustion', 'joint_power', 'rolling_hr_mean', 'simulated_HR', '5thfinger_power_ratio', 'rolling_exhaustion', 'L_SHOULDER_ROM', 'wrist_power_ratio', '1stfinger_power_ratio']\n",
      "INFO: Test Load: Features for R_ELBOW_energy_by_trial_exhaustion_score: ['rolling_hr_mean', 'joint_power', 'exhaustion_lag1', 'ema_exhaustion', '5thfinger_power_ratio', 'elbow_power_ratio', '1stfinger_power_ratio', 'energy_acceleration', 'simulated_HR', 'wrist_power_ratio']\n",
      "INFO: Test Load: Features for L_KNEE_energy_by_trial_exhaustion_score: ['ema_exhaustion', 'joint_power', 'rolling_hr_mean', 'exhaustion_lag1', 'simulated_HR', 'energy_acceleration', 'L_SHOULDER_ROM', '5thfinger_power_ratio', 'elbow_power_ratio', 'time_since_start']\n",
      "INFO: Test Load: Features for R_KNEE_energy_by_trial_exhaustion_score: ['ema_exhaustion', 'rolling_hr_mean', 'exhaustion_lag1', 'simulated_HR', 'energy_acceleration', 'wrist_power_ratio', 'L_SHOULDER_ROM', 'elbow_power_ratio', 'power_avg_5', 'L_KNEE_ROM_deviation']\n",
      "INFO: Test Load: Features for L_HIP_energy_by_trial_exhaustion_score: ['exhaustion_lag1', 'ema_exhaustion', 'rolling_hr_mean', 'simulated_HR', 'energy_acceleration', 'R_SHOULDER_ROM', 'elbow_power_ratio', '1stfinger_power_ratio', 'L_SHOULDER_ROM', 'time_since_start']\n",
      "INFO: Test Load: Features for R_HIP_energy_by_trial_exhaustion_score: ['exhaustion_lag1', 'ema_exhaustion', 'joint_power', 'rolling_hr_mean', 'simulated_HR', 'energy_acceleration', '1stfinger_power_ratio', '5thfinger_power_ratio', 'hip_asymmetry', 'wrist_power_ratio']\n",
      "INFO: === Trial Summary Dataset Analysis ===\n",
      "INFO: Skipping analysis for Trial Summary Data; using pre-saved feature lists from ..\\..\\data\\Deep_Learning_Final\\feature_lists\\trial_summary\n",
      "INFO: Loaded feature list for 'exhaustion_rate': ['by_trial_exhaustion_score', 'power_avg_5', 'L_KNEE_angle', 'R_ELBOW_angle', 'joint_energy', 'R_ELBOW_energy', 'joint_power', 'wrist_asymmetry', 'R_ELBOW_ongoing_power', 'L_SHOULDER_ROM']\n",
      "INFO: [Trial Summary Data] Loaded top features for target 'exhaustion_rate': ['by_trial_exhaustion_score', 'power_avg_5', 'L_KNEE_angle', 'R_ELBOW_angle', 'joint_energy', 'R_ELBOW_energy', 'joint_power', 'wrist_asymmetry', 'R_ELBOW_ongoing_power', 'L_SHOULDER_ROM']\n",
      "INFO: Loaded feature list for 'injury_risk': ['by_trial_exhaustion_score', 'R_WRIST_ROM', 'elbow_asymmetry', 'energy_acceleration', 'L_WRIST_angle', 'rolling_power_std', 'wrist_asymmetry', 'R_KNEE_angle', 'L_HIP_ROM', 'R_ELBOW_angle']\n",
      "INFO: [Trial Summary Data] Loaded top features for target 'injury_risk': ['by_trial_exhaustion_score', 'R_WRIST_ROM', 'elbow_asymmetry', 'energy_acceleration', 'L_WRIST_angle', 'rolling_power_std', 'wrist_asymmetry', 'R_KNEE_angle', 'L_HIP_ROM', 'R_ELBOW_angle']\n",
      "INFO: === Shot Phase Summary Dataset Analysis ===\n",
      "INFO: Skipping analysis for Shot Phase Summary Data; using pre-saved feature lists from ..\\..\\data\\Deep_Learning_Final\\feature_lists\\shot_phase_summary\n",
      "INFO: Loaded feature list for 'exhaustion_rate': ['by_trial_exhaustion_score', 'power_avg_5', 'joint_power', 'R_ELBOW_energy', 'L_SHOULDER_ROM', 'R_SHOULDER_ROM', 'R_WRIST_ongoing_power', 'L_KNEE_angle', 'R_KNEE_angle', 'rolling_hr_mean']\n",
      "INFO: [Shot Phase Summary Data] Loaded top features for target 'exhaustion_rate': ['by_trial_exhaustion_score', 'power_avg_5', 'joint_power', 'R_ELBOW_energy', 'L_SHOULDER_ROM', 'R_SHOULDER_ROM', 'R_WRIST_ongoing_power', 'L_KNEE_angle', 'R_KNEE_angle', 'rolling_hr_mean']\n",
      "INFO: Loaded feature list for 'injury_risk': ['by_trial_exhaustion_score', 'power_avg_5', 'rolling_hr_mean', 'R_HIP_energy', 'R_WRIST_angle', 'elbow_asymmetry', 'R_HIP_ongoing_power', 'R_WRIST_ROM', 'L_ELBOW_energy', 'L_ELBOW_angle']\n",
      "INFO: [Shot Phase Summary Data] Loaded top features for target 'injury_risk': ['by_trial_exhaustion_score', 'power_avg_5', 'rolling_hr_mean', 'R_HIP_energy', 'R_WRIST_angle', 'elbow_asymmetry', 'R_HIP_ongoing_power', 'R_WRIST_ROM', 'L_ELBOW_energy', 'L_ELBOW_angle']\n",
      "INFO: Performed temporal train-test split with test size = 0.2\n",
      "INFO: Training data shape: (2364, 321), Testing data shape: (592, 321)\n",
      "INFO: Features provided for training exhaustion model: ['joint_power', 'joint_energy', 'elbow_asymmetry', 'L_WRIST_angle', 'R_WRIST_angle', 'exhaustion_lag1', 'power_avg_5', 'simulated_HR', 'player_height_in_meters', 'player_weight__in_kg']\n",
      "INFO: Available train_data columns: ['trial_id', 'result', 'landing_x', 'landing_y', 'entry_angle', 'frame_time', 'ball_x', 'ball_y', 'ball_z', 'R_EYE_x', 'R_EYE_y', 'R_EYE_z', 'L_EYE_x', 'L_EYE_y', 'L_EYE_z', 'NOSE_x', 'NOSE_y', 'NOSE_z', 'R_EAR_x', 'R_EAR_y', 'R_EAR_z', 'L_EAR_x', 'L_EAR_y', 'L_EAR_z', 'R_SHOULDER_x', 'R_SHOULDER_y', 'R_SHOULDER_z', 'L_SHOULDER_x', 'L_SHOULDER_y', 'L_SHOULDER_z', 'R_ELBOW_x', 'R_ELBOW_y', 'R_ELBOW_z', 'L_ELBOW_x', 'L_ELBOW_y', 'L_ELBOW_z', 'R_WRIST_x', 'R_WRIST_y', 'R_WRIST_z', 'L_WRIST_x', 'L_WRIST_y', 'L_WRIST_z', 'R_HIP_x', 'R_HIP_y', 'R_HIP_z', 'L_HIP_x', 'L_HIP_y', 'L_HIP_z', 'R_KNEE_x', 'R_KNEE_y', 'R_KNEE_z', 'L_KNEE_x', 'L_KNEE_y', 'L_KNEE_z', 'R_ANKLE_x', 'R_ANKLE_y', 'R_ANKLE_z', 'L_ANKLE_x', 'L_ANKLE_y', 'L_ANKLE_z', 'R_1STFINGER_x', 'R_1STFINGER_y', 'R_1STFINGER_z', 'R_5THFINGER_x', 'R_5THFINGER_y', 'R_5THFINGER_z', 'L_1STFINGER_x', 'L_1STFINGER_y', 'L_1STFINGER_z', 'L_5THFINGER_x', 'L_5THFINGER_y', 'L_5THFINGER_z', 'R_1STTOE_x', 'R_1STTOE_y', 'R_1STTOE_z', 'R_5THTOE_x', 'R_5THTOE_y', 'R_5THTOE_z', 'L_1STTOE_x', 'L_1STTOE_y', 'L_1STTOE_z', 'L_5THTOE_x', 'L_5THTOE_y', 'L_5THTOE_z', 'R_CALC_x', 'R_CALC_y', 'R_CALC_z', 'L_CALC_x', 'L_CALC_y', 'L_CALC_z', 'ball_speed', 'ball_velocity_x', 'ball_velocity_y', 'ball_velocity_z', 'overall_ball_velocity', 'ball_direction_x', 'ball_direction_y', 'ball_direction_z', 'computed_ball_velocity_x', 'computed_ball_velocity_y', 'computed_ball_velocity_z', 'dist_ball_R_1STFINGER', 'dist_ball_R_5THFINGER', 'dist_ball_L_1STFINGER', 'dist_ball_L_5THFINGER', 'ball_in_hands', 'shooting_motion', 'avg_shoulder_height', 'release_point_filter', 'dt', 'dx', 'dy', 'dz', 'L_ANKLE_ongoing_power', 'R_ANKLE_ongoing_power', 'L_KNEE_ongoing_power', 'R_KNEE_ongoing_power', 'L_HIP_ongoing_power', 'R_HIP_ongoing_power', 'L_ELBOW_ongoing_power', 'R_ELBOW_ongoing_power', 'L_WRIST_ongoing_power', 'R_WRIST_ongoing_power', 'L_1STFINGER_ongoing_power', 'L_5THFINGER_ongoing_power', 'R_1STFINGER_ongoing_power', 'R_5THFINGER_ongoing_power', 'L_ELBOW_angle', 'L_WRIST_angle', 'L_KNEE_angle', 'L_ELBOW_ongoing_angle', 'L_WRIST_ongoing_angle', 'L_KNEE_ongoing_angle', 'R_ELBOW_angle', 'R_WRIST_angle', 'R_KNEE_angle', 'R_ELBOW_ongoing_angle', 'R_WRIST_ongoing_angle', 'R_KNEE_ongoing_angle', 'shooting_phases', 'player_height_in_meters', 'player_height_ft', 'initial_release_angle', 'calculated_release_angle', 'angle_difference', 'distance_to_basket', 'optimal_release_angle', 'by_trial_time', 'continuous_frame_time', 'L_ANKLE_energy', 'R_ANKLE_energy', 'L_KNEE_energy', 'R_KNEE_energy', 'L_HIP_energy', 'R_HIP_energy', 'L_ELBOW_energy', 'R_ELBOW_energy', 'L_WRIST_energy', 'R_WRIST_energy', 'L_1STFINGER_energy', 'R_1STFINGER_energy', 'L_5THFINGER_energy', 'R_5THFINGER_energy', 'total_energy', 'by_trial_energy', 'by_trial_exhaustion_score', 'overall_cumulative_energy', 'overall_exhaustion_score', 'L_ANKLE_energy_by_trial', 'L_ANKLE_energy_by_trial_exhaustion_score', 'L_ANKLE_energy_overall_cumulative', 'L_ANKLE_energy_overall_exhaustion_score', 'R_ANKLE_energy_by_trial', 'R_ANKLE_energy_by_trial_exhaustion_score', 'R_ANKLE_energy_overall_cumulative', 'R_ANKLE_energy_overall_exhaustion_score', 'L_KNEE_energy_by_trial', 'L_KNEE_energy_by_trial_exhaustion_score', 'L_KNEE_energy_overall_cumulative', 'L_KNEE_energy_overall_exhaustion_score', 'R_KNEE_energy_by_trial', 'R_KNEE_energy_by_trial_exhaustion_score', 'R_KNEE_energy_overall_cumulative', 'R_KNEE_energy_overall_exhaustion_score', 'L_HIP_energy_by_trial', 'L_HIP_energy_by_trial_exhaustion_score', 'L_HIP_energy_overall_cumulative', 'L_HIP_energy_overall_exhaustion_score', 'R_HIP_energy_by_trial', 'R_HIP_energy_by_trial_exhaustion_score', 'R_HIP_energy_overall_cumulative', 'R_HIP_energy_overall_exhaustion_score', 'L_ELBOW_energy_by_trial', 'L_ELBOW_energy_by_trial_exhaustion_score', 'L_ELBOW_energy_overall_cumulative', 'L_ELBOW_energy_overall_exhaustion_score', 'R_ELBOW_energy_by_trial', 'R_ELBOW_energy_by_trial_exhaustion_score', 'R_ELBOW_energy_overall_cumulative', 'R_ELBOW_energy_overall_exhaustion_score', 'L_WRIST_energy_by_trial', 'L_WRIST_energy_by_trial_exhaustion_score', 'L_WRIST_energy_overall_cumulative', 'L_WRIST_energy_overall_exhaustion_score', 'R_WRIST_energy_by_trial', 'R_WRIST_energy_by_trial_exhaustion_score', 'R_WRIST_energy_overall_cumulative', 'R_WRIST_energy_overall_exhaustion_score', 'L_1STFINGER_energy_by_trial', 'L_1STFINGER_energy_by_trial_exhaustion_score', 'L_1STFINGER_energy_overall_cumulative', 'L_1STFINGER_energy_overall_exhaustion_score', 'R_1STFINGER_energy_by_trial', 'R_1STFINGER_energy_by_trial_exhaustion_score', 'R_1STFINGER_energy_overall_cumulative', 'R_1STFINGER_energy_overall_exhaustion_score', 'L_5THFINGER_energy_by_trial', 'L_5THFINGER_energy_by_trial_exhaustion_score', 'L_5THFINGER_energy_overall_cumulative', 'L_5THFINGER_energy_overall_exhaustion_score', 'R_5THFINGER_energy_by_trial', 'R_5THFINGER_energy_by_trial_exhaustion_score', 'R_5THFINGER_energy_overall_cumulative', 'R_5THFINGER_energy_overall_exhaustion_score', 'participant_id', 'L_SHOULDER_angle', 'R_SHOULDER_angle', 'L_HIP_angle', 'R_HIP_angle', 'L_ANKLE_angle', 'R_ANKLE_angle', 'datetime', 'player_weight__in_kg', 'joint_energy', 'joint_power', 'energy_acceleration', 'ankle_power_ratio', 'hip_asymmetry', 'ankle_asymmetry', 'wrist_asymmetry', 'elbow_asymmetry', 'knee_asymmetry', '1stfinger_asymmetry', '5thfinger_asymmetry', 'hip_power_ratio', 'wrist_power_ratio', 'elbow_power_ratio', 'knee_power_ratio', '1stfinger_power_ratio', '5thfinger_power_ratio', 'L_KNEE_ROM', 'L_KNEE_ROM_deviation', 'L_KNEE_ROM_extreme', 'R_KNEE_ROM', 'R_KNEE_ROM_deviation', 'R_KNEE_ROM_extreme', 'L_SHOULDER_ROM', 'L_SHOULDER_ROM_deviation', 'L_SHOULDER_ROM_extreme', 'R_SHOULDER_ROM', 'R_SHOULDER_ROM_deviation', 'R_SHOULDER_ROM_extreme', 'L_HIP_ROM', 'L_HIP_ROM_deviation', 'L_HIP_ROM_extreme', 'R_HIP_ROM', 'R_HIP_ROM_deviation', 'R_HIP_ROM_extreme', 'L_ANKLE_ROM', 'L_ANKLE_ROM_deviation', 'L_ANKLE_ROM_extreme', 'R_ANKLE_ROM', 'R_ANKLE_ROM_deviation', 'R_ANKLE_ROM_extreme', 'L_WRIST_ROM', 'L_WRIST_ROM_deviation', 'L_WRIST_ROM_extreme', 'R_WRIST_ROM', 'R_WRIST_ROM_deviation', 'R_WRIST_ROM_extreme', 'exhaustion_rate', 'simulated_HR', 'time_since_start', 'power_avg_5', 'rolling_power_std', 'rolling_hr_mean', 'rolling_energy_std', 'exhaustion_lag1', 'ema_exhaustion', 'rolling_exhaustion', 'injury_risk', 'L_ANKLE_exhaustion_rate', 'L_ANKLE_rolling_exhaustion', 'L_ANKLE_injury_risk', 'R_ANKLE_exhaustion_rate', 'R_ANKLE_rolling_exhaustion', 'R_ANKLE_injury_risk', 'L_WRIST_exhaustion_rate', 'L_WRIST_rolling_exhaustion', 'L_WRIST_injury_risk', 'R_WRIST_exhaustion_rate', 'R_WRIST_rolling_exhaustion', 'R_WRIST_injury_risk', 'L_ELBOW_exhaustion_rate', 'L_ELBOW_rolling_exhaustion', 'L_ELBOW_injury_risk', 'R_ELBOW_exhaustion_rate', 'R_ELBOW_rolling_exhaustion', 'R_ELBOW_injury_risk', 'L_KNEE_exhaustion_rate', 'L_KNEE_rolling_exhaustion', 'L_KNEE_injury_risk', 'R_KNEE_exhaustion_rate', 'R_KNEE_rolling_exhaustion', 'R_KNEE_injury_risk', 'L_HIP_exhaustion_rate', 'L_HIP_rolling_exhaustion', 'L_HIP_injury_risk', 'R_HIP_exhaustion_rate', 'R_HIP_rolling_exhaustion', 'R_HIP_injury_risk']\n",
      "INFO: Features have been scaled using StandardScaler.\n",
      "INFO: Created LSTM sequences: (2359, 5, 10), (2359, 1)\n",
      "INFO: Created LSTM sequences: (587, 5, 10), (587, 1)\n",
      "INFO: Training exhaustion model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Feature Analysis for Target: INJURY_RISK ===\n",
      "Permutation Top 10: ['rolling_exhaustion', 'ema_exhaustion', 'knee_power_ratio', 'exhaustion_lag1', 'hip_asymmetry', 'L_SHOULDER_ROM', 'power_avg_5', 'hip_power_ratio', '1stfinger_asymmetry', 'wrist_power_ratio']\n",
      "RFE Selected Features: ['knee_power_ratio', 'R_HIP_ROM', 'time_since_start', 'ema_exhaustion', 'rolling_exhaustion']\n",
      "SHAP Top 10: ['rolling_exhaustion', 'time_since_start', 'joint_energy', 'simulated_HR', 'ema_exhaustion', 'R_HIP_ROM', 'knee_power_ratio', 'ankle_power_ratio', 'rolling_hr_mean', 'exhaustion_lag1']\n",
      "Consensus Top 10: ['rolling_exhaustion', 'ema_exhaustion', 'knee_power_ratio', 'exhaustion_lag1', 'hip_asymmetry', 'power_avg_5', 'time_since_start', 'knee_asymmetry', 'elbow_power_ratio', 'L_SHOULDER_ROM']\n",
      "\n",
      "=== Feature Analysis for Target: BY_TRIAL_EXHAUSTION_SCORE ===\n",
      "Permutation Top 10: ['exhaustion_lag1', 'ema_exhaustion', 'rolling_hr_mean', 'joint_power', '5thfinger_power_ratio', 'wrist_power_ratio', 'simulated_HR', 'hip_power_ratio', '1stfinger_power_ratio', 'power_avg_5']\n",
      "RFE Selected Features: ['joint_energy', 'joint_power', 'exhaustion_lag1', 'rolling_hr_mean', 'ema_exhaustion']\n",
      "SHAP Top 10: ['joint_power', 'rolling_hr_mean', 'exhaustion_lag1', 'joint_energy', 'ema_exhaustion', 'simulated_HR', '5thfinger_power_ratio', 'wrist_power_ratio', 'power_avg_5', '1stfinger_power_ratio']\n",
      "Consensus Top 10: ['exhaustion_lag1', 'joint_power', 'rolling_hr_mean', 'ema_exhaustion', 'simulated_HR', '5thfinger_power_ratio', 'wrist_power_ratio', '1stfinger_power_ratio', 'power_avg_5', 'hip_power_ratio']\n",
      "\n",
      "=== Feature Analysis for Target: L_ANKLE_INJURY_RISK ===\n",
      "Permutation Top 10: ['rolling_exhaustion', 'R_SHOULDER_ROM', 'L_HIP_ROM', 'L_ANKLE_ROM', 'R_ANKLE_ROM_deviation', '1stfinger_power_ratio', 'L_ANKLE_ROM_deviation', '5thfinger_power_ratio', 'ankle_power_ratio', 'rolling_hr_mean']\n",
      "RFE Selected Features: ['L_HIP_ROM', 'R_HIP_ROM', 'exhaustion_lag1', 'time_since_start', 'rolling_exhaustion']\n",
      "SHAP Top 10: ['rolling_exhaustion', 'R_HIP_ROM', 'time_since_start', 'R_SHOULDER_ROM', 'L_HIP_ROM', 'L_ANKLE_ROM', 'exhaustion_lag1', 'L_ANKLE_ROM_deviation', 'ema_exhaustion', 'R_ANKLE_ROM']\n",
      "Consensus Top 10: ['rolling_exhaustion', 'R_SHOULDER_ROM', 'L_HIP_ROM', 'L_ANKLE_ROM', 'time_since_start', 'wrist_power_ratio', 'L_ANKLE_ROM_deviation', '5thfinger_power_ratio', 'joint_power', 'R_HIP_ROM']\n",
      "\n",
      "=== Feature Analysis for Target: R_ANKLE_INJURY_RISK ===\n",
      "Permutation Top 10: ['rolling_exhaustion', 'L_SHOULDER_ROM', 'exhaustion_lag1', 'L_HIP_ROM', 'joint_power', 'elbow_power_ratio', 'L_KNEE_ROM', 'joint_energy', 'R_SHOULDER_ROM', 'energy_acceleration']\n",
      "RFE Selected Features: ['L_KNEE_ROM_deviation', 'R_KNEE_ROM', 'exhaustion_lag1', 'time_since_start', 'rolling_exhaustion']\n",
      "SHAP Top 10: ['rolling_exhaustion', 'time_since_start', 'R_KNEE_ROM', 'R_KNEE_ROM_deviation', 'R_SHOULDER_ROM', 'L_SHOULDER_ROM', 'exhaustion_lag1', 'joint_energy', 'R_ANKLE_ROM_deviation', 'L_KNEE_ROM_deviation']\n",
      "Consensus Top 10: ['rolling_exhaustion', 'exhaustion_lag1', 'L_SHOULDER_ROM', 'joint_energy', 'R_SHOULDER_ROM', 'L_KNEE_ROM_deviation', 'joint_power', 'L_HIP_ROM', 'L_KNEE_ROM', 'rolling_hr_mean']\n",
      "\n",
      "=== Feature Analysis for Target: L_WRIST_INJURY_RISK ===\n",
      "Permutation Top 10: ['rolling_exhaustion', 'ema_exhaustion', 'exhaustion_lag1', 'L_ANKLE_ROM_deviation', 'rolling_hr_mean', 'L_ANKLE_ROM', 'power_avg_5', 'energy_acceleration', 'wrist_power_ratio', '5thfinger_power_ratio']\n",
      "RFE Selected Features: ['L_ANKLE_ROM_deviation', 'exhaustion_lag1', 'power_avg_5', 'time_since_start', 'rolling_exhaustion']\n",
      "SHAP Top 10: ['rolling_exhaustion', 'exhaustion_lag1', 'ema_exhaustion', 'power_avg_5', 'time_since_start', 'R_SHOULDER_ROM', 'rolling_hr_mean', '5thfinger_asymmetry', 'joint_energy', 'rolling_energy_std']\n",
      "Consensus Top 10: ['rolling_exhaustion', 'exhaustion_lag1', 'ema_exhaustion', 'power_avg_5', 'L_ANKLE_ROM_deviation', 'rolling_hr_mean', 'L_ANKLE_ROM', 'joint_energy', 'energy_acceleration', 'R_SHOULDER_ROM']\n",
      "\n",
      "=== Feature Analysis for Target: R_WRIST_INJURY_RISK ===\n",
      "Permutation Top 10: ['rolling_exhaustion', 'exhaustion_lag1', 'wrist_power_ratio', 'ema_exhaustion', '5thfinger_power_ratio', 'joint_power', '1stfinger_power_ratio', 'elbow_power_ratio', 'hip_power_ratio', '5thfinger_asymmetry']\n",
      "RFE Selected Features: ['elbow_power_ratio', 'wrist_power_ratio', 'R_HIP_ROM', 'time_since_start', 'rolling_exhaustion']\n",
      "SHAP Top 10: ['rolling_exhaustion', 'time_since_start', 'wrist_power_ratio', 'R_HIP_ROM', '5thfinger_power_ratio', 'elbow_power_ratio', '1stfinger_power_ratio', 'joint_power', 'exhaustion_lag1', 'knee_power_ratio']\n",
      "Consensus Top 10: ['rolling_exhaustion', 'wrist_power_ratio', '5thfinger_power_ratio', 'elbow_power_ratio', 'exhaustion_lag1', 'joint_power', 'R_HIP_ROM', '1stfinger_power_ratio', 'rolling_hr_mean', '5thfinger_asymmetry']\n",
      "\n",
      "=== Feature Analysis for Target: L_ELBOW_INJURY_RISK ===\n",
      "Permutation Top 10: ['rolling_exhaustion', 'ema_exhaustion', 'exhaustion_lag1', 'rolling_hr_mean', '5thfinger_asymmetry', 'energy_acceleration', 'rolling_energy_std', 'knee_asymmetry', 'elbow_power_ratio', 'R_HIP_ROM']\n",
      "RFE Selected Features: ['5thfinger_asymmetry', 'L_ANKLE_ROM_deviation', 'power_avg_5', 'time_since_start', 'rolling_exhaustion']\n",
      "SHAP Top 10: ['rolling_exhaustion', 'time_since_start', 'ema_exhaustion', 'R_SHOULDER_ROM', 'exhaustion_lag1', 'rolling_hr_mean', '5thfinger_asymmetry', 'power_avg_5', 'energy_acceleration', 'R_KNEE_ROM']\n",
      "Consensus Top 10: ['rolling_exhaustion', 'exhaustion_lag1', 'ema_exhaustion', '5thfinger_asymmetry', 'rolling_hr_mean', 'energy_acceleration', 'elbow_power_ratio', 'rolling_energy_std', 'R_ANKLE_ROM', 'time_since_start']\n",
      "\n",
      "=== Feature Analysis for Target: R_ELBOW_INJURY_RISK ===\n",
      "Permutation Top 10: ['rolling_exhaustion', 'wrist_power_ratio', 'energy_acceleration', 'joint_power', 'ankle_power_ratio', '5thfinger_power_ratio', 'joint_energy', 'elbow_power_ratio', 'power_avg_5', '1stfinger_power_ratio']\n",
      "RFE Selected Features: ['wrist_power_ratio', 'R_KNEE_ROM', 'L_ANKLE_ROM_deviation', 'time_since_start', 'rolling_exhaustion']\n",
      "SHAP Top 10: ['time_since_start', 'rolling_exhaustion', 'joint_power', 'elbow_power_ratio', 'wrist_power_ratio', '5thfinger_power_ratio', 'R_SHOULDER_ROM', 'exhaustion_lag1', 'energy_acceleration', '1stfinger_power_ratio']\n",
      "Consensus Top 10: ['rolling_exhaustion', 'wrist_power_ratio', 'joint_power', 'elbow_power_ratio', '5thfinger_power_ratio', 'energy_acceleration', 'R_SHOULDER_ROM', 'joint_energy', 'exhaustion_lag1', 'knee_power_ratio']\n",
      "\n",
      "=== Feature Analysis for Target: L_KNEE_INJURY_RISK ===\n",
      "Permutation Top 10: ['rolling_exhaustion', 'L_ANKLE_ROM', 'L_ANKLE_ROM_deviation', 'ema_exhaustion', 'L_HIP_ROM', 'R_KNEE_ROM_deviation', 'R_HIP_ROM', 'rolling_hr_mean', 'rolling_power_std', 'hip_power_ratio']\n",
      "RFE Selected Features: ['R_HIP_ROM', 'L_ANKLE_ROM', 'exhaustion_lag1', 'time_since_start', 'rolling_exhaustion']\n",
      "SHAP Top 10: ['rolling_exhaustion', 'time_since_start', 'R_HIP_ROM', 'L_ANKLE_ROM', 'ema_exhaustion', 'L_ANKLE_ROM_deviation', 'L_SHOULDER_ROM', 'L_HIP_ROM', 'R_KNEE_ROM_deviation', 'knee_asymmetry']\n",
      "Consensus Top 10: ['rolling_exhaustion', 'L_ANKLE_ROM', 'R_HIP_ROM', 'ema_exhaustion', 'L_HIP_ROM', 'L_ANKLE_ROM_deviation', 'rolling_hr_mean', 'R_KNEE_ROM_deviation', 'time_since_start', 'L_KNEE_ROM_deviation']\n",
      "\n",
      "=== Feature Analysis for Target: R_KNEE_INJURY_RISK ===\n",
      "Permutation Top 10: ['rolling_exhaustion', 'ema_exhaustion', 'R_HIP_ROM', '5thfinger_power_ratio', 'rolling_power_std', 'R_KNEE_ROM_deviation', 'joint_power', 'knee_power_ratio', 'exhaustion_lag1', 'L_HIP_ROM']\n",
      "RFE Selected Features: ['wrist_power_ratio', 'L_ANKLE_ROM', 'exhaustion_lag1', 'time_since_start', 'rolling_exhaustion']\n",
      "SHAP Top 10: ['rolling_exhaustion', 'time_since_start', 'exhaustion_lag1', 'rolling_hr_mean', 'wrist_power_ratio', 'R_KNEE_ROM_deviation', 'knee_power_ratio', 'R_SHOULDER_ROM', 'ema_exhaustion', 'hip_asymmetry']\n",
      "Consensus Top 10: ['rolling_exhaustion', 'exhaustion_lag1', 'R_HIP_ROM', 'knee_power_ratio', 'R_KNEE_ROM', '5thfinger_power_ratio', 'rolling_power_std', 'R_KNEE_ROM_deviation', 'rolling_hr_mean', 'wrist_power_ratio']\n",
      "\n",
      "=== Feature Analysis for Target: L_HIP_INJURY_RISK ===\n",
      "Permutation Top 10: ['rolling_exhaustion', 'ema_exhaustion', 'wrist_power_ratio', 'L_KNEE_ROM', 'L_ANKLE_ROM', 'L_KNEE_ROM_deviation', 'L_ANKLE_ROM_deviation', 'L_HIP_ROM', 'hip_asymmetry', 'ankle_asymmetry']\n",
      "RFE Selected Features: ['R_HIP_ROM', 'L_ANKLE_ROM', 'exhaustion_lag1', 'time_since_start', 'rolling_exhaustion']\n",
      "SHAP Top 10: ['rolling_exhaustion', 'time_since_start', 'ema_exhaustion', 'L_SHOULDER_ROM', 'R_KNEE_ROM', 'R_HIP_ROM', 'wrist_power_ratio', 'R_SHOULDER_ROM', 'rolling_hr_mean', 'exhaustion_lag1']\n",
      "Consensus Top 10: ['rolling_exhaustion', 'wrist_power_ratio', 'ema_exhaustion', 'L_ANKLE_ROM', 'L_KNEE_ROM_deviation', 'exhaustion_lag1', 'hip_asymmetry', 'time_since_start', 'ankle_asymmetry', 'L_KNEE_ROM']\n",
      "\n",
      "=== Feature Analysis for Target: R_HIP_INJURY_RISK ===\n",
      "Permutation Top 10: ['rolling_exhaustion', 'ema_exhaustion', 'wrist_power_ratio', 'exhaustion_lag1', 'L_SHOULDER_ROM', '5thfinger_power_ratio', 'L_KNEE_ROM_deviation', 'simulated_HR', 'energy_acceleration', 'L_HIP_ROM']\n",
      "RFE Selected Features: ['L_KNEE_ROM', 'R_HIP_ROM', 'exhaustion_lag1', 'time_since_start', 'rolling_exhaustion']\n",
      "SHAP Top 10: ['rolling_exhaustion', 'time_since_start', 'R_KNEE_ROM', 'ema_exhaustion', 'L_KNEE_ROM', 'R_KNEE_ROM_deviation', 'R_SHOULDER_ROM', 'exhaustion_lag1', 'L_KNEE_ROM_deviation', 'R_HIP_ROM']\n",
      "Consensus Top 10: ['rolling_exhaustion', 'exhaustion_lag1', 'ema_exhaustion', 'wrist_power_ratio', 'L_KNEE_ROM_deviation', 'R_SHOULDER_ROM', 'time_since_start', 'rolling_hr_mean', 'L_SHOULDER_ROM', 'hip_asymmetry']\n",
      "\n",
      "=== Feature Analysis for Target: L_ANKLE_ENERGY_BY_TRIAL_EXHAUSTION_SCORE ===\n",
      "Permutation Top 10: ['ema_exhaustion', 'joint_power', 'exhaustion_lag1', 'rolling_hr_mean', 'rolling_exhaustion', 'R_SHOULDER_ROM', 'L_SHOULDER_ROM', '5thfinger_power_ratio', 'ankle_power_ratio', 'simulated_HR']\n",
      "RFE Selected Features: ['joint_power', 'L_ANKLE_ROM', 'exhaustion_lag1', 'rolling_hr_mean', 'ema_exhaustion']\n",
      "SHAP Top 10: ['joint_power', 'ema_exhaustion', 'exhaustion_lag1', 'rolling_hr_mean', 'time_since_start', 'rolling_exhaustion', 'joint_energy', 'L_ANKLE_ROM_deviation', 'L_ANKLE_ROM', 'simulated_HR']\n",
      "Consensus Top 10: ['joint_power', 'ema_exhaustion', 'exhaustion_lag1', 'rolling_hr_mean', 'rolling_exhaustion', 'R_SHOULDER_ROM', 'simulated_HR', '1stfinger_asymmetry', '5thfinger_power_ratio', 'L_SHOULDER_ROM']\n",
      "\n",
      "=== Feature Analysis for Target: R_ANKLE_ENERGY_BY_TRIAL_EXHAUSTION_SCORE ===\n",
      "Permutation Top 10: ['ema_exhaustion', 'exhaustion_lag1', 'rolling_exhaustion', 'rolling_hr_mean', 'joint_power', 'R_KNEE_ROM_deviation', 'wrist_asymmetry', 'simulated_HR', 'R_KNEE_ROM', '1stfinger_asymmetry']\n",
      "RFE Selected Features: ['joint_power', 'L_SHOULDER_ROM', 'exhaustion_lag1', 'time_since_start', 'ema_exhaustion']\n",
      "SHAP Top 10: ['ema_exhaustion', 'exhaustion_lag1', 'joint_power', 'time_since_start', 'rolling_exhaustion', 'L_SHOULDER_ROM', 'rolling_hr_mean', 'joint_energy', 'L_ANKLE_ROM', 'R_KNEE_ROM_deviation']\n",
      "Consensus Top 10: ['ema_exhaustion', 'exhaustion_lag1', 'joint_power', 'rolling_hr_mean', 'rolling_exhaustion', 'R_KNEE_ROM_deviation', 'L_ANKLE_ROM_deviation', 'wrist_asymmetry', 'R_ANKLE_ROM_deviation', 'R_KNEE_ROM']\n",
      "\n",
      "=== Feature Analysis for Target: L_WRIST_ENERGY_BY_TRIAL_EXHAUSTION_SCORE ===\n",
      "Permutation Top 10: ['exhaustion_lag1', 'ema_exhaustion', 'L_SHOULDER_ROM', 'rolling_hr_mean', 'L_HIP_ROM', 'rolling_exhaustion', '1stfinger_power_ratio', 'simulated_HR', 'wrist_power_ratio', 'energy_acceleration']\n",
      "RFE Selected Features: ['joint_energy', 'joint_power', 'exhaustion_lag1', 'rolling_hr_mean', 'ema_exhaustion']\n",
      "SHAP Top 10: ['exhaustion_lag1', 'joint_power', 'ema_exhaustion', 'rolling_hr_mean', 'joint_energy', '1stfinger_power_ratio', 'L_SHOULDER_ROM', 'simulated_HR', 'wrist_power_ratio', 'rolling_exhaustion']\n",
      "Consensus Top 10: ['exhaustion_lag1', 'ema_exhaustion', 'rolling_hr_mean', '1stfinger_power_ratio', 'L_SHOULDER_ROM', 'simulated_HR', 'energy_acceleration', 'rolling_exhaustion', 'wrist_power_ratio', 'L_HIP_ROM']\n",
      "\n",
      "=== Feature Analysis for Target: R_WRIST_ENERGY_BY_TRIAL_EXHAUSTION_SCORE ===\n",
      "Permutation Top 10: ['exhaustion_lag1', 'rolling_hr_mean', 'ema_exhaustion', 'joint_power', 'R_ANKLE_ROM_deviation', 'wrist_power_ratio', 'R_ANKLE_ROM', 'simulated_HR', 'rolling_exhaustion', '1stfinger_power_ratio']\n",
      "RFE Selected Features: ['joint_energy', 'joint_power', 'wrist_power_ratio', 'exhaustion_lag1', 'rolling_hr_mean']\n",
      "SHAP Top 10: ['joint_power', 'rolling_hr_mean', 'joint_energy', 'exhaustion_lag1', 'ema_exhaustion', 'wrist_power_ratio', 'elbow_asymmetry', '5thfinger_power_ratio', '1stfinger_power_ratio', 'simulated_HR']\n",
      "Consensus Top 10: ['rolling_hr_mean', 'joint_power', 'exhaustion_lag1', 'wrist_power_ratio', 'ema_exhaustion', 'elbow_asymmetry', '5thfinger_power_ratio', '1stfinger_power_ratio', 'simulated_HR', 'energy_acceleration']\n",
      "\n",
      "=== Feature Analysis for Target: L_ELBOW_ENERGY_BY_TRIAL_EXHAUSTION_SCORE ===\n",
      "Permutation Top 10: ['exhaustion_lag1', 'ema_exhaustion', 'rolling_hr_mean', 'joint_power', 'rolling_exhaustion', 'L_SHOULDER_ROM', 'simulated_HR', 'hip_asymmetry', '5thfinger_power_ratio', 'wrist_power_ratio']\n",
      "RFE Selected Features: ['joint_energy', 'joint_power', 'exhaustion_lag1', 'rolling_hr_mean', 'ema_exhaustion']\n",
      "SHAP Top 10: ['exhaustion_lag1', 'joint_power', 'ema_exhaustion', 'rolling_hr_mean', 'joint_energy', 'simulated_HR', '5thfinger_power_ratio', 'L_SHOULDER_ROM', 'rolling_exhaustion', 'power_avg_5']\n",
      "Consensus Top 10: ['exhaustion_lag1', 'ema_exhaustion', 'joint_power', 'rolling_hr_mean', 'simulated_HR', '5thfinger_power_ratio', 'rolling_exhaustion', 'L_SHOULDER_ROM', 'wrist_power_ratio', '1stfinger_power_ratio']\n",
      "\n",
      "=== Feature Analysis for Target: R_ELBOW_ENERGY_BY_TRIAL_EXHAUSTION_SCORE ===\n",
      "Permutation Top 10: ['rolling_hr_mean', 'exhaustion_lag1', 'joint_power', 'ema_exhaustion', 'elbow_power_ratio', 'hip_power_ratio', '5thfinger_power_ratio', 'energy_acceleration', '1stfinger_power_ratio', '5thfinger_asymmetry']\n",
      "RFE Selected Features: ['joint_energy', 'joint_power', '1stfinger_power_ratio', 'exhaustion_lag1', 'rolling_hr_mean']\n",
      "SHAP Top 10: ['joint_power', 'rolling_hr_mean', 'joint_energy', 'exhaustion_lag1', 'ema_exhaustion', 'elbow_power_ratio', '1stfinger_power_ratio', '5thfinger_power_ratio', 'energy_acceleration', 'simulated_HR']\n",
      "Consensus Top 10: ['rolling_hr_mean', 'joint_power', 'exhaustion_lag1', 'ema_exhaustion', 'elbow_power_ratio', '1stfinger_power_ratio', '5thfinger_power_ratio', 'energy_acceleration', 'wrist_power_ratio', 'simulated_HR']\n",
      "\n",
      "=== Feature Analysis for Target: L_KNEE_ENERGY_BY_TRIAL_EXHAUSTION_SCORE ===\n",
      "Permutation Top 10: ['ema_exhaustion', 'exhaustion_lag1', 'rolling_hr_mean', 'joint_power', 'L_SHOULDER_ROM', 'energy_acceleration', 'simulated_HR', 'R_SHOULDER_ROM', 'L_ANKLE_ROM', '5thfinger_power_ratio']\n",
      "RFE Selected Features: ['joint_energy', 'joint_power', 'exhaustion_lag1', 'rolling_hr_mean', 'ema_exhaustion']\n",
      "SHAP Top 10: ['joint_power', 'rolling_hr_mean', 'ema_exhaustion', 'exhaustion_lag1', 'joint_energy', 'time_since_start', 'simulated_HR', '5thfinger_power_ratio', 'rolling_exhaustion', '5thfinger_asymmetry']\n",
      "Consensus Top 10: ['ema_exhaustion', 'joint_power', 'rolling_hr_mean', 'exhaustion_lag1', 'simulated_HR', 'energy_acceleration', 'L_SHOULDER_ROM', '5thfinger_power_ratio', 'elbow_power_ratio', 'time_since_start']\n",
      "\n",
      "=== Feature Analysis for Target: R_KNEE_ENERGY_BY_TRIAL_EXHAUSTION_SCORE ===\n",
      "Permutation Top 10: ['ema_exhaustion', 'exhaustion_lag1', 'rolling_hr_mean', 'simulated_HR', 'R_KNEE_ROM', 'energy_acceleration', 'R_SHOULDER_ROM', 'R_KNEE_ROM_deviation', 'elbow_power_ratio', 'wrist_power_ratio']\n",
      "RFE Selected Features: ['joint_energy', 'joint_power', 'exhaustion_lag1', 'rolling_hr_mean', 'ema_exhaustion']\n",
      "SHAP Top 10: ['joint_power', 'ema_exhaustion', 'rolling_hr_mean', 'joint_energy', 'exhaustion_lag1', 'simulated_HR', 'time_since_start', 'L_SHOULDER_ROM', 'wrist_power_ratio', '5thfinger_power_ratio']\n",
      "Consensus Top 10: ['ema_exhaustion', 'rolling_hr_mean', 'exhaustion_lag1', 'simulated_HR', 'energy_acceleration', 'wrist_power_ratio', 'L_SHOULDER_ROM', 'elbow_power_ratio', 'power_avg_5', 'L_KNEE_ROM_deviation']\n",
      "\n",
      "=== Feature Analysis for Target: L_HIP_ENERGY_BY_TRIAL_EXHAUSTION_SCORE ===\n",
      "Permutation Top 10: ['exhaustion_lag1', 'ema_exhaustion', 'rolling_hr_mean', 'R_SHOULDER_ROM', 'simulated_HR', 'power_avg_5', '1stfinger_power_ratio', 'energy_acceleration', 'L_SHOULDER_ROM', 'rolling_energy_std']\n",
      "RFE Selected Features: ['joint_energy', 'joint_power', 'exhaustion_lag1', 'rolling_hr_mean', 'ema_exhaustion']\n",
      "SHAP Top 10: ['exhaustion_lag1', 'ema_exhaustion', 'joint_power', 'rolling_hr_mean', 'joint_energy', 'time_since_start', 'simulated_HR', 'energy_acceleration', 'elbow_power_ratio', 'wrist_power_ratio']\n",
      "Consensus Top 10: ['exhaustion_lag1', 'ema_exhaustion', 'rolling_hr_mean', 'simulated_HR', 'energy_acceleration', 'R_SHOULDER_ROM', 'elbow_power_ratio', '1stfinger_power_ratio', 'L_SHOULDER_ROM', 'time_since_start']\n",
      "\n",
      "=== Feature Analysis for Target: R_HIP_ENERGY_BY_TRIAL_EXHAUSTION_SCORE ===\n",
      "Permutation Top 10: ['exhaustion_lag1', 'ema_exhaustion', 'rolling_hr_mean', 'R_SHOULDER_ROM', 'joint_power', 'power_avg_5', 'simulated_HR', 'R_KNEE_ROM', 'energy_acceleration', '1stfinger_power_ratio']\n",
      "RFE Selected Features: ['joint_energy', 'joint_power', 'exhaustion_lag1', 'rolling_hr_mean', 'ema_exhaustion']\n",
      "SHAP Top 10: ['exhaustion_lag1', 'joint_power', 'ema_exhaustion', 'rolling_hr_mean', 'joint_energy', 'simulated_HR', 'time_since_start', 'L_SHOULDER_ROM', 'energy_acceleration', '5thfinger_power_ratio']\n",
      "Consensus Top 10: ['exhaustion_lag1', 'ema_exhaustion', 'joint_power', 'rolling_hr_mean', 'simulated_HR', 'energy_acceleration', '1stfinger_power_ratio', '5thfinger_power_ratio', 'hip_asymmetry', 'wrist_power_ratio']\n",
      "Base Loaded Features: {'injury_risk': ['rolling_exhaustion', 'ema_exhaustion', 'knee_power_ratio', 'exhaustion_lag1', 'hip_asymmetry', 'power_avg_5', 'time_since_start', 'knee_asymmetry', 'L_SHOULDER_ROM', 'elbow_power_ratio'], 'by_trial_exhaustion_score': ['exhaustion_lag1', 'joint_power', 'rolling_hr_mean', 'ema_exhaustion', 'simulated_HR', '5thfinger_power_ratio', 'wrist_power_ratio', '1stfinger_power_ratio', 'power_avg_5', 'hip_power_ratio'], 'L_ANKLE_injury_risk': ['rolling_exhaustion', 'R_SHOULDER_ROM', 'L_HIP_ROM', 'L_ANKLE_ROM', 'time_since_start', 'wrist_power_ratio', 'L_ANKLE_ROM_deviation', '5thfinger_power_ratio', 'joint_power', 'R_HIP_ROM'], 'R_ANKLE_injury_risk': ['rolling_exhaustion', 'exhaustion_lag1', 'L_SHOULDER_ROM', 'joint_energy', 'R_SHOULDER_ROM', 'L_KNEE_ROM_deviation', 'joint_power', 'L_HIP_ROM', 'L_KNEE_ROM', 'rolling_hr_mean'], 'L_WRIST_injury_risk': ['rolling_exhaustion', 'exhaustion_lag1', 'ema_exhaustion', 'power_avg_5', 'L_ANKLE_ROM_deviation', 'rolling_hr_mean', 'L_ANKLE_ROM', 'joint_energy', 'energy_acceleration', 'R_SHOULDER_ROM'], 'R_WRIST_injury_risk': ['rolling_exhaustion', 'wrist_power_ratio', '5thfinger_power_ratio', 'elbow_power_ratio', 'exhaustion_lag1', 'joint_power', 'R_HIP_ROM', '1stfinger_power_ratio', 'rolling_hr_mean', '5thfinger_asymmetry'], 'L_ELBOW_injury_risk': ['rolling_exhaustion', 'exhaustion_lag1', 'ema_exhaustion', '5thfinger_asymmetry', 'rolling_hr_mean', 'energy_acceleration', 'elbow_power_ratio', 'rolling_energy_std', 'R_ANKLE_ROM', 'time_since_start'], 'R_ELBOW_injury_risk': ['rolling_exhaustion', 'wrist_power_ratio', 'joint_power', 'elbow_power_ratio', '5thfinger_power_ratio', 'energy_acceleration', 'R_SHOULDER_ROM', 'joint_energy', 'exhaustion_lag1', 'knee_power_ratio'], 'L_KNEE_injury_risk': ['rolling_exhaustion', 'L_ANKLE_ROM', 'R_HIP_ROM', 'ema_exhaustion', 'L_HIP_ROM', 'L_ANKLE_ROM_deviation', 'rolling_hr_mean', 'R_KNEE_ROM_deviation', 'time_since_start', 'L_KNEE_ROM_deviation'], 'R_KNEE_injury_risk': ['rolling_exhaustion', 'exhaustion_lag1', 'R_HIP_ROM', 'knee_power_ratio', 'R_KNEE_ROM', 'rolling_power_std', '5thfinger_power_ratio', 'rolling_hr_mean', 'R_KNEE_ROM_deviation', 'wrist_power_ratio'], 'L_HIP_injury_risk': ['rolling_exhaustion', 'wrist_power_ratio', 'ema_exhaustion', 'L_ANKLE_ROM', 'L_KNEE_ROM_deviation', 'exhaustion_lag1', 'hip_asymmetry', 'time_since_start', 'L_KNEE_ROM', 'ankle_asymmetry'], 'R_HIP_injury_risk': ['rolling_exhaustion', 'exhaustion_lag1', 'ema_exhaustion', 'wrist_power_ratio', 'L_KNEE_ROM_deviation', 'R_SHOULDER_ROM', 'time_since_start', 'rolling_hr_mean', 'L_SHOULDER_ROM', 'hip_asymmetry'], 'L_ANKLE_energy_by_trial_exhaustion_score': ['joint_power', 'ema_exhaustion', 'exhaustion_lag1', 'rolling_hr_mean', 'rolling_exhaustion', 'R_SHOULDER_ROM', 'simulated_HR', '1stfinger_asymmetry', '5thfinger_power_ratio', 'L_SHOULDER_ROM'], 'R_ANKLE_energy_by_trial_exhaustion_score': ['ema_exhaustion', 'exhaustion_lag1', 'joint_power', 'rolling_exhaustion', 'rolling_hr_mean', 'R_KNEE_ROM_deviation', 'L_ANKLE_ROM_deviation', 'wrist_asymmetry', 'R_ANKLE_ROM_deviation', 'time_since_start'], 'L_WRIST_energy_by_trial_exhaustion_score': ['exhaustion_lag1', 'ema_exhaustion', 'rolling_hr_mean', 'L_SHOULDER_ROM', '1stfinger_power_ratio', 'simulated_HR', 'energy_acceleration', 'rolling_exhaustion', 'wrist_power_ratio', 'L_HIP_ROM'], 'R_WRIST_energy_by_trial_exhaustion_score': ['rolling_hr_mean', 'joint_power', 'exhaustion_lag1', 'ema_exhaustion', 'wrist_power_ratio', 'elbow_asymmetry', '5thfinger_power_ratio', '1stfinger_power_ratio', 'simulated_HR', 'energy_acceleration'], 'L_ELBOW_energy_by_trial_exhaustion_score': ['exhaustion_lag1', 'ema_exhaustion', 'joint_power', 'rolling_hr_mean', 'simulated_HR', '5thfinger_power_ratio', 'rolling_exhaustion', 'L_SHOULDER_ROM', 'wrist_power_ratio', '1stfinger_power_ratio'], 'R_ELBOW_energy_by_trial_exhaustion_score': ['rolling_hr_mean', 'joint_power', 'exhaustion_lag1', 'ema_exhaustion', '5thfinger_power_ratio', 'elbow_power_ratio', '1stfinger_power_ratio', 'energy_acceleration', 'simulated_HR', 'wrist_power_ratio'], 'L_KNEE_energy_by_trial_exhaustion_score': ['ema_exhaustion', 'joint_power', 'rolling_hr_mean', 'exhaustion_lag1', 'simulated_HR', 'energy_acceleration', 'L_SHOULDER_ROM', '5thfinger_power_ratio', 'elbow_power_ratio', 'time_since_start'], 'R_KNEE_energy_by_trial_exhaustion_score': ['ema_exhaustion', 'rolling_hr_mean', 'exhaustion_lag1', 'simulated_HR', 'energy_acceleration', 'wrist_power_ratio', 'L_SHOULDER_ROM', 'elbow_power_ratio', 'power_avg_5', 'L_KNEE_ROM_deviation'], 'L_HIP_energy_by_trial_exhaustion_score': ['exhaustion_lag1', 'ema_exhaustion', 'rolling_hr_mean', 'simulated_HR', 'energy_acceleration', 'R_SHOULDER_ROM', 'elbow_power_ratio', '1stfinger_power_ratio', 'L_SHOULDER_ROM', 'time_since_start'], 'R_HIP_energy_by_trial_exhaustion_score': ['exhaustion_lag1', 'ema_exhaustion', 'joint_power', 'rolling_hr_mean', 'simulated_HR', 'energy_acceleration', '1stfinger_power_ratio', '5thfinger_power_ratio', 'hip_asymmetry', 'wrist_power_ratio']}\n",
      "Epoch 1/200\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6588 - val_loss: 1.7124\n",
      "Epoch 2/200\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4193 - val_loss: 1.7322\n",
      "Epoch 3/200\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3536 - val_loss: 1.7657\n",
      "Epoch 4/200\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3174 - val_loss: 1.7898\n",
      "Epoch 5/200\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3233 - val_loss: 1.6840\n",
      "Epoch 6/200\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2702 - val_loss: 1.5799\n",
      "Epoch 7/200\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2764 - val_loss: 1.4287\n",
      "Epoch 8/200\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2629 - val_loss: 1.1578\n",
      "Epoch 9/200\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2288 - val_loss: 1.1322\n",
      "Epoch 10/200\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2313 - val_loss: 1.1036\n",
      "Epoch 11/200\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2332 - val_loss: 0.9869\n",
      "Epoch 12/200\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2212 - val_loss: 0.9527\n",
      "Epoch 13/200\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2069 - val_loss: 0.8828\n",
      "Epoch 14/200\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2285 - val_loss: 0.9156\n",
      "Epoch 15/200\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2114 - val_loss: 0.9154\n",
      "Epoch 16/200\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2049 - val_loss: 0.8078\n",
      "Epoch 17/200\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2130 - val_loss: 0.9245\n",
      "Epoch 18/200\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2214 - val_loss: 0.7794\n",
      "Epoch 19/200\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2130 - val_loss: 0.7830\n",
      "Epoch 20/200\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2003 - val_loss: 0.7707\n",
      "Epoch 21/200\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1824 - val_loss: 0.7811\n",
      "Epoch 22/200\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2014 - val_loss: 0.7533\n",
      "Epoch 23/200\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2080 - val_loss: 0.7534\n",
      "Epoch 24/200\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1827 - val_loss: 0.8080\n",
      "Epoch 25/200\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2068 - val_loss: 0.7449\n",
      "Epoch 26/200\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1795 - val_loss: 0.7328\n",
      "Epoch 27/200\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1909 - val_loss: 0.7398\n",
      "Epoch 28/200\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1723 - val_loss: 0.8091\n",
      "Epoch 29/200\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1835 - val_loss: 0.7764\n",
      "Epoch 30/200\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1639 - val_loss: 0.7188\n",
      "Epoch 31/200\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1738 - val_loss: 0.7355\n",
      "Epoch 32/200\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1959 - val_loss: 0.7275\n",
      "Epoch 33/200\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1636 - val_loss: 0.7209\n",
      "Epoch 34/200\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1462 - val_loss: 0.7343\n",
      "Epoch 35/200\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1587 - val_loss: 0.8005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Features provided for training injury model: ['joint_power', 'joint_energy', 'elbow_asymmetry', 'knee_asymmetry', 'L_WRIST_angle', 'R_WRIST_angle', 'exhaustion_lag1', 'power_avg_5', 'simulated_HR', 'player_height_in_meters', 'player_weight__in_kg']\n",
      "INFO: Available train_data columns: ['trial_id', 'result', 'landing_x', 'landing_y', 'entry_angle', 'frame_time', 'ball_x', 'ball_y', 'ball_z', 'R_EYE_x', 'R_EYE_y', 'R_EYE_z', 'L_EYE_x', 'L_EYE_y', 'L_EYE_z', 'NOSE_x', 'NOSE_y', 'NOSE_z', 'R_EAR_x', 'R_EAR_y', 'R_EAR_z', 'L_EAR_x', 'L_EAR_y', 'L_EAR_z', 'R_SHOULDER_x', 'R_SHOULDER_y', 'R_SHOULDER_z', 'L_SHOULDER_x', 'L_SHOULDER_y', 'L_SHOULDER_z', 'R_ELBOW_x', 'R_ELBOW_y', 'R_ELBOW_z', 'L_ELBOW_x', 'L_ELBOW_y', 'L_ELBOW_z', 'R_WRIST_x', 'R_WRIST_y', 'R_WRIST_z', 'L_WRIST_x', 'L_WRIST_y', 'L_WRIST_z', 'R_HIP_x', 'R_HIP_y', 'R_HIP_z', 'L_HIP_x', 'L_HIP_y', 'L_HIP_z', 'R_KNEE_x', 'R_KNEE_y', 'R_KNEE_z', 'L_KNEE_x', 'L_KNEE_y', 'L_KNEE_z', 'R_ANKLE_x', 'R_ANKLE_y', 'R_ANKLE_z', 'L_ANKLE_x', 'L_ANKLE_y', 'L_ANKLE_z', 'R_1STFINGER_x', 'R_1STFINGER_y', 'R_1STFINGER_z', 'R_5THFINGER_x', 'R_5THFINGER_y', 'R_5THFINGER_z', 'L_1STFINGER_x', 'L_1STFINGER_y', 'L_1STFINGER_z', 'L_5THFINGER_x', 'L_5THFINGER_y', 'L_5THFINGER_z', 'R_1STTOE_x', 'R_1STTOE_y', 'R_1STTOE_z', 'R_5THTOE_x', 'R_5THTOE_y', 'R_5THTOE_z', 'L_1STTOE_x', 'L_1STTOE_y', 'L_1STTOE_z', 'L_5THTOE_x', 'L_5THTOE_y', 'L_5THTOE_z', 'R_CALC_x', 'R_CALC_y', 'R_CALC_z', 'L_CALC_x', 'L_CALC_y', 'L_CALC_z', 'ball_speed', 'ball_velocity_x', 'ball_velocity_y', 'ball_velocity_z', 'overall_ball_velocity', 'ball_direction_x', 'ball_direction_y', 'ball_direction_z', 'computed_ball_velocity_x', 'computed_ball_velocity_y', 'computed_ball_velocity_z', 'dist_ball_R_1STFINGER', 'dist_ball_R_5THFINGER', 'dist_ball_L_1STFINGER', 'dist_ball_L_5THFINGER', 'ball_in_hands', 'shooting_motion', 'avg_shoulder_height', 'release_point_filter', 'dt', 'dx', 'dy', 'dz', 'L_ANKLE_ongoing_power', 'R_ANKLE_ongoing_power', 'L_KNEE_ongoing_power', 'R_KNEE_ongoing_power', 'L_HIP_ongoing_power', 'R_HIP_ongoing_power', 'L_ELBOW_ongoing_power', 'R_ELBOW_ongoing_power', 'L_WRIST_ongoing_power', 'R_WRIST_ongoing_power', 'L_1STFINGER_ongoing_power', 'L_5THFINGER_ongoing_power', 'R_1STFINGER_ongoing_power', 'R_5THFINGER_ongoing_power', 'L_ELBOW_angle', 'L_WRIST_angle', 'L_KNEE_angle', 'L_ELBOW_ongoing_angle', 'L_WRIST_ongoing_angle', 'L_KNEE_ongoing_angle', 'R_ELBOW_angle', 'R_WRIST_angle', 'R_KNEE_angle', 'R_ELBOW_ongoing_angle', 'R_WRIST_ongoing_angle', 'R_KNEE_ongoing_angle', 'shooting_phases', 'player_height_in_meters', 'player_height_ft', 'initial_release_angle', 'calculated_release_angle', 'angle_difference', 'distance_to_basket', 'optimal_release_angle', 'by_trial_time', 'continuous_frame_time', 'L_ANKLE_energy', 'R_ANKLE_energy', 'L_KNEE_energy', 'R_KNEE_energy', 'L_HIP_energy', 'R_HIP_energy', 'L_ELBOW_energy', 'R_ELBOW_energy', 'L_WRIST_energy', 'R_WRIST_energy', 'L_1STFINGER_energy', 'R_1STFINGER_energy', 'L_5THFINGER_energy', 'R_5THFINGER_energy', 'total_energy', 'by_trial_energy', 'by_trial_exhaustion_score', 'overall_cumulative_energy', 'overall_exhaustion_score', 'L_ANKLE_energy_by_trial', 'L_ANKLE_energy_by_trial_exhaustion_score', 'L_ANKLE_energy_overall_cumulative', 'L_ANKLE_energy_overall_exhaustion_score', 'R_ANKLE_energy_by_trial', 'R_ANKLE_energy_by_trial_exhaustion_score', 'R_ANKLE_energy_overall_cumulative', 'R_ANKLE_energy_overall_exhaustion_score', 'L_KNEE_energy_by_trial', 'L_KNEE_energy_by_trial_exhaustion_score', 'L_KNEE_energy_overall_cumulative', 'L_KNEE_energy_overall_exhaustion_score', 'R_KNEE_energy_by_trial', 'R_KNEE_energy_by_trial_exhaustion_score', 'R_KNEE_energy_overall_cumulative', 'R_KNEE_energy_overall_exhaustion_score', 'L_HIP_energy_by_trial', 'L_HIP_energy_by_trial_exhaustion_score', 'L_HIP_energy_overall_cumulative', 'L_HIP_energy_overall_exhaustion_score', 'R_HIP_energy_by_trial', 'R_HIP_energy_by_trial_exhaustion_score', 'R_HIP_energy_overall_cumulative', 'R_HIP_energy_overall_exhaustion_score', 'L_ELBOW_energy_by_trial', 'L_ELBOW_energy_by_trial_exhaustion_score', 'L_ELBOW_energy_overall_cumulative', 'L_ELBOW_energy_overall_exhaustion_score', 'R_ELBOW_energy_by_trial', 'R_ELBOW_energy_by_trial_exhaustion_score', 'R_ELBOW_energy_overall_cumulative', 'R_ELBOW_energy_overall_exhaustion_score', 'L_WRIST_energy_by_trial', 'L_WRIST_energy_by_trial_exhaustion_score', 'L_WRIST_energy_overall_cumulative', 'L_WRIST_energy_overall_exhaustion_score', 'R_WRIST_energy_by_trial', 'R_WRIST_energy_by_trial_exhaustion_score', 'R_WRIST_energy_overall_cumulative', 'R_WRIST_energy_overall_exhaustion_score', 'L_1STFINGER_energy_by_trial', 'L_1STFINGER_energy_by_trial_exhaustion_score', 'L_1STFINGER_energy_overall_cumulative', 'L_1STFINGER_energy_overall_exhaustion_score', 'R_1STFINGER_energy_by_trial', 'R_1STFINGER_energy_by_trial_exhaustion_score', 'R_1STFINGER_energy_overall_cumulative', 'R_1STFINGER_energy_overall_exhaustion_score', 'L_5THFINGER_energy_by_trial', 'L_5THFINGER_energy_by_trial_exhaustion_score', 'L_5THFINGER_energy_overall_cumulative', 'L_5THFINGER_energy_overall_exhaustion_score', 'R_5THFINGER_energy_by_trial', 'R_5THFINGER_energy_by_trial_exhaustion_score', 'R_5THFINGER_energy_overall_cumulative', 'R_5THFINGER_energy_overall_exhaustion_score', 'participant_id', 'L_SHOULDER_angle', 'R_SHOULDER_angle', 'L_HIP_angle', 'R_HIP_angle', 'L_ANKLE_angle', 'R_ANKLE_angle', 'datetime', 'player_weight__in_kg', 'joint_energy', 'joint_power', 'energy_acceleration', 'ankle_power_ratio', 'hip_asymmetry', 'ankle_asymmetry', 'wrist_asymmetry', 'elbow_asymmetry', 'knee_asymmetry', '1stfinger_asymmetry', '5thfinger_asymmetry', 'hip_power_ratio', 'wrist_power_ratio', 'elbow_power_ratio', 'knee_power_ratio', '1stfinger_power_ratio', '5thfinger_power_ratio', 'L_KNEE_ROM', 'L_KNEE_ROM_deviation', 'L_KNEE_ROM_extreme', 'R_KNEE_ROM', 'R_KNEE_ROM_deviation', 'R_KNEE_ROM_extreme', 'L_SHOULDER_ROM', 'L_SHOULDER_ROM_deviation', 'L_SHOULDER_ROM_extreme', 'R_SHOULDER_ROM', 'R_SHOULDER_ROM_deviation', 'R_SHOULDER_ROM_extreme', 'L_HIP_ROM', 'L_HIP_ROM_deviation', 'L_HIP_ROM_extreme', 'R_HIP_ROM', 'R_HIP_ROM_deviation', 'R_HIP_ROM_extreme', 'L_ANKLE_ROM', 'L_ANKLE_ROM_deviation', 'L_ANKLE_ROM_extreme', 'R_ANKLE_ROM', 'R_ANKLE_ROM_deviation', 'R_ANKLE_ROM_extreme', 'L_WRIST_ROM', 'L_WRIST_ROM_deviation', 'L_WRIST_ROM_extreme', 'R_WRIST_ROM', 'R_WRIST_ROM_deviation', 'R_WRIST_ROM_extreme', 'exhaustion_rate', 'simulated_HR', 'time_since_start', 'power_avg_5', 'rolling_power_std', 'rolling_hr_mean', 'rolling_energy_std', 'exhaustion_lag1', 'ema_exhaustion', 'rolling_exhaustion', 'injury_risk', 'L_ANKLE_exhaustion_rate', 'L_ANKLE_rolling_exhaustion', 'L_ANKLE_injury_risk', 'R_ANKLE_exhaustion_rate', 'R_ANKLE_rolling_exhaustion', 'R_ANKLE_injury_risk', 'L_WRIST_exhaustion_rate', 'L_WRIST_rolling_exhaustion', 'L_WRIST_injury_risk', 'R_WRIST_exhaustion_rate', 'R_WRIST_rolling_exhaustion', 'R_WRIST_injury_risk', 'L_ELBOW_exhaustion_rate', 'L_ELBOW_rolling_exhaustion', 'L_ELBOW_injury_risk', 'R_ELBOW_exhaustion_rate', 'R_ELBOW_rolling_exhaustion', 'R_ELBOW_injury_risk', 'L_KNEE_exhaustion_rate', 'L_KNEE_rolling_exhaustion', 'L_KNEE_injury_risk', 'R_KNEE_exhaustion_rate', 'R_KNEE_rolling_exhaustion', 'R_KNEE_injury_risk', 'L_HIP_exhaustion_rate', 'L_HIP_rolling_exhaustion', 'L_HIP_injury_risk', 'R_HIP_exhaustion_rate', 'R_HIP_rolling_exhaustion', 'R_HIP_injury_risk']\n",
      "INFO: Features have been scaled using StandardScaler.\n",
      "INFO: Created LSTM sequences: (2359, 5, 11), (2359,)\n",
      "INFO: Created LSTM sequences: (587, 5, 11), (587,)\n",
      "INFO: Training injury risk model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 5ms/step - accuracy: 0.6902 - loss: 0.5914 - val_accuracy: 0.8279 - val_loss: 0.4220\n",
      "Epoch 2/200\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7867 - loss: 0.4492 - val_accuracy: 0.8620 - val_loss: 0.3333\n",
      "Epoch 3/200\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8029 - loss: 0.4207 - val_accuracy: 0.8739 - val_loss: 0.3094\n",
      "Epoch 4/200\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8317 - loss: 0.3741 - val_accuracy: 0.8756 - val_loss: 0.2957\n",
      "Epoch 5/200\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8347 - loss: 0.3675 - val_accuracy: 0.8688 - val_loss: 0.2961\n",
      "Epoch 6/200\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8334 - loss: 0.3548 - val_accuracy: 0.8825 - val_loss: 0.2814\n",
      "Epoch 7/200\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8432 - loss: 0.3376 - val_accuracy: 0.8859 - val_loss: 0.2636\n",
      "Epoch 8/200\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8408 - loss: 0.3393 - val_accuracy: 0.8842 - val_loss: 0.2612\n",
      "Epoch 9/200\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8567 - loss: 0.3177 - val_accuracy: 0.8944 - val_loss: 0.2512\n",
      "Epoch 10/200\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8497 - loss: 0.3447 - val_accuracy: 0.8790 - val_loss: 0.2696\n",
      "Epoch 11/200\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8596 - loss: 0.3056 - val_accuracy: 0.8825 - val_loss: 0.2521\n",
      "Epoch 12/200\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8644 - loss: 0.3203 - val_accuracy: 0.8637 - val_loss: 0.2666\n",
      "Epoch 13/200\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8584 - loss: 0.3233 - val_accuracy: 0.8586 - val_loss: 0.2898\n",
      "Epoch 14/200\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8548 - loss: 0.3231 - val_accuracy: 0.8756 - val_loss: 0.2617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Training joint-specific injury model for L_ANKLE_injury_risk using features: ['rolling_exhaustion', 'R_SHOULDER_ROM', 'L_HIP_ROM', 'L_ANKLE_ROM', 'time_since_start', 'wrist_power_ratio', 'L_ANKLE_ROM_deviation', '5thfinger_power_ratio', 'joint_power', 'R_HIP_ROM']\n",
      "INFO: Features provided for training injury model: ['rolling_exhaustion', 'R_SHOULDER_ROM', 'L_HIP_ROM', 'L_ANKLE_ROM', 'time_since_start', 'wrist_power_ratio', 'L_ANKLE_ROM_deviation', '5thfinger_power_ratio', 'joint_power', 'R_HIP_ROM']\n",
      "INFO: Available train_data columns: ['trial_id', 'result', 'landing_x', 'landing_y', 'entry_angle', 'frame_time', 'ball_x', 'ball_y', 'ball_z', 'R_EYE_x', 'R_EYE_y', 'R_EYE_z', 'L_EYE_x', 'L_EYE_y', 'L_EYE_z', 'NOSE_x', 'NOSE_y', 'NOSE_z', 'R_EAR_x', 'R_EAR_y', 'R_EAR_z', 'L_EAR_x', 'L_EAR_y', 'L_EAR_z', 'R_SHOULDER_x', 'R_SHOULDER_y', 'R_SHOULDER_z', 'L_SHOULDER_x', 'L_SHOULDER_y', 'L_SHOULDER_z', 'R_ELBOW_x', 'R_ELBOW_y', 'R_ELBOW_z', 'L_ELBOW_x', 'L_ELBOW_y', 'L_ELBOW_z', 'R_WRIST_x', 'R_WRIST_y', 'R_WRIST_z', 'L_WRIST_x', 'L_WRIST_y', 'L_WRIST_z', 'R_HIP_x', 'R_HIP_y', 'R_HIP_z', 'L_HIP_x', 'L_HIP_y', 'L_HIP_z', 'R_KNEE_x', 'R_KNEE_y', 'R_KNEE_z', 'L_KNEE_x', 'L_KNEE_y', 'L_KNEE_z', 'R_ANKLE_x', 'R_ANKLE_y', 'R_ANKLE_z', 'L_ANKLE_x', 'L_ANKLE_y', 'L_ANKLE_z', 'R_1STFINGER_x', 'R_1STFINGER_y', 'R_1STFINGER_z', 'R_5THFINGER_x', 'R_5THFINGER_y', 'R_5THFINGER_z', 'L_1STFINGER_x', 'L_1STFINGER_y', 'L_1STFINGER_z', 'L_5THFINGER_x', 'L_5THFINGER_y', 'L_5THFINGER_z', 'R_1STTOE_x', 'R_1STTOE_y', 'R_1STTOE_z', 'R_5THTOE_x', 'R_5THTOE_y', 'R_5THTOE_z', 'L_1STTOE_x', 'L_1STTOE_y', 'L_1STTOE_z', 'L_5THTOE_x', 'L_5THTOE_y', 'L_5THTOE_z', 'R_CALC_x', 'R_CALC_y', 'R_CALC_z', 'L_CALC_x', 'L_CALC_y', 'L_CALC_z', 'ball_speed', 'ball_velocity_x', 'ball_velocity_y', 'ball_velocity_z', 'overall_ball_velocity', 'ball_direction_x', 'ball_direction_y', 'ball_direction_z', 'computed_ball_velocity_x', 'computed_ball_velocity_y', 'computed_ball_velocity_z', 'dist_ball_R_1STFINGER', 'dist_ball_R_5THFINGER', 'dist_ball_L_1STFINGER', 'dist_ball_L_5THFINGER', 'ball_in_hands', 'shooting_motion', 'avg_shoulder_height', 'release_point_filter', 'dt', 'dx', 'dy', 'dz', 'L_ANKLE_ongoing_power', 'R_ANKLE_ongoing_power', 'L_KNEE_ongoing_power', 'R_KNEE_ongoing_power', 'L_HIP_ongoing_power', 'R_HIP_ongoing_power', 'L_ELBOW_ongoing_power', 'R_ELBOW_ongoing_power', 'L_WRIST_ongoing_power', 'R_WRIST_ongoing_power', 'L_1STFINGER_ongoing_power', 'L_5THFINGER_ongoing_power', 'R_1STFINGER_ongoing_power', 'R_5THFINGER_ongoing_power', 'L_ELBOW_angle', 'L_WRIST_angle', 'L_KNEE_angle', 'L_ELBOW_ongoing_angle', 'L_WRIST_ongoing_angle', 'L_KNEE_ongoing_angle', 'R_ELBOW_angle', 'R_WRIST_angle', 'R_KNEE_angle', 'R_ELBOW_ongoing_angle', 'R_WRIST_ongoing_angle', 'R_KNEE_ongoing_angle', 'shooting_phases', 'player_height_in_meters', 'player_height_ft', 'initial_release_angle', 'calculated_release_angle', 'angle_difference', 'distance_to_basket', 'optimal_release_angle', 'by_trial_time', 'continuous_frame_time', 'L_ANKLE_energy', 'R_ANKLE_energy', 'L_KNEE_energy', 'R_KNEE_energy', 'L_HIP_energy', 'R_HIP_energy', 'L_ELBOW_energy', 'R_ELBOW_energy', 'L_WRIST_energy', 'R_WRIST_energy', 'L_1STFINGER_energy', 'R_1STFINGER_energy', 'L_5THFINGER_energy', 'R_5THFINGER_energy', 'total_energy', 'by_trial_energy', 'by_trial_exhaustion_score', 'overall_cumulative_energy', 'overall_exhaustion_score', 'L_ANKLE_energy_by_trial', 'L_ANKLE_energy_by_trial_exhaustion_score', 'L_ANKLE_energy_overall_cumulative', 'L_ANKLE_energy_overall_exhaustion_score', 'R_ANKLE_energy_by_trial', 'R_ANKLE_energy_by_trial_exhaustion_score', 'R_ANKLE_energy_overall_cumulative', 'R_ANKLE_energy_overall_exhaustion_score', 'L_KNEE_energy_by_trial', 'L_KNEE_energy_by_trial_exhaustion_score', 'L_KNEE_energy_overall_cumulative', 'L_KNEE_energy_overall_exhaustion_score', 'R_KNEE_energy_by_trial', 'R_KNEE_energy_by_trial_exhaustion_score', 'R_KNEE_energy_overall_cumulative', 'R_KNEE_energy_overall_exhaustion_score', 'L_HIP_energy_by_trial', 'L_HIP_energy_by_trial_exhaustion_score', 'L_HIP_energy_overall_cumulative', 'L_HIP_energy_overall_exhaustion_score', 'R_HIP_energy_by_trial', 'R_HIP_energy_by_trial_exhaustion_score', 'R_HIP_energy_overall_cumulative', 'R_HIP_energy_overall_exhaustion_score', 'L_ELBOW_energy_by_trial', 'L_ELBOW_energy_by_trial_exhaustion_score', 'L_ELBOW_energy_overall_cumulative', 'L_ELBOW_energy_overall_exhaustion_score', 'R_ELBOW_energy_by_trial', 'R_ELBOW_energy_by_trial_exhaustion_score', 'R_ELBOW_energy_overall_cumulative', 'R_ELBOW_energy_overall_exhaustion_score', 'L_WRIST_energy_by_trial', 'L_WRIST_energy_by_trial_exhaustion_score', 'L_WRIST_energy_overall_cumulative', 'L_WRIST_energy_overall_exhaustion_score', 'R_WRIST_energy_by_trial', 'R_WRIST_energy_by_trial_exhaustion_score', 'R_WRIST_energy_overall_cumulative', 'R_WRIST_energy_overall_exhaustion_score', 'L_1STFINGER_energy_by_trial', 'L_1STFINGER_energy_by_trial_exhaustion_score', 'L_1STFINGER_energy_overall_cumulative', 'L_1STFINGER_energy_overall_exhaustion_score', 'R_1STFINGER_energy_by_trial', 'R_1STFINGER_energy_by_trial_exhaustion_score', 'R_1STFINGER_energy_overall_cumulative', 'R_1STFINGER_energy_overall_exhaustion_score', 'L_5THFINGER_energy_by_trial', 'L_5THFINGER_energy_by_trial_exhaustion_score', 'L_5THFINGER_energy_overall_cumulative', 'L_5THFINGER_energy_overall_exhaustion_score', 'R_5THFINGER_energy_by_trial', 'R_5THFINGER_energy_by_trial_exhaustion_score', 'R_5THFINGER_energy_overall_cumulative', 'R_5THFINGER_energy_overall_exhaustion_score', 'participant_id', 'L_SHOULDER_angle', 'R_SHOULDER_angle', 'L_HIP_angle', 'R_HIP_angle', 'L_ANKLE_angle', 'R_ANKLE_angle', 'datetime', 'player_weight__in_kg', 'joint_energy', 'joint_power', 'energy_acceleration', 'ankle_power_ratio', 'hip_asymmetry', 'ankle_asymmetry', 'wrist_asymmetry', 'elbow_asymmetry', 'knee_asymmetry', '1stfinger_asymmetry', '5thfinger_asymmetry', 'hip_power_ratio', 'wrist_power_ratio', 'elbow_power_ratio', 'knee_power_ratio', '1stfinger_power_ratio', '5thfinger_power_ratio', 'L_KNEE_ROM', 'L_KNEE_ROM_deviation', 'L_KNEE_ROM_extreme', 'R_KNEE_ROM', 'R_KNEE_ROM_deviation', 'R_KNEE_ROM_extreme', 'L_SHOULDER_ROM', 'L_SHOULDER_ROM_deviation', 'L_SHOULDER_ROM_extreme', 'R_SHOULDER_ROM', 'R_SHOULDER_ROM_deviation', 'R_SHOULDER_ROM_extreme', 'L_HIP_ROM', 'L_HIP_ROM_deviation', 'L_HIP_ROM_extreme', 'R_HIP_ROM', 'R_HIP_ROM_deviation', 'R_HIP_ROM_extreme', 'L_ANKLE_ROM', 'L_ANKLE_ROM_deviation', 'L_ANKLE_ROM_extreme', 'R_ANKLE_ROM', 'R_ANKLE_ROM_deviation', 'R_ANKLE_ROM_extreme', 'L_WRIST_ROM', 'L_WRIST_ROM_deviation', 'L_WRIST_ROM_extreme', 'R_WRIST_ROM', 'R_WRIST_ROM_deviation', 'R_WRIST_ROM_extreme', 'exhaustion_rate', 'simulated_HR', 'time_since_start', 'power_avg_5', 'rolling_power_std', 'rolling_hr_mean', 'rolling_energy_std', 'exhaustion_lag1', 'ema_exhaustion', 'rolling_exhaustion', 'injury_risk', 'L_ANKLE_exhaustion_rate', 'L_ANKLE_rolling_exhaustion', 'L_ANKLE_injury_risk', 'R_ANKLE_exhaustion_rate', 'R_ANKLE_rolling_exhaustion', 'R_ANKLE_injury_risk', 'L_WRIST_exhaustion_rate', 'L_WRIST_rolling_exhaustion', 'L_WRIST_injury_risk', 'R_WRIST_exhaustion_rate', 'R_WRIST_rolling_exhaustion', 'R_WRIST_injury_risk', 'L_ELBOW_exhaustion_rate', 'L_ELBOW_rolling_exhaustion', 'L_ELBOW_injury_risk', 'R_ELBOW_exhaustion_rate', 'R_ELBOW_rolling_exhaustion', 'R_ELBOW_injury_risk', 'L_KNEE_exhaustion_rate', 'L_KNEE_rolling_exhaustion', 'L_KNEE_injury_risk', 'R_KNEE_exhaustion_rate', 'R_KNEE_rolling_exhaustion', 'R_KNEE_injury_risk', 'L_HIP_exhaustion_rate', 'L_HIP_rolling_exhaustion', 'L_HIP_injury_risk', 'R_HIP_exhaustion_rate', 'R_HIP_rolling_exhaustion', 'R_HIP_injury_risk']\n",
      "INFO: Features have been scaled using StandardScaler.\n",
      "INFO: Created LSTM sequences: (2359, 5, 10), (2359,)\n",
      "INFO: Created LSTM sequences: (587, 5, 10), (587,)\n",
      "INFO: Training injury risk model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7639 - loss: 1.4319 - val_accuracy: 0.7853 - val_loss: 0.4504\n",
      "Epoch 2/200\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8096 - loss: 0.4523 - val_accuracy: 0.7666 - val_loss: 0.6584\n",
      "Epoch 3/200\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8220 - loss: 0.4447 - val_accuracy: 0.7206 - val_loss: 1.1985\n",
      "Epoch 4/200\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8166 - loss: 0.4845 - val_accuracy: 0.7394 - val_loss: 1.6518\n",
      "Epoch 5/200\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8414 - loss: 0.4450 - val_accuracy: 0.7462 - val_loss: 1.7212\n",
      "Epoch 6/200\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8585 - loss: 0.4054 - val_accuracy: 0.7700 - val_loss: 0.6526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Successfully trained joint model for L_ANKLE_injury_risk.\n",
      "INFO: Training joint-specific injury model for R_ANKLE_injury_risk using features: ['rolling_exhaustion', 'exhaustion_lag1', 'L_SHOULDER_ROM', 'joint_energy', 'R_SHOULDER_ROM', 'L_KNEE_ROM_deviation', 'joint_power', 'L_HIP_ROM', 'L_KNEE_ROM', 'rolling_hr_mean']\n",
      "INFO: Features provided for training injury model: ['rolling_exhaustion', 'exhaustion_lag1', 'L_SHOULDER_ROM', 'joint_energy', 'R_SHOULDER_ROM', 'L_KNEE_ROM_deviation', 'joint_power', 'L_HIP_ROM', 'L_KNEE_ROM', 'rolling_hr_mean']\n",
      "INFO: Available train_data columns: ['trial_id', 'result', 'landing_x', 'landing_y', 'entry_angle', 'frame_time', 'ball_x', 'ball_y', 'ball_z', 'R_EYE_x', 'R_EYE_y', 'R_EYE_z', 'L_EYE_x', 'L_EYE_y', 'L_EYE_z', 'NOSE_x', 'NOSE_y', 'NOSE_z', 'R_EAR_x', 'R_EAR_y', 'R_EAR_z', 'L_EAR_x', 'L_EAR_y', 'L_EAR_z', 'R_SHOULDER_x', 'R_SHOULDER_y', 'R_SHOULDER_z', 'L_SHOULDER_x', 'L_SHOULDER_y', 'L_SHOULDER_z', 'R_ELBOW_x', 'R_ELBOW_y', 'R_ELBOW_z', 'L_ELBOW_x', 'L_ELBOW_y', 'L_ELBOW_z', 'R_WRIST_x', 'R_WRIST_y', 'R_WRIST_z', 'L_WRIST_x', 'L_WRIST_y', 'L_WRIST_z', 'R_HIP_x', 'R_HIP_y', 'R_HIP_z', 'L_HIP_x', 'L_HIP_y', 'L_HIP_z', 'R_KNEE_x', 'R_KNEE_y', 'R_KNEE_z', 'L_KNEE_x', 'L_KNEE_y', 'L_KNEE_z', 'R_ANKLE_x', 'R_ANKLE_y', 'R_ANKLE_z', 'L_ANKLE_x', 'L_ANKLE_y', 'L_ANKLE_z', 'R_1STFINGER_x', 'R_1STFINGER_y', 'R_1STFINGER_z', 'R_5THFINGER_x', 'R_5THFINGER_y', 'R_5THFINGER_z', 'L_1STFINGER_x', 'L_1STFINGER_y', 'L_1STFINGER_z', 'L_5THFINGER_x', 'L_5THFINGER_y', 'L_5THFINGER_z', 'R_1STTOE_x', 'R_1STTOE_y', 'R_1STTOE_z', 'R_5THTOE_x', 'R_5THTOE_y', 'R_5THTOE_z', 'L_1STTOE_x', 'L_1STTOE_y', 'L_1STTOE_z', 'L_5THTOE_x', 'L_5THTOE_y', 'L_5THTOE_z', 'R_CALC_x', 'R_CALC_y', 'R_CALC_z', 'L_CALC_x', 'L_CALC_y', 'L_CALC_z', 'ball_speed', 'ball_velocity_x', 'ball_velocity_y', 'ball_velocity_z', 'overall_ball_velocity', 'ball_direction_x', 'ball_direction_y', 'ball_direction_z', 'computed_ball_velocity_x', 'computed_ball_velocity_y', 'computed_ball_velocity_z', 'dist_ball_R_1STFINGER', 'dist_ball_R_5THFINGER', 'dist_ball_L_1STFINGER', 'dist_ball_L_5THFINGER', 'ball_in_hands', 'shooting_motion', 'avg_shoulder_height', 'release_point_filter', 'dt', 'dx', 'dy', 'dz', 'L_ANKLE_ongoing_power', 'R_ANKLE_ongoing_power', 'L_KNEE_ongoing_power', 'R_KNEE_ongoing_power', 'L_HIP_ongoing_power', 'R_HIP_ongoing_power', 'L_ELBOW_ongoing_power', 'R_ELBOW_ongoing_power', 'L_WRIST_ongoing_power', 'R_WRIST_ongoing_power', 'L_1STFINGER_ongoing_power', 'L_5THFINGER_ongoing_power', 'R_1STFINGER_ongoing_power', 'R_5THFINGER_ongoing_power', 'L_ELBOW_angle', 'L_WRIST_angle', 'L_KNEE_angle', 'L_ELBOW_ongoing_angle', 'L_WRIST_ongoing_angle', 'L_KNEE_ongoing_angle', 'R_ELBOW_angle', 'R_WRIST_angle', 'R_KNEE_angle', 'R_ELBOW_ongoing_angle', 'R_WRIST_ongoing_angle', 'R_KNEE_ongoing_angle', 'shooting_phases', 'player_height_in_meters', 'player_height_ft', 'initial_release_angle', 'calculated_release_angle', 'angle_difference', 'distance_to_basket', 'optimal_release_angle', 'by_trial_time', 'continuous_frame_time', 'L_ANKLE_energy', 'R_ANKLE_energy', 'L_KNEE_energy', 'R_KNEE_energy', 'L_HIP_energy', 'R_HIP_energy', 'L_ELBOW_energy', 'R_ELBOW_energy', 'L_WRIST_energy', 'R_WRIST_energy', 'L_1STFINGER_energy', 'R_1STFINGER_energy', 'L_5THFINGER_energy', 'R_5THFINGER_energy', 'total_energy', 'by_trial_energy', 'by_trial_exhaustion_score', 'overall_cumulative_energy', 'overall_exhaustion_score', 'L_ANKLE_energy_by_trial', 'L_ANKLE_energy_by_trial_exhaustion_score', 'L_ANKLE_energy_overall_cumulative', 'L_ANKLE_energy_overall_exhaustion_score', 'R_ANKLE_energy_by_trial', 'R_ANKLE_energy_by_trial_exhaustion_score', 'R_ANKLE_energy_overall_cumulative', 'R_ANKLE_energy_overall_exhaustion_score', 'L_KNEE_energy_by_trial', 'L_KNEE_energy_by_trial_exhaustion_score', 'L_KNEE_energy_overall_cumulative', 'L_KNEE_energy_overall_exhaustion_score', 'R_KNEE_energy_by_trial', 'R_KNEE_energy_by_trial_exhaustion_score', 'R_KNEE_energy_overall_cumulative', 'R_KNEE_energy_overall_exhaustion_score', 'L_HIP_energy_by_trial', 'L_HIP_energy_by_trial_exhaustion_score', 'L_HIP_energy_overall_cumulative', 'L_HIP_energy_overall_exhaustion_score', 'R_HIP_energy_by_trial', 'R_HIP_energy_by_trial_exhaustion_score', 'R_HIP_energy_overall_cumulative', 'R_HIP_energy_overall_exhaustion_score', 'L_ELBOW_energy_by_trial', 'L_ELBOW_energy_by_trial_exhaustion_score', 'L_ELBOW_energy_overall_cumulative', 'L_ELBOW_energy_overall_exhaustion_score', 'R_ELBOW_energy_by_trial', 'R_ELBOW_energy_by_trial_exhaustion_score', 'R_ELBOW_energy_overall_cumulative', 'R_ELBOW_energy_overall_exhaustion_score', 'L_WRIST_energy_by_trial', 'L_WRIST_energy_by_trial_exhaustion_score', 'L_WRIST_energy_overall_cumulative', 'L_WRIST_energy_overall_exhaustion_score', 'R_WRIST_energy_by_trial', 'R_WRIST_energy_by_trial_exhaustion_score', 'R_WRIST_energy_overall_cumulative', 'R_WRIST_energy_overall_exhaustion_score', 'L_1STFINGER_energy_by_trial', 'L_1STFINGER_energy_by_trial_exhaustion_score', 'L_1STFINGER_energy_overall_cumulative', 'L_1STFINGER_energy_overall_exhaustion_score', 'R_1STFINGER_energy_by_trial', 'R_1STFINGER_energy_by_trial_exhaustion_score', 'R_1STFINGER_energy_overall_cumulative', 'R_1STFINGER_energy_overall_exhaustion_score', 'L_5THFINGER_energy_by_trial', 'L_5THFINGER_energy_by_trial_exhaustion_score', 'L_5THFINGER_energy_overall_cumulative', 'L_5THFINGER_energy_overall_exhaustion_score', 'R_5THFINGER_energy_by_trial', 'R_5THFINGER_energy_by_trial_exhaustion_score', 'R_5THFINGER_energy_overall_cumulative', 'R_5THFINGER_energy_overall_exhaustion_score', 'participant_id', 'L_SHOULDER_angle', 'R_SHOULDER_angle', 'L_HIP_angle', 'R_HIP_angle', 'L_ANKLE_angle', 'R_ANKLE_angle', 'datetime', 'player_weight__in_kg', 'joint_energy', 'joint_power', 'energy_acceleration', 'ankle_power_ratio', 'hip_asymmetry', 'ankle_asymmetry', 'wrist_asymmetry', 'elbow_asymmetry', 'knee_asymmetry', '1stfinger_asymmetry', '5thfinger_asymmetry', 'hip_power_ratio', 'wrist_power_ratio', 'elbow_power_ratio', 'knee_power_ratio', '1stfinger_power_ratio', '5thfinger_power_ratio', 'L_KNEE_ROM', 'L_KNEE_ROM_deviation', 'L_KNEE_ROM_extreme', 'R_KNEE_ROM', 'R_KNEE_ROM_deviation', 'R_KNEE_ROM_extreme', 'L_SHOULDER_ROM', 'L_SHOULDER_ROM_deviation', 'L_SHOULDER_ROM_extreme', 'R_SHOULDER_ROM', 'R_SHOULDER_ROM_deviation', 'R_SHOULDER_ROM_extreme', 'L_HIP_ROM', 'L_HIP_ROM_deviation', 'L_HIP_ROM_extreme', 'R_HIP_ROM', 'R_HIP_ROM_deviation', 'R_HIP_ROM_extreme', 'L_ANKLE_ROM', 'L_ANKLE_ROM_deviation', 'L_ANKLE_ROM_extreme', 'R_ANKLE_ROM', 'R_ANKLE_ROM_deviation', 'R_ANKLE_ROM_extreme', 'L_WRIST_ROM', 'L_WRIST_ROM_deviation', 'L_WRIST_ROM_extreme', 'R_WRIST_ROM', 'R_WRIST_ROM_deviation', 'R_WRIST_ROM_extreme', 'exhaustion_rate', 'simulated_HR', 'time_since_start', 'power_avg_5', 'rolling_power_std', 'rolling_hr_mean', 'rolling_energy_std', 'exhaustion_lag1', 'ema_exhaustion', 'rolling_exhaustion', 'injury_risk', 'L_ANKLE_exhaustion_rate', 'L_ANKLE_rolling_exhaustion', 'L_ANKLE_injury_risk', 'R_ANKLE_exhaustion_rate', 'R_ANKLE_rolling_exhaustion', 'R_ANKLE_injury_risk', 'L_WRIST_exhaustion_rate', 'L_WRIST_rolling_exhaustion', 'L_WRIST_injury_risk', 'R_WRIST_exhaustion_rate', 'R_WRIST_rolling_exhaustion', 'R_WRIST_injury_risk', 'L_ELBOW_exhaustion_rate', 'L_ELBOW_rolling_exhaustion', 'L_ELBOW_injury_risk', 'R_ELBOW_exhaustion_rate', 'R_ELBOW_rolling_exhaustion', 'R_ELBOW_injury_risk', 'L_KNEE_exhaustion_rate', 'L_KNEE_rolling_exhaustion', 'L_KNEE_injury_risk', 'R_KNEE_exhaustion_rate', 'R_KNEE_rolling_exhaustion', 'R_KNEE_injury_risk', 'L_HIP_exhaustion_rate', 'L_HIP_rolling_exhaustion', 'L_HIP_injury_risk', 'R_HIP_exhaustion_rate', 'R_HIP_rolling_exhaustion', 'R_HIP_injury_risk']\n",
      "INFO: Features have been scaled using StandardScaler.\n",
      "INFO: Created LSTM sequences: (2359, 5, 10), (2359,)\n",
      "INFO: Created LSTM sequences: (587, 5, 10), (587,)\n",
      "INFO: Training injury risk model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7157 - loss: 1.5796 - val_accuracy: 0.7700 - val_loss: 0.4051\n",
      "Epoch 2/200\n",
      "\u001b[1m 1/74\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.7812 - loss: 0.3781"
     ]
    }
   ],
   "source": [
    "# %%writefile ml/preprocess_train_predict/darts_models_for_comparison.py\n",
    "\n",
    "\n",
    "import torch\n",
    "import logging\n",
    "import pandas as pd\n",
    "import sys\n",
    "device_str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device_str)\n",
    "from darts.models import NBEATSModel, ExponentialSmoothing\n",
    "from darts import TimeSeries\n",
    "from darts.dataprocessing import Pipeline\n",
    "forecast_horizon = 36  # Set your forecast horizon as needed\n",
    "model_nbeats = NBEATSModel(\n",
    "    input_chunk_length=24,\n",
    "    output_chunk_length=forecast_horizon,\n",
    "    pl_trainer_kwargs={\"accelerator\": \"gpu\", \"devices\": [0]}\n",
    ")\n",
    "from ml.load_and_prepare_data.load_data_and_analyze import (\n",
    "    load_data, prepare_joint_features, \n",
    "    feature_engineering, summarize_data, check_and_drop_nulls,\n",
    "    prepare_base_datasets)\n",
    "\n",
    "from ml.feature_selection.feature_selection import (\n",
    "    load_top_features, perform_feature_importance_analysis, save_top_features,\n",
    "    analyze_joint_injury_features, check_for_invalid_values,\n",
    "    perform_feature_importance_analysis, analyze_and_display_top_features, \n",
    "    run_feature_importance_analysis, run_feature_import_and_load_top_features)\n",
    "\n",
    "from ml.preprocess_train_predict.base_training import (\n",
    "    temporal_train_test_split, scale_features, create_sequences, train_exhaustion_model, \n",
    "    train_injury_model,  train_joint_models, forecast_and_plot_exhaustion, forecast_and_plot_injury,\n",
    "    forecast_and_plot_joint, summarize_regression_model, summarize_classification_model, \n",
    "    summarize_joint_models, summarize_all_models, final_model_summary, \n",
    "    summarize_joint_exhaustion_models\n",
    "    )\n",
    "from ml.preprocess_train_predict.conformal_tights import (\n",
    "    train_conformal_model, predict_with_uncertainty, plot_conformal_results, add_time_series_forecasting,\n",
    "    add_conformal_to_exhaustion_model\n",
    "    )\n",
    "\n",
    "def preprocess_timeseries_darts(ts, transformers=None):\n",
    "    \"\"\"\n",
    "    Preprocesses a Darts TimeSeries object using a pipeline of transformers from Darts.\n",
    "    \n",
    "    Parameters:\n",
    "        ts (TimeSeries): Darts TimeSeries object to be transformed.\n",
    "        transformers (list): Optional list of transformer objects. If None, a default pipeline\n",
    "                             using MissingValuesFiller and Scaler is used.\n",
    "                             \n",
    "    Returns:\n",
    "        TimeSeries: The transformed TimeSeries.\n",
    "    \"\"\"\n",
    "    from darts.dataprocessing import Pipeline\n",
    "    from darts.dataprocessing.transformers import MissingValuesFiller, Scaler\n",
    "\n",
    "\n",
    "    # Use default transformers if none are provided.\n",
    "    if transformers is None:\n",
    "        transformers = [MissingValuesFiller(), Scaler()]\n",
    "\n",
    "    # Create and fit the transformation pipeline.\n",
    "    pipeline = Pipeline(transformers)\n",
    "    ts_transformed = pipeline.fit_transform(ts)\n",
    "    return ts_transformed\n",
    "\n",
    "\n",
    "\n",
    "def detect_anomalies_with_darts(time_series, training_window=24, high_quantile=0.99, k=2, window=5):\n",
    "    \"\"\"\n",
    "    Detects anomalies in a Darts TimeSeries using forecasting-based anomaly detection.\n",
    "    \n",
    "    This function utilizes the KMeansScorer and QuantileDetector from Darts AD module\n",
    "    to compute anomaly scores and then convert them into binary anomaly flags.\n",
    "    \n",
    "    Parameters:\n",
    "      - time_series (TimeSeries): Darts TimeSeries object containing the data.\n",
    "      - training_window (int): Window size for training the anomaly scorer.\n",
    "      - high_quantile (float): High quantile threshold for binary detection.\n",
    "      - k (int): Number of clusters for KMeansScorer.\n",
    "      - window (int): Rolling window size for anomaly scoring.\n",
    "    \n",
    "    Returns:\n",
    "      - binary_anomalies (np.array): Array of binary anomaly flags (1 for anomaly, 0 for normal).\n",
    "      - anomaly_scores (np.array): Anomaly scores computed for the time series.\n",
    "    \"\"\"\n",
    "    from darts.ad import KMeansScorer, QuantileDetector\n",
    "\n",
    "    # Use 80% of the series to train the anomaly scorer\n",
    "    train_length = int(0.8 * len(time_series))\n",
    "    training_data = time_series[:train_length]\n",
    "    new_data = time_series[train_length:]\n",
    "    \n",
    "    # Train the anomaly scorer on the training data\n",
    "    scorer = KMeansScorer(k=k, window=window)\n",
    "    scorer.fit(training_data)\n",
    "    \n",
    "    # Score new data for anomalies.\n",
    "    anomaly_scores = scorer.score(new_data)\n",
    "    \n",
    "    # Fit a detector on the training scores and detect anomalies on new data.\n",
    "    detector = QuantileDetector(high_quantile=high_quantile)\n",
    "    detector.fit(scorer.score(training_data))\n",
    "    binary_anomalies = detector.detect(anomaly_scores)\n",
    "    \n",
    "    return binary_anomalies, anomaly_scores\n",
    "\n",
    "\n",
    "from darts import TimeSeries\n",
    "from darts.dataprocessing import Pipeline\n",
    "from darts.dataprocessing.transformers import MissingValuesFiller, Scaler\n",
    "from darts.models import NBEATSModel, ExponentialSmoothing\n",
    "from darts.metrics import mae, mape, rmse, smape\n",
    "import pandas as pd\n",
    "\n",
    "def enhanced_forecasting_with_darts_and_metrics(\n",
    "    data,\n",
    "    timestamp_col='timestamp',\n",
    "    target_col='exhaustion_rate',\n",
    "    train_frac=0.8,\n",
    "    freq='33ms'\n",
    "):\n",
    "    \"\"\"\n",
    "    Splits the series, fits two Darts models on train, forecasts on test,\n",
    "    aligns timestamps, and returns forecasts + test_series + metrics_df.\n",
    "    \n",
    "    Parameters:\n",
    "        data (pd.DataFrame): Input data with timestamp and target columns.\n",
    "        timestamp_col (str): Name of the timestamp column.\n",
    "        target_col (str): Name of the target column to forecast.\n",
    "        train_frac (float): Fraction of data to use for training.\n",
    "        freq (str): Frequency string for timestamps.\n",
    "    \n",
    "    Returns:\n",
    "        train_series (TimeSeries): Training portion of the time series.\n",
    "        test_series (TimeSeries): Testing portion of the time series.\n",
    "        forecast_nbeats (TimeSeries): Forecast from the NBEATS model.\n",
    "        forecast_es (TimeSeries): Forecast from the Exponential Smoothing model.\n",
    "        metrics_df (pd.DataFrame): DataFrame containing metrics for both models.\n",
    "    \"\"\"\n",
    "    from darts import TimeSeries\n",
    "    from darts.dataprocessing import Pipeline\n",
    "    from darts.dataprocessing.transformers import MissingValuesFiller, Scaler\n",
    "    from darts.models import NBEATSModel, ExponentialSmoothing\n",
    "    from darts.metrics import mae, mape, rmse, smape\n",
    "    import pandas as pd\n",
    "    import logging\n",
    "\n",
    "    # Build and preprocess full series\n",
    "    full_series = TimeSeries.from_dataframe(\n",
    "        data, time_col=timestamp_col, value_cols=[target_col],\n",
    "        fill_missing_dates=True, freq=freq\n",
    "    )\n",
    "    pipeline = Pipeline([MissingValuesFiller(), Scaler()])\n",
    "    full_series = pipeline.fit_transform(full_series)\n",
    "\n",
    "    # Split\n",
    "    split_idx    = int(train_frac * len(full_series))\n",
    "    train_series = full_series[:split_idx]\n",
    "    test_series  = full_series[split_idx:]\n",
    "\n",
    "    # Instantiate & fit\n",
    "    model_nbeats = NBEATSModel(input_chunk_length=24, output_chunk_length=len(test_series))\n",
    "    model_es     = ExponentialSmoothing()\n",
    "    \n",
    "    logging.info(f\"Fitting NBEATS model for {target_col}...\")\n",
    "    model_nbeats.fit(train_series)\n",
    "    \n",
    "    logging.info(f\"Fitting ExponentialSmoothing model for {target_col}...\")\n",
    "    model_es.fit(train_series)\n",
    "\n",
    "    # Forecast exactly test length\n",
    "    logging.info(\"Generating forecasts...\")\n",
    "    forecast_nbeats = model_nbeats.predict(n=len(test_series))\n",
    "    forecast_es     = model_es.predict(n=len(test_series))\n",
    "\n",
    "    # Compute metrics\n",
    "    logging.info(\"Computing metrics...\")\n",
    "    metrics = {\n",
    "        \"NBEATS\": {\n",
    "            \"MAE\":   mae(test_series, forecast_nbeats),\n",
    "            \"RMSE\":  rmse(test_series, forecast_nbeats),\n",
    "            \"SMAPE\": smape(test_series, forecast_nbeats),\n",
    "        },\n",
    "        \"ExpSmoothing\": {\n",
    "            \"MAE\":   mae(test_series, forecast_es),\n",
    "            \"RMSE\":  rmse(test_series, forecast_es),\n",
    "            \"SMAPE\": smape(test_series, forecast_es),\n",
    "        }\n",
    "    }\n",
    "    metrics_df = pd.DataFrame(metrics).T\n",
    "    \n",
    "    logging.info(f\"Darts metrics for {target_col}:\\n{metrics_df}\")\n",
    "    \n",
    "    return train_series, test_series, forecast_nbeats, model_nbeats, forecast_es, metrics_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Main Script: Running Three Separate Analyses ---\n",
    "if __name__ == \"__main__\":\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from ml.load_and_prepare_data.load_data_and_analyze import (\n",
    "        load_data, prepare_joint_features, feature_engineering, summarize_data, check_and_drop_nulls\n",
    "    )\n",
    "    debug = True\n",
    "    importance_threshold = 0.01\n",
    "    csv_path = \"../../data/processed/final_granular_dataset.csv\"\n",
    "    json_path = \"../../data/basketball/freethrow/participant_information.json\"\n",
    "    output_dir = \"../../data/Deep_Learning_Final\"\n",
    "    \n",
    "    base_feature_dir = os.path.join(output_dir, \"feature_lists/base\")\n",
    "    trial_feature_dir = os.path.join(output_dir, \"feature_lists/trial_summary\")\n",
    "    shot_feature_dir = os.path.join(output_dir, \"feature_lists/shot_phase_summary\")\n",
    "    \n",
    "    data, trial_df, shot_df = prepare_base_datasets(csv_path, json_path, debug=debug)\n",
    "    numeric_features = data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    summary_targets = ['exhaustion_rate', 'injury_risk']\n",
    "    trial_summary_features = [col for col in trial_df.columns if col not in summary_targets]\n",
    "    trial_summary_features = [col for col in trial_summary_features if col in numeric_features]\n",
    "    shot_summary_features = [col for col in shot_df.columns if col not in summary_targets]\n",
    "    shot_summary_features = [col for col in shot_summary_features if col in numeric_features]\n",
    "    print(\"Available target columns:\", [c for c in data.columns if \"exhaustion\" in c])\n",
    "    # ========================================\n",
    "    # 1) Overall Base Dataset (including Joint-Specific Targets)\n",
    "    # ========================================\n",
    "    features = [\n",
    "        'joint_energy', 'joint_power', 'energy_acceleration',\n",
    "        'elbow_asymmetry', 'hip_asymmetry', 'ankle_asymmetry', 'wrist_asymmetry', 'knee_asymmetry', \n",
    "        '1stfinger_asymmetry', '5thfinger_asymmetry',\n",
    "        'elbow_power_ratio', 'hip_power_ratio', 'ankle_power_ratio', 'wrist_power_ratio', \n",
    "        'knee_power_ratio', '1stfinger_power_ratio', '5thfinger_power_ratio',\n",
    "        'L_KNEE_ROM', 'L_KNEE_ROM_deviation', 'L_KNEE_ROM_extreme',\n",
    "        'R_KNEE_ROM', 'R_KNEE_ROM_deviation', 'R_KNEE_ROM_extreme',\n",
    "        'L_SHOULDER_ROM', 'L_SHOULDER_ROM_deviation', 'L_SHOULDER_ROM_extreme',\n",
    "        'R_SHOULDER_ROM', 'R_SHOULDER_ROM_deviation', 'R_SHOULDER_ROM_extreme',\n",
    "        'L_HIP_ROM', 'L_HIP_ROM_deviation', 'L_HIP_ROM_extreme',\n",
    "        'R_HIP_ROM', 'R_HIP_ROM_deviation', 'R_HIP_ROM_extreme',\n",
    "        'L_ANKLE_ROM', 'L_ANKLE_ROM_deviation', 'L_ANKLE_ROM_extreme',\n",
    "        'R_ANKLE_ROM', 'R_ANKLE_ROM_deviation', 'R_ANKLE_ROM_extreme',\n",
    "        'exhaustion_lag1', 'power_avg_5', 'rolling_power_std', 'rolling_hr_mean',\n",
    "        'time_since_start', 'ema_exhaustion', 'rolling_exhaustion', 'rolling_energy_std',\n",
    "        'simulated_HR',\n",
    "        'player_height_in_meters', 'player_weight__in_kg'\n",
    "    ]\n",
    "    classification_targets = 'injury_risk'\n",
    "    regression_targets = 'by_trial_exhaustion_score' # exhaustion_rate\n",
    "    base_targets = [classification_targets] + [regression_targets]\n",
    "    joints = ['ANKLE', 'WRIST', 'ELBOW', 'KNEE', 'HIP']\n",
    "    joint_injury_targets = [f\"{side}_{joint}_{classification_targets}\" for joint in joints for side in ['L', 'R']]\n",
    "    joint_exhaustion_targets = [f\"{side}_{joint}_energy_{regression_targets}\" for joint in joints for side in ['L', 'R']] # by_trial_exhaustion_score\n",
    "    # joint_exhaustion_targets = [f\"{side}_{joint}_{regression_targets}\" for joint in joints for side in ['L', 'R']]  # exhaustion_rate\n",
    "    joint_targets = joint_injury_targets + joint_exhaustion_targets\n",
    "    all_targets = base_targets + joint_targets\n",
    "\n",
    "    logging.info(\"=== Base Dataset Analysis (Overall + Joint-Specific) ===\")\n",
    "    base_loaded_features = run_feature_import_and_load_top_features(\n",
    "        dataset=data,\n",
    "        features=features,\n",
    "        targets=all_targets,\n",
    "        base_output_dir=output_dir,\n",
    "        output_subdir=\"feature_lists/base\",\n",
    "        debug=debug,\n",
    "        dataset_label=\"Base Data\",\n",
    "        importance_threshold=importance_threshold,\n",
    "        n_top=10,\n",
    "        run_analysis=True\n",
    "    )\n",
    "    print(f\"Base Loaded Features: {base_loaded_features}\")\n",
    "    \n",
    "    joint_feature_dict = {}\n",
    "    for target in joint_targets:\n",
    "        try:\n",
    "            feat_loaded = base_loaded_features.get(target, [])\n",
    "            logging.info(f\"Test Load: Features for {target}: {feat_loaded}\")\n",
    "            joint_feature_dict[target] = feat_loaded\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading features for {target}: {e}\")\n",
    "    \n",
    "    # ========================================\n",
    "    # 2) Trial Summary Dataset Analysis\n",
    "    # ========================================\n",
    "\n",
    "    \n",
    "    logging.info(\"=== Trial Summary Dataset Analysis ===\")\n",
    "    trial_loaded_features = run_feature_import_and_load_top_features(\n",
    "        dataset=trial_df,\n",
    "        features=trial_summary_features,\n",
    "        targets=summary_targets,\n",
    "        base_output_dir=output_dir,\n",
    "        output_subdir=\"feature_lists/trial_summary\",\n",
    "        debug=debug,\n",
    "        dataset_label=\"Trial Summary Data\",\n",
    "        importance_threshold=importance_threshold,\n",
    "        n_top=10,\n",
    "        run_analysis=False\n",
    "    )\n",
    "    features_exhaustion_trial = trial_loaded_features.get('by_trial_exhaustion_score', [])\n",
    "    features_injury_trial = trial_loaded_features.get('injury_risk', [])\n",
    "    trial_summary_data = trial_df.copy()\n",
    "    \n",
    "    # ========================================\n",
    "    # 3) Shot Phase Summary Dataset Analysis\n",
    "    # ========================================\n",
    "\n",
    "    \n",
    "    logging.info(\"=== Shot Phase Summary Dataset Analysis ===\")\n",
    "    shot_loaded_features = run_feature_import_and_load_top_features(\n",
    "        dataset=shot_df,\n",
    "        features=shot_summary_features,\n",
    "        targets=summary_targets,\n",
    "        base_output_dir=output_dir,\n",
    "        output_subdir=\"feature_lists/shot_phase_summary\",\n",
    "        debug=debug,\n",
    "        dataset_label=\"Shot Phase Summary Data\",\n",
    "        importance_threshold=importance_threshold,\n",
    "        n_top=10,\n",
    "        run_analysis=False\n",
    "    )\n",
    "    features_exhaustion_shot = shot_loaded_features.get('by_trial_exhaustion_score', [])\n",
    "    features_injury_shot = shot_loaded_features.get('injury_risk', [])\n",
    "    shot_phase_summary_data = shot_df.copy()\n",
    "\n",
    "    # ------------------------------\n",
    "    # 5. Split Base Data for Training Models\n",
    "    # ------------------------------\n",
    "    train_data, test_data = temporal_train_test_split(data, test_size=0.2)\n",
    "    timesteps = 5\n",
    "\n",
    "    # Hyperparameters and architecture definitions.\n",
    "    hyperparams = {\n",
    "        \"epochs\": 200,\n",
    "        \"batch_size\": 32,\n",
    "        \"early_stop_patience\": 5\n",
    "    }\n",
    "    arch_exhaustion = {\n",
    "        \"num_lstm_layers\": 1,\n",
    "        \"lstm_units\": 64,\n",
    "        \"dropout_rate\": 0.2,\n",
    "        \"dense_units\": 1,\n",
    "        \"dense_activation\": None\n",
    "    }\n",
    "    arch_injury = {\n",
    "        \"num_lstm_layers\": 1,\n",
    "        \"lstm_units\": 64,\n",
    "        \"dropout_rate\": 0.2,\n",
    "        \"dense_units\": 1,\n",
    "        \"dense_activation\": \"sigmoid\"\n",
    "    }\n",
    "    \n",
    "    # For demonstration, define features/targets (you can adjust these as needed)\n",
    "    features_exhaustion = [\n",
    "        'joint_power', \n",
    "        'joint_energy', \n",
    "        'elbow_asymmetry',  \n",
    "        'L_WRIST_angle', 'R_WRIST_angle',  # Updated: removed \"wrist_angle\"\n",
    "        'exhaustion_lag1', \n",
    "        'power_avg_5',\n",
    "        'simulated_HR',\n",
    "        'player_height_in_meters',\n",
    "        'player_weight__in_kg'\n",
    "    ]\n",
    "    target_exhaustion = 'by_trial_exhaustion_score'\n",
    "\n",
    "    features_injury = [\n",
    "        'joint_power', \n",
    "        'joint_energy', \n",
    "        'elbow_asymmetry',  \n",
    "        'knee_asymmetry', \n",
    "        'L_WRIST_angle', 'R_WRIST_angle',  # Updated: removed \"wrist_angle\"\n",
    "        'exhaustion_lag1', \n",
    "        'power_avg_5',\n",
    "        'simulated_HR',\n",
    "        'player_height_in_meters',\n",
    "        'player_weight__in_kg'\n",
    "    ]\n",
    "    target_injury = 'injury_risk'\n",
    "    # ------------------------------\n",
    "    # 6a. Train Models on Base Data for Overall Exhaustion and Injury Risk\n",
    "    # ------------------------------\n",
    "    model_exhaustion, scaler_exhaustion, target_scaler, X_val_exh, y_val_exh = train_exhaustion_model(\n",
    "        train_data, test_data, features_exhaustion, timesteps,\n",
    "        epochs=hyperparams[\"epochs\"],\n",
    "        batch_size=hyperparams[\"batch_size\"],\n",
    "        early_stop_patience=hyperparams[\"early_stop_patience\"],\n",
    "        num_lstm_layers=arch_exhaustion[\"num_lstm_layers\"],\n",
    "        lstm_units=arch_exhaustion[\"lstm_units\"],\n",
    "        dropout_rate=arch_exhaustion[\"dropout_rate\"],\n",
    "        dense_units=arch_exhaustion[\"dense_units\"],\n",
    "        dense_activation=arch_exhaustion[\"dense_activation\"]\n",
    "    )\n",
    "    model_injury, scaler_injury, X_val_injury, y_val_injury = train_injury_model(\n",
    "        train_data, test_data, features_injury, timesteps,\n",
    "        epochs=hyperparams[\"epochs\"],\n",
    "        batch_size=hyperparams[\"batch_size\"],\n",
    "        num_lstm_layers=arch_injury[\"num_lstm_layers\"],\n",
    "        lstm_units=arch_injury[\"lstm_units\"],\n",
    "        dropout_rate=arch_injury[\"dropout_rate\"],\n",
    "        dense_units=arch_injury[\"dense_units\"],\n",
    "        dense_activation=arch_injury[\"dense_activation\"]\n",
    "    )\n",
    "    # For joint models, train using the base data and the corresponding feature lists.\n",
    "    joint_models = {}\n",
    "    for joint_target, features_list in joint_feature_dict.items():\n",
    "        try:\n",
    "            logging.info(f\"Training joint-specific injury model for {joint_target} using features: {features_list}\")\n",
    "            model, scaler, X_val_joint, y_val_joint = train_injury_model(\n",
    "                train_data, test_data,\n",
    "                features=features_list,\n",
    "                timesteps=timesteps, \n",
    "                epochs=hyperparams[\"epochs\"],\n",
    "                batch_size=hyperparams[\"batch_size\"],\n",
    "                early_stop_patience=hyperparams[\"early_stop_patience\"],\n",
    "                num_lstm_layers=arch_exhaustion[\"num_lstm_layers\"],\n",
    "                lstm_units=arch_exhaustion[\"lstm_units\"],\n",
    "                dropout_rate=arch_exhaustion[\"dropout_rate\"],\n",
    "                dense_units=arch_exhaustion[\"dense_units\"],\n",
    "                dense_activation=arch_exhaustion[\"dense_activation\"],\n",
    "                target_col=joint_target\n",
    "            )\n",
    "            joint_models[joint_target] = {\n",
    "                'model': model,\n",
    "                'features': features_list,\n",
    "                'scaler': scaler\n",
    "            }\n",
    "            logging.info(f\"Successfully trained joint model for {joint_target}.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error training joint model for {joint_target}: {e}\")\n",
    "\n",
    "    # 6c. Train joint exhaustion models (by_trial_exhaustion) for each joint.\n",
    "    joint_exhaustion_models = {}\n",
    "    for joint in joints:\n",
    "        for side in ['L', 'R']:\n",
    "            target_joint_exh = f\"{side}_{joint}_exhaustion_rate\"\n",
    "            try:\n",
    "                if target_joint_exh in joint_feature_dict:\n",
    "                    features_list = joint_feature_dict[target_joint_exh]\n",
    "                    logging.info(f\"Using preloaded features for {target_joint_exh}: {features_list}\")\n",
    "                else:\n",
    "                    features_list = load_top_features(target_joint_exh, feature_dir=base_feature_dir, df=data, n_top=10)\n",
    "                    logging.info(f\"Loaded features for {target_joint_exh}: {features_list}\")\n",
    "                \n",
    "                model_exh, scaler_exh, target_scaler_exh, X_val_joint_exh, y_val_joint_exh = train_exhaustion_model(\n",
    "                    train_data, test_data,\n",
    "                    features=features_list,\n",
    "                    timesteps=timesteps,\n",
    "                    epochs=hyperparams[\"epochs\"],\n",
    "                    batch_size=hyperparams[\"batch_size\"],\n",
    "                    early_stop_patience=hyperparams[\"early_stop_patience\"],\n",
    "                    num_lstm_layers=arch_exhaustion[\"num_lstm_layers\"],\n",
    "                    lstm_units=arch_exhaustion[\"lstm_units\"],\n",
    "                    dropout_rate=arch_exhaustion[\"dropout_rate\"],\n",
    "                    dense_units=arch_exhaustion[\"dense_units\"],\n",
    "                    dense_activation=arch_exhaustion[\"dense_activation\"],\n",
    "                    target_col=target_joint_exh\n",
    "                )\n",
    "                joint_exhaustion_models[target_joint_exh] = {\n",
    "                    'model': model_exh,\n",
    "                    'features': features_list,\n",
    "                    'scaler': scaler_exh\n",
    "                }\n",
    "                logging.info(f\"Successfully trained joint exhaustion model for {target_joint_exh}.\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error training joint exhaustion model for {target_joint_exh}: {e}\")\n",
    "\n",
    "    # ------------------------------\n",
    "    # 7a. Forecasting for Base Models\n",
    "    # ------------------------------\n",
    "    forecast_and_plot_exhaustion(\n",
    "        model=model_exhaustion,\n",
    "        test_data=test_data,\n",
    "        forecast_features=features_exhaustion,\n",
    "        scaler_exhaustion=scaler_exhaustion,\n",
    "        target_scaler=target_scaler,\n",
    "        timesteps=timesteps,\n",
    "        future_steps=50,\n",
    "        title=\"Overall Exhaustion Model Forecast\"\n",
    "    )\n",
    "    forecast_and_plot_injury(\n",
    "        model=model_injury,\n",
    "        test_data=test_data,\n",
    "        forecast_features=features_injury,\n",
    "        scaler_injury=scaler_injury,\n",
    "        timesteps=timesteps,\n",
    "        future_steps=50,\n",
    "        title=\"Overall Injury Risk Forecast\"\n",
    "    )\n",
    "    forecast_and_plot_joint(\n",
    "        joint_models=joint_models,\n",
    "        test_data=test_data,\n",
    "        timesteps=timesteps,\n",
    "        future_steps=50\n",
    "    )\n",
    "\n",
    "    # ------------------------------\n",
    "    # 8. Summarize Base Model Testing Results\n",
    "    # ------------------------------\n",
    "    summary_df = summarize_all_models(\n",
    "        model_exhaustion, X_val_exh, y_val_exh, target_scaler,\n",
    "        model_injury, X_val_injury, y_val_injury,\n",
    "        joint_models, test_data, timesteps, output_dir\n",
    "    )\n",
    "    print(\"=== Model Summaries (Base Data) ===\")\n",
    "    print(summary_df)\n",
    "\n",
    "\n",
    "    # ------------------------------\n",
    "    # Additional: Conformal Uncertainty Integration\n",
    "    # ------------------------------\n",
    "    logging.info(\"Running conformal uncertainty integration for exhaustion model...\")\n",
    "    conformal_model = add_conformal_to_exhaustion_model(train_data, test_data, target_col='by_trial_exhaustion_score',)\n",
    "    \n",
    "    try:\n",
    "        # Time series forecasting with uncertainty using Darts.\n",
    "        logging.info(\"Running time series forecasting with conformal uncertainty...\")\n",
    "        forecaster, forecast, test_target =add_time_series_forecasting(\n",
    "            data,\n",
    "            time_col='timestamp',\n",
    "            value_cols=['by_trial_exhaustion_score']\n",
    "        )\n",
    "\n",
    "        # Plot forecast vs. actual (using Darts built-in plotting).\n",
    "        forecast.plot(label=\"Forecast\")\n",
    "        test_target.plot(label=\"Actual\")\n",
    "        plt.title(\"Time Series Forecast with Conformal Uncertainty\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        logging.error(\"An error occurred in time series forecasting: \" + str(e))\n",
    "        sys.exit(1)\n",
    "\n",
    "\n",
    "    # --- A. Preprocess TimeSeries with Darts ---\n",
    "    from darts import TimeSeries\n",
    "    # Create a TimeSeries from the exhaustion target column.\n",
    "    ts_exhaustion = TimeSeries.from_dataframe(\n",
    "    data,\n",
    "    time_col='timestamp',\n",
    "    value_cols=['by_trial_exhaustion_score'],\n",
    "    fill_missing_dates=True,\n",
    "    freq='33ms'\n",
    ")\n",
    "\n",
    "    # Preprocess the TimeSeries using the Darts pipeline.\n",
    "    ts_preprocessed = preprocess_timeseries_darts(ts_exhaustion)\n",
    "    print(\"Preprocessed TimeSeries head:\")\n",
    "    print(ts_preprocessed.to_dataframe().head())\n",
    "\n",
    "    # --- B. Detect Anomalies with Darts ---\n",
    "    anomaly_flags, anomaly_scores = detect_anomalies_with_darts(ts_preprocessed)\n",
    "    print(\"Anomaly flags (first 10 values):\", anomaly_flags[:10])\n",
    "    print(\"Anomaly scores (first 10 values):\", anomaly_scores[:10])\n",
    "\n",
    "    # --- C. Enhanced Forecasting with Darts ---\n",
    "    # Get the trained model and series from enhanced_forecasting\n",
    "    train_series, test_series, forecast_nbeats, model_nbeats, forecast_es, metrics_df = enhanced_forecasting_with_darts_and_metrics(\n",
    "        data, timestamp_col='timestamp', \n",
    "        target_col='by_trial_exhaustion_score')\n",
    "\n",
    "    # And update the plotting code:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    test_series.plot(label=\"Actual\")\n",
    "    forecast_nbeats.plot(label=\"NBEATS Forecast\")\n",
    "    forecast_es.plot(label=\"ExpSmoothing Forecast\")\n",
    "    plt.title(\"Enhanced Forecasting with Darts\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # Explaining the model predictions\n",
    "    from darts.metrics import mae, mape, rmse, smape\n",
    "\n",
    "    errors = model_nbeats.backtest(\n",
    "        series=ts_exhaustion,\n",
    "        start=0.6,\n",
    "        forecast_horizon=36,\n",
    "        metric=smape,\n",
    "        retrain=False\n",
    "    )\n",
    "    print(\"Rolling MAPE distribution:\", errors)\n",
    "\n",
    "    # Print the metrics\n",
    "    print(\"\\nForecast Metrics:\")\n",
    "    print(metrics_df)\n",
    "\n",
    "    # ------------------------------\n",
    "    # 9. Train, Forecast, and Summarize Aggregated Models\n",
    "    # ------------------------------\n",
    "    # Instead of using a hard-coded summary_features list, we now load the top features\n",
    "    # specific to each aggregated dataset (which were saved using the threshold filter).\n",
    "    \n",
    "    # --- 9a. Process Trial Summary Data ---\n",
    "    trial_train_data, trial_test_data = temporal_train_test_split(trial_summary_data, test_size=0.2)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    model_exhaustion_trial, scaler_exhaustion_trial, target_scaler_trial, X_val_exh_trial, y_val_exh_trial = train_exhaustion_model(\n",
    "        trial_train_data, trial_test_data, features_exhaustion_trial, timesteps,\n",
    "        epochs=hyperparams[\"epochs\"],\n",
    "        batch_size=hyperparams[\"batch_size\"],\n",
    "        early_stop_patience=hyperparams[\"early_stop_patience\"],\n",
    "        num_lstm_layers=arch_exhaustion[\"num_lstm_layers\"],\n",
    "        lstm_units=arch_exhaustion[\"lstm_units\"],\n",
    "        dropout_rate=arch_exhaustion[\"dropout_rate\"],\n",
    "        dense_units=arch_exhaustion[\"dense_units\"],\n",
    "        dense_activation=arch_exhaustion[\"dense_activation\"]\n",
    "    )\n",
    "    model_injury_trial, scaler_injury_trial, X_val_injury_trial, y_val_injury_trial = train_injury_model(\n",
    "        trial_train_data, trial_test_data, features_injury_trial, timesteps,\n",
    "        epochs=hyperparams[\"epochs\"],\n",
    "        batch_size=hyperparams[\"batch_size\"],\n",
    "        num_lstm_layers=arch_injury[\"num_lstm_layers\"],\n",
    "        lstm_units=arch_injury[\"lstm_units\"],\n",
    "        dropout_rate=arch_injury[\"dropout_rate\"],\n",
    "        dense_units=arch_injury[\"dense_units\"],\n",
    "        dense_activation=arch_injury[\"dense_activation\"]\n",
    "    )\n",
    "    \n",
    "    forecast_and_plot_exhaustion(\n",
    "        model=model_exhaustion_trial,\n",
    "        test_data=trial_test_data,\n",
    "        forecast_features=features_exhaustion_trial,\n",
    "        scaler_exhaustion=scaler_exhaustion_trial,\n",
    "        target_scaler=target_scaler_trial,\n",
    "        timesteps=timesteps,\n",
    "        future_steps=50,\n",
    "        title=\"Trial Summary Aggregated Exhaustion Forecast\"\n",
    "    )\n",
    "    forecast_and_plot_injury(\n",
    "        model=model_injury_trial,\n",
    "        test_data=trial_test_data,\n",
    "        forecast_features=features_injury_trial,\n",
    "        scaler_injury=scaler_injury_trial,\n",
    "        timesteps=timesteps,\n",
    "        future_steps=50,\n",
    "        title=\"Trial Summary Aggregated Injury Forecast\"\n",
    "    )\n",
    "    \n",
    "    trial_summary_df = summarize_all_models(\n",
    "        model_exhaustion_trial, X_val_exh_trial, y_val_exh_trial, target_scaler_trial,\n",
    "        model_injury_trial, X_val_injury_trial, y_val_injury_trial,\n",
    "        joint_models, trial_test_data, timesteps, output_dir,\n",
    "        include_joint_models=False, debug=debug\n",
    "    )\n",
    "\n",
    "    print(\"=== Model Summaries (Trial Summary Aggregated Data) ===\")\n",
    "    print(trial_summary_df)\n",
    "    \n",
    "    # --- 9b. Process Shot Phase Summary Data ---\n",
    "    shot_train_data, shot_test_data = temporal_train_test_split(shot_phase_summary_data, test_size=0.2)\n",
    "    \n",
    "\n",
    "\n",
    "    model_exhaustion_shot, scaler_exhaustion_shot, target_scaler_shot, X_val_exh_shot, y_val_exh_shot = train_exhaustion_model(\n",
    "        shot_train_data, shot_test_data, features_exhaustion_shot, timesteps,\n",
    "        epochs=hyperparams[\"epochs\"],\n",
    "        batch_size=hyperparams[\"batch_size\"],\n",
    "        early_stop_patience=hyperparams[\"early_stop_patience\"],\n",
    "        num_lstm_layers=arch_exhaustion[\"num_lstm_layers\"],\n",
    "        lstm_units=arch_exhaustion[\"lstm_units\"],\n",
    "        dropout_rate=arch_exhaustion[\"dropout_rate\"],\n",
    "        dense_units=arch_exhaustion[\"dense_units\"],\n",
    "        dense_activation=arch_exhaustion[\"dense_activation\"]\n",
    "    )\n",
    "    model_injury_shot, scaler_injury_shot, X_val_injury_shot, y_val_injury_shot = train_injury_model(\n",
    "        shot_train_data, shot_test_data, features_injury_shot, timesteps,\n",
    "        epochs=hyperparams[\"epochs\"],\n",
    "        batch_size=hyperparams[\"batch_size\"],\n",
    "        num_lstm_layers=arch_injury[\"num_lstm_layers\"],\n",
    "        lstm_units=arch_injury[\"lstm_units\"],\n",
    "        dropout_rate=arch_injury[\"dropout_rate\"],\n",
    "        dense_units=arch_injury[\"dense_units\"],\n",
    "        dense_activation=arch_injury[\"dense_activation\"]\n",
    "    )\n",
    "    \n",
    "    forecast_and_plot_exhaustion(\n",
    "        model=model_exhaustion_shot,\n",
    "        test_data=shot_test_data,\n",
    "        forecast_features=features_exhaustion_shot,\n",
    "        scaler_exhaustion=scaler_exhaustion_shot,\n",
    "        target_scaler=target_scaler_shot,\n",
    "        timesteps=timesteps,\n",
    "        future_steps=50,\n",
    "        title=\"Shot Phase Summary Aggregated Exhaustion Forecast\"\n",
    "    )\n",
    "    forecast_and_plot_injury(\n",
    "        model=model_injury_shot,\n",
    "        test_data=shot_test_data,\n",
    "        forecast_features=features_injury_shot,\n",
    "        scaler_injury=scaler_injury_shot,\n",
    "        timesteps=timesteps,\n",
    "        future_steps=50,\n",
    "        title=\"Shot Phase Summary Aggregated Injury Forecast\"\n",
    "    )\n",
    "    \n",
    "    shot_summary_df = summarize_all_models(\n",
    "        model_exhaustion_shot, X_val_exh_shot, y_val_exh_shot, target_scaler_shot,\n",
    "        model_injury_shot, X_val_injury_shot, y_val_injury_shot,\n",
    "        joint_models, shot_test_data, timesteps, output_dir,\n",
    "        include_joint_models=False, debug=debug\n",
    "    )\n",
    "\n",
    "    print(\"=== Model Summaries (Shot Phase Summary Aggregated Data) ===\")\n",
    "    print(shot_summary_df)\n",
    "\n",
    "\n",
    "    # ------------------------------\n",
    "    # Final Step: Group and Compare Summaries Across Datasets\n",
    "    # ------------------------------\n",
    "\n",
    "    # Separate base summary into regression and classification parts.\n",
    "    base_reg = summary_df[summary_df[\"Type\"] == \"Regression\"]\n",
    "    base_class = summary_df[summary_df[\"Type\"] == \"Classification\"]\n",
    "\n",
    "    trial_reg = trial_summary_df[trial_summary_df[\"Type\"] == \"Regression\"]\n",
    "    trial_class = trial_summary_df[trial_summary_df[\"Type\"] == \"Classification\"]\n",
    "\n",
    "    shot_reg = shot_summary_df[shot_summary_df[\"Type\"] == \"Regression\"]\n",
    "    shot_class = shot_summary_df[shot_summary_df[\"Type\"] == \"Classification\"]\n",
    "\n",
    "    # Generate joint injury summary from the base test data.\n",
    "    joint_injury_dict = summarize_joint_models(joint_models, test_data, timesteps, debug=debug)\n",
    "    joint_injury_df = pd.DataFrame.from_dict(joint_injury_dict, orient='index').reset_index().rename(columns={'index': 'Model'})\n",
    "    joint_injury_df[\"Type\"] = \"Classification\"\n",
    "\n",
    "    # Generate joint exhaustion summary from the base test data.\n",
    "    joint_exh_dict = summarize_joint_exhaustion_models(joint_exhaustion_models, test_data, timesteps, debug=debug)\n",
    "    joint_exh_df = pd.DataFrame.from_dict(joint_exh_dict, orient='index').reset_index().rename(columns={'index': 'Model'})\n",
    "    joint_exh_df[\"Type\"] = \"Regression\"\n",
    "    \n",
    "\n",
    "    # Build lists for each group.\n",
    "    regression_summaries = [base_reg, trial_reg, shot_reg]\n",
    "    classification_summaries = [base_class, trial_class, shot_class]\n",
    "    # Combine both joint summaries into one list.\n",
    "    joint_summaries = [joint_injury_df, joint_exh_df]\n",
    "\n",
    "    # Provide names for each dataset.\n",
    "    dataset_names = [\"Base\", \"Trial Aggregated\", \"Shot Aggregated\"]\n",
    "    # For joint models, you may label them as \"Joint Injury\" and \"Joint Exhaustion\".\n",
    "    joint_names = [\"Joint Injury Models\", \"Joint Exhaustion Models\"]\n",
    "\n",
    "    # Get the final combined summaries (including joint summaries).\n",
    "    final_reg, final_class, final_joint, final_all = final_model_summary(\n",
    "        regression_summaries, classification_summaries, \n",
    "        regression_names=dataset_names, classification_names=dataset_names,\n",
    "        joint_summaries=joint_summaries, joint_names=joint_names\n",
    "    )\n",
    "\n",
    "    print(\"=== Final Regression Summary ===\")\n",
    "    print(final_reg)\n",
    "    print(\"\\n=== Final Classification Summary ===\")\n",
    "    print(final_class)\n",
    "    print(\"\\n=== Final Joint Summary ===\")\n",
    "    print(final_joint)\n",
    "    print(\"\\n=== Final Combined Summary ===\")\n",
    "    print(final_all) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add in the datapreprocessor to try the pad vs dtw vs set_window\n",
    "\n",
    "https://github.com/ghadfield32/ml_preprocessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile ml/preprocess_train_predict/datapreprocessor_lstm_experimental_training.py\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import json\n",
    "from datetime import datetime\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from datapreprocessor import DataPreprocessor\n",
    "from darts import TimeSeries\n",
    "from ml.load_and_prepare_data.load_data_and_analyze import (\n",
    "    load_data, prepare_joint_features, feature_engineering, summarize_data)\n",
    "\n",
    "from ml.feature_selection.feature_selection import (\n",
    "    load_top_features, perform_feature_importance_analysis, save_top_features,\n",
    "    analyze_joint_injury_features, check_for_invalid_values,\n",
    "    perform_feature_importance_analysis, analyze_and_display_top_features)\n",
    "\n",
    "from ml.preprocess_train_predict.base_training import (\n",
    "    temporal_train_test_split, scale_features, create_sequences, train_exhaustion_model, \n",
    "    train_injury_model,  train_joint_models, forecast_and_plot_exhaustion, forecast_and_plot_injury,\n",
    "    forecast_and_plot_joint, summarize_regression_model, summarize_classification_model, \n",
    "    summarize_joint_models, summarize_all_models, final_model_summary, \n",
    "    summarize_joint_exhaustion_models\n",
    "    )\n",
    "\n",
    "\n",
    "def check_for_nulls(df, step_msg=\"\"):\n",
    "    \"\"\"Prints the total number of nulls and lists columns with null values.\"\"\"\n",
    "    total_nulls = df.isnull().sum().sum()\n",
    "    if total_nulls > 0:\n",
    "        cols_with_nulls = [col for col in df.columns if df[col].isnull().sum() > 0]\n",
    "        print(f\"[{step_msg}] WARNING: Found {total_nulls} null values in columns: {cols_with_nulls}\")\n",
    "    else:\n",
    "        print(f\"[{step_msg}] No null values found.\")\n",
    "        \n",
    "# Check for extreme values that might cause instability\n",
    "def analyze_data_distribution(X, name=\"Data\"):\n",
    "    print(f\"\\n{name} Analysis:\")\n",
    "    \n",
    "    if np.isnan(X).any():\n",
    "        print(f\"WARNING: Contains {np.isnan(X).sum()} NaN values\")\n",
    "    \n",
    "    if np.isinf(X).any():\n",
    "        print(f\"WARNING: Contains {np.isinf(X).sum()} infinite values\")\n",
    "    \n",
    "    # Calculate statistics per feature\n",
    "    for i in range(X.shape[-1]):\n",
    "        feature_data = X[:,:,i].flatten()\n",
    "        feature_data = feature_data[~np.isnan(feature_data)]  # Remove NaNs for calculation\n",
    "        \n",
    "        if len(feature_data) > 0:\n",
    "            print(f\"Feature {i}:\")\n",
    "            print(f\"  Range: {np.min(feature_data):.4f} to {np.max(feature_data):.4f}\")\n",
    "            print(f\"  Mean: {np.mean(feature_data):.4f}, Std: {np.std(feature_data):.4f}\")\n",
    "            \n",
    "            # Check for potential outliers\n",
    "            q1, q3 = np.percentile(feature_data, [25, 75])\n",
    "            iqr = q3 - q1\n",
    "            outlier_count = np.sum((feature_data < q1 - 1.5*iqr) | (feature_data > q3 + 1.5*iqr))\n",
    "            print(f\"  Potential outliers: {outlier_count} ({outlier_count/len(feature_data)*100:.2f}%)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_model_metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Evaluate model predictions using common metrics: MAE, RMSE, and R².\n",
    "    \n",
    "    Args:\n",
    "        y_true (array-like): Ground truth target values.\n",
    "        y_pred (array-like): Predicted values.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary with keys 'mae', 'rmse', and 'r2' representing the evaluation metrics.\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "    import numpy as np\n",
    "\n",
    "    # Ensure dimensions are compatible using the existing helper function.\n",
    "    y_true, y_pred = ensure_compatible_dimensions(y_true, y_pred)\n",
    "    \n",
    "    # Compute metrics.\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    return {'mae': mae, 'rmse': rmse, 'r2': r2}\n",
    "\n",
    "def generate_final_report(report_data):\n",
    "    \"\"\"\n",
    "    Generate and print a summary report for model evaluation metrics.\n",
    "\n",
    "    Args:\n",
    "        report_data (list of dict): A list where each dictionary contains:\n",
    "            'test' (str): A descriptive test name,\n",
    "            'mae' (float): The Mean Absolute Error,\n",
    "            'rmse' (float): The Root Mean Squared Error,\n",
    "            'r2' (float): The R² score.\n",
    "    \"\"\"\n",
    "    print(\"\\n\\n==== Final Evaluation Report ====\")\n",
    "    for entry in report_data:\n",
    "        print(f\"Test: {entry['test']}\")\n",
    "        print(f\"  MAE: {entry['mae']:.4f}\")\n",
    "        print(f\"  RMSE: {entry['rmse']:.4f}\")\n",
    "        print(f\"  R²: {entry['r2']:.4f}\\n\")\n",
    "    print(\"==== End of Report ====\\n\")\n",
    "\n",
    "\n",
    "#---------------------------------------------\n",
    "# Custom functions for preprocessing\n",
    "#---------------------------------------------\n",
    "def debug_datasets(variables, max_sample_rows=5):\n",
    "    \"\"\"\n",
    "    Debug multiple datasets with detailed information.\n",
    "    \n",
    "    Args:\n",
    "        variables (dict): Dictionary of variable_name: variable_value pairs to debug\n",
    "        max_sample_rows (int, optional): Maximum number of sample rows to display\n",
    "    \"\"\"\n",
    "    print(\"\\n==== DATASET DEBUG INFORMATION ====\")\n",
    "    \n",
    "    for name, value in variables.items():\n",
    "        print(f\"\\n[{name}]:\")\n",
    "        print(f\"  Type: {type(value)}\")\n",
    "        \n",
    "        if hasattr(value, 'shape'):\n",
    "            print(f\"  Shape: {value.shape}\")\n",
    "        elif isinstance(value, dict):\n",
    "            print(f\"  Length: {len(value)} items\")\n",
    "        elif hasattr(value, '__len__'):\n",
    "            print(f\"  Length: {len(value)}\")\n",
    "        \n",
    "        # Handle different data types\n",
    "        if isinstance(value, pd.DataFrame):\n",
    "            print(\"\\n  Data Sample:\")\n",
    "            print(value.head(max_sample_rows))\n",
    "            print(\"\\n  Columns:\")\n",
    "            print(value.columns.tolist())\n",
    "            print(\"\\n  Data Types:\")\n",
    "            print(value.dtypes)\n",
    "            print(f\"\\n  Missing Values: {value.isna().sum().sum()} total\")\n",
    "        elif isinstance(value, np.ndarray):\n",
    "            print(\"\\n  Array Sample:\")\n",
    "            if value.ndim == 1:\n",
    "                print(value[:min(max_sample_rows, value.shape[0])])\n",
    "            elif value.ndim == 2:\n",
    "                print(value[:min(max_sample_rows, value.shape[0]), :min(10, value.shape[1])])\n",
    "            else:\n",
    "                print(f\"  {value.ndim}-dimensional array (sample not shown)\")\n",
    "            print(f\"\\n  Data Type: {value.dtype}\")\n",
    "            if np.isnan(value).any():\n",
    "                print(f\"  Warning: Contains {np.isnan(value).sum()} NaN values\")\n",
    "        elif isinstance(value, dict):\n",
    "            print(\"\\n  Dictionary Keys:\")\n",
    "            print(list(value.keys())[:min(20, len(value))])\n",
    "            if len(value) > 20:\n",
    "                print(f\"  ... and {len(value) - 20} more keys\")\n",
    "        \n",
    "        # Add more detailed information for model prediction results\n",
    "        if name.startswith('result') and isinstance(value, tuple):\n",
    "            print(\"\\n  Tuple Contents:\")\n",
    "            for i, item in enumerate(value):\n",
    "                print(f\"  Element {i}:\")\n",
    "                print(f\"    Type: {type(item)}\")\n",
    "                if hasattr(item, 'shape'):\n",
    "                    print(f\"    Shape: {item.shape}\")\n",
    "                if isinstance(item, np.ndarray) and item.size > 0:\n",
    "                    print(f\"    Sample: {item.flatten()[:min(5, item.size)]}\")\n",
    "    \n",
    "    print(\"\\n==== END DEBUG INFORMATION ====\")\n",
    "\n",
    "# Example usage:\n",
    "def debug_preprocessing_result(result, expected_shape=None):\n",
    "    print(f\"Type of result: {type(result)}\")\n",
    "    \n",
    "    # Create a dictionary to pass to our debug function\n",
    "    debug_data = {\n",
    "        'result': result,\n",
    "        'summary': summary,\n",
    "        'test_data': test_data,\n",
    "        'train_data': train_data\n",
    "    }\n",
    "    \n",
    "    if expected_shape:\n",
    "        debug_data['expected_shape'] = expected_shape\n",
    "        \n",
    "    # If result is a tuple, add each component separately\n",
    "    if isinstance(result, tuple):\n",
    "        for i, item in enumerate(result):\n",
    "            debug_data[f'result_element_{i}'] = item\n",
    "            \n",
    "    # If we have sequence data, add those too\n",
    "    if 'y_test_seq' in globals():\n",
    "        debug_data['y_test_seq'] = y_test_seq\n",
    "    if 'y_train_seq' in globals():\n",
    "        debug_data['y_train_seq'] = y_train_seq\n",
    "        \n",
    "    debug_datasets(debug_data)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Updated usage example:\n",
    "# result = dtw_date_predict.final_preprocessing(new_data, model_input_shape=expected_shape)\n",
    "# result = debug_preprocessing_result(result, expected_shape)\n",
    "\n",
    "def select_complete_test_data(full_data, n_trials=2):\n",
    "    \"\"\"\n",
    "    Select a subset of the data that contains complete sequences with all phases.\n",
    "    \n",
    "    Args:\n",
    "        full_data (pd.DataFrame): The complete dataset\n",
    "        n_trials (int): Number of complete trials to select\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: A subset containing complete sequences with all phases\n",
    "    \"\"\"\n",
    "    # Get all unique phases in the dataset\n",
    "    all_phases = full_data['shooting_phases'].unique()\n",
    "    print(f\"All phases in dataset: {all_phases}\")\n",
    "    \n",
    "    # Find trials that contain all required phases\n",
    "    complete_trials = []\n",
    "    \n",
    "    # Get unique trial/session combinations\n",
    "    trial_combinations = full_data[['session_biomech', 'trial_biomech']].drop_duplicates().values\n",
    "    \n",
    "    for session, trial in trial_combinations:\n",
    "        # Get data for this trial\n",
    "        trial_data = full_data[(full_data['session_biomech'] == session) & \n",
    "                              (full_data['trial_biomech'] == trial)]\n",
    "        \n",
    "        # Check if this trial has all phases\n",
    "        trial_phases = set(trial_data['shooting_phases'].unique())\n",
    "        \n",
    "        if len(trial_phases) >= len(all_phases) - 1:  # Allow for one missing phase\n",
    "            complete_trials.append((session, trial, len(trial_data)))\n",
    "    \n",
    "    print(f\"Found {len(complete_trials)} trials with complete phase data\")\n",
    "    \n",
    "    # Sort by data size (descending) and select the top n_trials\n",
    "    complete_trials.sort(key=lambda x: x[2], reverse=True)\n",
    "    selected_trials = complete_trials[:n_trials]\n",
    "    \n",
    "    # Create a new DataFrame with the selected trials\n",
    "    test_data = pd.DataFrame()\n",
    "    for session, trial, _ in selected_trials:\n",
    "        trial_data = full_data[(full_data['session_biomech'] == session) & \n",
    "                              (full_data['trial_biomech'] == trial)]\n",
    "        print(f\"Selected trial {session}/{trial} with {len(trial_data)} samples and phases: {trial_data['shooting_phases'].unique()}\")\n",
    "        test_data = pd.concat([test_data, trial_data])\n",
    "    \n",
    "    return test_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from datetime import datetime\n",
    "    import matplotlib.pyplot as plt\n",
    "    import os\n",
    "    import logging\n",
    "    import yaml\n",
    "    import shutil\n",
    "    from tensorflow.keras.models import Sequential, load_model\n",
    "    from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "    from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "    from ml.load_and_prepare_data.load_data_and_analyze import (\n",
    "        load_data, prepare_joint_features, \n",
    "        feature_engineering, summarize_data, check_and_drop_nulls,\n",
    "        prepare_base_datasets)\n",
    "\n",
    "    from ml.feature_selection.feature_selection import (\n",
    "        load_top_features, perform_feature_importance_analysis, save_top_features,\n",
    "        analyze_joint_injury_features, check_for_invalid_values,\n",
    "        perform_feature_importance_analysis, analyze_and_display_top_features, \n",
    "        run_feature_importance_analysis, run_feature_import_and_load_top_features)\n",
    "\n",
    "    from ml.preprocess_train_predict.base_training import (\n",
    "        temporal_train_test_split, scale_features, create_sequences, train_exhaustion_model, \n",
    "        train_injury_model,  train_joint_models, forecast_and_plot_exhaustion, forecast_and_plot_injury,\n",
    "        forecast_and_plot_joint, summarize_regression_model, summarize_classification_model, \n",
    "        summarize_joint_models, summarize_all_models, final_model_summary, \n",
    "        summarize_joint_exhaustion_models\n",
    "        )\n",
    "    from ml.preprocess_train_predict.conformal_tights import (\n",
    "        train_conformal_model, predict_with_uncertainty, plot_conformal_results, add_time_series_forecasting,\n",
    "        add_conformal_to_exhaustion_model\n",
    "        )\n",
    "    from ml.preprocess_train_predict.darts_models_for_comparison import ( \n",
    "        preprocess_timeseries_darts, detect_anomalies_with_darts, enhanced_forecasting_with_darts_and_metrics)\n",
    "\n",
    "\n",
    "    graphs_output_dir=\"../../data/Deep_Learning_Final/graphs\"\n",
    "    transformers_dir=\"../../data/Deep_Learning_Final/transformers\"\n",
    "    debug = True\n",
    "    importance_threshold = 0.01\n",
    "    csv_path = \"../../data/processed/final_granular_dataset.csv\"\n",
    "    json_path = \"../../data/basketball/freethrow/participant_information.json\"\n",
    "    output_dir = \"../../data/Deep_Learning_Final\"\n",
    "    \n",
    "    base_feature_dir = os.path.join(output_dir, \"feature_lists/base\")\n",
    "    trial_feature_dir = os.path.join(output_dir, \"feature_lists/trial_summary\")\n",
    "    shot_feature_dir = os.path.join(output_dir, \"feature_lists/shot_phase_summary\")\n",
    "    \n",
    "    data, trial_df, shot_df = prepare_base_datasets(csv_path, json_path, debug=debug)\n",
    "    # check uniques in shooting_phases\n",
    "    print(\"Unique shooting phases:\", data['shooting_phases'].unique())\n",
    "    PHASE_MAP = {\n",
    "    'leg_cock':      'leg_cock',\n",
    "    'arm_cock':      'arm_cock',\n",
    "    'arm_release':   'arm_release',\n",
    "    'wrist_release': 'wrist_release'\n",
    "    }\n",
    "    EXPECTED_PHASES = {'leg_cock', 'arm_cock', 'arm_release', 'wrist_release'}\n",
    "\n",
    "    # Group the data first by trial_id (or another appropriate column)\n",
    "    for group_key, group_df in data.groupby('trial_id'):\n",
    "        raw_phases = group_df['shooting_phases'].unique().tolist()\n",
    "        normalized_phases = [PHASE_MAP.get(p, p) for p in raw_phases]\n",
    "        unique_phases = set(normalized_phases)\n",
    "\n",
    "        print(f\"[DEBUG] Group {group_key}\")\n",
    "        print(f\"  raw_phases      : {raw_phases}\")\n",
    "        print(f\"  normalized      : {normalized_phases}\")\n",
    "        print(f\"  unique_phases   : {unique_phases}\")\n",
    "        print(f\"  expected        : {EXPECTED_PHASES}\")\n",
    "\n",
    "        missing = EXPECTED_PHASES - unique_phases\n",
    "        if missing:\n",
    "            print(f\"  → DROPPING: missing {missing}\")\n",
    "            continue\n",
    "        # else: proceed to build sequences\n",
    "\n",
    "\n",
    "    numeric_features = data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    summary_targets = ['exhaustion_rate', 'injury_risk']\n",
    "    trial_summary_features = [col for col in trial_df.columns if col not in summary_targets]\n",
    "    trial_summary_features = [col for col in trial_summary_features if col in numeric_features]\n",
    "    shot_summary_features = [col for col in shot_df.columns if col not in summary_targets]\n",
    "    shot_summary_features = [col for col in shot_summary_features if col in numeric_features]\n",
    "    print(\"Available target columns:\", [c for c in data.columns if \"exhaustion\" in c])\n",
    "    # ========================================\n",
    "    # 1) Overall Base Dataset (including Joint-Specific Targets)\n",
    "    # ========================================\n",
    "    features = [\n",
    "        'joint_energy', 'joint_power', 'energy_acceleration',\n",
    "        'elbow_asymmetry', 'hip_asymmetry', 'ankle_asymmetry', 'wrist_asymmetry', 'knee_asymmetry', \n",
    "        '1stfinger_asymmetry', '5thfinger_asymmetry',\n",
    "        'elbow_power_ratio', 'hip_power_ratio', 'ankle_power_ratio', 'wrist_power_ratio', \n",
    "        'knee_power_ratio', '1stfinger_power_ratio', '5thfinger_power_ratio',\n",
    "        'L_KNEE_ROM', 'L_KNEE_ROM_deviation', 'L_KNEE_ROM_extreme',\n",
    "        'R_KNEE_ROM', 'R_KNEE_ROM_deviation', 'R_KNEE_ROM_extreme',\n",
    "        'L_SHOULDER_ROM', 'L_SHOULDER_ROM_deviation', 'L_SHOULDER_ROM_extreme',\n",
    "        'R_SHOULDER_ROM', 'R_SHOULDER_ROM_deviation', 'R_SHOULDER_ROM_extreme',\n",
    "        'L_HIP_ROM', 'L_HIP_ROM_deviation', 'L_HIP_ROM_extreme',\n",
    "        'R_HIP_ROM', 'R_HIP_ROM_deviation', 'R_HIP_ROM_extreme',\n",
    "        'L_ANKLE_ROM', 'L_ANKLE_ROM_deviation', 'L_ANKLE_ROM_extreme',\n",
    "        'R_ANKLE_ROM', 'R_ANKLE_ROM_deviation', 'R_ANKLE_ROM_extreme',\n",
    "        'exhaustion_lag1', 'power_avg_5', 'rolling_power_std', 'rolling_hr_mean',\n",
    "        'time_since_start', 'ema_exhaustion', 'rolling_exhaustion', 'rolling_energy_std',\n",
    "        'simulated_HR',\n",
    "        'player_height_in_meters', 'player_weight__in_kg'\n",
    "    ]\n",
    "    base_targets = ['exhaustion_rate', 'injury_risk']\n",
    "    joints = ['ANKLE', 'WRIST', 'ELBOW', 'KNEE', 'HIP']\n",
    "    joint_injury_targets = [f\"{side}_{joint}_injury_risk\" for joint in joints for side in ['L', 'R']]\n",
    "    joint_exhaustion_targets = [f\"{side}_{joint}_exhaustion_rate\" for joint in joints for side in ['L', 'R']]\n",
    "    joint_targets = joint_injury_targets + joint_exhaustion_targets\n",
    "    all_targets = base_targets + joint_targets\n",
    "\n",
    "    logging.info(\"=== Base Dataset Analysis (Overall + Joint-Specific) ===\")\n",
    "    base_loaded_features = run_feature_import_and_load_top_features(\n",
    "        dataset=data,\n",
    "        features=features,\n",
    "        targets=all_targets,\n",
    "        base_output_dir=output_dir,\n",
    "        output_subdir=\"feature_lists/base\",\n",
    "        debug=debug,\n",
    "        dataset_label=\"Base Data\",\n",
    "        importance_threshold=importance_threshold,\n",
    "        n_top=10,\n",
    "        run_analysis=False\n",
    "    )\n",
    "    print(f\"Base Loaded Features: {base_loaded_features}\")\n",
    "    \n",
    "    joint_feature_dict = {}\n",
    "    for target in joint_targets:\n",
    "        try:\n",
    "            feat_loaded = base_loaded_features.get(target, [])\n",
    "            logging.info(f\"Test Load: Features for {target}: {feat_loaded}\")\n",
    "            joint_feature_dict[target] = feat_loaded\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading features for {target}: {e}\")\n",
    "    \n",
    "    # ========================================\n",
    "    # 2) Trial Summary Dataset Analysis\n",
    "    # ========================================\n",
    "\n",
    "    \n",
    "    logging.info(\"=== Trial Summary Dataset Analysis ===\")\n",
    "    trial_loaded_features = run_feature_import_and_load_top_features(\n",
    "        dataset=trial_df,\n",
    "        features=trial_summary_features,\n",
    "        targets=summary_targets,\n",
    "        base_output_dir=output_dir,\n",
    "        output_subdir=\"feature_lists/trial_summary\",\n",
    "        debug=debug,\n",
    "        dataset_label=\"Trial Summary Data\",\n",
    "        importance_threshold=importance_threshold,\n",
    "        n_top=10,\n",
    "        run_analysis=False\n",
    "    )\n",
    "    features_exhaustion_trial = trial_loaded_features.get('exhaustion_rate', [])\n",
    "    features_injury_trial = trial_loaded_features.get('injury_risk', [])\n",
    "    trial_summary_data = trial_df.copy()\n",
    "    \n",
    "    # ========================================\n",
    "    # 3) Shot Phase Summary Dataset Analysis\n",
    "    # ========================================\n",
    "\n",
    "    \n",
    "    logging.info(\"=== Shot Phase Summary Dataset Analysis ===\")\n",
    "    shot_loaded_features = run_feature_import_and_load_top_features(\n",
    "        dataset=shot_df,\n",
    "        features=shot_summary_features,\n",
    "        targets=summary_targets,\n",
    "        base_output_dir=output_dir,\n",
    "        output_subdir=\"feature_lists/shot_phase_summary\",\n",
    "        debug=debug,\n",
    "        dataset_label=\"Shot Phase Summary Data\",\n",
    "        importance_threshold=importance_threshold,\n",
    "        n_top=10,\n",
    "        run_analysis=False\n",
    "    )\n",
    "    features_exhaustion_shot = shot_loaded_features.get('exhaustion_rate', [])\n",
    "    features_injury_shot = shot_loaded_features.get('injury_risk', [])\n",
    "    shot_phase_summary_data = shot_df.copy()\n",
    "    # Nominal/Categorical variables: For example, identifiers or labels (none of these apply here)\n",
    "    nominal_categorical = ['player_height_in_meters', 'player_weight__in_kg', 'shooting_phases']\n",
    "\n",
    "    # Ordinal/Categorical variables: Categorical variables with a natural order (none of these apply here)\n",
    "    ordinal_categorical = []\n",
    "\n",
    "    # Numerical variables: All of your features are continuous numerical measurements.\n",
    "    numerical = [\n",
    "        'joint_energy',\n",
    "        'joint_power',\n",
    "        'energy_acceleration',\n",
    "        'hip_asymmetry',\n",
    "        'wrist_asymmetry',\n",
    "        'rolling_power_std',\n",
    "        'rolling_hr_mean',\n",
    "        'rolling_energy_std',\n",
    "        'simulated_HR'\n",
    "    ]\n",
    "    # Set up logging for debugging purposes.\n",
    "    logging.basicConfig(level=logging.DEBUG, format='%(asctime)s [%(levelname)s] %(message)s')\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    features = nominal_categorical + ordinal_categorical + numerical\n",
    "    base_targets = ['exhaustion_rate', 'injury_risk']\n",
    "    \n",
    "    \n",
    "    # Load your training data\n",
    "    logger.info(f\"Training data loaded from {csv_path}. Shape: {data.shape}\")\n",
    "    \n",
    "    y_variable = [base_targets[0]]\n",
    "    ordinal_categoricals=ordinal_categorical\n",
    "    nominal_categoricals=nominal_categorical\n",
    "    numericals=numerical\n",
    "\n",
    "    from tensorflow.keras.losses import MeanSquaredError\n",
    "    # Define model building function\n",
    "    def build_lstm_model(input_shape, horizon=1):\n",
    "        \"\"\"\n",
    "        Build an LSTM model with an output layer that matches the specified horizon.\n",
    "        \n",
    "        Args:\n",
    "            input_shape: Tuple defining the input shape (timesteps, features)\n",
    "            horizon: Number of future timesteps to predict (output dimension)\n",
    "            \n",
    "        Returns:\n",
    "            A compiled Keras Sequential model\n",
    "        \"\"\"\n",
    "        model = Sequential([\n",
    "            LSTM(64, input_shape=input_shape, return_sequences=True),\n",
    "            Dropout(0.2),\n",
    "            LSTM(32),\n",
    "            Dropout(0.2),\n",
    "            Dense(horizon)  # Output dimension now dynamically set by horizon\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss=MeanSquaredError(), metrics=['mae'])\n",
    "        return model\n",
    "\n",
    "    def ensure_compatible_dimensions(targets, predictions):\n",
    "        \"\"\"\n",
    "        Ensure that targets and predictions have compatible dimensions for error metric calculation.\n",
    "        \n",
    "        This function converts inputs to NumPy arrays, squeezes the last dimension if it is 1\n",
    "        (to convert a (samples, time_steps, 1) array to (samples, time_steps)), truncates both arrays\n",
    "        to the minimum number of samples if they differ, and reshapes 1D arrays to 2D if needed.\n",
    "        \n",
    "        Args:\n",
    "            targets (array-like): Ground truth target values.\n",
    "            predictions (array-like): Predicted values.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple[np.ndarray, np.ndarray]: The adjusted target and prediction arrays.\n",
    "        \"\"\"\n",
    "        import numpy as np\n",
    "\n",
    "        # Convert inputs to NumPy arrays\n",
    "        targets = np.array(targets)\n",
    "        predictions = np.array(predictions)\n",
    "\n",
    "        # If targets or predictions have an extra dimension of size 1, squeeze that axis.\n",
    "        if targets.ndim == 3 and targets.shape[2] == 1:\n",
    "            targets = targets.squeeze(axis=2)\n",
    "        if predictions.ndim == 3 and predictions.shape[2] == 1:\n",
    "            predictions = predictions.squeeze(axis=2)\n",
    "\n",
    "        # If number of samples (first axis) differ, truncate both arrays to the minimum count.\n",
    "        if targets.shape[0] != predictions.shape[0]:\n",
    "            n_samples = min(targets.shape[0], predictions.shape[0])\n",
    "            targets = targets[:n_samples]\n",
    "            predictions = predictions[:n_samples]\n",
    "\n",
    "        # If one array is 1D and the other 2D, reshape the 1D array to 2D.\n",
    "        if targets.ndim == 1 and predictions.ndim == 2:\n",
    "            targets = targets.reshape(-1, 1)\n",
    "        elif predictions.ndim == 1 and targets.ndim == 2:\n",
    "            predictions = predictions.reshape(-1, 1)\n",
    "\n",
    "        # Debug print the adjusted shapes\n",
    "        print(f\"Adjusted shapes - targets: {targets.shape}, predictions: {predictions.shape}\")\n",
    "\n",
    "        return targets, predictions\n",
    "\n",
    "\n",
    "\n",
    "    def get_horizon_from_preprocessor(preprocessor):\n",
    "        \"\"\"\n",
    "        Extract the horizon parameter from the preprocessor.\n",
    "        \n",
    "        For DTW or pad modes, if the horizon has not been computed yet,\n",
    "        it returns the product of horizon_sequence_number and sequence_length.\n",
    "        Otherwise, it returns the computed horizon.\n",
    "        \"\"\"\n",
    "        if hasattr(preprocessor, 'time_series_sequence_mode') and preprocessor.time_series_sequence_mode in [\"dtw\", \"pad\"]:\n",
    "            if preprocessor.horizon is not None:\n",
    "                return preprocessor.horizon\n",
    "            else:\n",
    "                return preprocessor.horizon_sequence_number * preprocessor.sequence_length\n",
    "        elif hasattr(preprocessor, 'options') and isinstance(preprocessor.options, dict):\n",
    "            return preprocessor.options.get('horizon', 1)\n",
    "        elif hasattr(preprocessor, 'horizon'):\n",
    "            return preprocessor.horizon\n",
    "        else:\n",
    "            return 1  # Default horizon if not specified\n",
    "\n",
    "\n",
    "    \n",
    "    # ---------- Test 1: Percentage-based Split ----------\n",
    "    print(\"\\n\\n=== Test 1: Percentage-based Split (80/20) ===\")\n",
    "\n",
    "    # Clean transformers directory\n",
    "    shutil.rmtree('./transformers', ignore_errors=True)\n",
    "    os.makedirs('./transformers', exist_ok=True)\n",
    "    # Calculate the index to split the dataset into thirds\n",
    "    split_index = int(len(data) * (2 / 3))\n",
    "\n",
    "    # Set new_data as the last third of the dataset\n",
    "    new_data = data.iloc[split_index:].copy()\n",
    "    \n",
    "    # list columns\n",
    "    print(\"New data columns:\", new_data.columns.tolist())\n",
    "\n",
    "    # Debugging information\n",
    "    print(f\"Total dataset size: {len(data)}\")\n",
    "    print(f\"Split index (start of last third): {split_index}\")\n",
    "    print(f\"New data (last third) shape: {new_data.shape}\")\n",
    "\n",
    "    # Configure the preprocessor for training without explicit window_size, step_size, or horizon parameters.\n",
    "    preprocessor = DataPreprocessor(\n",
    "        model_type=\"LSTM\",\n",
    "        y_variable=y_variable,\n",
    "        ordinal_categoricals=ordinal_categoricals,\n",
    "        nominal_categoricals=nominal_categoricals,\n",
    "        numericals=numericals,\n",
    "        mode=\"train\",\n",
    "        options={\n",
    "            \"enabled\": True,\n",
    "            \"time_column\": \"datetime\",\n",
    "            \"horizon\": 10,  # Horizon value is provided here\n",
    "            \"step_size\": 1,  # Step size provided here\n",
    "            \"sequence_modes\": {\n",
    "                \"set_window\": {\n",
    "                    \"window_size\": 10,  # Window size provided here\n",
    "                    \"max_sequence_length\": 10\n",
    "                }\n",
    "            },\n",
    "            \"ts_sequence_mode\": \"set_window\",\n",
    "            \"split_dataset\": {\n",
    "                \"test_size\": 0.2,\n",
    "                \"random_state\": 42\n",
    "            },\n",
    "            \"time_series_split\": {\n",
    "                \"method\": \"standard\"\n",
    "            }\n",
    "        },\n",
    "        sequence_categorical=[\"trial_id\"],\n",
    "        sub_sequence_categorical=[\"shooting_phases\"],\n",
    "        time_series_sequence_mode=\"set_window\",\n",
    "        debug=True\n",
    "    )\n",
    "\n",
    "    # Preprocess the training data\n",
    "    X_train_seq, X_test_seq, y_train_seq, y_test_seq, recommendations, _ = preprocessor.final_ts_preprocessing(data)\n",
    "    \n",
    "    # Debug on the data\n",
    "    analyze_data_distribution(X_train_seq, \"Training Features\")\n",
    "    analyze_data_distribution(y_train_seq, \"Training Targets\")\n",
    "\n",
    "    print(f\"Train shapes - X: {X_train_seq.shape}, y: {y_train_seq.shape}\")\n",
    "    print(f\"Test shapes - X: {X_test_seq.shape}, y: {y_test_seq.shape}\")\n",
    "    \n",
    "    # Train a model\n",
    "    print(\"Training LSTM model with percentage-based split...\")\n",
    "    # Extract horizon from preprocessor\n",
    "    horizon = get_horizon_from_preprocessor(preprocessor)\n",
    "    print(f\"Using horizon of {horizon} for model output dimension\")\n",
    "\n",
    "    # Build the LSTM model using the extracted horizon\n",
    "    model1 = build_lstm_model((X_train_seq.shape[1], X_train_seq.shape[2]), horizon=horizon)\n",
    "\n",
    "    model1.fit(\n",
    "        X_train_seq, y_train_seq, \n",
    "        validation_data=(X_test_seq, y_test_seq),\n",
    "        epochs=10, batch_size=32, verbose=1\n",
    "    )\n",
    "    model1.save('./transformers/model_percentage_split.h5')\n",
    "    \n",
    "    # Predict using the test set\n",
    "    predictions = model1.predict(X_test_seq)\n",
    "    print(f\"Predictions shape: {predictions.shape}, Target shape: {y_test_seq.shape}\")\n",
    "    if predictions.shape[-1] != y_test_seq.shape[-1]:\n",
    "        print(f\"WARNING: Shape mismatch detected: predictions {predictions.shape} vs targets {y_test_seq.shape}\")\n",
    "        \n",
    "    # Apply dimension compatibility function\n",
    "    y_test_seq, predictions = ensure_compatible_dimensions(y_test_seq, predictions)\n",
    "\n",
    "    mae = mean_absolute_error(y_test_seq, predictions)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_seq, predictions))\n",
    "    print(f\"Model evaluation - MAE: {mae:.4f}, RMSE: {rmse:.4f}\")\n",
    "\n",
    "    \n",
    "    # Predict using predict mode\n",
    "    print(\"\\nTesting prediction mode with new data...\")\n",
    "    \n",
    "    # Take the last segment of data as \"new\" data for prediction\n",
    "    # new_data = data.iloc[-48:].copy()  \n",
    "    \n",
    "    # Configure the preprocessor for prediction\n",
    "    predict_preprocessor = DataPreprocessor(\n",
    "        model_type=\"LSTM\",\n",
    "        y_variable=y_variable,\n",
    "        ordinal_categoricals=ordinal_categoricals,\n",
    "        nominal_categoricals=nominal_categoricals,\n",
    "        numericals=numericals,\n",
    "        mode=\"predict\",\n",
    "        options={\n",
    "            \"enabled\": True,\n",
    "            \"time_column\": \"datetime\",\n",
    "            \"horizon\": 10,              # Number of time steps ahead to predict\n",
    "            \"step_size\": 1,             # Step size for moving the window\n",
    "            \"sequence_modes\": {         # Window configuration for sequence mode\n",
    "                \"set_window\": {\n",
    "                    \"window_size\": 10,         # Size of each window\n",
    "                    \"max_sequence_length\": 10  # Maximum sequence length\n",
    "                }\n",
    "            },\n",
    "            \"ts_sequence_mode\": \"set_window\"\n",
    "        },\n",
    "        sequence_categorical=[\"trial_id\"],\n",
    "        sub_sequence_categorical=[\"shooting_phases\"],\n",
    "        transformers_dir=\"./transformers\"\n",
    "    )\n",
    "\n",
    "    \n",
    "\n",
    "    # Make predictions\n",
    "    model1 = load_model('./transformers/model_percentage_split.h5')\n",
    "    expected_shape = model1.input_shape\n",
    "    print(f\"Expected model input shape: {expected_shape}\")\n",
    "\n",
    "    # Preprocess new data for prediction\n",
    "    # results = predict_preprocessor.final_ts_preprocessing(new_data, model_input_shape=expected_shape)\n",
    "    # X_new_preprocessed = results[0]\n",
    "    X_new_preprocessed, recommendations, X_inversed = predict_preprocessor.final_ts_preprocessing(new_data, model_input_shape=expected_shape)\n",
    "    \n",
    "    \n",
    "    print(f\"Prediction data shape: {X_new_preprocessed.shape}\")\n",
    "    \n",
    "    predictions = model1.predict(X_new_preprocessed)\n",
    "    print(f\"Prediction results shape: {predictions.shape}\")\n",
    "    print(f\"Predictions: {predictions[:5].flatten()}\")\n",
    "    # Compute and print evaluation metrics using the new function.\n",
    "    model1_metrics = evaluate_model_metrics(y_test_seq, predictions)\n",
    "    print(f\"Model evaluation metrics - MAE: {model1_metrics['mae']:.4f}, RMSE: {model1_metrics['rmse']:.4f}, R²: {model1_metrics['r2']:.4f}\")\n",
    "\n",
    "    \n",
    "    # ---------- Test 2: Date-based Split ----------\n",
    "    print(\"\\n\\n=== Test 2: Date-based Split (2025-02-14 11:00) ===\")\n",
    "    \n",
    "    # Clean transformers directory\n",
    "    shutil.rmtree('./transformers', ignore_errors=True)\n",
    "    os.makedirs('./transformers', exist_ok=True)\n",
    "\n",
    "    new_data = new_data.copy()\n",
    "    # Configure the preprocessor for training with date-based split\n",
    "    preprocessor = DataPreprocessor(\n",
    "        model_type=\"LSTM\",\n",
    "        y_variable=y_variable,\n",
    "        ordinal_categoricals=ordinal_categoricals,\n",
    "        nominal_categoricals=nominal_categoricals,\n",
    "        numericals=numericals,\n",
    "        mode=\"train\",\n",
    "        options={\n",
    "            \"enabled\": True,\n",
    "            \"time_column\": \"datetime\",\n",
    "            \"horizon\": 10,\n",
    "            \"step_size\": 1,\n",
    "            \"sequence_modes\": {\n",
    "                \"set_window\": {\n",
    "                    \"window_size\": 10,  # 1 day window\n",
    "                    \"max_sequence_length\": 10\n",
    "                }\n",
    "            },\n",
    "            \"ts_sequence_mode\": \"set_window\",\n",
    "            \"split_dataset\": {\n",
    "                \"test_size\": 0.2,  # Not used for date-based split\n",
    "                \"random_state\": 42,\n",
    "                \"time_split_column\": \"datetime\",\n",
    "                \"time_split_value\": pd.Timestamp(\"2025-02-14 11:50:00\")\n",
    "            },\n",
    "            \"time_series_split\": {\n",
    "                \"method\": \"standard\"\n",
    "            }\n",
    "        },\n",
    "        sequence_categorical=[\"trial_id\"],\n",
    "        sub_sequence_categorical=[\"shooting_phases\"],\n",
    "        time_series_sequence_mode=\"set_window\",\n",
    "        debug=True\n",
    "    )\n",
    "    \n",
    "    # Preprocess the training data\n",
    "    X_train_seq, X_test_seq, y_train_seq, y_test_seq, recommendations, _ = preprocessor.final_ts_preprocessing(data)\n",
    "    \n",
    "    print(f\"Train shapes - X: {X_train_seq.shape}, y: {y_train_seq.shape}\")\n",
    "    print(f\"Test shapes - X: {X_test_seq.shape}, y: {y_test_seq.shape}\")\n",
    "    \n",
    "    # Train a model\n",
    "    print(\"Training LSTM model with date-based split...\")\n",
    "    # Extract horizon from preprocessor\n",
    "    horizon = get_horizon_from_preprocessor(preprocessor)\n",
    "    print(f\"Using horizon of {horizon} for model output dimension\")\n",
    "\n",
    "    # Build the LSTM model using the extracted horizon\n",
    "    model2 = build_lstm_model((X_train_seq.shape[1], X_train_seq.shape[2]), horizon=horizon)\n",
    "\n",
    "    model2.fit(\n",
    "        X_train_seq, y_train_seq, \n",
    "        validation_data=(X_test_seq, y_test_seq),\n",
    "        epochs=10, batch_size=32, verbose=1\n",
    "    )\n",
    "    model2.save('./transformers/model_date_split.h5')\n",
    "    \n",
    "    # Test prediction mode\n",
    "    print(\"\\nTesting prediction mode with new data...\")\n",
    "    \n",
    "    # Take the last segment of data as \"new\" data for prediction\n",
    "    # new_data = data.iloc[-48:].copy()  \n",
    "    \n",
    "    # Configure the preprocessor for prediction\n",
    "    predict_preprocessor = DataPreprocessor(\n",
    "        model_type=\"LSTM\",\n",
    "        y_variable=y_variable,\n",
    "        ordinal_categoricals=ordinal_categoricals,\n",
    "        nominal_categoricals=nominal_categoricals,\n",
    "        numericals=numericals,\n",
    "        mode=\"predict\",\n",
    "        options={\n",
    "            \"enabled\": True,\n",
    "            \"time_column\": \"datetime\",\n",
    "            \"horizon\": 10,              # Number of time steps ahead to predict\n",
    "            \"step_size\": 1,             # Step size for moving the window\n",
    "            \"sequence_modes\": {         # Window configuration for sequence mode\n",
    "                \"set_window\": {\n",
    "                    \"window_size\": 10,         # Size of each window\n",
    "                    \"max_sequence_length\": 10  # Maximum sequence length\n",
    "                }\n",
    "            },\n",
    "            \"ts_sequence_mode\": \"set_window\"\n",
    "        },\n",
    "        sequence_categorical=[\"trial_id\"],\n",
    "        sub_sequence_categorical=[\"shooting_phases\"],\n",
    "        transformers_dir=\"./transformers\"\n",
    "    )\n",
    "\n",
    "    \n",
    "    # Preprocess new data for prediction\n",
    "    # results = predict_preprocessor.final_ts_preprocessing(new_data, model_input_shape=expected_shape)\n",
    "    # X_new_preprocessed = results[0]\n",
    "    X_new_preprocessed, recommendations, X_inversed = predict_preprocessor.final_ts_preprocessing(new_data, model_input_shape=expected_shape)\n",
    "    \n",
    "    # Make predictions\n",
    "    model2 = load_model('./transformers/model_date_split.h5')\n",
    "    expected_shape = model2.input_shape\n",
    "    print(f\"Expected model input shape: {expected_shape}\")\n",
    "    predictions = model2.predict(X_new_preprocessed)\n",
    "    print(f\"Prediction results shape: {predictions.shape}\")\n",
    "    # Apply dimension compatibility function\n",
    "    y_test_seq, predictions = ensure_compatible_dimensions(y_test_seq, predictions)\n",
    "\n",
    "    # Compute and print evaluation metrics using the new function.\n",
    "    model2_metrics = evaluate_model_metrics(y_test_seq, predictions)\n",
    "    print(f\"Model evaluation metrics - MAE: {model2_metrics['mae']:.4f}, RMSE: {model2_metrics['rmse']:.4f}, R²: {model2_metrics['r2']:.4f}\")\n",
    "\n",
    "    # ---------- Test 3: PSI-based Split with Feature-Engine ----------\n",
    "    print(\"\\n\\n=== Test 3: PSI-based Split with Feature-Engine ===\")\n",
    "    \n",
    "    # Clean transformers directory\n",
    "    shutil.rmtree('./transformers', ignore_errors=True)\n",
    "    os.makedirs('./transformers', exist_ok=True)\n",
    "\n",
    "    new_data = new_data.copy()\n",
    "    # Configure the preprocessor for training with PSI-based split\n",
    "    preprocessor = DataPreprocessor(\n",
    "        model_type=\"LSTM\",\n",
    "        y_variable=y_variable,\n",
    "        ordinal_categoricals=ordinal_categoricals,\n",
    "        nominal_categoricals=nominal_categoricals,\n",
    "        numericals=numericals,\n",
    "        mode=\"train\",\n",
    "        options={\n",
    "            \"enabled\": True,\n",
    "            \"time_column\": \"datetime\",\n",
    "            \"horizon\": 10,\n",
    "            \"step_size\": 1,\n",
    "            \"sequence_modes\": {\n",
    "                \"set_window\": {\n",
    "                    \"window_size\": 10,  # 1 day window\n",
    "                    \"max_sequence_length\": 10\n",
    "                }\n",
    "            },\n",
    "            \"ts_sequence_mode\": \"set_window\",\n",
    "            \"psi_feature_selection\": {\n",
    "                \"enabled\": True,\n",
    "                \"threshold\": 0.25,\n",
    "                \"split_frac\": 0.75,\n",
    "                \"split_distinct\": False,\n",
    "                \"apply_before_split\": True\n",
    "            },\n",
    "            \"feature_engine_split\": {\n",
    "                \"enabled\": True,\n",
    "                \"split_frac\": 0.75,\n",
    "                \"split_distinct\": False\n",
    "            },\n",
    "            \"time_series_split\": {\n",
    "                \"method\": \"feature_engine\"\n",
    "            }\n",
    "        },\n",
    "        # sequence_categorical=[\"trial_id\"],\n",
    "        # sub_sequence_categorical=[\"shooting_phases\"],\n",
    "        time_series_sequence_mode=\"set_window\",\n",
    "        debug=True,\n",
    "        graphs_output_dir=\"./plots\"\n",
    "    )\n",
    "    \n",
    "    # Preprocess the training data\n",
    "    X_train_seq, X_test_seq, y_train_seq, y_test_seq, recommendations, _ = preprocessor.final_ts_preprocessing(data)\n",
    "    \n",
    "    print(f\"Train shapes - X: {X_train_seq.shape}, y: {y_train_seq.shape}\")\n",
    "    print(f\"Test shapes - X: {X_test_seq.shape}, y: {y_test_seq.shape}\")\n",
    "    \n",
    "    # Visualize PSI results if the method was run\n",
    "    preprocessor.visualize_psi_results(data, top_n=5)\n",
    "    \n",
    "    # Train a model\n",
    "    print(\"Training LSTM model with PSI-based split...\")\n",
    "    # Extract horizon from preprocessor\n",
    "    horizon = get_horizon_from_preprocessor(preprocessor)\n",
    "    print(f\"Using horizon of {horizon} for model output dimension\")\n",
    "\n",
    "    # Build the LSTM model using the extracted horizon\n",
    "    model3 = build_lstm_model((X_train_seq.shape[1], X_train_seq.shape[2]), horizon=horizon)\n",
    "\n",
    "    model3.fit(\n",
    "        X_train_seq, y_train_seq, \n",
    "        validation_data=(X_test_seq, y_test_seq),\n",
    "        epochs=10, batch_size=32, verbose=1\n",
    "    )\n",
    "    model3.save('./transformers/model_psi_split.h5')\n",
    "    \n",
    "    # Test prediction mode\n",
    "    print(\"\\nTesting prediction mode with new data...\")\n",
    "    \n",
    "    # Take the last segment of data as \"new\" data for prediction\n",
    "    # new_data = data.iloc[-48:].copy()  \n",
    "    \n",
    "    # Configure the preprocessor for prediction\n",
    "    predict_preprocessor = DataPreprocessor(\n",
    "        model_type=\"LSTM\",\n",
    "        y_variable=y_variable,\n",
    "        ordinal_categoricals=ordinal_categoricals,\n",
    "        nominal_categoricals=nominal_categoricals,\n",
    "        numericals=numericals,\n",
    "        mode=\"predict\",\n",
    "        options={\n",
    "            \"enabled\": True,\n",
    "            \"time_column\": \"datetime\",\n",
    "            \"horizon\": 10,              # Number of time steps ahead to predict\n",
    "            \"step_size\": 1,             # Step size for moving the window\n",
    "            \"sequence_modes\": {         # Window configuration for sequence mode\n",
    "                \"set_window\": {\n",
    "                    \"window_size\": 10,         # Size of each window\n",
    "                    \"max_sequence_length\": 10  # Maximum sequence length\n",
    "                }\n",
    "            },\n",
    "            \"ts_sequence_mode\": \"set_window\"\n",
    "        },\n",
    "        sequence_categorical=[\"trial_id\"],\n",
    "        sub_sequence_categorical=[\"shooting_phases\"],\n",
    "        transformers_dir=\"./transformers\"\n",
    "    )\n",
    "\n",
    "    \n",
    "    # Preprocess new data for prediction\n",
    "    # results = predict_preprocessor.final_ts_preprocessing(new_data, model_input_shape=expected_shape)\n",
    "    # X_new_preprocessed = results[0]\n",
    "    X_new_preprocessed, recommendations, X_inversed = predict_preprocessor.final_ts_preprocessing(new_data, model_input_shape=expected_shape)\n",
    "    \n",
    "    # Make predictions\n",
    "    model3 = load_model('./transformers/model_psi_split.h5')\n",
    "    expected_shape = model3.input_shape\n",
    "    print(f\"Expected model input shape: {expected_shape}\")\n",
    "    predictions = model3.predict(X_new_preprocessed)\n",
    "    print(f\"Prediction results shape: {predictions.shape}\")\n",
    "\n",
    "    # Apply dimension compatibility function\n",
    "    y_test_seq, predictions = ensure_compatible_dimensions(y_test_seq, predictions)\n",
    "\n",
    "    # Compute and print evaluation metrics using the new function.\n",
    "    model3_metrics = evaluate_model_metrics(y_test_seq, predictions)\n",
    "    print(f\"Model evaluation metrics - MAE: {model3_metrics['mae']:.4f}, RMSE: {model3_metrics['rmse']:.4f}, R²: {model3_metrics['r2']:.4f}\")\n",
    "\n",
    "    # ---------- Test 4: DTW/Pad Mode with PSI-based Split ----------\n",
    "    print(\"\\n\\n=== Test 4: DTW/Pad Mode with PSI-based Split ===\")\n",
    "    \n",
    "    # Clean transformers directory\n",
    "    shutil.rmtree('./transformers', ignore_errors=True)\n",
    "    os.makedirs('./transformers', exist_ok=True)\n",
    "\n",
    "    new_data = new_data.copy()\n",
    "    # Configure the preprocessor for training with DTW mode\n",
    "    preprocessor = DataPreprocessor(\n",
    "        model_type=\"LSTM\",\n",
    "        y_variable=y_variable,\n",
    "        ordinal_categoricals=ordinal_categoricals,\n",
    "        nominal_categoricals=nominal_categoricals,\n",
    "        numericals=numericals,\n",
    "        mode=\"train\",\n",
    "        options={\n",
    "            \"enabled\": True,\n",
    "            \"time_column\": \"datetime\",\n",
    "            # \"horizon\": 379,\n",
    "            \"use_horizon_sequence\": True,\n",
    "            \"horizon_sequence_number\": 1,\n",
    "            \"step_size\": 1,\n",
    "            \"sequence_modes\": {\n",
    "                \"pad\": {\n",
    "                    \"pad_threshold\": 1.2,  # Allows up to 90% padding\n",
    "                    \"padding_side\": \"post\"\n",
    "                }\n",
    "            },\n",
    "            \"ts_sequence_mode\": \"pad\",\n",
    "            \"psi_feature_selection\": {\n",
    "                \"enabled\": True,\n",
    "                \"threshold\": 0.25,\n",
    "                \"split_frac\": 0.75,\n",
    "                \"split_distinct\": False,\n",
    "                \"apply_before_split\": True\n",
    "            },\n",
    "            \"feature_engine_split\": {\n",
    "                \"enabled\": True,\n",
    "                \"split_frac\": 0.75,\n",
    "                \"split_distinct\": False\n",
    "            },\n",
    "            \"time_series_split\": {\n",
    "                \"method\": \"feature_engine\"\n",
    "            }\n",
    "        },\n",
    "        sequence_categorical=[\"trial_id\"],\n",
    "        sub_sequence_categorical=[\"shooting_phases\"],\n",
    "        time_series_sequence_mode=\"pad\",\n",
    "        debug=True,\n",
    "        graphs_output_dir=\"./plots\"\n",
    "    )\n",
    "    \n",
    "    # Preprocess the training data\n",
    "    X_train_seq, X_test_seq, y_train_seq, y_test_seq, recommendations, _ = preprocessor.final_ts_preprocessing(data)\n",
    "    \n",
    "    print(f\"Train shapes - X: {X_train_seq.shape}, y: {y_train_seq.shape}\")\n",
    "    print(f\"Test shapes - X: {X_test_seq.shape}, y: {y_test_seq.shape}\")\n",
    "    \n",
    "    # Train a model\n",
    "    print(\"Training LSTM model with DTW/Pad mode...\")\n",
    "    # Extract horizon from preprocessor\n",
    "    horizon = get_horizon_from_preprocessor(preprocessor)\n",
    "    print(f\"Using horizon of {horizon} for model output dimension\")\n",
    "\n",
    "    print(f\"X_train_seq shape: {X_train_seq.shape}, X_test_seq shape: {X_test_seq.shape}\")\n",
    "    print(f\"y_train_seq shape: {y_train_seq.shape}, y_test_seq shape: {y_test_seq.shape}\")\n",
    "\n",
    "    # Build the LSTM model using the extracted horizon\n",
    "    model4 = build_lstm_model((X_train_seq.shape[1], X_train_seq.shape[2]), horizon=horizon)\n",
    "\n",
    "    model4.fit(\n",
    "        X_train_seq, y_train_seq, \n",
    "        validation_data=(X_test_seq, y_test_seq),\n",
    "        epochs=10, batch_size=32, verbose=1\n",
    "    )\n",
    "    model4.save('./transformers/model_dtw_pad.h5')\n",
    "    \n",
    "    # Test prediction mode\n",
    "    print(\"\\nTesting prediction mode with new data...\")\n",
    "    \n",
    "    # Take the last segment of data as \"new\" data for prediction\n",
    "    # new_data = data.iloc[-48:].copy()  \n",
    "    \n",
    "    # Configure the preprocessor for prediction\n",
    "    predict_preprocessor = DataPreprocessor(\n",
    "        model_type=\"LSTM\",\n",
    "        y_variable=y_variable,\n",
    "        ordinal_categoricals=ordinal_categoricals,\n",
    "        nominal_categoricals=nominal_categoricals,\n",
    "        numericals=numericals,\n",
    "        mode=\"predict\",\n",
    "        options={\n",
    "            \"enabled\": True,\n",
    "            \"time_column\": \"datetime\",\n",
    "            \"ts_sequence_mode\": \"pad\"\n",
    "        },\n",
    "        sequence_categorical=[\"trial_id\"],\n",
    "        sub_sequence_categorical=[\"shooting_phases\"],\n",
    "        time_series_sequence_mode=\"pad\",\n",
    "        transformers_dir=\"./transformers\"\n",
    "    )\n",
    "    \n",
    "    # Preprocess new data for prediction\n",
    "    # results = predict_preprocessor.final_ts_preprocessing(new_data, model_input_shape=expected_shape)\n",
    "    # X_new_preprocessed = results[0]\n",
    "    X_new_preprocessed, recommendations, X_inversed = predict_preprocessor.final_ts_preprocessing(new_data, model_input_shape=expected_shape)\n",
    "    \n",
    "    # Make predictions\n",
    "    model4 = load_model('./transformers/model_dtw_pad.h5')\n",
    "    expected_shape = model4.input_shape\n",
    "    print(f\"Expected model input shape: {expected_shape}\")\n",
    "    predictions = model4.predict(X_new_preprocessed)\n",
    "    print(f\"Prediction results shape: {predictions.shape}\")\n",
    "    \n",
    "    print(\"\\n\\nAll tests completed successfully!\")\n",
    "\n",
    "    # Apply dimension compatibility function\n",
    "    y_test_seq, predictions = ensure_compatible_dimensions(y_test_seq, predictions)\n",
    "\n",
    "    # Compute and print evaluation metrics using the new function.\n",
    "    model4_metrics = evaluate_model_metrics(y_test_seq, predictions)\n",
    "    print(f\"Model evaluation metrics - MAE: {model4_metrics['mae']:.4f}, RMSE: {model4_metrics['rmse']:.4f}, R²: {model4_metrics['r2']:.4f}\")\n",
    "\n",
    "\n",
    "    # Clean transformers directory\n",
    "    shutil.rmtree('./transformers', ignore_errors=True)\n",
    "    os.makedirs('./transformers', exist_ok=True)\n",
    "\n",
    "    new_data = new_data.copy()\n",
    "    # Configure the preprocessor for training with DTW mode\n",
    "    preprocessor = DataPreprocessor(\n",
    "        model_type=\"LSTM\",\n",
    "        y_variable=y_variable,\n",
    "        ordinal_categoricals=ordinal_categoricals,\n",
    "        nominal_categoricals=nominal_categoricals,\n",
    "        numericals=numericals,\n",
    "        mode=\"train\",\n",
    "        options={\n",
    "            \"enabled\": True,\n",
    "            \"time_column\": \"datetime\",\n",
    "            # \"horizon\": 379,\n",
    "            \"use_horizon_sequence\": True,\n",
    "            \"horizon_sequence_number\": 1,\n",
    "            \"step_size\": 1,\n",
    "            \"sequence_modes\": {\n",
    "                \"dtw\": {\n",
    "                    \"reference_sequence\": \"mean\",  # Use mean sequence as reference\n",
    "                    \"dtw_threshold\": 3.0          # DTW threshold for sequences\n",
    "                }\n",
    "            },\n",
    "            \"ts_sequence_mode\": \"dtw\",\n",
    "            \"psi_feature_selection\": {\n",
    "                \"enabled\": True,\n",
    "                \"threshold\": 0.25,\n",
    "                \"split_frac\": 0.75,\n",
    "                \"split_distinct\": False,\n",
    "                \"apply_before_split\": True\n",
    "            },\n",
    "            \"feature_engine_split\": {\n",
    "                \"enabled\": True,\n",
    "                \"split_frac\": 0.75,\n",
    "                \"split_distinct\": False\n",
    "            },\n",
    "            \"time_series_split\": {\n",
    "                \"method\": \"feature_engine\"\n",
    "            }\n",
    "        },\n",
    "        sequence_categorical=[\"trial_id\"],\n",
    "        sub_sequence_categorical=[\"shooting_phases\"],\n",
    "        time_series_sequence_mode=\"dtw\",\n",
    "        debug=True,\n",
    "        graphs_output_dir=\"./plots\"\n",
    "    )\n",
    "    \n",
    "    # Preprocess the training data\n",
    "    X_train_seq, X_test_seq, y_train_seq, y_test_seq, recommendations, _ = preprocessor.final_ts_preprocessing(data)\n",
    "    \n",
    "    print(f\"Train shapes - X: {X_train_seq.shape}, y: {y_train_seq.shape}\")\n",
    "    print(f\"Test shapes - X: {X_test_seq.shape}, y: {y_test_seq.shape}\")\n",
    "    \n",
    "    # Train a model\n",
    "    print(\"Training LSTM model with DTW/Pad mode...\")\n",
    "    # Extract horizon from preprocessor\n",
    "    horizon = get_horizon_from_preprocessor(preprocessor)\n",
    "    print(f\"Using horizon of {horizon} for model output dimension\")\n",
    "\n",
    "    # Build the LSTM model using the extracted horizon\n",
    "    model5 = build_lstm_model((X_train_seq.shape[1], X_train_seq.shape[2]), horizon=horizon)\n",
    "\n",
    "    model5.fit(\n",
    "        X_train_seq, y_train_seq, \n",
    "        validation_data=(X_test_seq, y_test_seq),\n",
    "        epochs=10, batch_size=32, verbose=1\n",
    "    )\n",
    "    model5.save('./transformers/model_dtw.h5')\n",
    "    \n",
    "    # Test prediction mode\n",
    "    print(\"\\nTesting prediction mode with new data...\")\n",
    "    \n",
    "    # Take the last segment of data as \"new\" data for prediction\n",
    "    # new_data = data.iloc[-48:].copy()  \n",
    "    \n",
    "    # Configure the preprocessor for prediction\n",
    "    predict_preprocessor = DataPreprocessor(\n",
    "        model_type=\"LSTM\",\n",
    "        y_variable=y_variable,\n",
    "        ordinal_categoricals=ordinal_categoricals,\n",
    "        nominal_categoricals=nominal_categoricals,\n",
    "        numericals=numericals,\n",
    "        mode=\"predict\",\n",
    "        options={\n",
    "            \"enabled\": True,\n",
    "            \"time_column\": \"datetime\",\n",
    "            \"ts_sequence_mode\": \"dtw\"\n",
    "        },\n",
    "        sequence_categorical=[\"trial_id\"],\n",
    "        sub_sequence_categorical=[\"shooting_phases\"],\n",
    "        time_series_sequence_mode=\"dtw\",\n",
    "        transformers_dir=\"./transformers\"\n",
    "    )\n",
    "    # Load your model to extract the input shape\n",
    "    model5 = load_model('./transformers/model_dtw.h5')\n",
    "    expected_shape = model5.input_shape\n",
    "    print(f\"Expected model input shape: {expected_shape}\")\n",
    "\n",
    "    # Preprocess new data for prediction\n",
    "    # results = predict_preprocessor.final_ts_preprocessing(new_data, model_input_shape=expected_shape)\n",
    "    # X_new_preprocessed = results[0]\n",
    "    X_new_preprocessed, recommendations, X_inversed = predict_preprocessor.final_ts_preprocessing(new_data, model_input_shape=expected_shape)\n",
    "    \n",
    "    # Make predictions\n",
    "    # model5 = load_model('./transformers/model_dtw.h5')\n",
    "    # expected_shape = model5.input_shape\n",
    "    # print(f\"Expected model input shape: {expected_shape}\")\n",
    "    predictions = model5.predict(X_new_preprocessed)\n",
    "    print(f\"Prediction results shape: {predictions.shape}\")\n",
    "    \n",
    "    print(\"\\n\\nAll tests completed successfully!\")\n",
    "\n",
    "    # Test 5: Pad Mode with Percentage-Based Sequence-Aware Split\n",
    "    print(\"\\n\\n=== Test 5: Pad Mode with Percentage-Based Sequence-Aware Split ===\")\n",
    "\n",
    "    # Clean transformers directory\n",
    "    shutil.rmtree('./transformers', ignore_errors=True)\n",
    "    os.makedirs('./transformers', exist_ok=True)\n",
    "    new_data = new_data.copy()\n",
    "    # Configure preprocessor for training with pad mode and percentage-based split\n",
    "    pad_pct_preprocessor = DataPreprocessor(\n",
    "        model_type=\"LSTM\",\n",
    "        y_variable=y_variable,\n",
    "        ordinal_categoricals=ordinal_categoricals,\n",
    "        nominal_categoricals=nominal_categoricals,\n",
    "        numericals=numericals,\n",
    "        mode=\"train\",\n",
    "        options={\n",
    "            \"enabled\": True,\n",
    "            \"time_column\": \"datetime\",\n",
    "            # \"horizon\": 379,\n",
    "            \"use_horizon_sequence\": True,\n",
    "            \"horizon_sequence_number\": 1,\n",
    "            \"step_size\": 1,\n",
    "            \"sequence_modes\": {\n",
    "                \"pad\": {\n",
    "                    \"pad_threshold\": 0.3,  # Allows up to 90% padding\n",
    "                    \"padding_side\": \"post\"\n",
    "                }\n",
    "            },\n",
    "            \"time_series_split\": {\n",
    "                \"method\": \"sequence_aware\",  # Use sequence-aware splitting\n",
    "                # \"test_size\": 0.2,            # Use 20% of sequences for testing\n",
    "                'target_train_fraction': 0.8,  # Aim for 80% training, 20% testing\n",
    "                \"debug_phases\": True         # Enable detailed phase debugging\n",
    "            }\n",
    "        },\n",
    "        sequence_categorical=[\"trial_id\"],\n",
    "        sub_sequence_categorical=[\"shooting_phases\"],\n",
    "        time_series_sequence_mode=\"pad\",\n",
    "        debug=True,\n",
    "        graphs_output_dir=\"./plots\"\n",
    "    )\n",
    "\n",
    "    # Preprocess training data\n",
    "    X_train_seq, X_test_seq, y_train_seq, y_test_seq, recommendations, _ = pad_pct_preprocessor.final_ts_preprocessing(data)\n",
    "\n",
    "    print(f\"Train shapes - X: {X_train_seq.shape}, y: {y_train_seq.shape}\")\n",
    "    print(f\"Test shapes - X: {X_test_seq.shape}, y: {y_test_seq.shape}\")\n",
    "\n",
    "    # Train model\n",
    "    print(\"Training LSTM model with pad mode and percentage-based split...\")\n",
    "    horizon = get_horizon_from_preprocessor(pad_pct_preprocessor)\n",
    "    print(f\"Using horizon of {horizon} for model output dimension\")\n",
    "\n",
    "    model5 = build_lstm_model((X_train_seq.shape[1], X_train_seq.shape[2]), horizon=horizon)\n",
    "    model5.fit(\n",
    "        X_train_seq, y_train_seq, \n",
    "        validation_data=(X_test_seq, y_test_seq),\n",
    "        epochs=10, batch_size=32, verbose=1\n",
    "    )\n",
    "    model5.save('./transformers/model_pad_pct.h5')\n",
    "\n",
    "    # Test prediction\n",
    "    print(\"\\nTesting prediction mode with new data...\")\n",
    "    # new_data = data.iloc[-48:].copy()\n",
    "\n",
    "    pad_pct_predict = DataPreprocessor(\n",
    "        model_type=\"LSTM\",\n",
    "        y_variable=y_variable,\n",
    "        ordinal_categoricals=ordinal_categoricals,\n",
    "        nominal_categoricals=nominal_categoricals,\n",
    "        numericals=numericals,\n",
    "        mode=\"predict\",\n",
    "        options={\n",
    "            \"enabled\": True,\n",
    "            \"time_column\": \"datetime\",\n",
    "            \"time_series_split\": {\n",
    "                \"method\": \"sequence_aware\",  # Use sequence-aware splitting\n",
    "                # \"test_size\": 0.2,            # Use 20% of sequences for testing\n",
    "                'target_train_fraction': 0.8,  # Aim for 80% training, 20% testing\n",
    "                \"debug_phases\": True         # Enable detailed phase debugging\n",
    "            }\n",
    "        },\n",
    "        sequence_categorical=[\"trial_id\"],\n",
    "        sub_sequence_categorical=[\"shooting_phases\"],\n",
    "        time_series_sequence_mode=\"pad\",\n",
    "        transformers_dir=\"./transformers\"\n",
    "    )\n",
    "\n",
    "    # result = pad_pct_predict.final_ts_preprocessing(new_data, model_input_shape=expected_shape)\n",
    "    # X_new_preprocessed = result[0]\n",
    "    X_new_preprocessed, recommendations, X_inversed = pad_pct_predict.final_ts_preprocessing(new_data, model_input_shape=expected_shape)\n",
    "    \n",
    "    model5 = load_model('./transformers/model_pad_pct.h5')\n",
    "    expected_shape = model5.input_shape\n",
    "    print(f\"Expected model input shape: {expected_shape}\")\n",
    "    predictions = model5.predict(X_new_preprocessed)\n",
    "    print(f\"Prediction results shape: {predictions.shape}\")\n",
    "    # Apply dimension compatibility function\n",
    "    y_test_seq, predictions = ensure_compatible_dimensions(y_test_seq, predictions)\n",
    "\n",
    "    # Compute and print evaluation metrics using the new function.\n",
    "    model5_metrics = evaluate_model_metrics(y_test_seq, predictions)\n",
    "    print(f\"Model evaluation model5_metrics - MAE: {model5_metrics['mae']:.4f}, RMSE: {model5_metrics['rmse']:.4f}, R²: {model5_metrics['r2']:.4f}\")\n",
    "\n",
    "    # Test 6: Pad Mode with Date-Based Sequence-Aware Split\n",
    "    print(\"\\n\\n=== Test 6: Pad Mode with Date-Based Sequence-Aware Split ===\")\n",
    "\n",
    "    # Clean transformers directory\n",
    "    shutil.rmtree('./transformers', ignore_errors=True)\n",
    "    os.makedirs('./transformers', exist_ok=True)\n",
    "\n",
    "    new_data = new_data.copy()\n",
    "    # Calculate median date for splitting\n",
    "    median_date = data['datetime'].median()\n",
    "    print(f\"Using median date as split point: {median_date}\")\n",
    "\n",
    "    # Configure preprocessor for training with pad mode and date-based split\n",
    "    pad_date_preprocessor = DataPreprocessor(\n",
    "        model_type=\"LSTM\",\n",
    "        y_variable=y_variable,\n",
    "        ordinal_categoricals=ordinal_categoricals,\n",
    "        nominal_categoricals=nominal_categoricals,\n",
    "        numericals=numericals,\n",
    "        mode=\"train\",\n",
    "        options={\n",
    "            \"enabled\": True,\n",
    "            \"time_column\": \"datetime\",\n",
    "            # \"horizon\": 379,\n",
    "            \"use_horizon_sequence\": True,\n",
    "            \"horizon_sequence_number\": 1,\n",
    "            \"step_size\": 1,\n",
    "            \"sequence_modes\": {\n",
    "                \"pad\": {\n",
    "                    \"pad_threshold\": 0.3,  # Allows up to 90% padding\n",
    "                    \"padding_side\": \"post\"\n",
    "                }\n",
    "            },\n",
    "            \"time_series_split\": {\n",
    "                \"method\": \"sequence_aware\",   # Use sequence-aware splitting\n",
    "                \"split_date\": str(median_date), # Split at the median date\n",
    "                \"debug_phases\": True          # Enable detailed phase debugging\n",
    "            }\n",
    "        },\n",
    "        sequence_categorical=[\"trial_id\"],\n",
    "        sub_sequence_categorical=[\"shooting_phases\"],\n",
    "        time_series_sequence_mode=\"pad\",\n",
    "        debug=True,\n",
    "        graphs_output_dir=\"./plots\"\n",
    "    )\n",
    "\n",
    "    # Analyze potential split points first\n",
    "    print(\"Analyzing potential split points...\")\n",
    "    split_options = pad_date_preprocessor.analyze_split_options(data)\n",
    "    for i, option in enumerate(split_options[:3]):  # Show top 3\n",
    "        print(f\"Option {i+1}: Split at {option['split_time']} - Train fraction: {option['train_fraction']:.2f}\")\n",
    "\n",
    "    # Preprocess training data\n",
    "    X_train_seq, X_test_seq, y_train_seq, y_test_seq, recommendations, _ = pad_date_preprocessor.final_ts_preprocessing(data)\n",
    "\n",
    "    print(f\"Train shapes - X: {X_train_seq.shape}, y: {y_train_seq.shape}\")\n",
    "    print(f\"Test shapes - X: {X_test_seq.shape}, y: {y_test_seq.shape}\")\n",
    "\n",
    "    # Train model\n",
    "    print(\"Training LSTM model with pad mode and date-based split...\")\n",
    "    horizon = get_horizon_from_preprocessor(pad_date_preprocessor)\n",
    "    print(f\"Using horizon of {horizon} for model output dimension\")\n",
    "\n",
    "    model6 = build_lstm_model((X_train_seq.shape[1], X_train_seq.shape[2]), horizon=horizon)\n",
    "    model6.fit(\n",
    "        X_train_seq, y_train_seq, \n",
    "        validation_data=(X_test_seq, y_test_seq),\n",
    "        epochs=10, batch_size=32, verbose=1\n",
    "    )\n",
    "    model6.save('./transformers/model_pad_date.h5')\n",
    "\n",
    "    # Test prediction\n",
    "    print(\"\\nTesting prediction mode with new data...\")\n",
    "    # new_data = data.iloc[-48:].copy()\n",
    "\n",
    "    pad_date_predict = DataPreprocessor(\n",
    "        model_type=\"LSTM\",\n",
    "        y_variable=y_variable,\n",
    "        ordinal_categoricals=ordinal_categoricals,\n",
    "        nominal_categoricals=nominal_categoricals,\n",
    "        numericals=numericals,\n",
    "        mode=\"predict\",\n",
    "        options={\n",
    "            \"enabled\": True,\n",
    "            \"time_column\": \"datetime\",\n",
    "            \"time_series_split\": {\n",
    "                \"method\": \"sequence_aware\",   # Use sequence-aware splitting\n",
    "                \"split_date\": str(median_date), # Split at the median date\n",
    "                \"debug_phases\": True          # Enable detailed phase debugging\n",
    "            }\n",
    "        },\n",
    "        sequence_categorical=[\"trial_id\"],\n",
    "        sub_sequence_categorical=[\"shooting_phases\"],\n",
    "        time_series_sequence_mode=\"pad\",\n",
    "        transformers_dir=\"./transformers\"\n",
    "    )\n",
    "\n",
    "    # result = pad_date_predict.final_ts_preprocessing(new_data, model_input_shape=expected_shape)\n",
    "    # X_new_preprocessed = result[0]\n",
    "    X_new_preprocessed, recommendations, X_inversed = pad_date_predict.final_ts_preprocessing(new_data, model_input_shape=expected_shape)\n",
    "    \n",
    "    model6 = load_model('./transformers/model_pad_date.h5')\n",
    "    expected_shape = model6.input_shape\n",
    "    print(f\"Expected model input shape: {expected_shape}\")\n",
    "    predictions = model6.predict(X_new_preprocessed)\n",
    "    print(f\"Prediction results shape: {predictions.shape}\")\n",
    "    # Apply dimension compatibility function\n",
    "    y_test_seq, predictions = ensure_compatible_dimensions(y_test_seq, predictions)\n",
    "\n",
    "    # Compute and print evaluation metrics using the new function.\n",
    "    model6_metrics = evaluate_model_metrics(y_test_seq, predictions)\n",
    "    print(f\"Model evaluation model6_metrics - MAE: {model6_metrics['mae']:.4f}, RMSE: {model6_metrics['rmse']:.4f}, R²: {model6_metrics['r2']:.4f}\")\n",
    "\n",
    "    # Test 7: DTW Mode with Percentage-Based Sequence-Aware Split\n",
    "    print(\"\\n\\n=== Test 7: DTW Mode with Percentage-Based Sequence-Aware Split ===\")\n",
    "\n",
    "    # Clean transformers directory\n",
    "    shutil.rmtree('./transformers', ignore_errors=True)\n",
    "    os.makedirs('./transformers', exist_ok=True)\n",
    "    \n",
    "    new_data = new_data.copy()\n",
    "    # Configure preprocessor for training with DTW mode and percentage-based split\n",
    "    dtw_pct_preprocessor = DataPreprocessor(\n",
    "        model_type=\"LSTM\",\n",
    "        y_variable=y_variable,\n",
    "        ordinal_categoricals=ordinal_categoricals,\n",
    "        nominal_categoricals=nominal_categoricals,\n",
    "        numericals=numericals,\n",
    "        mode=\"train\",\n",
    "        options={\n",
    "            \"enabled\": True,\n",
    "            \"time_column\": \"datetime\",\n",
    "            # \"horizon\": 379,\n",
    "            \"use_horizon_sequence\": True,\n",
    "            \"horizon_sequence_number\": 1,\n",
    "            \"step_size\": 1,\n",
    "            \"sequence_modes\": {\n",
    "                \"dtw\": {\n",
    "                    \"reference_sequence\": \"max\",  # Use max length sequence as reference\n",
    "                    \"dtw_threshold\": 0.3          # DTW threshold for sequences\n",
    "                }\n",
    "            },\n",
    "            \"time_series_split\": {\n",
    "                \"method\": \"sequence_aware\",  # Use sequence-aware splitting\n",
    "                # \"test_size\": 0.2,            # Use 20% of sequences for testing\n",
    "                'target_train_fraction': 0.75,  # Aim for 80% training, 20% testing\n",
    "                \"debug_phases\": True         # Enable detailed phase debugging\n",
    "            }\n",
    "        },\n",
    "        sequence_categorical=[\"trial_id\"],\n",
    "        sub_sequence_categorical=[\"shooting_phases\"],\n",
    "        time_series_sequence_mode=\"dtw\",\n",
    "        debug=True,\n",
    "        graphs_output_dir=\"./plots\"\n",
    "    )\n",
    "\n",
    "    # Preprocess training data\n",
    "    X_train_seq, X_test_seq, y_train_seq, y_test_seq, recommendations, _ = dtw_pct_preprocessor.final_ts_preprocessing(data)\n",
    "\n",
    "    print(f\"Train shapes - X: {X_train_seq.shape}, y: {y_train_seq.shape}\")\n",
    "    print(f\"Test shapes - X: {X_test_seq.shape}, y: {y_test_seq.shape}\")\n",
    "\n",
    "    # Train model\n",
    "    print(\"Training LSTM model with DTW mode and percentage-based split...\")\n",
    "    horizon = get_horizon_from_preprocessor(dtw_pct_preprocessor)\n",
    "    print(f\"Using horizon of {horizon} for model output dimension\")\n",
    "\n",
    "    model7 = build_lstm_model((X_train_seq.shape[1], X_train_seq.shape[2]), horizon=horizon)\n",
    "    model7.fit(\n",
    "        X_train_seq, y_train_seq, \n",
    "        validation_data=(X_test_seq, y_test_seq),\n",
    "        epochs=10, batch_size=32, verbose=1\n",
    "    )\n",
    "    model7.save('./transformers/model_dtw_pct.h5')\n",
    "\n",
    "    # Test prediction\n",
    "    print(\"\\nTesting prediction mode with new data...\")\n",
    "    # new_data = data.iloc[-48:].copy()\n",
    "\n",
    "    dtw_pct_predict = DataPreprocessor(\n",
    "        model_type=\"LSTM\",\n",
    "        y_variable=y_variable,\n",
    "        ordinal_categoricals=ordinal_categoricals,\n",
    "        nominal_categoricals=nominal_categoricals,\n",
    "        numericals=numericals,\n",
    "        mode=\"predict\",\n",
    "        options={\n",
    "            \"enabled\": True,\n",
    "            \"time_column\": \"datetime\",\n",
    "            \"time_series_split\": {\n",
    "                \"method\": \"sequence_aware\",  # Use sequence-aware splitting\n",
    "                # \"test_size\": 0.2,            # Use 20% of sequences for testing\n",
    "                'target_train_fraction': 0.75,  # Aim for 80% training, 20% testing\n",
    "                \"debug_phases\": True         # Enable detailed phase debugging\n",
    "            }\n",
    "        },\n",
    "        sequence_categorical=[\"trial_id\"],\n",
    "        sub_sequence_categorical=[\"shooting_phases\"],\n",
    "        time_series_sequence_mode=\"dtw\",\n",
    "        transformers_dir=\"./transformers\"\n",
    "    )\n",
    "    \n",
    "    # result = dtw_pct_predict.final_ts_preprocessing(new_data, model_input_shape=expected_shape)\n",
    "    # X_new_preprocessed = result[0]\n",
    "    X_new_preprocessed, recommendations, X_inversed = dtw_pct_predict.final_ts_preprocessing(new_data, model_input_shape=expected_shape)\n",
    "    model7 = load_model('./transformers/model_dtw_pct.h5')\n",
    "    expected_shape = model7.input_shape\n",
    "    print(f\"Expected model input shape: {expected_shape}\")\n",
    "    predictions = model7.predict(X_new_preprocessed)\n",
    "    print(f\"Prediction results shape: {predictions.shape}\")\n",
    "    # Apply dimension compatibility function\n",
    "    y_test_seq, predictions = ensure_compatible_dimensions(y_test_seq, predictions)\n",
    "\n",
    "    # Compute and print evaluation metrics using the new function.\n",
    "    model7_metrics = evaluate_model_metrics(y_test_seq, predictions)\n",
    "    print(f\"Model evaluation model7_metrics - MAE: {model7_metrics['mae']:.4f}, RMSE: {model7_metrics['rmse']:.4f}, R²: {model7_metrics['r2']:.4f}\")\n",
    "\n",
    "    # Test 8: DTW Mode with Date-Based Sequence-Aware Split\n",
    "    print(\"\\n\\n=== Test 8: DTW Mode with Date-Based Sequence-Aware Split ===\")\n",
    "\n",
    "    # Clean transformers directory\n",
    "    shutil.rmtree('./transformers', ignore_errors=True)\n",
    "    os.makedirs('./transformers', exist_ok=True)\n",
    "\n",
    "    # Configure preprocessor for training with DTW mode and date-based split\n",
    "    dtw_date_preprocessor = DataPreprocessor(\n",
    "        model_type=\"LSTM\",\n",
    "        y_variable=y_variable,\n",
    "        ordinal_categoricals=ordinal_categoricals,\n",
    "        nominal_categoricals=nominal_categoricals,\n",
    "        numericals=numericals,\n",
    "        mode=\"train\",\n",
    "        options={\n",
    "            \"enabled\": True,\n",
    "            \"time_column\": \"datetime\",\n",
    "            # \"horizon\": 379,\n",
    "            \"use_horizon_sequence\": True,\n",
    "            \"horizon_sequence_number\": 1,\n",
    "            \"step_size\": 1,\n",
    "            \"sequence_modes\": {\n",
    "                \"dtw\": {\n",
    "                    \"reference_sequence\": \"max\",  # Use max length sequence as reference\n",
    "                    \"dtw_threshold\": 0.3          # DTW threshold for sequences\n",
    "                }\n",
    "            },\n",
    "            \"time_series_split\": {\n",
    "                \"method\": \"sequence_aware\",      # Use sequence-aware splitting\n",
    "                \"split_date\": str(median_date),   # Split at the calculated date\n",
    "                \"debug_phases\": True             # Enable detailed phase debugging\n",
    "            }\n",
    "        },\n",
    "        sequence_categorical=[\"trial_id\"],\n",
    "        sub_sequence_categorical=[\"shooting_phases\"],\n",
    "        time_series_sequence_mode=\"dtw\",\n",
    "        debug=True,\n",
    "        graphs_output_dir=\"./plots\"\n",
    "    )\n",
    "\n",
    "    # Analyze potential split points first\n",
    "    print(\"Analyzing potential split points...\")\n",
    "    split_options = dtw_date_preprocessor.analyze_split_options(data)\n",
    "    for i, option in enumerate(split_options[:3]):  # Show top 3\n",
    "        print(f\"Option {i+1}: Split at {option['split_time']} - Train fraction: {option['train_fraction']:.2f}\")\n",
    "\n",
    "    # Preprocess training data\n",
    "    X_train_seq, X_test_seq, y_train_seq, y_test_seq, recommendations, _ = dtw_date_preprocessor.final_ts_preprocessing(data)\n",
    "\n",
    "    print(f\"Train shapes - X: {X_train_seq.shape}, y: {y_train_seq.shape}\")\n",
    "    print(f\"Test shapes - X: {X_test_seq.shape}, y: {y_test_seq.shape}\")\n",
    "\n",
    "    # Train model\n",
    "    print(\"Training LSTM model with DTW mode and date-based split...\")\n",
    "    horizon = get_horizon_from_preprocessor(dtw_date_preprocessor)\n",
    "    print(f\"Using horizon of {horizon} for model output dimension\")\n",
    "\n",
    "    model8 = build_lstm_model((X_train_seq.shape[1], X_train_seq.shape[2]), horizon=horizon)\n",
    "    model8.fit(\n",
    "        X_train_seq, y_train_seq, \n",
    "        validation_data=(X_test_seq, y_test_seq),\n",
    "        epochs=10, batch_size=32, verbose=1\n",
    "    )\n",
    "    model8.save('./transformers/model_dtw_date.h5')\n",
    "\n",
    "    # Test prediction\n",
    "    print(\"\\nTesting prediction mode with new data...\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    dtw_date_predict = DataPreprocessor(\n",
    "        model_type=\"LSTM\",\n",
    "        y_variable=y_variable,\n",
    "        ordinal_categoricals=ordinal_categoricals,\n",
    "        nominal_categoricals=nominal_categoricals,\n",
    "        numericals=numericals,\n",
    "        mode=\"predict\",\n",
    "        options={\n",
    "            \"enabled\": True,\n",
    "            \"time_column\": \"datetime\",\n",
    "            \"time_series_split\": {\n",
    "                \"method\": \"sequence_aware\",      # Use sequence-aware splitting\n",
    "                \"split_date\": str(median_date),   # Split at the calculated date\n",
    "                \"debug_phases\": True             # Enable detailed phase debugging\n",
    "            }\n",
    "        },\n",
    "        sequence_categorical=[\"trial_id\"],\n",
    "        sub_sequence_categorical=[\"shooting_phases\"],\n",
    "        time_series_sequence_mode=\"dtw\",\n",
    "        transformers_dir=\"./transformers\"\n",
    "    )\n",
    "\n",
    "\n",
    "    expected_shape = model8.input_shape\n",
    "    X_new_preprocessed, recommendations, X_inversed = dtw_date_predict.final_ts_preprocessing(new_data, model_input_shape=expected_shape)\n",
    "    print(f\"Expected model input shape: {expected_shape}\")\n",
    "    # result = dtw_date_predict.final_ts_preprocessing(new_data, model_input_shape=expected_shape)\n",
    "    # result = debug_preprocessing_result(result, expected_shape)\n",
    "                \n",
    "\n",
    "    # X_new_preprocessed = result[0]\n",
    "    # print(f\"Type of result: {type(result)}\")\n",
    "    # if isinstance(result, tuple):\n",
    "    #     print(f\"Result contains {len(result)} elements\")\n",
    "    #     for i, item in enumerate(result):\n",
    "    #         print(f\"Item {i} is of type {type(item)}\")\n",
    "    #         if hasattr(item, 'shape'):\n",
    "    #             print(f\"  Shape: {item.shape}\")\n",
    "    model8 = load_model('./transformers/model_dtw_date.h5')\n",
    "    predictions = model8.predict(X_new_preprocessed)\n",
    "    print(f\"Prediction results shape: {predictions.shape}\")\n",
    "    # Apply dimension compatibility function\n",
    "    y_test_seq, predictions = ensure_compatible_dimensions(y_test_seq, predictions)\n",
    "\n",
    "    # Compute and print evaluation metrics using the new function.\n",
    "    model8_metrics = evaluate_model_metrics(y_test_seq, predictions)\n",
    "    print(f\"Model evaluation metrics - MAE: {model8_metrics['mae']:.4f}, RMSE: {model8_metrics['rmse']:.4f}, R²: {model8_metrics['r2']:.4f}\")\n",
    "\n",
    "\n",
    "    print(\"\\n\\nAll tests completed successfully!\")\n",
    "\n",
    "\n",
    "    # At the end of all tests, collect each model's metrics into a report list.\n",
    "    evaluation_report = []\n",
    "    evaluation_report.append({\n",
    "        'test': 'Model 1: Percentage-based Split',\n",
    "        'mae': model1_metrics['mae'],\n",
    "        'rmse': model1_metrics['rmse'],\n",
    "        'r2': model1_metrics['r2']\n",
    "    })\n",
    "    evaluation_report.append({\n",
    "        'test': 'Model 2: Date-based Split',\n",
    "        'mae': model2_metrics['mae'],\n",
    "        'rmse': model2_metrics['rmse'],\n",
    "        'r2': model2_metrics['r2']\n",
    "    })\n",
    "    evaluation_report.append({\n",
    "        'test': 'Model 3: PSI-based Split with Feature-Engine',\n",
    "        'mae': model3_metrics['mae'],\n",
    "        'rmse': model3_metrics['rmse'],\n",
    "        'r2': model3_metrics['r2']\n",
    "    })\n",
    "    evaluation_report.append({\n",
    "        'test': 'Model 4: DTW/Pad Mode with PSI-based Split',\n",
    "        'mae': model4_metrics['mae'],\n",
    "        'rmse': model4_metrics['rmse'],\n",
    "        'r2': model4_metrics['r2']\n",
    "    })\n",
    "    evaluation_report.append({\n",
    "        'test': 'Model 5: DTW Mode with Feature-Engine Split',\n",
    "        'mae': model5_metrics['mae'],\n",
    "        'rmse': model5_metrics['rmse'],\n",
    "        'r2': model5_metrics['r2']\n",
    "    })\n",
    "    evaluation_report.append({\n",
    "        'test': 'Model 6: Pad Mode with Date-based Sequence-Aware Split',\n",
    "        'mae': model6_metrics['mae'],\n",
    "        'rmse': model6_metrics['rmse'],\n",
    "        'r2': model6_metrics['r2']\n",
    "    })\n",
    "    evaluation_report.append({\n",
    "        'test': 'Model 7: DTW Mode with Percentage-Based Sequence-Aware Split',\n",
    "        'mae': model7_metrics['mae'],\n",
    "        'rmse': model7_metrics['rmmse'] if 'rmmse' in model7_metrics else model7_metrics['rmse'],  # ensuring consistency\n",
    "        'r2': model7_metrics['r2']\n",
    "    })\n",
    "    evaluation_report.append({\n",
    "        'test': 'Model 8: DTW Mode with Date-Based Sequence-Aware Split',\n",
    "        'mae': model8_metrics['mae'],\n",
    "        'rmse': model8_metrics['rmse'],\n",
    "        'r2': model8_metrics['r2']\n",
    "    })\n",
    "\n",
    "    # Generate the final summary report.\n",
    "    generate_final_report(evaluation_report)\n",
    "\n",
    "def get_horizon_from_preprocessor(preprocessor):\n",
    "    \"\"\"\n",
    "    Extract the horizon parameter from the preprocessor.\n",
    "    \n",
    "    For DTW or pad modes, if the horizon has not been computed yet,\n",
    "    it returns the product of horizon_sequence_number and sequence_length.\n",
    "    Otherwise, it returns the computed horizon.\n",
    "    \"\"\"\n",
    "    if hasattr(preprocessor, 'time_series_sequence_mode') and preprocessor.time_series_sequence_mode in [\"dtw\", \"pad\"]:\n",
    "        if preprocessor.horizon is not None:\n",
    "            return preprocessor.horizon\n",
    "        else:\n",
    "            # Compute dynamic horizon as horizon_sequence_number * sequence_length\n",
    "            return preprocessor.horizon_sequence_number * preprocessor.sequence_length\n",
    "    elif hasattr(preprocessor, 'options') and isinstance(preprocessor.options, dict):\n",
    "        return preprocessor.options.get('horizon', 1)\n",
    "    elif hasattr(preprocessor, 'horizon'):\n",
    "        return preprocessor.horizon\n",
    "    else:\n",
    "        return 1  # Default horizon if not specified\n",
    "\n",
    "        \n",
    "# -----------------------------------------------------------------------------\n",
    "# Helper function to ensure targets and predictions have compatible dimensions.\n",
    "def ensure_compatible_dimensions(targets, predictions):\n",
    "    import numpy as np\n",
    "\n",
    "    targets = np.array(targets)\n",
    "    predictions = np.array(predictions)\n",
    "\n",
    "    if targets.ndim == 3 and targets.shape[2] == 1:\n",
    "        targets = targets.squeeze(axis=2)\n",
    "    if predictions.ndim == 3 and predictions.shape[2] == 1:\n",
    "        predictions = predictions.squeeze(axis=2)\n",
    "\n",
    "    if targets.shape[0] != predictions.shape[0]:\n",
    "        n_samples = min(targets.shape[0], predictions.shape[0])\n",
    "        targets = targets[:n_samples]\n",
    "        predictions = predictions[:n_samples]\n",
    "\n",
    "    if targets.ndim == 1 and predictions.ndim == 2:\n",
    "        targets = targets.reshape(-1, 1)\n",
    "    elif predictions.ndim == 1 and targets.ndim == 2:\n",
    "        predictions = predictions.reshape(-1, 1)\n",
    "\n",
    "    print(f\"Adjusted shapes - targets: {targets.shape}, predictions: {predictions.shape}\")\n",
    "    return targets, predictions\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def forecast_model(model, predict_preprocessor, forecast_data):\n",
    "    \"\"\"\n",
    "    Generate forecast predictions on unseen forecast_data using the provided model and preprocessor.\n",
    "    \n",
    "    Args:\n",
    "         model: A trained Keras model.\n",
    "         predict_preprocessor: A DataPreprocessor instance configured for prediction.\n",
    "         forecast_data: A pandas DataFrame containing unseen future data.\n",
    "         \n",
    "    Returns:\n",
    "         forecast: Numpy array of forecast predictions.\n",
    "    \"\"\"\n",
    "    # Get the expected input shape from the model.\n",
    "    expected_shape = model.input_shape\n",
    "    print(f\"[Forecast Model] Expected model input shape: {expected_shape}\")\n",
    "    \n",
    "    # Preprocess the forecast data using final_ts_preprocessing.\n",
    "    X_forecast, recommendations, X_inversed = predict_preprocessor.final_ts_preprocessing(\n",
    "        forecast_data, model_input_shape=expected_shape)\n",
    "    print(f\"[Forecast Model] Processed forecast data shape: {X_forecast.shape}\")\n",
    "    \n",
    "    # Generate predictions (the forecast) with the model.\n",
    "    forecast = model.predict(X_forecast)\n",
    "    print(f\"[Forecast Model] Forecast prediction shape: {forecast.shape}\")\n",
    "    \n",
    "    return forecast\n",
    "\n",
    "\n",
    "def plot_forecasts(actual, predicted, forecast, title=\"Actual vs Predicted & Forecast\"):\n",
    "    \"\"\"\n",
    "    Plot actual and predicted values for a given time span along with forecast predictions.\n",
    "    \n",
    "    In this example, we assume that the provided arrays represent the forecast period.\n",
    "    The x-axis is indexed by time steps.\n",
    "    \n",
    "    Args:\n",
    "        actual (np.array): Actual target values for the forecast period (1D array).  \n",
    "                           (If not available, pass an array of zeros.)\n",
    "        predicted (np.array): Predicted target values for the forecast period (1D array).\n",
    "        forecast (np.array): Forecast predictions for the forecast period (1D array).\n",
    "        title (str): Title for the plot.\n",
    "    \"\"\"\n",
    "    T = len(actual)\n",
    "    x = list(range(T))\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(x, actual, label=\"Actual Forecast\", marker='o')\n",
    "    plt.plot(x, predicted, label=\"Predicted Forecast\", marker='x')\n",
    "    plt.plot(x, forecast, label=\"Forecast\", marker='s')\n",
    "    plt.xlabel(\"Time Step\")\n",
    "    plt.ylabel(\"Target Value\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def run_forecasting_experiments(data):\n",
    "    \"\"\"\n",
    "    Run forecast experiments for multiple trained models on a hold-out forecast segment,\n",
    "    and plot the forecast results.\n",
    "    \n",
    "    For each forecasting test (each model/sequence mode combination), this function:\n",
    "      - Loads the saved model.\n",
    "      - Creates a prediction preprocessor with matching settings.\n",
    "      - Runs forecast_model() on a forecast data segment (e.g. the last 48 rows).\n",
    "      - If the forecast_data contains the true target column (e.g. \"exhaustion_rate\"),\n",
    "        it will extract those values and plot a graph showing actual vs predicted vs forecast.\n",
    "    \n",
    "    Args:\n",
    "         data (pd.DataFrame): The full dataset from which a forecast segment is taken.\n",
    "    \n",
    "    Returns:\n",
    "         forecasts_dict: Dictionary mapping test names to forecast prediction arrays.\n",
    "    \"\"\"\n",
    "    from tensorflow.keras.models import load_model\n",
    "    import pandas as pd\n",
    "\n",
    "    # Mapping of descriptive test names to (model file path, sequence mode).\n",
    "    # (Adjust the model file names and sequence modes to match your saved models.)\n",
    "    forecasting_tests = {\n",
    "        \"Percentage-based (set_window)\": (\"./transformers/model_percentage_split.h5\", \"set_window\"),\n",
    "        \"DTW (Feature-Engine)\": (\"./transformers/model_dtw.h5\", \"dtw\"),\n",
    "        \"Pad (Date-based)\": (\"./transformers/model_pad_date.h5\", \"pad\"),\n",
    "        \"DTW (Date-based)\": (\"./transformers/model_dtw_date.h5\", \"dtw\"),\n",
    "    }\n",
    "    \n",
    "    # Define a hold-out forecast segment.\n",
    "    # For this example, we choose the last 48 rows of the DataFrame.\n",
    "    forecast_data = data.iloc[-48:].copy()\n",
    "    print(f\"[Forecast Experiment] Forecasting on unseen data segment with shape: {forecast_data.shape}\")\n",
    "    \n",
    "    forecasts_dict = {}\n",
    "    \n",
    "    # Loop over each forecasting test.\n",
    "    for test_name, (model_path, seq_mode) in forecasting_tests.items():\n",
    "        print(f\"\\n[Forecast Experiment] Running forecast test: {test_name}\")\n",
    "        # Create a prediction preprocessor using a configuration matching the model.\n",
    "        predict_preprocessor = DataPreprocessor(\n",
    "            model_type=\"LSTM\",\n",
    "            y_variable=[\"exhaustion_rate\"],  # adjust if your target is different\n",
    "            ordinal_categoricals=[],\n",
    "            nominal_categoricals=nominal_categorical,\n",
    "            numericals=numerical,\n",
    "            mode=\"predict\",\n",
    "            options={\n",
    "                \"enabled\": True,\n",
    "                \"time_column\": \"datetime\",\n",
    "                \"ts_sequence_mode\": seq_mode\n",
    "            },\n",
    "            sequence_categorical=[\"trial_id\"],\n",
    "            sub_sequence_categorical=[\"shooting_phases\"],\n",
    "            transformers_dir=\"./transformers\"\n",
    "        )\n",
    "        # Load the trained model.\n",
    "        model_instance = load_model(model_path)\n",
    "        print(f\"[Forecast Experiment] Loaded model from {model_path}\")\n",
    "        \n",
    "        # Run the forecast using the helper function.\n",
    "        forecast_pred = forecast_model(model_instance, predict_preprocessor, forecast_data)\n",
    "        forecasts_dict[test_name] = forecast_pred\n",
    "        \n",
    "        # If forecast_data contains the actual target, extract it and plot.\n",
    "        if \"exhaustion_rate\" in forecast_data.columns:\n",
    "            # Obtain the actual forecast target values.\n",
    "            # Here we assume the forecast segment’s target values can be extracted directly.\n",
    "            forecast_actual = forecast_data[\"exhaustion_rate\"].values  \n",
    "            # Since the preprocessor reshapes the data into sequences,\n",
    "            # we assume forecast_pred is of shape (n_seq, time_steps, [1]) or (n_seq, time_steps).\n",
    "            # For simplicity, we take the first sequence from forecast_pred.\n",
    "            if forecast_pred.ndim == 3:\n",
    "                fp = forecast_pred[0].flatten()\n",
    "            elif forecast_pred.ndim == 2:\n",
    "                fp = forecast_pred[0].flatten()\n",
    "            else:\n",
    "                fp = forecast_pred.flatten()\n",
    "            # We then slice the actual target to match the length of the forecast predictions.\n",
    "            T_forecast = len(fp)\n",
    "            fa = forecast_actual[-T_forecast:]\n",
    "            # Use the same actual values for both \"actual\" and \"predicted\" lines on the forecast period\n",
    "            # if a separate “predicted” value is not available for the forecast segment.\n",
    "            # (Alternatively, if you have a separate forecast prediction from an evaluation split, you can use it.)\n",
    "            plot_forecasts(fa, fa, fp, title=f\"{test_name} - Forecast (Actual vs Predicted)\")\n",
    "        else:\n",
    "            # If there is no actual target column, plot forecast predictions against zeros.\n",
    "            print(f\"[Forecast Experiment] No actual target found in forecast_data for {test_name}; plotting forecast only.\")\n",
    "            dummy_actual = np.zeros_like(forecast_pred.flatten())\n",
    "            plot_forecasts(dummy_actual, dummy_actual, forecast_pred.flatten(), title=f\"{test_name} - Forecast (Predicted Only)\")\n",
    "        \n",
    "    return forecasts_dict\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Function to train an LSTM (or TCN-LSTM) model.\n",
    "def train_lstm_model(X_train, y_train, ts_params, config, use_tcn=False, bidirectional=False):\n",
    "    \"\"\"\n",
    "    Train an LSTM or TCN-LSTM model based on provided configuration.\n",
    "\n",
    "    Args:\n",
    "        X_train (np.ndarray): Training data of shape (samples, timesteps, features).\n",
    "        y_train (np.ndarray): Target training data.\n",
    "        ts_params (dict): Time series parameters dictionary, used to extract horizon.\n",
    "        config (dict): Configuration dictionary.\n",
    "        use_tcn (bool): If True, use TCN layer before LSTM.\n",
    "        bidirectional (bool): If True, use Bidirectional wrapper for LSTM layers.\n",
    "\n",
    "    Returns:\n",
    "        model: Trained Keras model.\n",
    "    \"\"\"\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "    if bidirectional:\n",
    "        from tensorflow.keras.layers import Bidirectional\n",
    "\n",
    "    horizon = ts_params.get(\"horizon\", 1)\n",
    "    input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "    model = Sequential()\n",
    "    \n",
    "    if use_tcn:\n",
    "        try:\n",
    "            from tcn import TCN\n",
    "        except ImportError:\n",
    "            raise ImportError(\"TCN layer not found. Please install the tcn package.\")\n",
    "        \n",
    "        # Add a TCN layer; wrap with Bidirectional if needed.\n",
    "        if bidirectional:\n",
    "            model.add(Bidirectional(TCN(nb_filters=64, return_sequences=True), input_shape=input_shape))\n",
    "        else:\n",
    "            model.add(TCN(nb_filters=64, return_sequences=True, input_shape=input_shape))\n",
    "        model.add(Dropout(0.2))\n",
    "        \n",
    "        # Follow with an LSTM layer.\n",
    "        if bidirectional:\n",
    "            model.add(Bidirectional(LSTM(32)))\n",
    "        else:\n",
    "            model.add(LSTM(32))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(horizon))\n",
    "    else:\n",
    "        # Standard LSTM architecture.\n",
    "        if bidirectional:\n",
    "            model.add(Bidirectional(LSTM(64, return_sequences=True), input_shape=input_shape))\n",
    "        else:\n",
    "            model.add(LSTM(64, return_sequences=True, input_shape=input_shape))\n",
    "        model.add(Dropout(0.2))\n",
    "        if bidirectional:\n",
    "            model.add(Bidirectional(LSTM(32)))\n",
    "        else:\n",
    "            model.add(LSTM(32))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(horizon))\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    \n",
    "    # Train the model using default training parameters.\n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Experiment function for running models in different sequence modes.\n",
    "def run_sequence_mode_experiment(data, sequence_mode, model_architectures):\n",
    "    \"\"\"\n",
    "    Run an experiment for a given sequence mode by preprocessing the data,\n",
    "    training a model for each architecture, and evaluating predictions.\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): The complete dataset.\n",
    "        sequence_mode (str): The sequence mode (e.g., \"set_window\", \"dtw\", \"pad\").\n",
    "        model_architectures (list): List of architecture configurations to test.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary of results with metrics and shapes for each architecture.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    # Set up sequence-specific parameters.\n",
    "    if sequence_mode in [\"set_window\"]:\n",
    "        ts_params = {\n",
    "            \"enabled\": True,\n",
    "            \"time_column\": \"datetime\",\n",
    "            \"horizon\": 5,  # initial dummy horizon; will be updated later\n",
    "            \"step_size\": 1,\n",
    "            \"window_size\": 10,\n",
    "            \"sequence_modes\": {},\n",
    "            \"ts_sequence_mode\": sequence_mode,\n",
    "            \"split_dataset\": {\"test_size\": 0.2, \"random_state\": 42},\n",
    "            \"time_series_split\": {\"method\": \"standard\"}\n",
    "        }\n",
    "        ts_params[\"sequence_modes\"][\"set_window\"] = {\"window_size\": 10, \"max_sequence_length\": 10}\n",
    "    elif sequence_mode == \"pad\":\n",
    "        ts_params = {\n",
    "            \"enabled\": True,\n",
    "            \"time_column\": \"datetime\",\n",
    "            \"use_horizon_sequence\": True,\n",
    "            \"horizon_sequence_number\": 1,\n",
    "            \"step_size\": 1,\n",
    "            \"sequence_modes\": {},\n",
    "            \"ts_sequence_mode\": sequence_mode,\n",
    "            \"split_dataset\": {\"test_size\": 0.2, \"random_state\": 42},\n",
    "            \"time_series_split\": {\"method\": \"standard\"}\n",
    "        }\n",
    "        ts_params[\"sequence_modes\"][\"pad\"] = {\"pad_threshold\": 0.3, \"padding_side\": \"post\"}\n",
    "    elif sequence_mode == \"dtw\":\n",
    "        ts_params = {\n",
    "            \"enabled\": True,\n",
    "            \"time_column\": \"datetime\",\n",
    "            \"use_horizon_sequence\": True,\n",
    "            \"horizon_sequence_number\": 1,\n",
    "            \"step_size\": 1,\n",
    "            \"sequence_modes\": {},\n",
    "            \"ts_sequence_mode\": sequence_mode,\n",
    "            \"split_dataset\": {\"test_size\": 0.2, \"random_state\": 42},\n",
    "            \"time_series_split\": {\"method\": \"standard\"}\n",
    "        }\n",
    "        ts_params[\"sequence_modes\"][\"dtw\"] = {\"reference_sequence\": \"max\", \"dtw_threshold\": 0.3}\n",
    "\n",
    "    try:\n",
    "        # Create the DataPreprocessor for training.\n",
    "        preprocessor = DataPreprocessor(\n",
    "            model_type=\"LSTM\",\n",
    "            y_variable=y_variable,\n",
    "            ordinal_categoricals=ordinal_categoricals,\n",
    "            nominal_categoricals=nominal_categoricals,\n",
    "            numericals=numericals,\n",
    "            mode=\"train\",\n",
    "            options=ts_params,\n",
    "            sequence_categorical=[\"trial_id\"],\n",
    "            sub_sequence_categorical=[\"shooting_phases\"],\n",
    "            time_series_sequence_mode=sequence_mode,\n",
    "            debug=True,\n",
    "            graphs_output_dir=graphs_output_dir,\n",
    "            transformers_dir=transformers_dir\n",
    "        )\n",
    "\n",
    "        # Call final_ts_preprocessing; this call updates preprocessor.horizon dynamically\n",
    "        X_train_seq, X_test_seq, y_train_seq, y_test_seq, recommendations, _ = preprocessor.final_ts_preprocessing(data)\n",
    "\n",
    "        # --- NEW STEP: Update ts_params with the computed horizon ---\n",
    "        ts_params[\"horizon\"] = preprocessor.horizon\n",
    "        preprocessor.logger.info(f\"Updated ts_params horizon to: {ts_params['horizon']}\")\n",
    "\n",
    "        # Loop over each model architecture to train and evaluate.\n",
    "        for arch in model_architectures:\n",
    "            arch_name = f\"{'TCN-' if arch['use_tcn'] else ''}{'Bi' if arch['bidirectional'] else ''}LSTM\"\n",
    "            # Use the updated horizon from ts_params when building the model.\n",
    "            model = build_lstm_model(\n",
    "                (X_train_seq.shape[1], X_train_seq.shape[2]),\n",
    "                horizon=ts_params.get(\"horizon\", 1)\n",
    "            )\n",
    "            model.fit(\n",
    "                X_train_seq, y_train_seq, \n",
    "                validation_data=(X_test_seq, y_test_seq),\n",
    "                epochs=10, batch_size=32, verbose=1\n",
    "            )\n",
    "            predictions = model.predict(X_test_seq)\n",
    "            # Ensure targets and predictions have compatible dimensions.\n",
    "            y_test_seq, predictions = ensure_compatible_dimensions(y_test_seq, predictions)\n",
    "            metrics = {\n",
    "                'mae': mean_absolute_error(y_test_seq, predictions),\n",
    "                'rmse': np.sqrt(mean_squared_error(y_test_seq, predictions)),\n",
    "                'r2': r2_score(y_test_seq, predictions)\n",
    "            }\n",
    "            results[arch_name] = {\n",
    "                'metrics': metrics,\n",
    "                'train_shape': X_train_seq.shape,\n",
    "                'test_shape': X_test_seq.shape,\n",
    "                'architecture': arch\n",
    "            }\n",
    "            del model\n",
    "            tf.keras.backend.clear_session()\n",
    "\n",
    "    except Exception as e:\n",
    "        results['preprocessing_error'] = str(e)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "def save_experiment_results(results, config):\n",
    "    \"\"\"Save experiment results to a JSON file.\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"experiment_results_{timestamp}.json\"\n",
    "    filepath = os.path.join(config[\"paths\"][\"training_output_dir\"], filename)\n",
    "    \n",
    "    def convert_to_serializable(obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return obj\n",
    "    \n",
    "    serializable_results = json.loads(json.dumps(results, default=convert_to_serializable))\n",
    "    \n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(serializable_results, f, indent=2)\n",
    "    \n",
    "    print(f\"Results saved to {filepath}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "def print_experiment_summary(all_results):\n",
    "    \"\"\"Print a summary of all experiment results.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"EXPERIMENT SUMMARY\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    for sequence_mode, results in all_results.items():\n",
    "        print(f\"\\nSequence Mode: {sequence_mode}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        if 'preprocessing_error' in results:\n",
    "            print(f\"ERROR: {results['preprocessing_error']}\")\n",
    "            continue\n",
    "            \n",
    "        for arch_name, arch_results in results.items():\n",
    "            if 'error' in arch_results:\n",
    "                print(f\"{arch_name}: ERROR - {arch_results['error']}\")\n",
    "                continue\n",
    "                \n",
    "            metrics = arch_results['metrics']\n",
    "            print(f\"\\n{arch_name}:\")\n",
    "            print(f\"  Sequence Shape: {arch_results['train_shape']}\")\n",
    "            print(f\"  MAE: {metrics['mae']:.4f}\")\n",
    "            print(f\"  RMSE: {metrics['rmse']:.4f}\")\n",
    "            print(f\"  R²: {metrics['r2']:.4f}\")\n",
    "            # Assuming MAPE is part of metrics if computed elsewhere.\n",
    "            if 'mape' in metrics:\n",
    "                print(f\"  MAPE: {metrics['mape']:.4f}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Define model architectures to test\n",
    "model_architectures = [\n",
    "    {'use_tcn': False, 'bidirectional': False},  # LSTM\n",
    "    {'use_tcn': False, 'bidirectional': True},   # BiLSTM\n",
    "    {'use_tcn': True, 'bidirectional': False},   # TCN-LSTM\n",
    "    {'use_tcn': True, 'bidirectional': True}       # TCN-BiLSTM\n",
    "]\n",
    "\n",
    "# Sequence modes to test\n",
    "sequence_modes = [\"set_window\", \"dtw\", \"pad\"]\n",
    "\n",
    "# Run experiments for each sequence mode and collect results.\n",
    "all_results = {}\n",
    "for mode in sequence_modes:\n",
    "    all_results[mode] = run_sequence_mode_experiment(data, mode, model_architectures)\n",
    "\n",
    "# Print summary of all experiments.\n",
    "print_experiment_summary(all_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ml/preprocess_train_predict/auto_ts_training.py\n",
    "# adding Autots\n",
    "\n",
    "# https://github.com/winedarksea/AutoTS\n",
    "\n",
    "from autots import AutoTS\n",
    "import pandas as pd\n",
    "from autots import AutoTS, load_daily\n",
    "\n",
    "# sample datasets can be used in either of the long or wide import shapes\n",
    "# long = False\n",
    "# df = load_daily(long=long)\n",
    "# df = data.copy()\n",
    "df = trial_summary_df.copy()\n",
    "print(df.columns.tolist())\n",
    "\n",
    "\n",
    "# Convert 'trial_id' to numeric ordering (e.g., T0001 → 1)\n",
    "df['trial_order'] = df['trial_id'].str.extract('T(\\d+)').astype(int)\n",
    "\n",
    "# Normalize 'frame_time' within each trial\n",
    "df['normalized_frame_time'] = df.groupby('trial_order')['frame_time'].transform(lambda x: x - x.min())\n",
    "\n",
    "# Create global time by offsetting each trial’s frame_time based on trial_order\n",
    "# Assume a buffer between trials (e.g., 10,000 units of time)\n",
    "buffer_between_trials = 10000\n",
    "df['global_time'] = df['trial_order'] * buffer_between_trials + df['normalized_frame_time']\n",
    "\n",
    "# Create a pseudo datetime column starting from an arbitrary start time\n",
    "start_time = pd.Timestamp(\"2024-01-01 00:00:00\")\n",
    "df['datetime'] = start_time + pd.to_timedelta(df['global_time'], unit='ms')\n",
    "\n",
    "\n",
    "\n",
    "print(df.head())\n",
    "print(df.columns)\n",
    "print(df.info())\n",
    "print(df.describe())\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Option 1: For wide format data (DatetimeIndex with series as columns)\n",
    "# Assuming df is your pandas DataFrame with DatetimeIndex\n",
    "model = AutoTS(\n",
    "    forecast_length=5,  # Forecast horizon (days/periods ahead)\n",
    "    frequency=\"infer\",   # Can be set to 'D', 'M', 'H', etc.\n",
    "    prediction_interval=0.9,\n",
    "    ensemble=None,       # Set to None for single models, or 'simple', 'horizontal', etc.\n",
    "    model_list=\"fast\",   # Try \"superfast\", \"fast\", \"default\" based on your needs\n",
    "    transformer_list=\"fast\",\n",
    "    max_generations=10,  # More generations = more thorough model search\n",
    "    num_validations=2\n",
    ")\n",
    "\n",
    "# Assuming you have a column like 'date' or 'timestamp'\n",
    "date_col = 'datetime'  # replace with your actual date column name\n",
    "\n",
    "# Make sure it's datetime type\n",
    "df[date_col] = pd.to_datetime(df[date_col])\n",
    "\n",
    "# check on datetime duplicates\n",
    "print(\"date column duplicates======================\"  ,df[date_col].duplicated().sum())\n",
    "# check on datetime nulls\n",
    "print(df[date_col].isnull().sum())\n",
    "\n",
    "# Set it as the index\n",
    "data_wide = df.set_index(date_col)\n",
    "\n",
    "# Now fit the model with proper DatetimeIndex\n",
    "model = model.fit(data_wide)\n",
    "\n",
    "\n",
    "# Option 2: For long format data\n",
    "# Assuming df_long has columns 'date', 'series_id', 'value'\n",
    "model = AutoTS(\n",
    "    forecast_length=5,\n",
    "    frequency=\"infer\",\n",
    "    prediction_interval=0.9,\n",
    "    ensemble=None,\n",
    "    model_list=\"fast\",\n",
    "    transformer_list=\"fast\",\n",
    "    max_generations=10,\n",
    "    num_validations=1,\n",
    "    min_allowed_train_percent=0.5  # Default is 1.0\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Fit the model (for long format)\n",
    "model_long = model_long.fit(\n",
    "    df,\n",
    "    date_col='date',     # Your date column name\n",
    "    value_col='value',   # Your value column name\n",
    "    id_col='series_id'   # Your series identifier column\n",
    ")\n",
    "\n",
    "# Generate forecasts\n",
    "prediction = model.predict()\n",
    "\n",
    "# Access the forecasts\n",
    "forecast_df = prediction.forecast\n",
    "upper_forecast = prediction.upper_forecast  # Upper bound of prediction interval\n",
    "lower_forecast = prediction.lower_forecast  # Lower bound of prediction interval\n",
    "\n",
    "# Get model details\n",
    "print(model)\n",
    "\n",
    "# Plot a specific series (first column in this example)\n",
    "prediction.plot(model.df_wide_numeric, series=model.df_wide_numeric.columns[0])\n",
    "\n",
    "# Get model performance metrics\n",
    "model_results = model.results()\n",
    "validation_results = model.results(\"validation\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science_ft_bio_predictions",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
