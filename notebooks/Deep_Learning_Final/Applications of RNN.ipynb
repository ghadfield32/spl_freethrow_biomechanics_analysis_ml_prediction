{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"s3cP-C2lcaNZ"},"outputs":[],"source":["!pip install setuptools==65.5.0 \"wheel<0.40.0\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v16EQXROcjAt"},"outputs":[],"source":["!pip install d2l==1.0.0b0"]},{"cell_type":"markdown","metadata":{"id":"Kqg8IO77c8q6"},"source":["### Long Short-Term Memory (LSTM)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Boh-lXTadB5L"},"outputs":[],"source":["import tensorflow as tf\n","from d2l import tensorflow as d2l"]},{"cell_type":"markdown","metadata":{"id":"oBDOAJvAeG4N"},"source":["#### Implementation from Scratch"]},{"cell_type":"markdown","metadata":{"id":"IFlbVLTFdIPB"},"source":["Now let's implement an LSTM from scratch."]},{"cell_type":"markdown","metadata":{"id":"ibmD9wq_dUoT"},"source":["**Initializing Model Parameters**\n","\n","Next, we need to define and initialize the model parameters. As previously, the hyperparameter `num_hiddens` dictates the number of hidden units. We initialize weights following a Gaussian distribution with 0.01 standard deviation, and we set the biases to 0."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rLjhwX7odi3Y"},"outputs":[],"source":["class LSTMScratch(d2l.Module):\n","    def __init__(self, num_inputs, num_hiddens, sigma=0.01):\n","        super().__init__()\n","        self.save_hyperparameters()\n","\n","        init_weight = lambda *shape: tf.Variable(tf.random.normal(shape) * sigma)\n","        triple = lambda: (init_weight(num_inputs, num_hiddens),\n","                          init_weight(num_hiddens, num_hiddens),\n","                          tf.Variable(tf.zeros(num_hiddens)))\n","\n","        self.W_xi, self.W_hi, self.b_i = triple()  # Input gate\n","        self.W_xf, self.W_hf, self.b_f = triple()  # Forget gate\n","        self.W_xo, self.W_ho, self.b_o = triple()  # Output gate\n","        self.W_xc, self.W_hc, self.b_c = triple()  # Input node"]},{"cell_type":"markdown","metadata":{"id":"h5ai4F4pdnwU"},"source":["The actual model is defined as described above, consisting of three gates and an input node. Note that only the hidden state is passed to the output layer."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VRl348SvdpAy"},"outputs":[],"source":["@d2l.add_to_class(LSTMScratch)\n","def forward(self, inputs, H_C=None):\n","    if H_C is None:\n","        # Initial state with shape: (batch_size, num_hiddens)\n","        H = tf.zeros((inputs.shape[1], self.num_hiddens))\n","        C = tf.zeros((inputs.shape[1], self.num_hiddens))\n","    else:\n","        H, C = H_C\n","    outputs = []\n","    for X in inputs:\n","        I = tf.sigmoid(tf.matmul(X, self.W_xi) +\n","                        tf.matmul(H, self.W_hi) + self.b_i)\n","        F = tf.sigmoid(tf.matmul(X, self.W_xf) +\n","                        tf.matmul(H, self.W_hf) + self.b_f)\n","        O = tf.sigmoid(tf.matmul(X, self.W_xo) +\n","                        tf.matmul(H, self.W_ho) + self.b_o)\n","        C_tilde = tf.tanh(tf.matmul(X, self.W_xc) +\n","                           tf.matmul(H, self.W_hc) + self.b_c)\n","        C = F * C + I * C_tilde\n","        H = O * tf.tanh(C)\n","        outputs.append(H)\n","    return outputs, (H, C)"]},{"cell_type":"markdown","metadata":{"id":"iXygSOdKdyoK"},"source":["**Training and Prediction**\n","\n","Letâ€™s train an LSTM model by instantiating the RNNLMScratch class"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sOM02ps2d6EM"},"outputs":[],"source":["data = d2l.TimeMachine(batch_size=1024, num_steps=32)\n","with d2l.try_gpu():\n","    lstm = LSTMScratch(num_inputs=len(data.vocab), num_hiddens=32)\n","    model = d2l.RNNLMScratch(lstm, vocab_size=len(data.vocab), lr=4)\n","trainer = d2l.Trainer(max_epochs=50, gradient_clip_val=1)\n","trainer.fit(model, data)"]},{"cell_type":"markdown","metadata":{"id":"B2ihU5sXeVQR"},"source":["#### Concise Implementation"]},{"cell_type":"markdown","metadata":{"id":"qO3EfPL6eegd"},"source":["Using high-level APIs, we can directly instantiate an LSTM model. This encapsulates all the configuration details that we made explicit above. The code is significantly faster as it uses compiled operators rather than Python for many details that we spelled out before."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"je60oQiuecO_"},"outputs":[],"source":["class LSTM(d2l.RNN):\n","    def __init__(self, num_hiddens):\n","        d2l.Module.__init__(self)\n","        self.save_hyperparameters()\n","        self.rnn = tf.keras.layers.LSTM(\n","                num_hiddens, return_sequences=True,\n","                return_state=True, time_major=True)\n","\n","    def forward(self, inputs, H_C=None):\n","        outputs, *H_C = self.rnn(inputs, H_C)\n","        return outputs, H_C\n","\n","lstm = LSTM(num_hiddens=32)\n","with d2l.try_gpu():\n","    model = d2l.RNNLM(lstm, vocab_size=len(data.vocab), lr=4)\n","trainer.fit(model, data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I-GxzS7relLS"},"outputs":[],"source":["model.predict('it has', 20, data.vocab)"]},{"cell_type":"markdown","metadata":{"id":"1aypQJNLfika"},"source":["### Gated Recurrent Units (GRU)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cOHwr7E1gDp3"},"outputs":[],"source":["import tensorflow as tf\n","from d2l import tensorflow as d2l"]},{"cell_type":"markdown","metadata":{"id":"yw3KpkgdgLrE"},"source":["#### Implementation from Scratch"]},{"cell_type":"markdown","metadata":{"id":"hMWpxIFCgRka"},"source":["**Initializing Model Parameters**\n","\n","The first step is to initialize the model parameters. We draw the weights from a Gaussian distribution with standard deviation to be sigma and set the bias to 0. The hyperparameter `num_hiddens` defines the number of hidden units. We instantiate all weights and biases relating to the update gate, the reset gate, and the candidate hidden state."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FEU4F3hNgao4"},"outputs":[],"source":["class GRUScratch(d2l.Module):\n","    def __init__(self, num_inputs, num_hiddens, sigma=0.01):\n","        super().__init__()\n","        self.save_hyperparameters()\n","\n","        init_weight = lambda *shape: tf.Variable(tf.random.normal(shape) * sigma)\n","        triple = lambda: (init_weight(num_inputs, num_hiddens),\n","                          init_weight(num_hiddens, num_hiddens),\n","                          tf.Variable(tf.zeros(num_hiddens)))\n","\n","        self.W_xz, self.W_hz, self.b_z = triple()  # Update gate\n","        self.W_xr, self.W_hr, self.b_r = triple()  # Reset gate\n","        self.W_xh, self.W_hh, self.b_h = triple()  # Candidate hidden state"]},{"cell_type":"markdown","metadata":{"id":"Hl4GWFQJgeuU"},"source":["**Defining the Model**\n","\n","Now we are ready to define the GRU forward computation. Its structure is the same as that of the basic RNN cell, except that the update equations are more complex."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iIJaUYfOgm4f"},"outputs":[],"source":["@d2l.add_to_class(GRUScratch)\n","def forward(self, inputs, H=None):\n","    if H is None:\n","        # Initial state with shape: (batch_size, num_hiddens)\n","        H = tf.zeros((inputs.shape[1], self.num_hiddens))\n","    outputs = []\n","    for X in inputs:\n","        Z = tf.sigmoid(tf.matmul(X, self.W_xz) +\n","                        tf.matmul(H, self.W_hz) + self.b_z)\n","        R = tf.sigmoid(tf.matmul(X, self.W_xr) +\n","                        tf.matmul(H, self.W_hr) + self.b_r)\n","        H_tilde = tf.tanh(tf.matmul(X, self.W_xh) +\n","                           tf.matmul(R * H, self.W_hh) + self.b_h)\n","        H = Z * H + (1 - Z) * H_tilde\n","        outputs.append(H)\n","    return outputs, H"]},{"cell_type":"markdown","metadata":{"id":"mTZhyuhPgriH"},"source":["**Training**\n","\n","Training a language model on The Time Machine dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x7im6pMpgwWs"},"outputs":[],"source":["data = d2l.TimeMachine(batch_size=1024, num_steps=32)\n","with d2l.try_gpu():\n","    gru = GRUScratch(num_inputs=len(data.vocab), num_hiddens=32)\n","    model = d2l.RNNLMScratch(gru, vocab_size=len(data.vocab), lr=4)\n","trainer = d2l.Trainer(max_epochs=50, gradient_clip_val=1)\n","trainer.fit(model, data)"]},{"cell_type":"markdown","metadata":{"id":"CmCOZSfYg19T"},"source":["#### Concise Implementation"]},{"cell_type":"markdown","metadata":{"id":"SZUNcjBEg6NL"},"source":["In high-level APIs, we can directly instantiate a GRU model. This encapsulates all the configuration detail that we made explicit above."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HFM7Ctt7g-2r"},"outputs":[],"source":["class GRU(d2l.RNN):\n","    def __init__(self, num_inputs, num_hiddens):\n","        d2l.Module.__init__(self)\n","        self.save_hyperparameters()\n","        self.rnn = tf.keras.layers.GRU(num_hiddens, return_sequences=True,\n","                                       return_state=True)"]},{"cell_type":"markdown","metadata":{"id":"rGoOoBeHhiLO"},"source":["The code is significantly faster in training as it uses compiled operators rather than Python."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"-JJydgP9gLEg"},"outputs":[],"source":["gru = GRU(num_inputs=len(data.vocab), num_hiddens=32)\n","with d2l.try_gpu():\n","    model = d2l.RNNLM(gru, vocab_size=len(data.vocab), lr=4)\n","trainer.fit(model, data)"]},{"cell_type":"markdown","metadata":{"id":"XhuKMQljijtJ"},"source":["After training, we print out the perplexity on the training set and the predicted sequence following the provided prefix."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UN6c6PGqikc2"},"outputs":[],"source":["model.predict('it has', 20, data.vocab)"]},{"cell_type":"markdown","source":["### Deep Recurrent Neural Networks"],"metadata":{"id":"N87BMwmKPyM3"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z7uwZNsChkVO"},"outputs":[],"source":["import tensorflow as tf\n","from d2l import tensorflow as d2l"]},{"cell_type":"markdown","source":["#### Implementation from Scratch"],"metadata":{"id":"aJy3SundQBPY"}},{"cell_type":"markdown","source":["To implement a multi-layer RNN from scratch, we can treat each layer as an `RNNScratch` instance with its own learnable parameters."],"metadata":{"id":"Ggfp-oUSQEc1"}},{"cell_type":"code","source":["class StackedRNNScratch(d2l.Module):\n","    def __init__(self, num_inputs, num_hiddens, num_layers, sigma=0.01):\n","        super().__init__()\n","        self.save_hyperparameters()\n","        self.rnns = [d2l.RNNScratch(num_inputs if i==0 else num_hiddens,\n","                                    num_hiddens, sigma)\n","                     for i in range(num_layers)]"],"metadata":{"id":"16mchPGZQKFy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The multi-layer forward computation simply performs forward computation layer by layer."],"metadata":{"id":"1hJRIIFMQLh-"}},{"cell_type":"code","source":["@d2l.add_to_class(StackedRNNScratch)\n","def forward(self, inputs, Hs=None):\n","    outputs = inputs\n","    if Hs is None: Hs = [None] * self.num_layers\n","    for i in range(self.num_layers):\n","        outputs, Hs[i] = self.rnns[i](outputs, Hs[i])\n","        outputs = tf.stack(outputs, 0)\n","    return outputs, Hs"],"metadata":{"id":"sWIAeVl-P9gA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["As an example, we train a deep GRU model on The Time Machine dataset. To keep things simple we set the number of layers to 2."],"metadata":{"id":"eAJ6HlR3QR-O"}},{"cell_type":"code","source":["data = d2l.TimeMachine(batch_size=1024, num_steps=32)\n","with d2l.try_gpu():\n","    rnn_block = StackedRNNScratch(num_inputs=len(data.vocab),\n","                              num_hiddens=32, num_layers=2)\n","    model = d2l.RNNLMScratch(rnn_block, vocab_size=len(data.vocab), lr=2)\n","trainer = d2l.Trainer(max_epochs=100, gradient_clip_val=1)\n","trainer.fit(model, data)"],"metadata":{"id":"GidAtxWkQUM8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Concise Implementation"],"metadata":{"id":"AXSJ6PfHQXm7"}},{"cell_type":"markdown","source":["Fortunately many of the logistical details required to implement multiple layers of an RNN are readily available in high-level APIs. Our concise implementation will use such built-in functionalities. The code allows specification of the number of layers explicitly rather than picking the default of a single layer."],"metadata":{"id":"fyDQN1Y-QcYJ"}},{"cell_type":"code","source":["class GRU(d2l.RNN):\n","    \"\"\"The multi-layer GRU model.\"\"\"\n","    def __init__(self, num_hiddens, num_layers, dropout=0):\n","        d2l.Module.__init__(self)\n","        self.save_hyperparameters()\n","        gru_cells = [tf.keras.layers.GRUCell(num_hiddens, dropout=dropout)\n","                     for _ in range(num_layers)]\n","        self.rnn = tf.keras.layers.RNN(gru_cells, return_sequences=True,\n","                                       return_state=True, time_major=True)\n","\n","    def forward(self, X, state=None):\n","        outputs, *state = self.rnn(X, state)\n","        return outputs, state"],"metadata":{"id":"mxTV69JOQaNK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The architectural decisions such as choosing hyperparameters are very similar to those of GRU. We pick the same number of inputs and outputs as we have distinct tokens, i.e., `vocab_size`. The number of hidden units is still 32. The only difference is that we now select a nontrivial number of hidden layers by specifying the value of `num_layers`."],"metadata":{"id":"4FeErLn5Qrf1"}},{"cell_type":"code","source":["gru = GRU(num_hiddens=32, num_layers=2)\n","with d2l.try_gpu():\n","    model = d2l.RNNLM(gru, vocab_size=len(data.vocab), lr=2)\n","trainer.fit(model, data)"],"metadata":{"id":"UrFwevucQ599"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.predict('it has', 20, data.vocab)"],"metadata":{"id":"GjyxpOfnQ-fx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Bidirectional Recurrent Neural Networks"],"metadata":{"id":"xsD9hJcLRM_K"}},{"cell_type":"code","source":["import tensorflow as tf\n","from d2l import tensorflow as d2l"],"metadata":{"id":"tnyFub0ARQ6t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Implementation from Scratch"],"metadata":{"id":"06icWMxJRVf8"}},{"cell_type":"markdown","source":["To implement a bidirectional RNN from scratch, we can include two unidirectional `RNNScratch` instances with separate learnable parameters."],"metadata":{"id":"P2INWWAuRc7Q"}},{"cell_type":"code","source":["class BiRNNScratch(d2l.Module):\n","    def __init__(self, num_inputs, num_hiddens, sigma=0.01):\n","        super().__init__()\n","        self.save_hyperparameters()\n","        self.f_rnn = d2l.RNNScratch(num_inputs, num_hiddens, sigma)\n","        self.b_rnn = d2l.RNNScratch(num_inputs, num_hiddens, sigma)\n","        self.num_hiddens *= 2  # The output dimension will be doubled"],"metadata":{"id":"9xq0toM5Rad4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["States of forward and backward RNNs are updated separately, while outputs of these two RNNs are concatenated."],"metadata":{"id":"PFGOyC_rRjeQ"}},{"cell_type":"code","source":["@d2l.add_to_class(BiRNNScratch)\n","def forward(self, inputs, Hs=None):\n","    f_H, b_H = Hs if Hs is not None else (None, None)\n","    f_outputs, f_H = self.f_rnn(inputs, f_H)\n","    b_outputs, b_H = self.b_rnn(reversed(inputs), b_H)\n","    outputs = [tf.concat((f, b), -1) for f, b in zip(\n","        f_outputs, reversed(b_outputs))]\n","    return outputs, (f_H, b_H)"],"metadata":{"id":"GuZtunQNRkDZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"iBohNsNIRpxi"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}