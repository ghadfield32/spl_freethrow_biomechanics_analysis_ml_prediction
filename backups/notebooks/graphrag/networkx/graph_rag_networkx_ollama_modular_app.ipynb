{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Free Ollama GraphRag with Llama 3.2 currently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspired by: https://github.com/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques_runnable_scripts/graph_rag.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step-by-Step Breakdown of the Process\n",
    "\n",
    "    Document Loading and Processing:\n",
    "        Loading Documents: The PyPDFLoader loads the specified PDF document. In this case, it is Understanding Climate Change.pdf.\n",
    "        Splitting and Embedding: The DocumentProcessor class splits the document into smaller chunks (e.g., 1000 characters) using RecursiveCharacterTextSplitter. Each chunk is then embedded using the OllamaEmbeddings model (llama3.2).\n",
    "        Vector Store Creation: The processed chunks are stored in a Chroma vector store, which enables fast similarity-based document retrieval.\n",
    "\n",
    "    Knowledge Graph Construction:\n",
    "        Building the Graph: The KnowledgeGraph class constructs a graph using the document chunks as nodes. Each node is represented by its content and connected to other nodes based on similarity and shared concepts.\n",
    "        Concept Extraction: Concepts are extracted from each document chunk using a combination of the spaCy NLP model and the ChatOllama language model.\n",
    "        Edge Creation: Edges are added between nodes based on the cosine similarity of their embeddings and shared concepts. The edge weight is calculated using a custom formula that combines similarity and concept overlap.\n",
    "\n",
    "    Query Handling and Graph Traversal:\n",
    "        Retrieving Relevant Documents: When a query is issued, the system first retrieves the most relevant documents from the vector store using similarity-based search.\n",
    "        Expanding Context with Graph Traversal: The system then traverses the knowledge graph starting from the most relevant nodes, expanding the context by visiting related nodes based on edge weights and priority. This traversal is managed using a priority queue (heap) that ensures the most promising nodes are explored first.\n",
    "        Answer Generation: As the graph is traversed, the accumulated context is checked to see if it provides a complete answer. If not, the traversal continues until an answer is found or the context is expanded enough to generate a final answer using the language model.\n",
    "\n",
    "    Visualization:\n",
    "        The graph traversal is visualized using networkx and matplotlib. Nodes are colored to represent different stages of the traversal (e.g., start node, end node, regular nodes), and edges are displayed based on their weights.\n",
    "        The graph in the image shows the path taken through the nodes, highlighting the connections and shared concepts.\n",
    "\n",
    "Output Analysis\n",
    "\n",
    "    The system's output indicates that the main cause of climate change, based on the provided context, is primarily driven by human activities, particularly the emission of greenhouse gases. This conclusion is reached by traversing nodes that cover topics like greenhouse gases, deforestation, and carbon storage.\n",
    "\n",
    "    The graph visualization shows the traversal path through the nodes. The starting node (green) and the ending node (red) highlight the trajectory taken through the graph. The concepts and content of each node are displayed, giving a clear picture of how the traversal occurred."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Potentials\n",
    "\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../../src/graphrag/networkx/examples/document_processor.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../../src/graphrag/networkx/examples/document_processor.py\n",
    "\n",
    "import nltk\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import OllamaEmbeddings  # Updated import\n",
    "from langchain_community.vectorstores import Chroma  # Updated import\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "\n",
    "class DocumentProcessor:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the DocumentProcessor with a text splitter and embeddings.\n",
    "        \"\"\"\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "        self.embeddings = OllamaEmbeddings(model=\"llama3.2\")\n",
    "\n",
    "    def process_documents(self, documents):\n",
    "        splits = self.text_splitter.split_documents(documents)\n",
    "        vector_store = Chroma.from_documents(\n",
    "            splits,\n",
    "            self.embeddings,\n",
    "            persist_directory=\"../../../data/graph_chroma_dbs\"\n",
    "        )\n",
    "        return splits, vector_store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../../src/graphrag/networkx/examples/knowledge_graph.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../../src/graphrag/networkx/examples/knowledge_graph.py\n",
    "\n",
    "import nltk\n",
    "import spacy\n",
    "import networkx as nx\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Dict\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.schema import AIMessage\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "\n",
    "class KnowledgeGraph:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the KnowledgeGraph with a graph, lemmatizer, and NLP model.\n",
    "        \"\"\"\n",
    "        self.graph = nx.Graph()\n",
    "        self.lemmatizer = nltk.WordNetLemmatizer()\n",
    "        self.concept_cache = {}\n",
    "        self.nlp = self._load_spacy_model()\n",
    "        self.edges_threshold = 0.8\n",
    "\n",
    "    def build_graph(self, splits, llm, embedding_model):\n",
    "        self._add_nodes(splits)\n",
    "        embeddings = self._create_embeddings(splits, embedding_model)\n",
    "        self._extract_concepts(splits, llm)\n",
    "        self._add_edges(embeddings)\n",
    "\n",
    "    def _add_nodes(self, splits):\n",
    "        \"\"\"\n",
    "        Adds nodes to the graph from the document splits.\n",
    "        \"\"\"\n",
    "        for i, split in enumerate(splits):\n",
    "            self.graph.add_node(i, content=split.page_content)\n",
    "\n",
    "    def _create_embeddings(self, splits, embedding_model):\n",
    "        \"\"\"\n",
    "        Creates embeddings for the document splits using the embedding model.\n",
    "        \"\"\"\n",
    "        texts = [split.page_content for split in splits]\n",
    "        return embedding_model.embed_documents(texts)\n",
    "\n",
    "    def _compute_similarities(self, embeddings):\n",
    "        \"\"\"\n",
    "        Computes the cosine similarity matrix for the embeddings.\n",
    "        \"\"\"\n",
    "        from sklearn.metrics.pairwise import cosine_similarity\n",
    "        return cosine_similarity(embeddings)\n",
    "\n",
    "    def _load_spacy_model(self):\n",
    "        \"\"\"\n",
    "        Loads the spaCy NLP model, downloading it if necessary.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return spacy.load(\"en_core_web_sm\")\n",
    "        except OSError:\n",
    "            print(\"Downloading spaCy model...\")\n",
    "            spacy.cli.download(\"en_core_web_sm\")\n",
    "            return spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    def _extract_concepts_and_entities(self, content, llm):\n",
    "        \"\"\"\n",
    "        Extracts concepts and named entities from the content using spaCy and a large language model.\n",
    "        \"\"\"\n",
    "        if content in self.concept_cache:\n",
    "            return self.concept_cache[content]\n",
    "\n",
    "        # Extract named entities using spaCy\n",
    "        doc = self.nlp(content)\n",
    "        named_entities = [ent.text for ent in doc.ents if ent.label_ in [\"PERSON\", \"ORG\", \"GPE\", \"WORK_OF_ART\"]]\n",
    "\n",
    "        # Extract general concepts using LLM\n",
    "        concept_extraction_prompt = PromptTemplate(\n",
    "            input_variables=[\"text\"],\n",
    "            template=\"Extract key concepts (excluding named entities) from the following text:\\n\\n{text}\\n\\nKey concepts:\"\n",
    "        )\n",
    "        concept_chain = concept_extraction_prompt | llm\n",
    "        response = concept_chain.invoke({\"text\": content})\n",
    "\n",
    "        # Check if response is an AIMessage object and get content\n",
    "        if isinstance(response, AIMessage):\n",
    "            general_concepts = response.content\n",
    "        else:\n",
    "            general_concepts = response\n",
    "\n",
    "        # Split the response into individual concepts\n",
    "        general_concepts = [concept.strip() for concept in general_concepts.split(',')]\n",
    "\n",
    "        # Combine named entities and general concepts\n",
    "        all_concepts = list(set(named_entities + general_concepts))\n",
    "\n",
    "        self.concept_cache[content] = all_concepts\n",
    "        return all_concepts\n",
    "\n",
    "    def _extract_concepts(self, splits, llm):\n",
    "        \"\"\"\n",
    "        Extracts concepts for all document splits using multi-threading.\n",
    "        \"\"\"\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            future_to_node = {executor.submit(self._extract_concepts_and_entities, split.page_content, llm): i\n",
    "                              for i, split in enumerate(splits)}\n",
    "\n",
    "            for future in tqdm(as_completed(future_to_node), total=len(splits),\n",
    "                               desc=\"Extracting concepts and entities\"):\n",
    "                node = future_to_node[future]\n",
    "                concepts = future.result()\n",
    "                self.graph.nodes[node]['concepts'] = concepts\n",
    "\n",
    "    def _add_edges(self, embeddings):\n",
    "        \"\"\"\n",
    "        Adds edges to the graph based on the similarity of embeddings and shared concepts.\n",
    "        \"\"\"\n",
    "        similarity_matrix = self._compute_similarities(embeddings)\n",
    "        num_nodes = len(self.graph.nodes)\n",
    "\n",
    "        for node1 in tqdm(range(num_nodes), desc=\"Adding edges\"):\n",
    "            for node2 in range(node1 + 1, num_nodes):\n",
    "                similarity_score = similarity_matrix[node1][node2]\n",
    "                if similarity_score > self.edges_threshold:\n",
    "                    shared_concepts = set(self.graph.nodes[node1]['concepts']) & set(\n",
    "                        self.graph.nodes[node2]['concepts'])\n",
    "                    edge_weight = self._calculate_edge_weight(node1, node2, similarity_score, shared_concepts)\n",
    "                    self.graph.add_edge(node1, node2, weight=edge_weight,\n",
    "                                        similarity=similarity_score,\n",
    "                                        shared_concepts=list(shared_concepts))\n",
    "\n",
    "    def _calculate_edge_weight(self, node1, node2, similarity_score, shared_concepts, alpha=0.7, beta=0.3):\n",
    "        \"\"\"\n",
    "        Calculates the weight of an edge based on similarity score and shared concepts.\n",
    "        \"\"\"\n",
    "        max_possible_shared = min(len(self.graph.nodes[node1]['concepts']), len(self.graph.nodes[node2]['concepts']))\n",
    "        normalized_shared_concepts = len(shared_concepts) / max_possible_shared if max_possible_shared > 0 else 0\n",
    "        return alpha * similarity_score + beta * normalized_shared_concepts\n",
    "\n",
    "    def _lemmatize_concept(self, concept):\n",
    "        \"\"\"\n",
    "        Lemmatizes a given concept.\n",
    "        \"\"\"\n",
    "        return ' '.join([self.lemmatizer.lemmatize(word) for word in concept.lower().split()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../../src/graphrag/networkx/examples/query_engine.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../../src/graphrag/networkx/examples/query_engine.py\n",
    "\n",
    "import heapq\n",
    "from typing import Tuple, List, Dict\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.schema import AIMessage\n",
    "\n",
    "class QueryEngine:\n",
    "    def __init__(self, vector_store, knowledge_graph):\n",
    "        self.vector_store = vector_store\n",
    "        self.knowledge_graph = knowledge_graph\n",
    "        self.llm = ChatOllama(model=\"llama3.2\", temperature=0)\n",
    "        self.max_context_length = 4000\n",
    "        self.answer_check_chain = self._create_answer_check_chain()\n",
    "\n",
    "    def _create_answer_check_chain(self):\n",
    "        \"\"\"\n",
    "        Creates a chain to check if the context provides a complete answer to the query.\n",
    "        \"\"\"\n",
    "        answer_check_prompt = PromptTemplate(\n",
    "            input_variables=[\"query\", \"context\"],\n",
    "            template=\"Given the query: '{query}'\\n\\nAnd the current context:\\n{context}\\n\\nDoes this context provide a complete answer to the query? If yes, provide the answer. If no, state that the answer is incomplete.\"\n",
    "        )\n",
    "        return answer_check_prompt | self.llm\n",
    "\n",
    "    def _check_answer(self, query: str, context: str) -> Tuple[bool, str]:\n",
    "        \"\"\"\n",
    "        Checks if the current context provides a complete answer to the query.\n",
    "        \"\"\"\n",
    "        response = self.answer_check_chain.invoke({\"query\": query, \"context\": context})\n",
    "        is_complete = \"Yes\" in response\n",
    "        answer = response.split(\"Answer:\")[-1].strip() if is_complete else \"\"\n",
    "        return is_complete, answer\n",
    "\n",
    "    def _expand_context(self, query: str, relevant_docs) -> Tuple[str, List[int], Dict[int, str], str]:\n",
    "        \"\"\"\n",
    "        Expands the context by traversing the knowledge graph using a Dijkstra-like approach.\n",
    "        \"\"\"\n",
    "        expanded_context = \"\"\n",
    "        traversal_path = []\n",
    "        visited_concepts = set()\n",
    "        filtered_content = {}\n",
    "        final_answer = \"\"\n",
    "\n",
    "        priority_queue = []\n",
    "        distances = {}\n",
    "\n",
    "        # Initialize priority queue with closest nodes from relevant docs\n",
    "        for doc in relevant_docs:\n",
    "            # Find the most similar node in the knowledge graph for each relevant document\n",
    "            closest_nodes = self.vector_store.similarity_search_with_score(doc.page_content, k=1)\n",
    "            closest_node_content, similarity_score = closest_nodes[0]\n",
    "\n",
    "            # Get the corresponding node in our knowledge graph\n",
    "            closest_node = next(n for n in self.knowledge_graph.graph.nodes if\n",
    "                                self.knowledge_graph.graph.nodes[n]['content'] == closest_node_content.page_content)\n",
    "\n",
    "            # Initialize priority (inverse of similarity score for min-heap behavior)\n",
    "            priority = 1 / similarity_score\n",
    "            heapq.heappush(priority_queue, (priority, closest_node))\n",
    "            distances[closest_node] = priority\n",
    "\n",
    "        while priority_queue:\n",
    "            # Get the node with the highest priority (lowest distance value)\n",
    "            current_priority, current_node = heapq.heappop(priority_queue)\n",
    "\n",
    "            # Skip if we've already found a better path to this node\n",
    "            if current_priority > distances.get(current_node, float('inf')):\n",
    "                continue\n",
    "\n",
    "            if current_node not in traversal_path:\n",
    "                traversal_path.append(current_node)\n",
    "                node_content = self.knowledge_graph.graph.nodes[current_node]['content']\n",
    "                node_concepts = self.knowledge_graph.graph.nodes[current_node]['concepts']\n",
    "\n",
    "                # Add node content to our accumulated context\n",
    "                filtered_content[current_node] = node_content\n",
    "                expanded_context += \"\\n\" + node_content if expanded_context else node_content\n",
    "\n",
    "                # Check if we have a complete answer with the current context\n",
    "                is_complete, answer = self._check_answer(query, expanded_context)\n",
    "                if is_complete:\n",
    "                    final_answer = answer\n",
    "                    break\n",
    "\n",
    "                # Process the concepts of the current node\n",
    "                node_concepts_set = set(self.knowledge_graph._lemmatize_concept(c) for c in node_concepts)\n",
    "                if not node_concepts_set.issubset(visited_concepts):\n",
    "                    visited_concepts.update(node_concepts_set)\n",
    "\n",
    "                    # Explore neighbors\n",
    "                    for neighbor in self.knowledge_graph.graph.neighbors(current_node):\n",
    "                        edge_data = self.knowledge_graph.graph[current_node][neighbor]\n",
    "                        edge_weight = edge_data['weight']\n",
    "\n",
    "                        # Calculate new distance (priority) to the neighbor\n",
    "                        distance = current_priority + (1 / edge_weight)\n",
    "\n",
    "                        # If we've found a stronger connection to the neighbor, update its distance\n",
    "                        if distance < distances.get(neighbor, float('inf')):\n",
    "                            distances[neighbor] = distance\n",
    "                            heapq.heappush(priority_queue, (distance, neighbor))\n",
    "\n",
    "            # If we found a final answer, break out of the main loop\n",
    "            if final_answer:\n",
    "                break\n",
    "\n",
    "        # If we haven't found a complete answer, generate one using the LLM\n",
    "        if not final_answer:\n",
    "            response_prompt = PromptTemplate(\n",
    "                input_variables=[\"query\", \"context\"],\n",
    "                template=\"Based on the following context, please answer the query.\\n\\nContext: {context}\\n\\nQuery: {query}\\n\\nAnswer:\"\n",
    "            )\n",
    "            response_chain = response_prompt | self.llm\n",
    "            input_data = {\"query\": query, \"context\": expanded_context}\n",
    "            final_answer = response_chain.invoke(input_data)\n",
    "\n",
    "        return expanded_context, traversal_path, filtered_content, final_answer\n",
    "\n",
    "    def query(self, query: str) -> Tuple[str, List[int], Dict[int, str]]:\n",
    "        \"\"\"\n",
    "        Processes a query by retrieving relevant documents, expanding the context, and generating the final answer.\n",
    "        \"\"\"\n",
    "        relevant_docs = self._retrieve_relevant_documents(query)\n",
    "        expanded_context, traversal_path, filtered_content, final_answer = self._expand_context(query, relevant_docs)\n",
    "        return final_answer, traversal_path, filtered_content\n",
    "\n",
    "    def _retrieve_relevant_documents(self, query: str):\n",
    "        \"\"\"\n",
    "        Retrieves relevant documents based on the query using the vector store.\n",
    "        \"\"\"\n",
    "        retriever = self.vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n",
    "        return retriever.get_relevant_documents(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../../src/graphrag/networkx/examples/visualizer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../../src/graphrag/networkx/examples/visualizer.py\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "class Visualizer:\n",
    "    @staticmethod\n",
    "    def visualize_traversal(graph, traversal_path):\n",
    "        \"\"\"\n",
    "        Visualizes the traversal path on the knowledge graph with nodes, edges, and traversal path highlighted.\n",
    "        \"\"\"\n",
    "        traversal_graph = nx.DiGraph()\n",
    "\n",
    "        # Add nodes and edges from the original graph\n",
    "        for node in graph.nodes():\n",
    "            traversal_graph.add_node(node)\n",
    "        for u, v, data in graph.edges(data=True):\n",
    "            traversal_graph.add_edge(u, v, **data)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(16, 12))\n",
    "\n",
    "        # Generate positions for all nodes\n",
    "        pos = nx.spring_layout(traversal_graph, k=1, iterations=50)\n",
    "\n",
    "        # Draw regular edges with color based on weight\n",
    "        edges = traversal_graph.edges()\n",
    "        edge_weights = [traversal_graph[u][v].get('weight', 0.5) for u, v in edges]\n",
    "        nx.draw_networkx_edges(traversal_graph, pos,\n",
    "                               edgelist=edges,\n",
    "                               edge_color=edge_weights,\n",
    "                               edge_cmap=plt.cm.Blues,\n",
    "                               width=2,\n",
    "                               ax=ax)\n",
    "\n",
    "        # Draw nodes\n",
    "        nx.draw_networkx_nodes(traversal_graph, pos,\n",
    "                               node_color='lightblue',\n",
    "                               node_size=3000,\n",
    "                               ax=ax)\n",
    "\n",
    "        # Draw traversal path with curved arrows\n",
    "        edge_offset = 0.1\n",
    "        for i in range(len(traversal_path) - 1):\n",
    "            start = traversal_path[i]\n",
    "            end = traversal_path[i + 1]\n",
    "            start_pos = pos[start]\n",
    "            end_pos = pos[end]\n",
    "\n",
    "            # Draw curved arrow\n",
    "            arrow = patches.FancyArrowPatch(start_pos, end_pos,\n",
    "                                            connectionstyle=f\"arc3,rad={0.3}\",\n",
    "                                            color='red',\n",
    "                                            arrowstyle=\"->\",\n",
    "                                            mutation_scale=20,\n",
    "                                            linestyle='--',\n",
    "                                            linewidth=2,\n",
    "                                            zorder=4)\n",
    "            ax.add_patch(arrow)\n",
    "\n",
    "        # Prepare labels for the nodes\n",
    "        labels = {}\n",
    "        for i, node in enumerate(traversal_path):\n",
    "            concepts = graph.nodes[node].get('concepts', [])\n",
    "            label = f\"{i + 1}. {concepts[0] if concepts else ''}\"\n",
    "            labels[node] = label\n",
    "\n",
    "        for node in traversal_graph.nodes():\n",
    "            if node not in labels:\n",
    "                concepts = graph.nodes[node].get('concepts', [])\n",
    "                labels[node] = concepts[0] if concepts else ''\n",
    "\n",
    "        # Draw labels\n",
    "        nx.draw_networkx_labels(traversal_graph, pos, labels, font_size=8, font_weight=\"bold\", ax=ax)\n",
    "\n",
    "        # Highlight start and end nodes\n",
    "        start_node = traversal_path[0]\n",
    "        end_node = traversal_path[-1]\n",
    "\n",
    "        nx.draw_networkx_nodes(traversal_graph, pos,\n",
    "                               nodelist=[start_node],\n",
    "                               node_color='lightgreen',\n",
    "                               node_size=3000,\n",
    "                               ax=ax)\n",
    "\n",
    "        nx.draw_networkx_nodes(traversal_graph, pos,\n",
    "                               nodelist=[end_node],\n",
    "                               node_color='lightcoral',\n",
    "                               node_size=3000,\n",
    "                               ax=ax)\n",
    "\n",
    "        ax.set_title(\"Graph Traversal Flow\")\n",
    "        ax.axis('off')\n",
    "\n",
    "        # Add colorbar for edge weights\n",
    "        sm = plt.cm.ScalarMappable(cmap=plt.cm.Blues,\n",
    "                                   norm=plt.Normalize(vmin=min(edge_weights), vmax=max(edge_weights)))\n",
    "        sm.set_array([])\n",
    "        cbar = fig.colorbar(sm, ax=ax, orientation='vertical', fraction=0.046, pad=0.04)\n",
    "        cbar.set_label('Edge Weight', rotation=270, labelpad=15)\n",
    "\n",
    "        # Add legend\n",
    "        regular_line = plt.Line2D([0], [0], color='blue', linewidth=2, label='Regular Edge')\n",
    "        traversal_line = plt.Line2D([0], [0], color='red', linewidth=2, linestyle='--', label='Traversal Path')\n",
    "        start_point = plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='lightgreen', markersize=15,\n",
    "                                 label='Start Node')\n",
    "        end_point = plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='lightcoral', markersize=15,\n",
    "                               label='End Node')\n",
    "        legend = plt.legend(handles=[regular_line, traversal_line, start_point, end_point], loc='upper left',\n",
    "                            bbox_to_anchor=(0, 1), ncol=2)\n",
    "        legend.get_frame().set_alpha(0.8)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        return fig, ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../../src/graphrag/networkx/examples/graph_rag.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../../src/graphrag/networkx/examples/graph_rag.py\n",
    "\n",
    "from document_processor import DocumentProcessor\n",
    "from knowledge_graph import KnowledgeGraph\n",
    "from query_engine import QueryEngine\n",
    "from visualizer import Visualizer\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "\n",
    "class GraphRAG:\n",
    "    def __init__(self, documents):\n",
    "        \"\"\"\n",
    "        Initializes the GraphRAG system.\n",
    "        \"\"\"\n",
    "        self.llm = ChatOllama(model=\"llama3.2\", temperature=0)\n",
    "        self.embedding_model = OllamaEmbeddings(model=\"llama3.2\")\n",
    "        self.document_processor = DocumentProcessor()  # Use the DocumentProcessor\n",
    "        self.knowledge_graph = KnowledgeGraph()\n",
    "        self.query_engine = None\n",
    "        self.visualizer = Visualizer()\n",
    "        self.process_documents(documents)\n",
    "\n",
    "    def process_documents(self, documents):\n",
    "        splits, vector_store = self.document_processor.process_documents(documents)\n",
    "        self.knowledge_graph.build_graph(splits, self.llm, self.embedding_model)\n",
    "        self.query_engine = QueryEngine(vector_store, self.knowledge_graph)\n",
    "\n",
    "\n",
    "    def query(self, query: str):\n",
    "        \"\"\"\n",
    "        Handles a query using the query engine.\n",
    "        \"\"\"\n",
    "        response, traversal_path, filtered_content = self.query_engine.query(query)\n",
    "        return response, traversal_path, filtered_content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../../src/graphrag/networkx/examples/app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../../src/graphrag/networkx/examples/app.py\n",
    "\n",
    "import streamlit as st\n",
    "import matplotlib.pyplot as plt\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.schema import Document\n",
    "import networkx as nx\n",
    "\n",
    "from graph_rag import GraphRAG  # Import your GraphRAG class\n",
    "from visualizer import Visualizer  # Import Visualizer for plotting\n",
    "\n",
    "\n",
    "# Initialize global variables\n",
    "graph_rag = None\n",
    "\n",
    "# Function to process uploaded PDF\n",
    "def process_pdf(file):\n",
    "    pdf_reader = PdfReader(file)\n",
    "    # Read content from all pages\n",
    "    documents = [Document(page_content=page.extract_text()) for page in pdf_reader.pages]\n",
    "    return documents\n",
    "\n",
    "# Set up the main Streamlit UI\n",
    "st.title(\"Knowledge Graph from PDF with LLM\")\n",
    "\n",
    "# PDF upload and processing\n",
    "uploaded_file = st.file_uploader(\"Upload a PDF file\", type=[\"pdf\"])\n",
    "\n",
    "if uploaded_file is not None:\n",
    "    # Process the uploaded PDF\n",
    "    documents = process_pdf(uploaded_file)\n",
    "    st.write(f\"Uploaded **{uploaded_file.name}** with **{len(documents)} pages** processed.\")\n",
    "\n",
    "    # Initialize GraphRAG with the processed documents\n",
    "    with st.spinner(\"Processing the PDF and building the knowledge graph...\"):\n",
    "        graph_rag = GraphRAG(documents)\n",
    "    st.success(\"PDF has been processed and the knowledge graph has been created.\")\n",
    "\n",
    "    # Visualization Section\n",
    "    st.write(\"### Knowledge Graph Visualization\")\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    pos = nx.spring_layout(graph_rag.knowledge_graph.graph, k=1, iterations=50)\n",
    "    nx.draw(graph_rag.knowledge_graph.graph, pos, with_labels=True, ax=ax,\n",
    "            node_size=500, node_color=\"lightblue\", edge_color=\"gray\")\n",
    "    st.pyplot(fig)\n",
    "\n",
    "# Query section\n",
    "if graph_rag is not None:\n",
    "    st.write(\"### Query the Knowledge Graph\")\n",
    "    query = st.text_input(\"Enter your query\", \"What is the main cause of climate change?\")\n",
    "\n",
    "    if st.button(\"Submit Query\"):\n",
    "        with st.spinner(\"Processing your query...\"):\n",
    "            response, traversal_path, filtered_content = graph_rag.query(query)\n",
    "            st.write(\"### Response to your query:\")\n",
    "            st.write(response)\n",
    "\n",
    "            # Visualize traversal\n",
    "            st.write(\"### Traversal Path Visualization\")\n",
    "            fig, ax = Visualizer.visualize_traversal(graph_rag.knowledge_graph.graph, traversal_path)\n",
    "            st.pyplot(fig)\n",
    "\n",
    "            # Show traversal logic\n",
    "            st.write(\"### Traversal Logic and Filtered Content\")\n",
    "            for i, node in enumerate(traversal_path):\n",
    "                st.write(f\"**Step {i + 1} - Node {node}:**\")\n",
    "                content = filtered_content.get(node, 'No content available')\n",
    "                st.write(content)\n",
    "                st.write(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x1200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2858/3428386683.py:94: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = self.llm([message])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LLM Analysis:\n",
      "I don't see any schema information provided. Please share the actual schema details, and I'll be happy to help you identify any redundant columns or similar tables.\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import MetaData\n",
    "from sqlalchemy.engine import create_engine\n",
    "import networkx as nx\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.schema import HumanMessage\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class DatabaseGraphAnalyzer:\n",
    "    def __init__(self, db_urls, llm_model=\"llama3.2\"):\n",
    "        \"\"\"\n",
    "        Initializes the DatabaseGraphAnalyzer with the provided database URLs.\n",
    "        \n",
    "        Parameters:\n",
    "        - db_urls (dict): Dictionary where keys are database names and values are their connection URLs.\n",
    "        - llm_model (str): LLM model name to use for semantic analysis.\n",
    "        \"\"\"\n",
    "        self.db_urls = db_urls\n",
    "        self.graph = nx.DiGraph()\n",
    "        self.llm = ChatOllama(model=llm_model)\n",
    "        \n",
    "        # Store metadata for each database\n",
    "        self.metadata = {}\n",
    "        for db_name, url in db_urls.items():\n",
    "            engine = create_engine(url)\n",
    "            metadata = MetaData()\n",
    "            metadata.reflect(bind=engine)\n",
    "            self.metadata[db_name] = metadata\n",
    "            self._add_metadata_to_graph(metadata, db_name)\n",
    "\n",
    "    def _add_metadata_to_graph(self, metadata, db_name):\n",
    "        \"\"\"\n",
    "        Adds database metadata to the graph, where each table and column are represented as nodes.\n",
    "        Relationships such as foreign keys are represented as edges.\n",
    "        \n",
    "        Parameters:\n",
    "        - metadata (MetaData): SQLAlchemy MetaData object containing the schema information.\n",
    "        - db_name (str): Name of the database to associate with these tables and columns.\n",
    "        \"\"\"\n",
    "        for table_name, table in metadata.tables.items():\n",
    "            # Add a table node\n",
    "            table_node = f\"{db_name}.{table_name}\"\n",
    "            self.graph.add_node(table_node, type=\"table\", db=db_name, label=f\"Table: {table_name} ({db_name})\")\n",
    "\n",
    "            # Add column nodes and link to the table\n",
    "            for column in table.columns:\n",
    "                column_node_id = f\"{db_name}.{table_name}.{column.name}\"\n",
    "                self.graph.add_node(column_node_id, type=\"column\", db=db_name, label=f\"Column: {column.name} ({table_name})\", data_type=str(column.type))\n",
    "                self.graph.add_edge(table_node, column_node_id, relationship=\"contains\")\n",
    "\n",
    "            # Link foreign keys between columns\n",
    "            for fk in table.foreign_keys:\n",
    "                parent_column = f\"{db_name}.{fk.parent.table.name}.{fk.parent.name}\"\n",
    "                referenced_column = f\"{db_name}.{fk.column.table.name}.{fk.column.name}\"\n",
    "                self.graph.add_edge(parent_column, referenced_column, relationship=\"foreign_key\")\n",
    "\n",
    "    def visualize_graph(self):\n",
    "        \"\"\"\n",
    "        Visualizes the combined database metadata graph with nodes and edges representing tables and columns.\n",
    "        \"\"\"\n",
    "        node_colors = ['skyblue' if data['type'] == 'table' else 'lightgreen' for _, data in self.graph.nodes(data=True)]\n",
    "        pos = nx.spring_layout(self.graph, k=0.5, iterations=50)\n",
    "        \n",
    "        plt.figure(figsize=(16, 12))\n",
    "        nx.draw_networkx_nodes(self.graph, pos, node_color=node_colors, node_size=1500, alpha=0.8)\n",
    "        nx.draw_networkx_labels(self.graph, pos, labels=nx.get_node_attributes(self.graph, 'label'), font_size=8, font_weight='bold')\n",
    "        nx.draw_networkx_edges(self.graph, pos, edgelist=self.graph.edges(data=True), arrowstyle='-|>', arrowsize=20, edge_color='black')\n",
    "        plt.title(\"Database Schema Graph\", fontsize=18)\n",
    "        plt.show()\n",
    "\n",
    "    def query_schema_with_prompt(self, custom_prompt):\n",
    "        \"\"\"\n",
    "        Query the LLM using a custom prompt and the schema information extracted.\n",
    "        \n",
    "        Parameters:\n",
    "        - custom_prompt (str): The custom prompt to send to the LLM for analysis.\n",
    "        \n",
    "        Returns:\n",
    "        str: LLM response to the custom prompt.\n",
    "        \"\"\"\n",
    "        # Extract table information\n",
    "        table_info = {node: {\"columns\": []} for node, data in self.graph.nodes(data=True) if data['type'] == 'table'}\n",
    "        for node, data in self.graph.nodes(data=True):\n",
    "            if data['type'] == 'column':\n",
    "                table_name = '.'.join(node.split('.')[:-1])  # Extract table name from column node\n",
    "                if table_name in table_info:\n",
    "                    table_info[table_name]['columns'].append(data['label'])\n",
    "        \n",
    "        # Format the schema information for the prompt\n",
    "        table_details = \"\\n\".join([f\"Table {key} has columns: {', '.join(value['columns'])}\" for key, value in table_info.items()])\n",
    "        prompt_content = f\"{custom_prompt}\\n\\nSchema Information:\\n{table_details}\"\n",
    "        \n",
    "        # Send the prompt to the LLM\n",
    "        message = HumanMessage(content=prompt_content)\n",
    "        response = self.llm([message])\n",
    "        \n",
    "        print(f\"\\nLLM Analysis:\\n{response.content}\")\n",
    "        return response.content\n",
    "\n",
    "# Main function to run the analyzer with sample databases\n",
    "def main():\n",
    "    # Example database URLs\n",
    "    db_urls = {\n",
    "        \"example1\": \"sqlite:///example1.db\",\n",
    "        \"example2\": \"sqlite:///example2.db\"\n",
    "    }\n",
    "\n",
    "    # Initialize and analyze the databases\n",
    "    analyzer = DatabaseGraphAnalyzer(db_urls)\n",
    "    analyzer.visualize_graph()\n",
    "\n",
    "    # Perform a custom LLM query on the schema\n",
    "    custom_prompt = \"Identify redundant columns or similar tables in this schema.\"\n",
    "    analyzer.query_schema_with_prompt(custom_prompt)\n",
    "\n",
    "# Run the main function if the script is executed\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science_ollama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
