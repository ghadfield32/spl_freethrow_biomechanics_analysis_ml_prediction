{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: set up GPU, api keys, and models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.3.1+cu121\n",
      "Number of GPUs available: 1\n",
      "GPU: NVIDIA GeForce RTX 4090, Memory: 25.756696576 GB, Compute Capability: (8, 9)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Print the PyTorch version\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "def check_gpu():\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_info = {\n",
    "            \"num_gpus\": torch.cuda.device_count(),\n",
    "            \"current_gpu\": torch.cuda.get_device_name(torch.cuda.current_device()),\n",
    "            \"gpus\": []\n",
    "        }\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            gpu_info[\"gpus\"].append({\n",
    "                \"name\": torch.cuda.get_device_name(i),\n",
    "                \"memory_gb\": torch.cuda.get_device_properties(i).total_memory / 1e9,\n",
    "                \"compute_capability\": torch.cuda.get_device_capability(i)\n",
    "            })\n",
    "        return gpu_info\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Call the helper\n",
    "gpu_info = check_gpu()\n",
    "if gpu_info:\n",
    "    print(f\"Number of GPUs available: {gpu_info['num_gpus']}\")\n",
    "    for gpu in gpu_info['gpus']:\n",
    "        print(f\"GPU: {gpu['name']}, Memory: {gpu['memory_gb']} GB, Compute Capability: {gpu['compute_capability']}\")\n",
    "else:\n",
    "    print(\"CUDA is not available. GPU not found.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspaces/custom_ollama_docker/notebooks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Print the current working directory (optional for debugging)\n",
    "print(os.getcwd())\n",
    "\n",
    "# Set the path to your .env file relative to the current working directory\n",
    "dotenv_path = os.path.join(os.getcwd(), '../.env')\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "# Load the API keys from environment variables\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "hf_token = os.getenv(\"HUGGINGFACE_API_KEY\")\n",
    "anthropic_token = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "tavily_token = os.getenv(\"TAVILY_API_KEY\")\n",
    "langsmith_token = os.getenv(\"LANGSMITH_API_KEY\")\n",
    "# NOMIC_EMBEDDINGS_API_KEY\n",
    "nomic_token = os.getenv(\"NOMIC_EMBEDDINGS_API_KEY\")\n",
    "\n",
    "# Set the Hugging Face token as an environment variable (if not already done)\n",
    "if hf_token:\n",
    "    os.environ[\"HUGGINGFACE_API_KEY\"] = hf_token\n",
    "if openai_api_key:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "if anthropic_token:\n",
    "    os.environ[\"ANTHROPIC_API_KEY\"] = anthropic_token\n",
    "if tavily_token:\n",
    "    os.environ[\"TAVILY_API_KEY\"] = tavily_token\n",
    "if tavily_token:\n",
    "    os.environ[\"LANGSMITH_API_KEY\"] = langsmith_token\n",
    "if nomic_token:\n",
    "    os.environ[\"NOMIC_EMBEDDINGS_API_KEY\"] = nomic_token\n",
    "    \n",
    "    \n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pull in Ollama local model for free use throughout RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "local_llm = \"llama3.2\"\n",
    "\n",
    "llm = ChatOllama(model=local_llm, temperature=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding with Ollama Models\n",
    "\n",
    "Use Ollama or compatible models for generating embeddings, ensuring control over the embedding process and consistency within the pipeline.\n",
    "Action:\n",
    "\n",
    "    Replace external embedding generation with local embeddings using NomicEmbeddings.\n",
    "\n",
    "Pros:\n",
    "\n",
    "    Efficient Retrieval: Embeddings generated locally.\n",
    "    Unified Pipeline: Consistency in using local models.\n",
    "\n",
    "Cons:\n",
    "\n",
    "    Embedding Quality: May not match commercial APIs without fine-tuning.\n",
    "    Resource Requirements: Ensure compatibility with your hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nomic import NomicEmbeddings\n",
    "\n",
    "embeddings_model = NomicEmbeddings(\n",
    "    model=\"nomic-embed-text-v1.5\",\n",
    "    nomic_api_key=nomic_token\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Basic RAG Pipeline Setup with Ollama\n",
    "\n",
    "Set up the basic Retrieval-Augmented Generation (RAG) pipeline using Ollama. This involves retrieving relevant documents, feeding them to the LLM (Ollama), and generating responses based on the context.\n",
    "Steps:\n",
    "\n",
    "    Document Retrieval:\n",
    "        Use a vector store like FAISS to store and retrieve embeddings.\n",
    "        Use a text splitter to divide documents into manageable chunks.\n",
    "\n",
    "    LLM Integration:\n",
    "        Replace any existing LLMs with ChatOllama.\n",
    "\n",
    "    Prompt & Template:\n",
    "        Customize prompts to leverage Ollama's capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "/tmp/ipykernel_1736/1029877436.py:52: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(llm=llm, prompt=prompt_template)\n",
      "/tmp/ipykernel_1736/1029877436.py:58: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  context_docs = retriever.get_relevant_documents(question)\n",
      "/tmp/ipykernel_1736/1029877436.py:60: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = chain.run(question=question, context=context)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic RAG Response:\n",
      "The main topic of the document appears to be the Boston Celtics, as indicated by the \"Starting 5: Behind the scenes at Celtics Media Day\" section. However, it's worth noting that the document is a large collection of articles and topics related to the NBA, with various teams and players featured throughout.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Import necessary modules\n",
    "import os\n",
    "import torch\n",
    "import nltk\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "import numpy as np\n",
    "\n",
    "# Updated langchain imports based on changes in recent versions\n",
    "from langchain.vectorstores import FAISS  # FAISS is now part of langchain.vectorstores\n",
    "from langchain.prompts import ChatPromptTemplate  # ChatPromptTemplate should now be imported from langchain.prompts\n",
    "from langchain.chains import LLMChain  # Chains module\n",
    "from langchain.chat_models import ChatOllama  # ChatOllama is still valid\n",
    "from langchain_nomic import NomicEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.schema import Document, HumanMessage\n",
    "\n",
    "# NBA API imports\n",
    "from nba_api.stats.endpoints import playercareerstats\n",
    "from nba_api.stats.static import players\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Load documents from the web\n",
    "loader = WebBaseLoader(\"https://www.nba.com/news\")\n",
    "docs = loader.load()\n",
    "\n",
    "# Split documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "documents = text_splitter.split_documents(docs)\n",
    "\n",
    "# Initialize FAISS vector store\n",
    "vector_store = FAISS.from_documents(documents, embeddings_model)\n",
    "\n",
    "# Create a retriever\n",
    "retriever = vector_store.as_retriever(search_type=\"similarity\")\n",
    "\n",
    "# Define the prompt template\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an assistant that answers questions based on retrieved context.\"),\n",
    "    (\"user\", \"Question: {question}\\n\\nContext:\\n{context}\")\n",
    "])\n",
    "\n",
    "\n",
    "# Set up the chain\n",
    "chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "def basic_rag_example():\n",
    "    question = \"What is the main topic of the document?\"\n",
    "    context_docs = retriever.get_relevant_documents(question)\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in context_docs[:5]])\n",
    "    response = chain.run(question=question, context=context)\n",
    "    print(\"Basic RAG Response:\")\n",
    "    print(response)\n",
    "\n",
    "# Call the example function\n",
    "basic_rag_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Option 1 - Using the NBA API\n",
    "2.1. NBAAPIDataLoader Class\n",
    "\n",
    "This class fetches player career stats using the NBA API and converts the data into a list of Document objects suitable for the RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NBAAPIDataLoader:\n",
    "    def __init__(self, player_name):\n",
    "        self.player_name = player_name\n",
    "\n",
    "    def fetch_player_career_stats(self):\n",
    "        # Find the player ID based on the player name\n",
    "        player_dict = players.find_players_by_full_name(self.player_name)\n",
    "        if not player_dict:\n",
    "            print(f\"No player found with name {self.player_name}\")\n",
    "            return None\n",
    "        player_id = player_dict[0]['id']\n",
    "\n",
    "        # Fetch career stats using the player ID\n",
    "        career = playercareerstats.PlayerCareerStats(player_id=player_id)\n",
    "        career_df = career.get_data_frames()[0]\n",
    "        return career_df\n",
    "\n",
    "    def df_to_documents(self, df):\n",
    "        # Convert each row of the DataFrame into a Document\n",
    "        documents = []\n",
    "        for index, row in df.iterrows():\n",
    "            content = row.to_string()\n",
    "            doc = Document(page_content=content)\n",
    "            documents.append(doc)\n",
    "        return documents\n",
    "\n",
    "    def load_data(self):\n",
    "        # Load data and convert it into Documents\n",
    "        career_df = self.fetch_player_career_stats()\n",
    "        if career_df is None:\n",
    "            return []\n",
    "        documents = self.df_to_documents(career_df)\n",
    "        return documents\n",
    "\n",
    "\n",
    "def nba_api_example():\n",
    "    # Initialize the data loader for a specific player\n",
    "    player_name = \"LeBron James\"\n",
    "    data_loader = NBAAPIDataLoader(player_name)\n",
    "    nba_documents = data_loader.load_data()\n",
    "    if not nba_documents:\n",
    "        return\n",
    "\n",
    "    # Use a text splitter if needed\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    nba_documents = text_splitter.split_documents(nba_documents)\n",
    "    print(nba_documents)\n",
    "    # Build a vector store with the NBA documents\n",
    "    nba_vector_store = FAISS.from_documents(nba_documents, embeddings_model)\n",
    "\n",
    "    # Create a retriever\n",
    "    nba_retriever = nba_vector_store.as_retriever(search_type=\"similarity\")\n",
    "\n",
    "    # Example query\n",
    "    question = \"What is LeBron James' average points per game?\"\n",
    "    context_docs = nba_retriever.get_relevant_documents(question)\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in context_docs[:5]])\n",
    "\n",
    "    # Generate a response using the RAG pipeline\n",
    "    response = chain.run(question=question, context=context)\n",
    "    print(\"NBA API RAG Response:\")\n",
    "    print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Option 2 - Web Scraping nba.com\n",
    "3.1. NBAWebScraperDataLoader Class\n",
    "\n",
    "This class scrapes player news articles from nba.com and converts them into Document objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NBAWebScraperDataLoader:\n",
    "    def __init__(self, player_name):\n",
    "        self.player_name = player_name\n",
    "\n",
    "    def scrape_nba_player_news(self):\n",
    "        # Build the search URL for the player\n",
    "        search_query = self.player_name.replace(' ', '+')\n",
    "        url = f\"https://www.nba.com/search?query={search_query}\"\n",
    "\n",
    "        # Send a GET request to nba.com\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to retrieve data from {url}\")\n",
    "            return []\n",
    "\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Extract news articles (modify selectors based on actual HTML structure)\n",
    "        articles = soup.find_all('div', class_='ArticleItem_headline')\n",
    "        documents = []\n",
    "        for article in articles:\n",
    "            title = article.get_text(strip=True)\n",
    "            link = article.find('a')['href']\n",
    "            # Combine title and link as the content\n",
    "            content = f\"{title}\\nLink: {link}\"\n",
    "            doc = Document(page_content=content)\n",
    "            documents.append(doc)\n",
    "\n",
    "        return documents\n",
    "\n",
    "    def load_data(self):\n",
    "        # Load data and convert it into Documents\n",
    "        documents = self.scrape_nba_player_news()\n",
    "        return documents\n",
    "\n",
    "def nba_web_scraping_example():\n",
    "    # Initialize the data loader for a specific player\n",
    "    player_name = \"LeBron James\"\n",
    "    data_loader = NBAWebScraperDataLoader(player_name)\n",
    "    nba_documents = data_loader.load_data()\n",
    "    if not nba_documents:\n",
    "        return\n",
    "\n",
    "    # Use a text splitter if needed\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    nba_documents = text_splitter.split_documents(nba_documents)\n",
    "\n",
    "    # Build a vector store with the NBA documents\n",
    "    nba_vector_store = FAISS.from_documents(nba_documents, embeddings_model)\n",
    "\n",
    "    # Create a retriever\n",
    "    nba_retriever = nba_vector_store.as_retriever(search_type=\"similarity\")\n",
    "\n",
    "    # Example query\n",
    "    question = \"What is the latest news about LeBron James?\"\n",
    "    context_docs = nba_retriever.get_relevant_documents(question)\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in context_docs[:5]])\n",
    "\n",
    "    # Generate a response using the RAG pipeline\n",
    "    response = chain.run(question=question, context=context)\n",
    "    print(\"NBA Web Scraping RAG Response:\")\n",
    "    print(response)\n",
    "\n",
    "\n",
    "nba_web_scraping_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Advanced Chunking: Proposition Chunking\n",
    "\n",
    "Introduce Proposition Chunking to break documents into smaller, meaningful sentences. This enhances retrieval accuracy and allows the LLM to generate more precise responses.\n",
    "\n",
    "- Where to Implement:\n",
    "\n",
    "    Replace fixed-length chunking with proposition-based chunking, breaking text into logical sentences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pros:\n",
    "\n",
    "    Improved Precision: Better alignment between query and content.\n",
    "    Contextual Clarity: Each chunk represents a complete thought.\n",
    "\n",
    "Cons:\n",
    "\n",
    "    Increased Number of Chunks: May lead to a larger number of documents to manage.\n",
    "    Potential Performance Impact: More documents can slow down retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks after proposition chunking: 8\n"
     ]
    }
   ],
   "source": [
    "class PropositionChunking:\n",
    "    def __init__(self, content):\n",
    "        self.content = content\n",
    "\n",
    "    def chunk(self):\n",
    "        # Tokenize the content into sentences\n",
    "        sentences = nltk.sent_tokenize(self.content)\n",
    "        # Create Document objects for each sentence\n",
    "        return [Document(page_content=sentence) for sentence in sentences]\n",
    "\n",
    "# Apply Proposition Chunking to the documents\n",
    "chunked_documents = []\n",
    "for doc in docs:\n",
    "    chunker = PropositionChunking(doc.page_content)\n",
    "    chunked_documents.extend(chunker.chunk())\n",
    "\n",
    "# Re-initialize the vector store with the new chunked_documents\n",
    "vector_store = FAISS.from_documents(chunked_documents, embeddings_model)\n",
    "retriever = vector_store.as_retriever(search_type=\"similarity\")\n",
    "\n",
    "# Example usage\n",
    "def proposition_chunking_example():\n",
    "    print(\"Number of chunks after proposition chunking:\", len(chunked_documents))\n",
    "\n",
    "# Call the example function\n",
    "proposition_chunking_example()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Query Transformations: HyDE Approach\n",
    "\n",
    "Implement Hypothetical Document Embeddings (HyDE) for query transformation. This method generates hypothetical documents based on the original query to improve retrieval alignment.\n",
    "Where to Implement:\n",
    "\n",
    "    Add a query transformation step before retrieval using the LLM.\n",
    "\n",
    "Pros:\n",
    "\n",
    "    Enhanced Retrieval: Aligns queries with document embeddings.\n",
    "    Improved Context Matching: Captures nuances of the query.\n",
    "\n",
    "Cons:\n",
    "\n",
    "    Additional Computation: Requires extra LLM calls.\n",
    "    Potential Latency Increase: May affect response times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1736/2096312822.py:11: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = self.llm(messages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hypothetical Document:\n",
      "Langsmith is an open-source tool that helps with testing and debugging of Python code, particularly for large-scale applications. It provides a set of features that make it easier to write unit tests, integration tests, and end-to-end tests for your application.\n",
      "\n",
      "Here are some ways Langsmith can help with testing:\n",
      "\n",
      "1. **Automated Testing**: Langsmith allows you to write automated tests for your application using Python's built-in unittest framework. You can create test cases for individual functions or classes, and run them automatically when you make changes to the code.\n",
      "2. **Test Coverage Analysis**: Langsmith provides a test coverage analysis feature that helps you identify areas of your code that are not being tested. This is useful for ensuring that all parts of your application are covered by tests.\n",
      "3. **Code Review**: Langsmith includes a code review feature that allows you to review changes to your code before they are committed. This helps catch bugs and errors early in the development process.\n",
      "4. **Test Generation**: Langsmith can generate test cases automatically for your code, based on the input parameters and expected output. This is useful for ensuring that all possible inputs are tested.\n",
      "5. **Integration Testing**: Langsmith provides a feature for integration testing, which allows you to test how different components of your application interact with each other.\n",
      "6. **End-to-End Testing**: Langsmith also supports end-to-end testing, which involves testing the entire application from start to finish. This ensures that all parts of the application work together seamlessly.\n",
      "7. **Test Reporting**: Langsmith provides detailed reports on test results, including pass/fail rates, coverage analysis, and more. This helps you identify areas where your tests are lacking and make improvements.\n",
      "\n",
      "Overall, Langsmith is a powerful tool for testing Python code, providing a range of features that can help improve the quality and reliability of your application.\n",
      "\n",
      "Here's an example of how you might use Langsmith to write automated tests for a simple function:\n",
      "\n",
      "```python\n",
      "import unittest\n",
      "from langsmith import test\n",
      "\n",
      "class Calculator:\n",
      "    def add(self, x, y):\n",
      "        return x + y\n",
      "\n",
      "@test.test_case\n",
      "def test_add():\n",
      "    calculator = Calculator()\n",
      "    assert calculator.add(1, 2) == 3\n",
      "    assert calculator.add(-1, 1) == 0\n",
      "```\n",
      "\n",
      "In this example, we define a `Calculator` class with an `add` method. We then use the `@test.test_case` decorator to mark the `test_add` function as a test case for our application. The test case creates an instance of the `Calculator` class and calls the `add` method with different inputs, asserting that the expected output is returned.\n",
      "\n",
      "When you run this test, Langsmith will automatically execute the test case and report any failures or errors.\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import Document, HumanMessage\n",
    "\n",
    "class HyDE:\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "\n",
    "    def transform(self, original_query):\n",
    "        # Generate a hypothetical answer\n",
    "        prompt = f\"Provide a detailed answer to the following question:\\n\\n{original_query}\"\n",
    "        messages = [HumanMessage(content=prompt)]\n",
    "        response = self.llm(messages)\n",
    "        hypothetical_doc = response.content.strip() if hasattr(response, 'content') else response.strip()\n",
    "        return hypothetical_doc\n",
    "\n",
    "# Example usage\n",
    "def hyde_example():\n",
    "    hyde_transformer = HyDE(llm)\n",
    "    transformed_query_doc = hyde_transformer.transform(\"Explain how LangSmith can help with testing.\")\n",
    "    print(\"Hypothetical Document:\")\n",
    "    print(transformed_query_doc)\n",
    "    return transformed_query_doc\n",
    "\n",
    "# Call the example function\n",
    "transformed_query_doc = hyde_example()\n",
    "\n",
    "# Embed the hypothetical document\n",
    "transformed_embedding = embeddings_model.embed_query(transformed_query_doc)\n",
    "\n",
    "# Retrieve documents using the transformed embedding\n",
    "retrieved_docs = vector_store.similarity_search_by_vector(transformed_embedding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Advanced Retrieval: Multi-Chunk Segment Extraction\n",
    "\n",
    "Use Relevant Segment Extraction (RSE) to retrieve multi-chunk segments, providing better context for the LLM during generation.\n",
    "Where to Implement:\n",
    "\n",
    "    After initial retrieval, combine relevant chunks into larger segments.\n",
    "\n",
    "Pros:\n",
    "\n",
    "    Contextual Richness: Provides comprehensive information.\n",
    "    Better Answer Generation: Improves the LLM's ability to generate accurate responses.\n",
    "\n",
    "Cons:\n",
    "\n",
    "    Potential for Exceeding Context Window: Be mindful of the LLM's maximum input size.\n",
    "    Processing Overhead: Combining large texts may increase computation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelevantSegmentExtraction:\n",
    "    def __init__(self, documents):\n",
    "        self.documents = documents\n",
    "\n",
    "    def extract(self):\n",
    "        # Combine the content of the top relevant documents\n",
    "        combined_content = \" \".join([doc.page_content for doc in self.documents])\n",
    "        return combined_content\n",
    "\n",
    "# Example usage\n",
    "def relevant_segment_extraction_example():\n",
    "    # Assuming 'retrieved_docs' is obtained from previous steps\n",
    "    rse = RelevantSegmentExtraction(retrieved_docs)\n",
    "    segments = rse.extract()\n",
    "    print(\"Combined Segments:\")\n",
    "    print(segments)\n",
    "\n",
    "# Call the example function (make sure 'retrieved_docs' is defined)\n",
    "# relevant_segment_extraction_example()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Intelligent Re-ranking for Better Retrieval\n",
    "\n",
    "Use Intelligent Re-ranking to reorder retrieved documents based on their relevance, ensuring the most pertinent information is considered first.\n",
    "Where to Implement:\n",
    "\n",
    "    After retrieval, re-score documents using the LLM and re-rank them.\n",
    "\n",
    "Pros:\n",
    "\n",
    "    Increased Accuracy: Prioritizes the most relevant information.\n",
    "    Dynamic Adaptation: Adjusts to nuances in the query.\n",
    "\n",
    "Cons:\n",
    "\n",
    "    Computational Overhead: Additional LLM calls for scoring.\n",
    "    Latency: May increase total response time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Document After Re-ranking:\n",
      "Check out a day-by-day breakdown of the calendar to find out.Load moreTop StoriesHoops high school names court for Knicks' RandleEmirates NBA Cup 2024Emirates NBA Cup, Group Play: 12 matchups to watch2024 Free AgencyReports: Jamal Murray agrees to max dealPodcastsSee allQuick LinksKey DatesNBA CommunicationsAll NBA TransactionsNBA HistoryLockervisionNBA OrganizationNBA IDNBA OfficialNBA CareersNBA Fan Code of ConductNBA InitiativesNBA CaresJr.\n",
      "[Document(metadata={}, page_content=\"Check out a day-by-day breakdown of the calendar to find out.Load moreTop StoriesHoops high school names court for Knicks' RandleEmirates NBA Cup 2024Emirates NBA Cup, Group Play: 12 matchups to watch2024 Free AgencyReports: Jamal Murray agrees to max dealPodcastsSee allQuick LinksKey DatesNBA CommunicationsAll NBA TransactionsNBA HistoryLockervisionNBA OrganizationNBA IDNBA OfficialNBA CareersNBA Fan Code of ConductNBA InitiativesNBA CaresJr.\"), Document(metadata={}, page_content=\"Top 5 scoring leaders for the Atlanta HawksAtlanta Hawks' storied history has been marked by several outstanding players.NBA Jersey Day returns on Oct. 21The NBA will hold its 5th annual NBA Jersey Day on Oct. 21, encouraging fans to wear their favorite jersey.Top 5 All-Time Wizards Assist LeadersWashington's top assist leaders have played crucial roles in defining the team's offensive strategies and success historically.Film Study: 2021 Draft picks enter key seasonCade Cunningham, Jalen Green and Evan Mobley are all looking to reach a new level this season.30 Teams in 30 Days: Atlanta HawksAtlanta was active this offseason adding the No.\"), Document(metadata={}, page_content=\"NBA News - Latest team, player and league news | NBA.com\\n\\nNavigation ToggleNBAHomeTicketsKey Dates2024-25 Regular SeasonEmirates NBA Cup ScheduleLeague Pass ScheduleNational TV GamesFeaturedNBA TVHomeTop Stories30 Teams In 30 DaysFree AgencyDraftTransactionsHistoryEventsFeaturesWriter ArchiveMoreStats Home2023-24 StandingsPlayersTeamsLeadersStats 101Cume StatsLineups ToolMedia Central Game StatsDraftQuick LinksContact UsAtlanticBoston CelticsBrooklyn NetsNew York KnicksPhiladelphia 76ersToronto RaptorsCentralChicago BullsCleveland CavaliersDetroit PistonsIndiana PacersMilwaukee BucksSoutheastAtlanta HawksCharlotte HornetsMiami HeatOrlando MagicWashington WizardsNorthwestDenver NuggetsMinnesota TimberwolvesOklahoma City ThunderPortland Trail BlazersUtah JazzPacificGolden State WarriorsLA ClippersLos Angeles LakersPhoenix SunsSacramento KingsSouthwestDallas MavericksHouston RocketsMemphis GrizzliesNew Orleans PelicansSan Antonio SpursPlayers HomePlayer StatsStarting LineupsFree Agent TrackerTransactionsHomeFull Court GuessRankIQPlayer PathBlastHoop ConnectFantasy HomeFantasy NewsNBA Pick'EmPlay Yahoo FantasyPlay Sorare NBAFanDuel DFSDraft Kings DFSNBA StoreJerseysMenWomenKidsCustom ShopHardwood ClassicsHatsFootwearAuctionsNBA Game WornNYC StoreNBA Photo StoreNBATickets.comOfficial Tickets by TicketmasterNBA ExperiencesNBA G LeagueWNBANBA 2K LeagueBasketball Africa LeagueGamesScheduleNBA CupWatchNewsFree AgencyStatsTeamsPlayersNBA PlayFuture Starts NowFantasyLeague PassStoreTicketsAffiliatesNewsHomeTop Stories30 Teams In 30 DaysFree AgencyDraftTransactionsHistoryEventsHall of Fame Class of 20242024 Paris Olympics2024 Summer League2024 Playoffs2024 NBA All-StarFeaturesNBA AwardsPower RankingsRace to the MVPRookie LadderTrending TopicsHorry ScaleNBA MailbagWriter ArchiveMoreFantasyPodcastsLearn The GameNBA OfficialLatestStarting 5: Behind the scenes at Celtics Media DayThe best photos, quotes and stories from Celtics Media Day.\"), Document(metadata={}, page_content=\"Plus, a WNBA Finals rematch between the Aces and Liberty ensues.Best moments from Celtics Media DayFollow all of the news from Boston as the reigning champs tip off the 2024-25 Media Day schedule.30 Teams in 30 Days: Charlotte HornetsStocked with young players and a star guard in LaMelo Ball, Charlotte has plenty of pieces to build around.Starting 5: Celtics tip off preseason Media DaysInside the Celtics' title defense, Abu Dhabi Games and NBA preseason look ahead, plus WNBA Playoffs.NBA leaders in blocks in multiple seasonsThere's a select group of players that have managed to lead the league in blocks per game multiple times.\")]\n"
     ]
    }
   ],
   "source": [
    "class IntelligentReranking:\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "\n",
    "    def rerank(self, query, retrieved_docs):\n",
    "        scored_docs = []\n",
    "        for doc in retrieved_docs:\n",
    "            prompt = f\"On a scale of 1 to 10, how relevant is the following document to the query?\\n\\nQuery: {query}\\n\\nDocument: {doc.page_content}\\n\\nRelevance Score:\"\n",
    "            messages = [HumanMessage(content=prompt)]\n",
    "            response = self.llm(messages)\n",
    "            score_text = response.content.strip() if hasattr(response, 'content') else response.strip()\n",
    "            try:\n",
    "                score = float(score_text)\n",
    "            except ValueError:\n",
    "                score = 0  # Default to 0 if parsing fails\n",
    "            scored_docs.append((doc, score))\n",
    "        # Sort documents by score\n",
    "        reranked_docs = [doc for doc, score in sorted(scored_docs, key=lambda x: x[1], reverse=True)]\n",
    "        return reranked_docs\n",
    "\n",
    "# Intelligent Re-ranking Example\n",
    "def intelligent_reranking_example():\n",
    "    reranker = IntelligentReranking(llm)\n",
    "    question = \"Explain how LangSmith can help with testing.\"\n",
    "    reranked_docs = reranker.rerank(question, retrieved_docs)\n",
    "    top_docs = reranked_docs[:5]\n",
    "    print(\"Top Document After Re-ranking:\")\n",
    "    print(reranked_docs[0].page_content)\n",
    "    return top_docs\n",
    "\n",
    "# Call the example function and get 'top_docs'\n",
    "top_docs = intelligent_reranking_example()\n",
    "\n",
    "print(top_docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Ensemble Retrieval for Robustness\n",
    "\n",
    "Incorporate Ensemble Retrieval by combining multiple retrieval methods, such as keyword-based and vector-based retrieval, to enhance the retrieval robustness.\n",
    "Where to Implement:\n",
    "\n",
    "    After individual retrievals, combine and re-rank results.\n",
    "\n",
    "Pros:\n",
    "\n",
    "    Robustness: Captures documents that might be missed by one method.\n",
    "    Improved Recall: Increases the chance of retrieving relevant documents.\n",
    "\n",
    "Cons:\n",
    "\n",
    "    Complexity: Requires managing multiple retrieval systems.\n",
    "    Potential Redundancy: May retrieve overlapping information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Retrieval Response:\n",
      "It seems like we have a bit of repetition here!\n",
      "\n",
      "To answer your question, LangSmith is not directly related to testing. However, I can provide some general information about how data analysis and visualization tools like LangSmith (which appears to be a tool for analyzing NBA player birthdays) could be used in testing contexts.\n",
      "\n",
      "In the context of testing, data analysis and visualization tools like LangSmith could help identify patterns or trends in test results, such as:\n",
      "\n",
      "1. Identifying which players were born on specific days of the year, as you mentioned.\n",
      "2. Analyzing the distribution of birthdays across different months or seasons.\n",
      "3. Visualizing the relationship between player performance and their birthdate.\n",
      "\n",
      "These insights could be used to inform testing strategies, identify potential biases or trends in test results, or even help develop new tests that target specific characteristics of players born on certain days of the year.\n",
      "\n",
      "However, without more context about how LangSmith is being used in a testing scenario, it's difficult to provide more specific information. If you have any additional details or clarification about your question, I'd be happy to try and assist further!\n"
     ]
    }
   ],
   "source": [
    "def ensemble_retrieval_example():\n",
    "    from rank_bm25 import BM25Okapi\n",
    "\n",
    "    # Prepare documents for BM25\n",
    "    tokenized_corpus = [doc.page_content.split() for doc in chunked_documents]\n",
    "    bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "    query = \"Explain how LangSmith can help with testing.\"\n",
    "    tokenized_query = query.split()\n",
    "    bm25_scores = bm25.get_scores(tokenized_query)\n",
    "    bm25_scores = np.array(bm25_scores)\n",
    "    bm25_top_indices = bm25_scores.argsort()[-5:][::-1]\n",
    "    bm25_top_docs = [chunked_documents[i] for i in bm25_top_indices]\n",
    "\n",
    "    # Retrieve using vector store\n",
    "    vector_retrieved_docs = retriever.get_relevant_documents(query)\n",
    "\n",
    "    # Combine results without duplicates\n",
    "    def combine_documents(doc_list1, doc_list2):\n",
    "        combined_docs = []\n",
    "        seen_contents = set()\n",
    "        for doc in doc_list1 + doc_list2:\n",
    "            content = doc.page_content  # Use the content as a unique identifier\n",
    "            if content not in seen_contents:\n",
    "                seen_contents.add(content)\n",
    "                combined_docs.append(doc)\n",
    "        return combined_docs\n",
    "\n",
    "    combined_docs = combine_documents(vector_retrieved_docs, bm25_top_docs)\n",
    "\n",
    "    # Assign weights\n",
    "    vector_weight = 0.7\n",
    "    bm25_weight = 0.3\n",
    "\n",
    "    # Create a scoring function\n",
    "    combined_scores = {}\n",
    "    for doc in combined_docs:\n",
    "        content = doc.page_content  # Use document content as the key\n",
    "        vector_score = 1 if doc in vector_retrieved_docs else 0\n",
    "        bm25_score = 1 if doc in bm25_top_docs else 0\n",
    "        combined_score = vector_weight * vector_score + bm25_weight * bm25_score\n",
    "        combined_scores[content] = combined_score  # Use the content as the key\n",
    "\n",
    "    # Sort documents based on combined scores\n",
    "    ensemble_results = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_docs = [doc for doc_content, score in ensemble_results[:5]]\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in top_docs])\n",
    "\n",
    "    # Update the prompt\n",
    "    prompt_template = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are an assistant that answers questions based on retrieved context.\"),\n",
    "        (\"user\", \"Question: {question}\\n\\nContext:\\n{context}\")\n",
    "    ])\n",
    "\n",
    "    # Set up the chain\n",
    "    chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "    # Invoke the chain\n",
    "    response = chain.run(question=query, context=context)\n",
    "    print(\"Ensemble Retrieval Response:\")\n",
    "    print(response)\n",
    "\n",
    "# Call the example function\n",
    "ensemble_retrieval_example()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Explainable Retrieval for Transparency\n",
    "\n",
    "Add Explainable Retrieval to provide insights into why certain documents were retrieved, enhancing transparency and user trust.\n",
    "Where to Implement:\n",
    "\n",
    "    After retrieval and before presenting results to the user.\n",
    "\n",
    "Pros:\n",
    "\n",
    "    Transparency: Users understand the relevance of retrieved documents.\n",
    "    Trust Building: Enhances user confidence in the system.\n",
    "\n",
    "Cons:\n",
    "\n",
    "    Additional Computation: Generating explanations requires extra processing.\n",
    "    Possible Latency Increase: May affect overall response time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document: Check out a day-by-day breakdown of the calendar to find out.Load moreTop StoriesHoops high school names court for Knicks' RandleEmirates NBA Cup 2024Emirates NBA Cup, Group Play: 12 matchups to watch2024 Free AgencyReports: Jamal Murray agrees to max dealPodcastsSee allQuick LinksKey DatesNBA CommunicationsAll NBA TransactionsNBA HistoryLockervisionNBA OrganizationNBA IDNBA OfficialNBA CareersNBA Fan Code of ConductNBA InitiativesNBA CaresJr.\n",
      "Explanation: This document is relevant to the query because it provides information on LangSmith, a testing tool, which can help with testing.\n",
      "--------------------------------------------------------------------------------\n",
      "Document: Top 5 scoring leaders for the Atlanta HawksAtlanta Hawks' storied history has been marked by several outstanding players.NBA Jersey Day returns on Oct. 21The NBA will hold its 5th annual NBA Jersey Day on Oct. 21, encouraging fans to wear their favorite jersey.Top 5 All-Time Wizards Assist LeadersWashington's top assist leaders have played crucial roles in defining the team's offensive strategies and success historically.Film Study: 2021 Draft picks enter key seasonCade Cunningham, Jalen Green and Evan Mobley are all looking to reach a new level this season.30 Teams in 30 Days: Atlanta HawksAtlanta was active this offseason adding the No.\n",
      "Explanation: This document is not relevant to the query about how LangSmith can help with testing, as it appears to be unrelated to software testing or development and instead focuses on NBA teams and players.\n",
      "--------------------------------------------------------------------------------\n",
      "Document: NBA News - Latest team, player and league news | NBA.com\n",
      "\n",
      "Navigation ToggleNBAHomeTicketsKey Dates2024-25 Regular SeasonEmirates NBA Cup ScheduleLeague Pass ScheduleNational TV GamesFeaturedNBA TVHomeTop Stories30 Teams In 30 DaysFree AgencyDraftTransactionsHistoryEventsFeaturesWriter ArchiveMoreStats Home2023-24 StandingsPlayersTeamsLeadersStats 101Cume StatsLineups ToolMedia Central Game StatsDraftQuick LinksContact UsAtlanticBoston CelticsBrooklyn NetsNew York KnicksPhiladelphia 76ersToronto RaptorsCentralChicago BullsCleveland CavaliersDetroit PistonsIndiana PacersMilwaukee BucksSoutheastAtlanta HawksCharlotte HornetsMiami HeatOrlando MagicWashington WizardsNorthwestDenver NuggetsMinnesota TimberwolvesOklahoma City ThunderPortland Trail BlazersUtah JazzPacificGolden State WarriorsLA ClippersLos Angeles LakersPhoenix SunsSacramento KingsSouthwestDallas MavericksHouston RocketsMemphis GrizzliesNew Orleans PelicansSan Antonio SpursPlayers HomePlayer StatsStarting LineupsFree Agent TrackerTransactionsHomeFull Court GuessRankIQPlayer PathBlastHoop ConnectFantasy HomeFantasy NewsNBA Pick'EmPlay Yahoo FantasyPlay Sorare NBAFanDuel DFSDraft Kings DFSNBA StoreJerseysMenWomenKidsCustom ShopHardwood ClassicsHatsFootwearAuctionsNBA Game WornNYC StoreNBA Photo StoreNBATickets.comOfficial Tickets by TicketmasterNBA ExperiencesNBA G LeagueWNBANBA 2K LeagueBasketball Africa LeagueGamesScheduleNBA CupWatchNewsFree AgencyStatsTeamsPlayersNBA PlayFuture Starts NowFantasyLeague PassStoreTicketsAffiliatesNewsHomeTop Stories30 Teams In 30 DaysFree AgencyDraftTransactionsHistoryEventsHall of Fame Class of 20242024 Paris Olympics2024 Summer League2024 Playoffs2024 NBA All-StarFeaturesNBA AwardsPower RankingsRace to the MVPRookie LadderTrending TopicsHorry ScaleNBA MailbagWriter ArchiveMoreFantasyPodcastsLearn The GameNBA OfficialLatestStarting 5: Behind the scenes at Celtics Media DayThe best photos, quotes and stories from Celtics Media Day.\n",
      "Explanation: This document is relevant to the query because it provides information on various NBA teams, players, and news, which could be useful for testing LangSmith's capabilities in understanding and processing sports-related data.\n",
      "--------------------------------------------------------------------------------\n",
      "Document: Plus, a WNBA Finals rematch between the Aces and Liberty ensues.Best moments from Celtics Media DayFollow all of the news from Boston as the reigning champs tip off the 2024-25 Media Day schedule.30 Teams in 30 Days: Charlotte HornetsStocked with young players and a star guard in LaMelo Ball, Charlotte has plenty of pieces to build around.Starting 5: Celtics tip off preseason Media DaysInside the Celtics' title defense, Abu Dhabi Games and NBA preseason look ahead, plus WNBA Playoffs.NBA leaders in blocks in multiple seasonsThere's a select group of players that have managed to lead the league in blocks per game multiple times.\n",
      "Explanation: This document is relevant to the query because it mentions \"WNBA Playoffs\", which implies that LangSmith (presumably referring to Langston \"Lang\" Smith, an NBA coach) may be involved with a WNBA team, potentially providing coaching or testing services.\n",
      "--------------------------------------------------------------------------------\n",
      "Explainable Retrieval Response:\n",
      "Unfortunately, I couldn't find any information on how LangSmith can help with testing in the provided documents. However, based on my general knowledge and context, I can provide some possible explanations.\n",
      "\n",
      "LangSmith is likely a software testing tool, but without more specific information, it's difficult to say exactly what features or capabilities it offers. If LangSmith is designed for testing sports-related data, such as player statistics or game outcomes, it may be able to help with tasks like:\n",
      "\n",
      "* Data validation and verification\n",
      "* Automated testing of statistical models or algorithms\n",
      "* Integration testing with other software systems\n",
      "\n",
      "In this case, the relevant documents (e.g., NBA News - Latest team, player and league news | NBA.com) provide some context on how LangSmith might be used in a sports-related setting. For example, if LangSmith is being used to test data for a WNBA team, it may help with tasks like:\n",
      "\n",
      "* Verifying player statistics or game outcomes\n",
      "* Testing the accuracy of predictive models for game outcomes\n",
      "* Integrating with other software systems to provide real-time data analysis\n",
      "\n",
      "However, without more specific information on how LangSmith works and what features it offers, it's difficult to provide a more detailed explanation. If you have any additional context or information on LangSmith, I'd be happy to try and help further!\n"
     ]
    }
   ],
   "source": [
    "class ExplainableRetrieval:\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "\n",
    "    def retrieve_with_explanation(self, query, retrieved_docs):\n",
    "        explanations = []\n",
    "        for doc in retrieved_docs:\n",
    "            prompt = f\"Explain in one sentence why the following document is relevant to the query.\\n\\nQuery: {query}\\n\\nDocument: {doc.page_content}\\n\\nExplanation:\"\n",
    "            messages = [HumanMessage(content=prompt)]\n",
    "            response = self.llm(messages)\n",
    "            explanation = response.content.strip() if hasattr(response, 'content') else response.strip()\n",
    "            explanations.append((doc, explanation))\n",
    "        return explanations\n",
    "\n",
    "# Explainable Retrieval Example\n",
    "def explainable_retrieval_example(top_docs):\n",
    "    explainable_retriever = ExplainableRetrieval(llm)\n",
    "    question = \"Explain how LangSmith can help with testing.\"\n",
    "    explanations_and_docs = explainable_retriever.retrieve_with_explanation(question, top_docs)\n",
    "    for doc, explanation in explanations_and_docs:\n",
    "        print(f\"Document: {doc.page_content}\\nExplanation: {explanation}\\n{'-'*80}\")\n",
    "\n",
    "    context_with_explanations = \"\\n\\n\".join([\n",
    "        f\"Document: {doc.page_content}\\nExplanation: {explanation}\"\n",
    "        for doc, explanation in explanations_and_docs\n",
    "    ])\n",
    "\n",
    "    prompt_template = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are an assistant that answers questions based on retrieved context and explanations.\"),\n",
    "        (\"user\", \"Question: {question}\\n\\nContext:\\n{context_with_explanations}\")\n",
    "    ])\n",
    "\n",
    "    chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "    # Invoke the chain\n",
    "    response = chain.run(question=question, context_with_explanations=context_with_explanations)\n",
    "    print(\"Explainable Retrieval Response:\")\n",
    "    print(response)\n",
    "\n",
    "# Call the example function with 'top_docs'\n",
    "explainable_retrieval_example(top_docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Main Function Integrating All Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Answer:\n",
      "Based on the context provided, which is a research article by Lilian Weng titled \"LLM-powered Autonomous Agents\" (2023), I can tell you about some of the documents mentioned:\n",
      "\n",
      "1. **The Article**: The main document is the research article itself, written by Lilian Weng and published on her website (`lilianweng.github.io`). The article discusses the concept of autonomous agents and how they can be powered using Large Language Models (LLMs).\n",
      "\n",
      "2. **Message GPT Agent**: A specific implementation or example of an agent mentioned in the article is called \"message_agent\". This agent uses a Message GPT model, which is likely a variant of the LLM used to power the agent.\n",
      "\n",
      "3. **Reference [1]**: Although not explicitly stated, it appears that there is a reference to another research work by Wei et al., but unfortunately, the details of this reference are not provided in the context.\n",
      "\n",
      "If you'd like more information about any specific aspect of the article or the Message GPT Agent, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Load documents\n",
    "    loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "    docs = loader.load()\n",
    "\n",
    "    # Proposition Chunking\n",
    "    chunked_documents = []\n",
    "    for doc in docs:\n",
    "        chunker = PropositionChunking(doc.page_content)\n",
    "        chunked_documents.extend(chunker.chunk())\n",
    "\n",
    "    # Build vector store\n",
    "    vector_store = FAISS.from_documents(chunked_documents, embeddings_model)\n",
    "    retriever = vector_store.as_retriever(search_type=\"similarity\")\n",
    "\n",
    "    # HyDE query transformation\n",
    "    hyde_transformer = HyDE(llm)\n",
    "    original_query = \"Tell me about the documents within: 2023-06-23-agent\"\n",
    "    transformed_query_doc = hyde_transformer.transform(original_query)\n",
    "\n",
    "    # Embed the hypothetical document\n",
    "    transformed_embedding = embeddings_model.embed_query(transformed_query_doc)\n",
    "\n",
    "    # Retrieve documents using the transformed embedding\n",
    "    retrieved_docs = vector_store.similarity_search_by_vector(transformed_embedding)\n",
    "\n",
    "    # Intelligent re-ranking\n",
    "    reranker = IntelligentReranking(llm)\n",
    "    reranked_docs = reranker.rerank(original_query, retrieved_docs)\n",
    "\n",
    "    # Prepare top documents for context\n",
    "    top_docs_content = \"\\n\\n\".join([doc.page_content for doc in reranked_docs[:5]])\n",
    "\n",
    "    # Generate answer\n",
    "    prompt_template = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are an assistant that answers questions based on retrieved context.\"),\n",
    "        (\"user\", \"Question: {question}\\n\\nContext:\\n{top_docs_content}\")\n",
    "    ])\n",
    "\n",
    "    chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "    # Invoke the chain\n",
    "    response = chain.run(question=original_query, top_docs_content=top_docs_content)\n",
    "    print(\"Final Answer:\")\n",
    "    print(response)\n",
    "\n",
    "# Call the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fun Nba api example: Moved to it's own notebook to see about the success of this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Answer:\n",
      "To calculate LeBron James's average points per game, we need to add up all his points and divide by the total number of games played.\n",
      "\n",
      "From the provided data:\n",
      "\n",
      "- In the 2023-24 season: 1822 points in 71 games.\n",
      "- In the 2003-04 season: 1654 points in 79 games.\n",
      "- In the 2004-05 season: 2175 points in 80 games.\n",
      "- In the 2022-23 season: 1590 points in 55 games.\n",
      "\n",
      "Total points = 1822 + 1654 + 2175 + 1590 = 6641\n",
      "\n",
      "Total games played = 71 + 79 + 80 + 55 = 285\n",
      "\n",
      "Average points per game = Total points / Total games played\n",
      "= 6641 / 285\n",
      " 23.3\n"
     ]
    }
   ],
   "source": [
    "def integrate_data_into_pipeline(data_documents):\n",
    "    # Use Proposition Chunking to split documents into sentences\n",
    "    chunked_documents = []\n",
    "    for doc in data_documents:\n",
    "        chunker = PropositionChunking(doc.page_content)\n",
    "        chunked_documents.extend(chunker.chunk())\n",
    "\n",
    "    # Build a vector store with the chunked documents\n",
    "    vector_store = FAISS.from_documents(chunked_documents, embeddings_model)\n",
    "    retriever = vector_store.as_retriever(search_type=\"similarity\")\n",
    "\n",
    "    return retriever\n",
    "\n",
    "\n",
    "def main_with_data_source():\n",
    "    # Choose data source and player name\n",
    "    player_name = \"LeBron James\"\n",
    "    data_source = \"nba_api\"  # Options: \"nba_api\" or \"web_scraping\"\n",
    "\n",
    "    # Initialize the appropriate data loader\n",
    "    if data_source == \"nba_api\":\n",
    "        data_loader = NBAAPIDataLoader(player_name)\n",
    "    elif data_source == \"web_scraping\":\n",
    "        data_loader = NBAWebScraperDataLoader(player_name)\n",
    "    else:\n",
    "        print(\"Invalid data source selected.\")\n",
    "        return\n",
    "\n",
    "    # Load data into Documents\n",
    "    data_documents = data_loader.load_data()\n",
    "    if not data_documents:\n",
    "        print(\"No data loaded.\")\n",
    "        return\n",
    "\n",
    "    # Integrate data into the pipeline\n",
    "    retriever = integrate_data_into_pipeline(data_documents)\n",
    "\n",
    "    # Proceed with the rest of the pipeline\n",
    "    # HyDE query transformation\n",
    "    hyde_transformer = HyDE(llm)\n",
    "    original_query = f\"What is {player_name}'s average points per game?\"\n",
    "    transformed_query_doc = hyde_transformer.transform(original_query)\n",
    "\n",
    "    # Embed the hypothetical document\n",
    "    transformed_embedding = embeddings_model.embed_query(transformed_query_doc)\n",
    "\n",
    "    # Retrieve documents using the transformed embedding\n",
    "    retrieved_docs = retriever.get_relevant_documents(original_query)\n",
    "\n",
    "    # Intelligent re-ranking\n",
    "    reranker = IntelligentReranking(llm)\n",
    "    reranked_docs = reranker.rerank(original_query, retrieved_docs)\n",
    "\n",
    "    # Prepare top documents for context\n",
    "    top_docs_content = \"\\n\\n\".join([doc.page_content for doc in reranked_docs[:5]])\n",
    "\n",
    "    # Generate an answer using the RAG pipeline\n",
    "    prompt_template = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are an assistant that answers questions based on retrieved context.\"),\n",
    "        (\"user\", \"Question: {question}\\n\\nContext:\\n{top_docs_content}\")\n",
    "    ])\n",
    "\n",
    "    chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "    # Invoke the chain to get the final answer\n",
    "    response = chain.run(question=original_query, top_docs_content=top_docs_content)\n",
    "    print(\"Final Answer:\")\n",
    "    print(response)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_with_data_source()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science_ollama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
