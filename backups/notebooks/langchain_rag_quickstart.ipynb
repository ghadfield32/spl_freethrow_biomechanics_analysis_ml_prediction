{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: set up GPU, api keys, and models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.3.1+cu121\n",
      "CUDA is available! GPU is ready to be used.\n",
      "Number of GPUs available: 1\n",
      "Current GPU: NVIDIA GeForce RTX 4090\n",
      "GPU 0: NVIDIA GeForce RTX 4090\n",
      "  - Total Memory: 25.756696576 GB\n",
      "  - Compute Capability: (8, 9)\n",
      "Tensor on GPU: tensor([[0.7068, 0.6939, 0.9230],\n",
      "        [0.2045, 0.8537, 0.6757],\n",
      "        [0.7674, 0.0699, 0.5267]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Print the PyTorch version\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Check if CUDA (GPU support) is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available! GPU is ready to be used.\")\n",
    "    print(f\"Number of GPUs available: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current GPU: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
    "else:\n",
    "    print(\"CUDA is not available. GPU is not set up correctly.\")\n",
    "\n",
    "# Print additional GPU details\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"  - Total Memory: {torch.cuda.get_device_properties(i).total_memory / 1e9} GB\")\n",
    "        print(f\"  - Compute Capability: {torch.cuda.get_device_capability(i)}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # Create a random tensor and move it to the GPU\n",
    "    tensor = torch.rand(3, 3).cuda()\n",
    "    print(\"Tensor on GPU:\", tensor)\n",
    "else:\n",
    "    print(\"GPU is not available, cannot move tensor to GPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspaces/custom_ollama_docker/notebooks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Print the current working directory (optional for debugging)\n",
    "print(os.getcwd())\n",
    "\n",
    "# Set the path to your .env file relative to the current working directory\n",
    "dotenv_path = os.path.join(os.getcwd(), '../.env')\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "# Load the API keys from environment variables\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "hf_token = os.getenv(\"HUGGINGFACE_API_KEY\")\n",
    "anthropic_token = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "tavily_token = os.getenv(\"TAVILY_API_KEY\")\n",
    "langsmith_token = os.getenv(\"LANGSMITH_API_KEY\")\n",
    "# NOMIC_EMBEDDINGS_API_KEY\n",
    "nomic_token = os.getenv(\"NOMIC_EMBEDDINGS_API_KEY\")\n",
    "\n",
    "# Set the Hugging Face token as an environment variable (if not already done)\n",
    "if hf_token:\n",
    "    os.environ[\"HUGGINGFACE_API_KEY\"] = hf_token\n",
    "if openai_api_key:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "if anthropic_token:\n",
    "    os.environ[\"ANTHROPIC_API_KEY\"] = anthropic_token\n",
    "if tavily_token:\n",
    "    os.environ[\"TAVILY_API_KEY\"] = tavily_token\n",
    "if tavily_token:\n",
    "    os.environ[\"LANGSMITH_API_KEY\"] = langsmith_token\n",
    "if nomic_token:\n",
    "    os.environ[\"NOMIC_EMBEDDINGS_API_KEY\"] = nomic_token\n",
    "    \n",
    "    \n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pull in Ollama local model for free use throughout RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "local_llm = \"llama3.1\"\n",
    "\n",
    "llm = ChatOllama(model=local_llm, temperature=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding with Ollama Models\n",
    "\n",
    "Use Ollama or compatible models for generating embeddings, ensuring control over the embedding process and consistency within the pipeline.\n",
    "Action:\n",
    "\n",
    "    Replace external embedding generation with local embeddings using NomicEmbeddings.\n",
    "\n",
    "Pros:\n",
    "\n",
    "    Efficient Retrieval: Embeddings generated locally.\n",
    "    Unified Pipeline: Consistency in using local models.\n",
    "\n",
    "Cons:\n",
    "\n",
    "    Embedding Quality: May not match commercial APIs without fine-tuning.\n",
    "    Resource Requirements: Ensure compatibility with your hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nomic import NomicEmbeddings\n",
    "\n",
    "embeddings_model = NomicEmbeddings(\n",
    "    model=\"nomic-embed-text-v1.5\",\n",
    "    nomic_api_key=nomic_token\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Basic RAG Pipeline Setup with Ollama\n",
    "\n",
    "Set up the basic Retrieval-Augmented Generation (RAG) pipeline using Ollama. This involves retrieving relevant documents, feeding them to the LLM (Ollama), and generating responses based on the context.\n",
    "Steps:\n",
    "\n",
    "    Document Retrieval:\n",
    "        Use a vector store like FAISS to store and retrieve embeddings.\n",
    "        Use a text splitter to divide documents into manageable chunks.\n",
    "\n",
    "    LLM Integration:\n",
    "        Replace any existing LLMs with ChatOllama.\n",
    "\n",
    "    Prompt & Template:\n",
    "        Customize prompts to leverage Ollama's capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic RAG Response:\n",
      "Based on the retrieved context, the main topic of the document is writing a Super Mario game in Python using MVC components split in separate files with keyboard control.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Import necessary modules\n",
    "import os\n",
    "import torch\n",
    "import nltk\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "import numpy as np\n",
    "\n",
    "# Updated langchain imports based on changes in recent versions\n",
    "from langchain.vectorstores import FAISS  # FAISS is now part of langchain.vectorstores\n",
    "from langchain.prompts import ChatPromptTemplate  # ChatPromptTemplate should now be imported from langchain.prompts\n",
    "from langchain.chains import LLMChain  # Chains module\n",
    "from langchain.chat_models import ChatOllama  # ChatOllama is still valid\n",
    "from langchain_nomic import NomicEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.schema import Document, HumanMessage\n",
    "\n",
    "# NBA API imports\n",
    "from nba_api.stats.endpoints import playercareerstats\n",
    "from nba_api.stats.static import players\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load documents from the web\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "docs = loader.load()\n",
    "\n",
    "# Split documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "documents = text_splitter.split_documents(docs)\n",
    "\n",
    "# Initialize FAISS vector store\n",
    "vector_store = FAISS.from_documents(documents, embeddings_model)\n",
    "\n",
    "# Create a retriever\n",
    "retriever = vector_store.as_retriever(search_type=\"similarity\")\n",
    "\n",
    "# Define the prompt template\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an assistant that answers questions based on retrieved context.\"),\n",
    "    (\"user\", \"Question: {question}\\n\\nContext:\\n{context}\")\n",
    "])\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = ChatOllama(model=\"llama3.1\", temperature=0.7)\n",
    "\n",
    "# Set up the chain\n",
    "chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "def basic_rag_example():\n",
    "    question = \"What is the main topic of the document?\"\n",
    "    context_docs = retriever.get_relevant_documents(question)\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in context_docs[:5]])\n",
    "    response = chain.run(question=question, context=context)\n",
    "    print(\"Basic RAG Response:\")\n",
    "    print(response)\n",
    "\n",
    "# Call the example function\n",
    "basic_rag_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Option 1 - Using the NBA API\n",
    "2.1. NBAAPIDataLoader Class\n",
    "\n",
    "This class fetches player career stats using the NBA API and converts the data into a list of Document objects suitable for the RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NBAAPIDataLoader:\n",
    "    def __init__(self, player_name):\n",
    "        self.player_name = player_name\n",
    "\n",
    "    def fetch_player_career_stats(self):\n",
    "        # Find the player ID based on the player name\n",
    "        player_dict = players.find_players_by_full_name(self.player_name)\n",
    "        if not player_dict:\n",
    "            print(f\"No player found with name {self.player_name}\")\n",
    "            return None\n",
    "        player_id = player_dict[0]['id']\n",
    "\n",
    "        # Fetch career stats using the player ID\n",
    "        career = playercareerstats.PlayerCareerStats(player_id=player_id)\n",
    "        career_df = career.get_data_frames()[0]\n",
    "        return career_df\n",
    "\n",
    "    def df_to_documents(self, df):\n",
    "        # Convert each row of the DataFrame into a Document\n",
    "        documents = []\n",
    "        for index, row in df.iterrows():\n",
    "            content = row.to_string()\n",
    "            doc = Document(page_content=content)\n",
    "            documents.append(doc)\n",
    "        return documents\n",
    "\n",
    "    def load_data(self):\n",
    "        # Load data and convert it into Documents\n",
    "        career_df = self.fetch_player_career_stats()\n",
    "        if career_df is None:\n",
    "            return []\n",
    "        documents = self.df_to_documents(career_df)\n",
    "        return documents\n",
    "\n",
    "\n",
    "def nba_api_example():\n",
    "    # Initialize the data loader for a specific player\n",
    "    player_name = \"LeBron James\"\n",
    "    data_loader = NBAAPIDataLoader(player_name)\n",
    "    nba_documents = data_loader.load_data()\n",
    "    if not nba_documents:\n",
    "        return\n",
    "\n",
    "    # Use a text splitter if needed\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    nba_documents = text_splitter.split_documents(nba_documents)\n",
    "\n",
    "    # Build a vector store with the NBA documents\n",
    "    nba_vector_store = FAISS.from_documents(nba_documents, embeddings_model)\n",
    "\n",
    "    # Create a retriever\n",
    "    nba_retriever = nba_vector_store.as_retriever(search_type=\"similarity\")\n",
    "\n",
    "    # Example query\n",
    "    question = \"What is LeBron James' average points per game?\"\n",
    "    context_docs = nba_retriever.get_relevant_documents(question)\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in context_docs[:5]])\n",
    "\n",
    "    # Generate a response using the RAG pipeline\n",
    "    response = chain.run(question=question, context=context)\n",
    "    print(\"NBA API RAG Response:\")\n",
    "    print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Option 2 - Web Scraping nba.com\n",
    "3.1. NBAWebScraperDataLoader Class\n",
    "\n",
    "This class scrapes player news articles from nba.com and converts them into Document objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NBAWebScraperDataLoader:\n",
    "    def __init__(self, player_name):\n",
    "        self.player_name = player_name\n",
    "\n",
    "    def scrape_nba_player_news(self):\n",
    "        # Build the search URL for the player\n",
    "        search_query = self.player_name.replace(' ', '+')\n",
    "        url = f\"https://www.nba.com/search?query={search_query}\"\n",
    "\n",
    "        # Send a GET request to nba.com\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to retrieve data from {url}\")\n",
    "            return []\n",
    "\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Extract news articles (modify selectors based on actual HTML structure)\n",
    "        articles = soup.find_all('div', class_='ArticleItem_headline')\n",
    "        documents = []\n",
    "        for article in articles:\n",
    "            title = article.get_text(strip=True)\n",
    "            link = article.find('a')['href']\n",
    "            # Combine title and link as the content\n",
    "            content = f\"{title}\\nLink: {link}\"\n",
    "            doc = Document(page_content=content)\n",
    "            documents.append(doc)\n",
    "\n",
    "        return documents\n",
    "\n",
    "    def load_data(self):\n",
    "        # Load data and convert it into Documents\n",
    "        documents = self.scrape_nba_player_news()\n",
    "        return documents\n",
    "\n",
    "def nba_web_scraping_example():\n",
    "    # Initialize the data loader for a specific player\n",
    "    player_name = \"LeBron James\"\n",
    "    data_loader = NBAWebScraperDataLoader(player_name)\n",
    "    nba_documents = data_loader.load_data()\n",
    "    if not nba_documents:\n",
    "        return\n",
    "\n",
    "    # Use a text splitter if needed\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    nba_documents = text_splitter.split_documents(nba_documents)\n",
    "\n",
    "    # Build a vector store with the NBA documents\n",
    "    nba_vector_store = FAISS.from_documents(nba_documents, embeddings_model)\n",
    "\n",
    "    # Create a retriever\n",
    "    nba_retriever = nba_vector_store.as_retriever(search_type=\"similarity\")\n",
    "\n",
    "    # Example query\n",
    "    question = \"What is the latest news about LeBron James?\"\n",
    "    context_docs = nba_retriever.get_relevant_documents(question)\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in context_docs[:5]])\n",
    "\n",
    "    # Generate a response using the RAG pipeline\n",
    "    response = chain.run(question=question, context=context)\n",
    "    print(\"NBA Web Scraping RAG Response:\")\n",
    "    print(response)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Advanced Chunking: Proposition Chunking\n",
    "\n",
    "Introduce Proposition Chunking to break documents into smaller, meaningful sentences. This enhances retrieval accuracy and allows the LLM to generate more precise responses.\n",
    "\n",
    "- Where to Implement:\n",
    "\n",
    "    Replace fixed-length chunking with proposition-based chunking, breaking text into logical sentences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pros:\n",
    "\n",
    "    Improved Precision: Better alignment between query and content.\n",
    "    Contextual Clarity: Each chunk represents a complete thought.\n",
    "\n",
    "Cons:\n",
    "\n",
    "    Increased Number of Chunks: May lead to a larger number of documents to manage.\n",
    "    Potential Performance Impact: More documents can slow down retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks after proposition chunking: 426\n"
     ]
    }
   ],
   "source": [
    "class PropositionChunking:\n",
    "    def __init__(self, content):\n",
    "        self.content = content\n",
    "\n",
    "    def chunk(self):\n",
    "        # Tokenize the content into sentences\n",
    "        sentences = nltk.sent_tokenize(self.content)\n",
    "        # Create Document objects for each sentence\n",
    "        return [Document(page_content=sentence) for sentence in sentences]\n",
    "\n",
    "# Apply Proposition Chunking to the documents\n",
    "chunked_documents = []\n",
    "for doc in docs:\n",
    "    chunker = PropositionChunking(doc.page_content)\n",
    "    chunked_documents.extend(chunker.chunk())\n",
    "\n",
    "# Re-initialize the vector store with the new chunked_documents\n",
    "vector_store = FAISS.from_documents(chunked_documents, embeddings_model)\n",
    "retriever = vector_store.as_retriever(search_type=\"similarity\")\n",
    "\n",
    "# Example usage\n",
    "def proposition_chunking_example():\n",
    "    print(\"Number of chunks after proposition chunking:\", len(chunked_documents))\n",
    "\n",
    "# Call the example function\n",
    "proposition_chunking_example()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Query Transformations: HyDE Approach\n",
    "\n",
    "Implement Hypothetical Document Embeddings (HyDE) for query transformation. This method generates hypothetical documents based on the original query to improve retrieval alignment.\n",
    "Where to Implement:\n",
    "\n",
    "    Add a query transformation step before retrieval using the LLM.\n",
    "\n",
    "Pros:\n",
    "\n",
    "    Enhanced Retrieval: Aligns queries with document embeddings.\n",
    "    Improved Context Matching: Captures nuances of the query.\n",
    "\n",
    "Cons:\n",
    "\n",
    "    Additional Computation: Requires extra LLM calls.\n",
    "    Potential Latency Increase: May affect response times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hypothetical Document:\n",
      "I couldn't find any information on \"LangSmith\". It's possible that it's a fictional or non-existent tool, or perhaps a lesser-known or emerging solution in the field of software development and testing.\n",
      "\n",
      "However, I can provide a general answer to how tools like LangSmith (if it exists) could potentially help with testing:\n",
      "\n",
      "**Assuming LangSmith is a software testing tool**\n",
      "\n",
      "In today's fast-paced software development landscape, ensuring the quality and reliability of software applications is more crucial than ever. Testing plays a vital role in this process, helping developers identify bugs, ensure compatibility, and guarantee that their product meets user expectations.\n",
      "\n",
      "If LangSmith were a real-world solution, it could potentially offer various features to facilitate testing processes, such as:\n",
      "\n",
      "1. **Automated Testing**: LangSmith might provide an automated testing framework that enables developers to write tests for their code using a specific programming language or domain-specific language (DSL). This would allow them to focus on writing code rather than manually creating test cases.\n",
      "2. **Code Analysis**: The tool could include advanced code analysis capabilities, enabling developers to identify potential issues, such as security vulnerabilities, performance bottlenecks, and coding best practices violations.\n",
      "3. **Test Data Generation**: LangSmith might offer a built-in feature for generating test data, which would save time and effort when creating scenarios for testing various software components.\n",
      "4. **Collaborative Testing**: The tool could facilitate collaboration among developers by allowing them to share test cases, discuss test results, and work together on improving the overall testing process.\n",
      "5. **Continuous Integration/Continuous Deployment (CI/CD)**: LangSmith might integrate with popular CI/CD pipelines, enabling seamless integration with existing workflows and automating the testing process for each code commit.\n",
      "6. **Reporting and Analytics**: The tool could provide detailed reports on test results, highlighting areas that require improvement and offering insights into the overall quality of the software application.\n",
      "7. **Integration with Agile Methodologies**: LangSmith might be designed to work seamlessly with agile methodologies like Scrum or Kanban, enabling developers to incorporate testing into their development process and ensure that testing is an integral part of each sprint.\n",
      "\n",
      "Keep in mind that this is a hypothetical explanation, as I couldn't find any information on a tool called \"LangSmith.\" If you have more context or details about LangSmith, please let me know, and I'll be happy to provide a more accurate answer.\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import Document, HumanMessage\n",
    "\n",
    "class HyDE:\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "\n",
    "    def transform(self, original_query):\n",
    "        # Generate a hypothetical answer\n",
    "        prompt = f\"Provide a detailed answer to the following question:\\n\\n{original_query}\"\n",
    "        messages = [HumanMessage(content=prompt)]\n",
    "        response = self.llm(messages)\n",
    "        hypothetical_doc = response.content.strip() if hasattr(response, 'content') else response.strip()\n",
    "        return hypothetical_doc\n",
    "\n",
    "# Example usage\n",
    "def hyde_example():\n",
    "    hyde_transformer = HyDE(llm)\n",
    "    transformed_query_doc = hyde_transformer.transform(\"Explain how LangSmith can help with testing.\")\n",
    "    print(\"Hypothetical Document:\")\n",
    "    print(transformed_query_doc)\n",
    "    return transformed_query_doc\n",
    "\n",
    "# Call the example function\n",
    "transformed_query_doc = hyde_example()\n",
    "\n",
    "# Embed the hypothetical document\n",
    "transformed_embedding = embeddings_model.embed_query(transformed_query_doc)\n",
    "\n",
    "# Retrieve documents using the transformed embedding\n",
    "retrieved_docs = vector_store.similarity_search_by_vector(transformed_embedding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Advanced Retrieval: Multi-Chunk Segment Extraction\n",
    "\n",
    "Use Relevant Segment Extraction (RSE) to retrieve multi-chunk segments, providing better context for the LLM during generation.\n",
    "Where to Implement:\n",
    "\n",
    "    After initial retrieval, combine relevant chunks into larger segments.\n",
    "\n",
    "Pros:\n",
    "\n",
    "    Contextual Richness: Provides comprehensive information.\n",
    "    Better Answer Generation: Improves the LLM's ability to generate accurate responses.\n",
    "\n",
    "Cons:\n",
    "\n",
    "    Potential for Exceeding Context Window: Be mindful of the LLM's maximum input size.\n",
    "    Processing Overhead: Combining large texts may increase computation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelevantSegmentExtraction:\n",
    "    def __init__(self, documents):\n",
    "        self.documents = documents\n",
    "\n",
    "    def extract(self):\n",
    "        # Combine the content of the top relevant documents\n",
    "        combined_content = \" \".join([doc.page_content for doc in self.documents])\n",
    "        return combined_content\n",
    "\n",
    "# Example usage\n",
    "def relevant_segment_extraction_example():\n",
    "    # Assuming 'retrieved_docs' is obtained from previous steps\n",
    "    rse = RelevantSegmentExtraction(retrieved_docs)\n",
    "    segments = rse.extract()\n",
    "    print(\"Combined Segments:\")\n",
    "    print(segments)\n",
    "\n",
    "# Call the example function (make sure 'retrieved_docs' is defined)\n",
    "# relevant_segment_extraction_example()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Intelligent Re-ranking for Better Retrieval\n",
    "\n",
    "Use Intelligent Re-ranking to reorder retrieved documents based on their relevance, ensuring the most pertinent information is considered first.\n",
    "Where to Implement:\n",
    "\n",
    "    After retrieval, re-score documents using the LLM and re-rank them.\n",
    "\n",
    "Pros:\n",
    "\n",
    "    Increased Accuracy: Prioritizes the most relevant information.\n",
    "    Dynamic Adaptation: Adjusts to nuances in the query.\n",
    "\n",
    "Cons:\n",
    "\n",
    "    Computational Overhead: Additional LLM calls for scoring.\n",
    "    Latency: May increase total response time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Document After Re-ranking:\n",
      "The results highlight when the external symbolic tools can work reliably, knowing when to and how to use the tools are crucial, determined by the LLM capability.\n",
      "[Document(metadata={}, page_content='The results highlight when the external symbolic tools can work reliably, knowing when to and how to use the tools are crucial, determined by the LLM capability.'), Document(metadata={}, page_content='ChatGPT Plugins and OpenAI API  function calling are good examples of LLMs augmented with tool use capability working in practice.'), Document(metadata={}, page_content='The workflow, implemented in LangChain, reflects what was previously described in the ReAct and MRKLs and combines CoT reasoning with tools relevant to the tasks:\\n\\nThe LLM is provided with a list of tool names, descriptions of their utility, and details about the expected input/output.'), Document(metadata={}, page_content='This agent can use tools to browse the Internet, read documentation, execute code, call robotics experimentation APIs and leverage other LLMs.')]\n"
     ]
    }
   ],
   "source": [
    "class IntelligentReranking:\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "\n",
    "    def rerank(self, query, retrieved_docs):\n",
    "        scored_docs = []\n",
    "        for doc in retrieved_docs:\n",
    "            prompt = f\"On a scale of 1 to 10, how relevant is the following document to the query?\\n\\nQuery: {query}\\n\\nDocument: {doc.page_content}\\n\\nRelevance Score:\"\n",
    "            messages = [HumanMessage(content=prompt)]\n",
    "            response = self.llm(messages)\n",
    "            score_text = response.content.strip() if hasattr(response, 'content') else response.strip()\n",
    "            try:\n",
    "                score = float(score_text)\n",
    "            except ValueError:\n",
    "                score = 0  # Default to 0 if parsing fails\n",
    "            scored_docs.append((doc, score))\n",
    "        # Sort documents by score\n",
    "        reranked_docs = [doc for doc, score in sorted(scored_docs, key=lambda x: x[1], reverse=True)]\n",
    "        return reranked_docs\n",
    "\n",
    "# Intelligent Re-ranking Example\n",
    "def intelligent_reranking_example():\n",
    "    reranker = IntelligentReranking(llm)\n",
    "    question = \"Explain how LangSmith can help with testing.\"\n",
    "    reranked_docs = reranker.rerank(question, retrieved_docs)\n",
    "    top_docs = reranked_docs[:5]\n",
    "    print(\"Top Document After Re-ranking:\")\n",
    "    print(reranked_docs[0].page_content)\n",
    "    return top_docs\n",
    "\n",
    "# Call the example function and get 'top_docs'\n",
    "top_docs = intelligent_reranking_example()\n",
    "\n",
    "print(top_docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Ensemble Retrieval for Robustness\n",
    "\n",
    "Incorporate Ensemble Retrieval by combining multiple retrieval methods, such as keyword-based and vector-based retrieval, to enhance the retrieval robustness.\n",
    "Where to Implement:\n",
    "\n",
    "    After individual retrievals, combine and re-rank results.\n",
    "\n",
    "Pros:\n",
    "\n",
    "    Robustness: Captures documents that might be missed by one method.\n",
    "    Improved Recall: Increases the chance of retrieving relevant documents.\n",
    "\n",
    "Cons:\n",
    "\n",
    "    Complexity: Requires managing multiple retrieval systems.\n",
    "    Potential Redundancy: May retrieve overlapping information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Retrieval Response:\n",
      "Based on the provided context, it appears that we're discussing a topic unrelated to LangSmith or testing. However, I'm assuming that you'd like me to retrieve context about something called \"LangSmith\" and provide information on how it can help with testing.\n",
      "\n",
      "Unfortunately, there's no relevant context provided about LangSmith. Nevertheless, if I were to make an educated guess, I might say that LangSmith could be a tool or service that assists with automated testing or quality assurance processes.\n",
      "\n",
      "In the absence of concrete information, here are some hypothetical ways LangSmith might aid in testing:\n",
      "\n",
      "1. **Automated Testing**: LangSmith could potentially help automate various types of tests, such as unit tests, integration tests, or end-to-end tests.\n",
      "2. **Test Case Management**: It may enable users to manage and organize test cases more efficiently, making it easier to maintain a comprehensive set of tests.\n",
      "3. **Code Review and Analysis**: This tool might perform code reviews and provide insights into potential issues that could impact the testing process.\n",
      "4. **Performance Optimization**: LangSmith could assist in identifying performance bottlenecks within an application, which can inform test planning and execution.\n",
      "\n",
      "Please note that these are speculative suggestions based on no concrete information about LangSmith. If you have specific details or context about this topic, I'd be happy to provide a more accurate and relevant response!\n"
     ]
    }
   ],
   "source": [
    "def ensemble_retrieval_example():\n",
    "    from rank_bm25 import BM25Okapi\n",
    "\n",
    "    # Prepare documents for BM25\n",
    "    tokenized_corpus = [doc.page_content.split() for doc in chunked_documents]\n",
    "    bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "    query = \"Explain how LangSmith can help with testing.\"\n",
    "    tokenized_query = query.split()\n",
    "    bm25_scores = bm25.get_scores(tokenized_query)\n",
    "    bm25_scores = np.array(bm25_scores)\n",
    "    bm25_top_indices = bm25_scores.argsort()[-5:][::-1]\n",
    "    bm25_top_docs = [chunked_documents[i] for i in bm25_top_indices]\n",
    "\n",
    "    # Retrieve using vector store\n",
    "    vector_retrieved_docs = retriever.get_relevant_documents(query)\n",
    "\n",
    "    # Combine results without duplicates\n",
    "    def combine_documents(doc_list1, doc_list2):\n",
    "        combined_docs = []\n",
    "        seen_contents = set()\n",
    "        for doc in doc_list1 + doc_list2:\n",
    "            content = doc.page_content  # Use the content as a unique identifier\n",
    "            if content not in seen_contents:\n",
    "                seen_contents.add(content)\n",
    "                combined_docs.append(doc)\n",
    "        return combined_docs\n",
    "\n",
    "    combined_docs = combine_documents(vector_retrieved_docs, bm25_top_docs)\n",
    "\n",
    "    # Assign weights\n",
    "    vector_weight = 0.7\n",
    "    bm25_weight = 0.3\n",
    "\n",
    "    # Create a scoring function\n",
    "    combined_scores = {}\n",
    "    for doc in combined_docs:\n",
    "        content = doc.page_content  # Use document content as the key\n",
    "        vector_score = 1 if doc in vector_retrieved_docs else 0\n",
    "        bm25_score = 1 if doc in bm25_top_docs else 0\n",
    "        combined_score = vector_weight * vector_score + bm25_weight * bm25_score\n",
    "        combined_scores[content] = combined_score  # Use the content as the key\n",
    "\n",
    "    # Sort documents based on combined scores\n",
    "    ensemble_results = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_docs = [doc for doc_content, score in ensemble_results[:5]]\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in top_docs])\n",
    "\n",
    "    # Update the prompt\n",
    "    prompt_template = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are an assistant that answers questions based on retrieved context.\"),\n",
    "        (\"user\", \"Question: {question}\\n\\nContext:\\n{context}\")\n",
    "    ])\n",
    "\n",
    "    # Set up the chain\n",
    "    chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "    # Invoke the chain\n",
    "    response = chain.run(question=query, context=context)\n",
    "    print(\"Ensemble Retrieval Response:\")\n",
    "    print(response)\n",
    "\n",
    "# Call the example function\n",
    "ensemble_retrieval_example()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Explainable Retrieval for Transparency\n",
    "\n",
    "Add Explainable Retrieval to provide insights into why certain documents were retrieved, enhancing transparency and user trust.\n",
    "Where to Implement:\n",
    "\n",
    "    After retrieval and before presenting results to the user.\n",
    "\n",
    "Pros:\n",
    "\n",
    "    Transparency: Users understand the relevance of retrieved documents.\n",
    "    Trust Building: Enhances user confidence in the system.\n",
    "\n",
    "Cons:\n",
    "\n",
    "    Additional Computation: Generating explanations requires extra processing.\n",
    "    Possible Latency Increase: May affect overall response time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document: The results highlight when the external symbolic tools can work reliably, knowing when to and how to use the tools are crucial, determined by the LLM capability.\n",
      "Explanation: The document is relevant to the query because it provides information on how to utilize external symbolic tools effectively for testing, which is a key aspect of LangSmith's capabilities.\n",
      "--------------------------------------------------------------------------------\n",
      "Document: ChatGPT Plugins and OpenAI API  function calling are good examples of LLMs augmented with tool use capability working in practice.\n",
      "Explanation: This document is relevant because it provides an example of how AI models, like LangSmith, can be used to augment human capabilities, specifically in the context of testing.\n",
      "--------------------------------------------------------------------------------\n",
      "Document: The workflow, implemented in LangChain, reflects what was previously described in the ReAct and MRKLs and combines CoT reasoning with tools relevant to the tasks:\n",
      "\n",
      "The LLM is provided with a list of tool names, descriptions of their utility, and details about the expected input/output.\n",
      "Explanation: This document is relevant to the query because it describes how LangChain, which includes tools like LangSmith, can be used to provide information for testing and evaluation purposes.\n",
      "--------------------------------------------------------------------------------\n",
      "Document: This agent can use tools to browse the Internet, read documentation, execute code, call robotics experimentation APIs and leverage other LLMs.\n",
      "Explanation: The document is relevant to the query because it describes capabilities that could be used by LangSmith for automated testing purposes.\n",
      "--------------------------------------------------------------------------------\n",
      "Explainable Retrieval Response:\n",
      "Based on the retrieved context and explanations, I can explain how LangSmith can help with testing.\n",
      "\n",
      "LangSmith can assist with testing in several ways:\n",
      "\n",
      "1. **Utilizing external symbolic tools effectively**: According to the first document, LangSmith can work reliably with external symbolic tools, knowing when and how to use them is crucial, and determined by LLM (Large Language Model) capability. This suggests that LangSmith can leverage these tools to perform various testing tasks.\n",
      "2. **Augmenting human capabilities**: As mentioned in the second document, ChatGPT Plugins and OpenAI API function calling are good examples of LLMs augmented with tool use capability working in practice. This implies that LangSmith can be used to augment human capabilities in testing, potentially making the process more efficient and effective.\n",
      "3. **Providing information for testing and evaluation**: The third document describes how LangChain, which includes tools like LangSmith, can provide a list of tool names, descriptions of their utility, and details about expected input/output. This suggests that LangSmith can help with testing by providing necessary information and resources.\n",
      "4. **Automated testing capabilities**: The fourth document states that this agent (presumably LangSmith) can use various tools to browse the Internet, read documentation, execute code, call APIs, and leverage other LLMs. This indicates that LangSmith has automated testing capabilities, which can be used to perform tasks such as test case execution, test data generation, and test result analysis.\n",
      "\n",
      "In summary, LangSmith can help with testing by utilizing external symbolic tools effectively, augmenting human capabilities, providing information for testing and evaluation, and performing automated testing tasks.\n"
     ]
    }
   ],
   "source": [
    "class ExplainableRetrieval:\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "\n",
    "    def retrieve_with_explanation(self, query, retrieved_docs):\n",
    "        explanations = []\n",
    "        for doc in retrieved_docs:\n",
    "            prompt = f\"Explain in one sentence why the following document is relevant to the query.\\n\\nQuery: {query}\\n\\nDocument: {doc.page_content}\\n\\nExplanation:\"\n",
    "            messages = [HumanMessage(content=prompt)]\n",
    "            response = self.llm(messages)\n",
    "            explanation = response.content.strip() if hasattr(response, 'content') else response.strip()\n",
    "            explanations.append((doc, explanation))\n",
    "        return explanations\n",
    "\n",
    "# Explainable Retrieval Example\n",
    "def explainable_retrieval_example(top_docs):\n",
    "    explainable_retriever = ExplainableRetrieval(llm)\n",
    "    question = \"Explain how LangSmith can help with testing.\"\n",
    "    explanations_and_docs = explainable_retriever.retrieve_with_explanation(question, top_docs)\n",
    "    for doc, explanation in explanations_and_docs:\n",
    "        print(f\"Document: {doc.page_content}\\nExplanation: {explanation}\\n{'-'*80}\")\n",
    "\n",
    "    context_with_explanations = \"\\n\\n\".join([\n",
    "        f\"Document: {doc.page_content}\\nExplanation: {explanation}\"\n",
    "        for doc, explanation in explanations_and_docs\n",
    "    ])\n",
    "\n",
    "    prompt_template = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are an assistant that answers questions based on retrieved context and explanations.\"),\n",
    "        (\"user\", \"Question: {question}\\n\\nContext:\\n{context_with_explanations}\")\n",
    "    ])\n",
    "\n",
    "    chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "    # Invoke the chain\n",
    "    response = chain.run(question=question, context_with_explanations=context_with_explanations)\n",
    "    print(\"Explainable Retrieval Response:\")\n",
    "    print(response)\n",
    "\n",
    "# Call the example function with 'top_docs'\n",
    "explainable_retrieval_example(top_docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Main Function Integrating All Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Answer:\n",
      "Based on the provided context, I've retrieved some relevant information about documents within `2023-06-23-agent`.\n",
      "\n",
      "It seems like there's a focus on organizing and managing files across different classes or projects. Some key points to take away are:\n",
      "\n",
      "* Different classes should be stored in separate files (point 13).\n",
      "* Important information should be saved immediately to files due to short-term memory limitations.\n",
      "* The code within these files should be compatible with each other.\n",
      "\n",
      "However, I couldn't find any specific documents or files mentioned within `2023-06-23-agent`. If you'd like to provide more context or details about what's inside this folder/file, I'll do my best to assist further!\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Load documents\n",
    "    loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "    docs = loader.load()\n",
    "\n",
    "    # Proposition Chunking\n",
    "    chunked_documents = []\n",
    "    for doc in docs:\n",
    "        chunker = PropositionChunking(doc.page_content)\n",
    "        chunked_documents.extend(chunker.chunk())\n",
    "\n",
    "    # Build vector store\n",
    "    vector_store = FAISS.from_documents(chunked_documents, embeddings_model)\n",
    "    retriever = vector_store.as_retriever(search_type=\"similarity\")\n",
    "\n",
    "    # HyDE query transformation\n",
    "    hyde_transformer = HyDE(llm)\n",
    "    original_query = \"Tell me about the documents within: 2023-06-23-agent\"\n",
    "    transformed_query_doc = hyde_transformer.transform(original_query)\n",
    "\n",
    "    # Embed the hypothetical document\n",
    "    transformed_embedding = embeddings_model.embed_query(transformed_query_doc)\n",
    "\n",
    "    # Retrieve documents using the transformed embedding\n",
    "    retrieved_docs = vector_store.similarity_search_by_vector(transformed_embedding)\n",
    "\n",
    "    # Intelligent re-ranking\n",
    "    reranker = IntelligentReranking(llm)\n",
    "    reranked_docs = reranker.rerank(original_query, retrieved_docs)\n",
    "\n",
    "    # Prepare top documents for context\n",
    "    top_docs_content = \"\\n\\n\".join([doc.page_content for doc in reranked_docs[:5]])\n",
    "\n",
    "    # Generate answer\n",
    "    prompt_template = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are an assistant that answers questions based on retrieved context.\"),\n",
    "        (\"user\", \"Question: {question}\\n\\nContext:\\n{top_docs_content}\")\n",
    "    ])\n",
    "\n",
    "    chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "    # Invoke the chain\n",
    "    response = chain.run(question=original_query, top_docs_content=top_docs_content)\n",
    "    print(\"Final Answer:\")\n",
    "    print(response)\n",
    "\n",
    "# Call the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fun Nba api example: Moved to it's own notebook to see about the success of this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Answer:\n",
      "Based on the provided context, I have retrieved information for LeBron James's points per game in different seasons. \n",
      "\n",
      "To calculate the average points per game across all available data, we need to add up his total points and divide by the number of games played.\n",
      "\n",
      "Total points = 1822 + 1654 + 2175 + 1590 = 6241\n",
      "Total games played = 71 + 79 + 80 + 55 = 285\n",
      "\n",
      "Average points per game = Total points / Total games played \n",
      "= 6241 / 285 \n",
      "= approximately 21.9 \n",
      "\n",
      "So, LeBron James's average points per game across the provided seasons is around 21.9.\n"
     ]
    }
   ],
   "source": [
    "def integrate_data_into_pipeline(data_documents):\n",
    "    # Use Proposition Chunking to split documents into sentences\n",
    "    chunked_documents = []\n",
    "    for doc in data_documents:\n",
    "        chunker = PropositionChunking(doc.page_content)\n",
    "        chunked_documents.extend(chunker.chunk())\n",
    "\n",
    "    # Build a vector store with the chunked documents\n",
    "    vector_store = FAISS.from_documents(chunked_documents, embeddings_model)\n",
    "    retriever = vector_store.as_retriever(search_type=\"similarity\")\n",
    "\n",
    "    return retriever\n",
    "\n",
    "\n",
    "def main_with_data_source():\n",
    "    # Choose data source and player name\n",
    "    player_name = \"LeBron James\"\n",
    "    data_source = \"nba_api\"  # Options: \"nba_api\" or \"web_scraping\"\n",
    "\n",
    "    # Initialize the appropriate data loader\n",
    "    if data_source == \"nba_api\":\n",
    "        data_loader = NBAAPIDataLoader(player_name)\n",
    "    elif data_source == \"web_scraping\":\n",
    "        data_loader = NBAWebScraperDataLoader(player_name)\n",
    "    else:\n",
    "        print(\"Invalid data source selected.\")\n",
    "        return\n",
    "\n",
    "    # Load data into Documents\n",
    "    data_documents = data_loader.load_data()\n",
    "    if not data_documents:\n",
    "        print(\"No data loaded.\")\n",
    "        return\n",
    "\n",
    "    # Integrate data into the pipeline\n",
    "    retriever = integrate_data_into_pipeline(data_documents)\n",
    "\n",
    "    # Proceed with the rest of the pipeline\n",
    "    # HyDE query transformation\n",
    "    hyde_transformer = HyDE(llm)\n",
    "    original_query = f\"What is {player_name}'s average points per game?\"\n",
    "    transformed_query_doc = hyde_transformer.transform(original_query)\n",
    "\n",
    "    # Embed the hypothetical document\n",
    "    transformed_embedding = embeddings_model.embed_query(transformed_query_doc)\n",
    "\n",
    "    # Retrieve documents using the transformed embedding\n",
    "    retrieved_docs = retriever.get_relevant_documents(original_query)\n",
    "\n",
    "    # Intelligent re-ranking\n",
    "    reranker = IntelligentReranking(llm)\n",
    "    reranked_docs = reranker.rerank(original_query, retrieved_docs)\n",
    "\n",
    "    # Prepare top documents for context\n",
    "    top_docs_content = \"\\n\\n\".join([doc.page_content for doc in reranked_docs[:5]])\n",
    "\n",
    "    # Generate an answer using the RAG pipeline\n",
    "    prompt_template = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are an assistant that answers questions based on retrieved context.\"),\n",
    "        (\"user\", \"Question: {question}\\n\\nContext:\\n{top_docs_content}\")\n",
    "    ])\n",
    "\n",
    "    chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "    # Invoke the chain to get the final answer\n",
    "    response = chain.run(question=original_query, top_docs_content=top_docs_content)\n",
    "    print(\"Final Answer:\")\n",
    "    print(response)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_with_data_source()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science_ollama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
