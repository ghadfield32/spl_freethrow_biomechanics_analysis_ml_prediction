{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAG, RAG with Memory, Adaptive RAG, Corrective RAG, self-RAG, Agentive RAG... are you lost? Let me help you with this guide.\n",
    "\n",
    "1/ Simple RAG\n",
    "Retrieves relevant documents based on the query and uses them to generate an answer.\n",
    "\n",
    "2/ Simple RAG with Memory\n",
    "Extends Simple RAG by maintaining context from previous interactions.\n",
    "\n",
    "3/ Branched RAG\n",
    "Performs multiple retrieval steps, refining the search based on intermediate results.\n",
    "\n",
    "4/ HyDE (Hypothetical Document Embedding)\n",
    "Generates a hypothetical ideal document before retrieval to improve search relevance.\n",
    "\n",
    "5/ Adaptive RAG\n",
    "Dynamically adjusts retrieval and generation strategies based on the query type or difficulty.\n",
    "\n",
    "6/ Corrective RAG (CRAG)\n",
    "Iteratively refines generated responses by fact-checking against retrieved information.\n",
    "\n",
    "7/ Self-RAG\n",
    "The model critiques and improves its own responses using self-reflection and retrieval.\n",
    "\n",
    "8/ Agentic RAG\n",
    "Combines RAG with agentic behavior, allowing for more complex, multi-step problem-solving.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://python.langchain.com/v0.1/docs/get_started/quickstart/\n",
    "\n",
    "langchain quick start ^\n",
    "\n",
    "\n",
    "https://python.langchain.com/docs/integrations/providers/ollama/\n",
    "\n",
    "Ollama integrations ^\n",
    "\n",
    "Tool calling:\n",
    "https://ollama.com/blog/tool-support\n",
    "https://python.langchain.com/docs/how_to/tool_calling/\n",
    "\n",
    "\n",
    "- Easy example:\n",
    "https://github.com/Shubhamsaboo/awesome-llm-apps/blob/main/llama3.1_local_rag/llama3.1_local_rag.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.4.1+cu121\n",
      "CUDA is available! GPU is ready to be used.\n",
      "Number of GPUs available: 1\n",
      "Current GPU: NVIDIA GeForce RTX 4090\n",
      "GPU 0: NVIDIA GeForce RTX 4090\n",
      "  - Total Memory: 25.756696576 GB\n",
      "  - Compute Capability: (8, 9)\n",
      "Tensor on GPU: tensor([[0.7830, 0.3323, 0.5044],\n",
      "        [0.1345, 0.2114, 0.7690],\n",
      "        [0.1381, 0.9195, 0.4452]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Print the PyTorch version\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Check if CUDA (GPU support) is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available! GPU is ready to be used.\")\n",
    "    print(f\"Number of GPUs available: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current GPU: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
    "else:\n",
    "    print(\"CUDA is not available. GPU is not set up correctly.\")\n",
    "\n",
    "# Print additional GPU details\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"  - Total Memory: {torch.cuda.get_device_properties(i).total_memory / 1e9} GB\")\n",
    "        print(f\"  - Compute Capability: {torch.cuda.get_device_capability(i)}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # Create a random tensor and move it to the GPU\n",
    "    tensor = torch.rand(3, 3).cuda()\n",
    "    print(\"Tensor on GPU:\", tensor)\n",
    "else:\n",
    "    print(\"GPU is not available, cannot move tensor to GPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspaces/custom_ollama_docker/notebooks/sports_news_rag\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "# Print the current working directory (optional for debugging)\n",
    "print(os.getcwd())\n",
    "\n",
    "# Set the path to your .env file relative to the current working directory\n",
    "dotenv_path = os.path.join(os.getcwd(), '../../.env')\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "\n",
    "# Set up API keys\n",
    "os.environ[\"TAVILY_API_KEY\"] = os.getenv(\"TAVILY_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Possible idea: pull in current repo files and make recommendations to fix it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Branches: ['main']\n",
      "Files: ['/workspaces/custom_ollama_docker/.devcontainer/Dockerfile', '/workspaces/custom_ollama_docker/.devcontainer/devcontainer.env', '/workspaces/custom_ollama_docker/.devcontainer/devcontainer.json', '/workspaces/custom_ollama_docker/.devcontainer/environment.yml', '/workspaces/custom_ollama_docker/.devcontainer/install_dependencies.sh']\n",
      "Skipping binary or non-text file: /workspaces/custom_ollama_docker/.devcontainer/Dockerfile\n",
      "Skipping binary or non-text file: /workspaces/custom_ollama_docker/.devcontainer/devcontainer.env\n",
      "Skipping binary or non-text file: /workspaces/custom_ollama_docker/.devcontainer/devcontainer.json\n",
      "Skipping binary or non-text file: /workspaces/custom_ollama_docker/.devcontainer/environment.yml\n",
      "Skipping binary or non-text file: /workspaces/custom_ollama_docker/chroma_db/chroma.sqlite3\n",
      "Skipping binary or non-text file: /workspaces/custom_ollama_docker/custom_ollama_docker.code-workspace\n",
      "Skipping binary or non-text file: /workspaces/custom_ollama_docker/data/chroma_dbs/6160174d-4048-4c8e-ac22-be804b0833b4/data_level0.bin\n",
      "Skipping binary or non-text file: /workspaces/custom_ollama_docker/data/chroma_dbs/6160174d-4048-4c8e-ac22-be804b0833b4/header.bin\n",
      "Skipping binary or non-text file: /workspaces/custom_ollama_docker/data/chroma_dbs/6160174d-4048-4c8e-ac22-be804b0833b4/length.bin\n",
      "Skipping binary or non-text file: /workspaces/custom_ollama_docker/data/chroma_dbs/6160174d-4048-4c8e-ac22-be804b0833b4/link_lists.bin\n",
      "Skipping binary or non-text file: /workspaces/custom_ollama_docker/data/chroma_dbs/chroma.sqlite3\n",
      "Skipping binary or non-text file: /workspaces/custom_ollama_docker/data/databases/database_creator.ipynb\n",
      "Skipping binary or non-text file: /workspaces/custom_ollama_docker/data/databases/db1.sqlite\n",
      "Skipping binary or non-text file: /workspaces/custom_ollama_docker/data/databases/db2.sqlite\n",
      "Skipping binary or non-text file: /workspaces/custom_ollama_docker/data/ollama_models/blobs/sha256-2bada8a7450677000f678be90653b85d364de7db25eb5ea54136ada5f3933730\n",
      "Skipping binary or non-text file: /workspaces/custom_ollama_docker/data/ollama_models/blobs/sha256-2f15b3218f0552c60647ce60ada83632d2c09755b16259b13e3e4458e9ae419d\n",
      "Skipping binary or non-text file: /workspaces/custom_ollama_docker/data/ollama_models/blobs/sha256-3f8eb4da87fa7a3c9da615036b0dc418d31fef2a30b115ff33562588b32c691d\n",
      "Skipping binary or non-text file: /workspaces/custom_ollama_docker/data/ollama_models/blobs/sha256-42347cd80dc868877d2807869c0e9c90034392b2f1f001cae1563488021e2e19\n",
      "Skipping binary or non-text file: /workspaces/custom_ollama_docker/data/ollama_models/blobs/sha256-43070e2d4e532684de521b885f385d0841030efa2b1a20bafb76133a5e1379c1\n",
      "Skipping binary or non-text file: /workspaces/custom_ollama_docker/data/ollama_models/blobs/sha256-491dfa501e59ed17239711477601bdc7f559de5407fbd4a2a79078b271045621\n",
      "Skipping binary or non-text file: /workspaces/custom_ollama_docker/data/ollama_models/blobs/sha256-4fa551d4f938f68b8c1e6afa9d28befb70e3f33f75d0753248d530364aeea40f\n",
      "Skipping binary or non-text file: /workspaces/custom_ollama_docker/data/ollama_models/blobs/sha256-577073ffcc6ce95b9981eacc77d1039568639e5638e83044994560d9ef82ce1b\n",
      "Skipping binary or non-text file: /workspaces/custom_ollama_docker/data/ollama_models/blobs/sha256-66b9ea09bd5b7099cbb4fc820f31b575c0366fa439b08245566692c6784e281e\n",
      "Skipping binary or non-text file: /workspaces/custom_ollama_docker/data/ollama_models/blobs/sha256-6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa\n",
      "Skipping binary or non-text file: /workspaces/custom_ollama_docker/data/ollama_models/blobs/sha256-832dd9e00a68dd83b3c3fb9f5588dad7dcf337a0db50f7d9483f310cd292e92e\n",
      "Skipping binary or non-text file: /workspaces/custom_ollama_docker/data/ollama_models/blobs/sha256-8ab4849b038cf0abc5b1c9b8ee1443dca6b93a045c2272180d985126eb40bf6f\n",
      "Skipping binary or non-text file: /workspaces/custom_ollama_docker/data/ollama_models/blobs/sha256-eb4402837c7829a690fa845de4d7f3fd842c2adee476d5341da8a46ea9255175\n",
      "Skipping binary or non-text file: /workspaces/custom_ollama_docker/data/ollama_models/blobs/sha256-ed11eda7790d05b49395598a42b155812b17e263214292f7b87d15e14003d337\n",
      "Skipping binary or non-text file: /workspaces/custom_ollama_docker/data/ollama_models/blobs/sha256-ff82381e2bea77d91c1b824c7afb83f6fb73e9f7de9dda631bcdbca564aa5435\n",
      "Skipping binary or non-text file: /workspaces/custom_ollama_docker/data/ollama_models/manifests/registry.ollama.ai/library/llama3/latest\n",
      "Skipping binary or non-text file: /workspaces/custom_ollama_docker/data/ollama_models/manifests/registry.ollama.ai/library/mistral/latest\n",
      "Skipping binary or non-text file: /workspaces/custom_ollama_docker/data/ollama_models/manifests/registry.ollama.ai/library/qwen2.5/latest\n",
      "Skipping binary or non-text file: /workspaces/custom_ollama_docker/docker-compose.yml\n",
      "Skipping binary or non-text file: /workspaces/custom_ollama_docker/images/Untitled.jpg\n",
      "Skipping binary or non-text file: /workspaces/custom_ollama_docker/images/codespace.gif\n",
      "Skipping binary or non-text file: /workspaces/custom_ollama_docker/images/python-template.gif\n",
      "Skipping binary or non-text file: /workspaces/custom_ollama_docker/images/use-template.gif\n",
      "Skipping binary or non-text file: /workspaces/custom_ollama_docker/notebooks/crawl4ai.ipynb\n",
      "Skipping binary or non-text file: /workspaces/custom_ollama_docker/notebooks/graphrag/langgraph_graphrag_agent_local.ipynb\n",
      "Skipping binary or non-text file: /workspaces/custom_ollama_docker/notebooks/graphrag/nir_diamante_graph_rag_networkx_langchain.ipynb\n",
      "Skipping binary or non-text file: /workspaces/custom_ollama_docker/notebooks/graphreader_neo4j_langgraph.ipynb\n",
      "Skipping binary or non-text file: /workspaces/custom_ollama_docker/notebooks/langchain_ollama.ipynb\n",
      "Skipping binary or non-text file: /workspaces/custom_ollama_docker/notebooks/langchain_quickstart_ollama_llamamodelandembed.ipynb\n",
      "Skipping binary or non-text file: /workspaces/custom_ollama_docker/notebooks/langchain_rag_nba_api.ipynb\n",
      "Skipping binary or non-text file: /workspaces/custom_ollama_docker/notebooks/langchain_rag_quickstart.ipynb\n",
      "Skipping binary or non-text file: /workspaces/custom_ollama_docker/notebooks/llama3.2_images.ipynb\n",
      "Skipping binary or non-text file: /workspaces/custom_ollama_docker/notebooks/neo4j_data_setup/nba_trade_data_neo4j_input.ipynb\n",
      "Skipping binary or non-text file: /workspaces/custom_ollama_docker/notebooks/neo4j_llama3.2_nomic.ipynb\n",
      "Skipping binary or non-text file: /workspaces/custom_ollama_docker/notebooks/neo4j_llama3_ollamaembed.ipynb\n",
      "Skipping binary or non-text file: /workspaces/custom_ollama_docker/notebooks/sports_news_rag/langchain_quickstart_ollama_llama_hyde_corrective.ipynb\n",
      "Skipping binary or non-text file: /workspaces/custom_ollama_docker/notebooks/sql_langchain bot.ipynb\n",
      "Skipping binary or non-text file: /workspaces/custom_ollama_docker/notebooks/table_augmented_retrieval_model.ipynb\n",
      "Skipping binary or non-text file: /workspaces/custom_ollama_docker/src/features/__pycache__/load_data.cpython-310.pyc\n",
      "Skipping binary or non-text file: /workspaces/custom_ollama_docker/src/features/__pycache__/preprocess_data.cpython-310.pyc\n",
      "Skipping binary or non-text file: /workspaces/custom_ollama_docker/src/models/__pycache__/evaluate_model.cpython-310.pyc\n",
      "Skipping binary or non-text file: /workspaces/custom_ollama_docker/src/models/__pycache__/train_model.cpython-310.pyc\n",
      "Skipping binary or non-text file: /workspaces/custom_ollama_docker/src/sports_news_rag/modules/__pycache__/corrective_rag.cpython-310.pyc\n",
      "Skipping binary or non-text file: /workspaces/custom_ollama_docker/src/sports_news_rag/modules/__pycache__/data_crawling.cpython-310.pyc\n",
      "Skipping binary or non-text file: /workspaces/custom_ollama_docker/src/sports_news_rag/modules/__pycache__/decision_mechanism.cpython-310.pyc\n",
      "Skipping binary or non-text file: /workspaces/custom_ollama_docker/src/sports_news_rag/modules/__pycache__/fact_checker.cpython-310.pyc\n",
      "Skipping binary or non-text file: /workspaces/custom_ollama_docker/src/sports_news_rag/modules/__pycache__/hyde_rag.cpython-310.pyc\n",
      "Skipping binary or non-text file: /workspaces/custom_ollama_docker/src/sports_news_rag/modules/__pycache__/self_rag.cpython-310.pyc\n",
      "Skipping binary or non-text file: /workspaces/custom_ollama_docker/src/sports_news_rag/modules/__pycache__/vector_store.cpython-310.pyc\n",
      "Skipping binary or non-text file: /workspaces/custom_ollama_docker/src/sports_news_rag/modules/__pycache__/web_search.cpython-310.pyc\n",
      "Skipping binary or non-text file: /workspaces/custom_ollama_docker/src/visualization/__pycache__/visualize_data.cpython-310.pyc\n",
      "Skipping binary or non-text file: /workspaces/custom_ollama_docker/tests/test2.ipynb\n",
      "Skipping binary or non-text file: /workspaces/custom_ollama_docker/tests/test3.qmd\n",
      "Skipping binary or non-text file: /workspaces/custom_ollama_docker/tests/test3_files/libs/bootstrap/bootstrap-icons.woff\n",
      "Number of valid documents created: 38\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'page_content'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 114\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# Create a vectorstore retriever for the filtered repository information\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m     retriever \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_vectorstore_from_repo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_info\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ve:\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError creating vector store: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mve\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[9], line 98\u001b[0m, in \u001b[0;36mcreate_vectorstore_from_repo\u001b[0;34m(repo_info, local_llm_model)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Use RecursiveCharacterTextSplitter to split code into smaller chunks\u001b[39;00m\n\u001b[1;32m     97\u001b[0m text_splitter \u001b[38;5;241m=\u001b[39m RecursiveCharacterTextSplitter()\n\u001b[0;32m---> 98\u001b[0m split_docs \u001b[38;5;241m=\u001b[39m \u001b[43mtext_splitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# Create embeddings using Ollama's local model\u001b[39;00m\n\u001b[1;32m    101\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m OllamaEmbeddings(model\u001b[38;5;241m=\u001b[39mlocal_llm_model)\n",
      "File \u001b[0;32m/opt/conda/envs/data_science_ollama/lib/python3.10/site-packages/langchain_text_splitters/base.py:94\u001b[0m, in \u001b[0;36mTextSplitter.split_documents\u001b[0;34m(self, documents)\u001b[0m\n\u001b[1;32m     92\u001b[0m texts, metadatas \u001b[38;5;241m=\u001b[39m [], []\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents:\n\u001b[0;32m---> 94\u001b[0m     texts\u001b[38;5;241m.\u001b[39mappend(\u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpage_content\u001b[49m)\n\u001b[1;32m     95\u001b[0m     metadatas\u001b[38;5;241m.\u001b[39mappend(doc\u001b[38;5;241m.\u001b[39mmetadata)\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_documents(texts, metadatas\u001b[38;5;241m=\u001b[39mmetadatas)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'page_content'"
     ]
    }
   ],
   "source": [
    "import git\n",
    "import os\n",
    "import mimetypes\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Function to extract the Git tree structure, ignoring dotfiles if specified\n",
    "def extract_git_tree(repo_path=\".\", include_dotfiles=False):\n",
    "    \"\"\"\n",
    "    Extracts the git tree structure including branch names, file paths, and commit history.\n",
    "\n",
    "    Args:\n",
    "    - repo_path (str): The path to the repository.\n",
    "    - include_dotfiles (bool): Whether to include dotfiles in the extracted file paths.\n",
    "\n",
    "    Returns:\n",
    "    - dict: A dictionary with keys \"branches\", \"files\", and \"commits\".\n",
    "    \"\"\"\n",
    "    repo = git.Repo(repo_path)\n",
    "    branches = [branch.name for branch in repo.branches]\n",
    "    files = repo.git.ls_tree(\"-r\", \"--name-only\", \"HEAD\").splitlines()\n",
    "\n",
    "    # Filter out dotfiles if include_dotfiles is False\n",
    "    if not include_dotfiles:\n",
    "        files = [f for f in files if not os.path.basename(f).startswith(\".\")]\n",
    "\n",
    "    commit_history = list(repo.iter_commits(max_count=5))\n",
    "\n",
    "    # Convert relative paths to absolute paths using the repo's working directory\n",
    "    abs_files = [os.path.join(repo.working_dir, file) for file in files]\n",
    "    return {\"branches\": branches, \"files\": abs_files, \"commits\": commit_history}\n",
    "\n",
    "# Function to check if a file is a text file\n",
    "def is_text_file(file_path):\n",
    "    \"\"\"\n",
    "    Determines if a given file is a text file by checking its MIME type.\n",
    "\n",
    "    Args:\n",
    "    - file_path (str): The path to the file.\n",
    "\n",
    "    Returns:\n",
    "    - bool: True if the file is a text file, False otherwise.\n",
    "    \"\"\"\n",
    "    mime_type, _ = mimetypes.guess_type(file_path)\n",
    "    # Consider text files as those having 'text/*' MIME type or no MIME type (unrecognized files)\n",
    "    if mime_type and mime_type.startswith(\"text\"):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# Create embeddings for code files and store them in a vector store\n",
    "def create_vectorstore_from_repo(repo_info, local_llm_model=\"llama3.2\"):\n",
    "    \"\"\"\n",
    "    Create embeddings for code files and store them in a Chroma vector store.\n",
    "\n",
    "    Args:\n",
    "    - repo_info (dict): The repository information containing file paths.\n",
    "    - local_llm_model (str): The local LLM model to use for creating embeddings.\n",
    "\n",
    "    Returns:\n",
    "    - retriever (object): A retriever for retrieving documents based on queries.\n",
    "    \"\"\"\n",
    "    file_contents = []\n",
    "    valid_files = []  # Keep track of successfully read files\n",
    "    \n",
    "    for file_path in repo_info[\"files\"]:\n",
    "        if is_text_file(file_path):\n",
    "            try:\n",
    "                # Attempt to open and read the file using the absolute path\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    content = f.read()\n",
    "                    file_contents.append(content)\n",
    "                    valid_files.append(file_path)  # Only add to valid_files if read is successful\n",
    "            except FileNotFoundError:\n",
    "                print(f\"File not found: {file_path}. Skipping.\")\n",
    "            except UnicodeDecodeError:\n",
    "                print(f\"Error decoding file {file_path}. Skipping.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading file {file_path}: {e}. Skipping.\")\n",
    "        else:\n",
    "            print(f\"Skipping binary or non-text file: {file_path}\")\n",
    "    \n",
    "    # Create a list of document objects only for valid files\n",
    "    documents = [{\"page_content\": content, \"metadata\": {\"source\": file_path}} for content, file_path in zip(file_contents, valid_files)]\n",
    "    \n",
    "    # Print the number of valid documents to ensure we have content\n",
    "    print(f\"Number of valid documents created: {len(documents)}\")\n",
    "    \n",
    "    # Check if there are any documents to process\n",
    "    if not documents:\n",
    "        raise ValueError(\"No valid documents were found. Ensure that the repository files are accessible and readable.\")\n",
    "    \n",
    "    # Use RecursiveCharacterTextSplitter to split code into smaller chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter()\n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "    \n",
    "    # Create embeddings using Ollama's local model\n",
    "    embeddings = OllamaEmbeddings(model=local_llm_model)\n",
    "    \n",
    "    # Create a Chroma vector store from the split document chunks\n",
    "    vectorstore = Chroma.from_documents(documents=split_docs, embedding=embeddings)\n",
    "    return vectorstore.as_retriever()\n",
    "\n",
    "# Example usage: Initialize retriever for a repository, ignoring dotfiles\n",
    "repo_info = extract_git_tree(\"../../\", include_dotfiles=False)\n",
    "print(f\"Branches: {repo_info['branches']}\")\n",
    "print(f\"Files: {repo_info['files'][:5]}\")  # Print first 5 files for brevity\n",
    "\n",
    "# Create a vectorstore retriever for the filtered repository information\n",
    "try:\n",
    "    retriever = create_vectorstore_from_repo(repo_info)\n",
    "except ValueError as ve:\n",
    "    print(f\"Error creating vector store: {ve}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Branches: ['main']\n",
      "Files: ['/workspaces/custom_ollama_docker/.devcontainer/Dockerfile', '/workspaces/custom_ollama_docker/.devcontainer/devcontainer.env', '/workspaces/custom_ollama_docker/.devcontainer/devcontainer.json', '/workspaces/custom_ollama_docker/.devcontainer/environment.yml', '/workspaces/custom_ollama_docker/.devcontainer/install_dependencies.sh']\n",
      "Error extracting code structure from /workspaces/custom_ollama_docker/tests/test3_files/libs/bootstrap/bootstrap.min.js: closing parenthesis ']' does not match opening parenthesis '(' (<unknown>, line 6). Skipping.\n",
      "Error extracting code structure from /workspaces/custom_ollama_docker/tests/test3_files/libs/clipboard/clipboard.min.js: invalid character '©' (U+00A9) (<unknown>, line 5). Skipping.\n",
      "Error extracting code structure from /workspaces/custom_ollama_docker/tests/test3_files/libs/quarto-html/anchor.min.js: leading zeros in decimal integer literals are not permitted; use an 0o prefix for octal integers (<unknown>, line 3). Skipping.\n",
      "Error extracting code structure from /workspaces/custom_ollama_docker/tests/test3_files/libs/quarto-html/popper.min.js: invalid syntax (<unknown>, line 1). Skipping.\n",
      "Error extracting code structure from /workspaces/custom_ollama_docker/tests/test3_files/libs/quarto-html/quarto.js: unterminated string literal (detected at line 80) (<unknown>, line 80). Skipping.\n",
      "Error extracting code structure from /workspaces/custom_ollama_docker/tests/test3_files/libs/quarto-html/tippy.umd.min.js: invalid syntax (<unknown>, line 1). Skipping.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'page_content'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 124\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# Create a vectorstore retriever for the code structure information\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 124\u001b[0m     retriever \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_vectorstore_from_code\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_info\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode vector store created successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;66;03m# Provide a sample code recommendation\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[10], line 85\u001b[0m, in \u001b[0;36mcreate_vectorstore_from_code\u001b[0;34m(repo_info, local_llm_model)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# Use RecursiveCharacterTextSplitter to split code descriptions into smaller chunks\u001b[39;00m\n\u001b[1;32m     84\u001b[0m text_splitter \u001b[38;5;241m=\u001b[39m RecursiveCharacterTextSplitter()\n\u001b[0;32m---> 85\u001b[0m split_docs \u001b[38;5;241m=\u001b[39m \u001b[43mtext_splitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode_descriptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# Create embeddings using Ollama's local model\u001b[39;00m\n\u001b[1;32m     88\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m OllamaEmbeddings(model\u001b[38;5;241m=\u001b[39mlocal_llm_model)\n",
      "File \u001b[0;32m/opt/conda/envs/data_science_ollama/lib/python3.10/site-packages/langchain_text_splitters/base.py:94\u001b[0m, in \u001b[0;36mTextSplitter.split_documents\u001b[0;34m(self, documents)\u001b[0m\n\u001b[1;32m     92\u001b[0m texts, metadatas \u001b[38;5;241m=\u001b[39m [], []\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents:\n\u001b[0;32m---> 94\u001b[0m     texts\u001b[38;5;241m.\u001b[39mappend(\u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpage_content\u001b[49m)\n\u001b[1;32m     95\u001b[0m     metadatas\u001b[38;5;241m.\u001b[39mappend(doc\u001b[38;5;241m.\u001b[39mmetadata)\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_documents(texts, metadatas\u001b[38;5;241m=\u001b[39mmetadatas)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'page_content'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ast\n",
    "import mimetypes\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Define a function to parse Python code and extract its structure\n",
    "def extract_python_code_structure(file_path):\n",
    "    \"\"\"\n",
    "    Extracts structure from a Python code file, including functions, classes, and docstrings.\n",
    "\n",
    "    Args:\n",
    "    - file_path (str): The path to the Python code file.\n",
    "\n",
    "    Returns:\n",
    "    - str: A structured representation of the code in natural language.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        code = file.read()\n",
    "    \n",
    "    # Parse the code using AST\n",
    "    tree = ast.parse(code)\n",
    "    \n",
    "    # Initialize an empty list to hold code descriptions\n",
    "    code_structure = []\n",
    "    \n",
    "    for node in ast.walk(tree):\n",
    "        # Extract function definitions\n",
    "        if isinstance(node, ast.FunctionDef):\n",
    "            func_info = f\"Function: {node.name}\\n\"\n",
    "            if ast.get_docstring(node):\n",
    "                func_info += f\"Docstring: {ast.get_docstring(node)}\\n\"\n",
    "            code_structure.append(func_info)\n",
    "        \n",
    "        # Extract class definitions\n",
    "        elif isinstance(node, ast.ClassDef):\n",
    "            class_info = f\"Class: {node.name}\\n\"\n",
    "            if ast.get_docstring(node):\n",
    "                class_info += f\"Docstring: {ast.get_docstring(node)}\\n\"\n",
    "            code_structure.append(class_info)\n",
    "    \n",
    "    # Join all code descriptions into a single string\n",
    "    return \"\\n\".join(code_structure)\n",
    "\n",
    "# Define a function to determine if a file is a code file (e.g., .py, .js, .java)\n",
    "def is_code_file(file_path):\n",
    "    \"\"\"\n",
    "    Determines if a given file is a code file based on its extension.\n",
    "\n",
    "    Args:\n",
    "    - file_path (str): The path to the file.\n",
    "\n",
    "    Returns:\n",
    "    - bool: True if the file is a code file, False otherwise.\n",
    "    \"\"\"\n",
    "    code_extensions = {\".py\", \".js\", \".java\", \".cpp\", \".c\", \".ts\", \".go\", \".rb\"}\n",
    "    return os.path.splitext(file_path)[1] in code_extensions\n",
    "\n",
    "# Create embeddings for code structures and store them in a vector store\n",
    "def create_vectorstore_from_code(repo_info, local_llm_model=\"llama3.2\"):\n",
    "    \"\"\"\n",
    "    Create embeddings for code structures and store them in a Chroma vector store.\n",
    "\n",
    "    Args:\n",
    "    - repo_info (dict): The repository information containing file paths.\n",
    "    - local_llm_model (str): The local LLM model to use for creating embeddings.\n",
    "\n",
    "    Returns:\n",
    "    - retriever (object): A retriever for retrieving documents based on queries.\n",
    "    \"\"\"\n",
    "    code_descriptions = []\n",
    "    for file_path in repo_info[\"files\"]:\n",
    "        if is_code_file(file_path):\n",
    "            try:\n",
    "                # Extract the code structure from the file\n",
    "                code_structure = extract_python_code_structure(file_path)\n",
    "                if code_structure:\n",
    "                    code_descriptions.append({\"page_content\": code_structure, \"metadata\": {\"source\": file_path}})\n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting code structure from {file_path}: {e}. Skipping.\")\n",
    "    \n",
    "    # Use RecursiveCharacterTextSplitter to split code descriptions into smaller chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter()\n",
    "    split_docs = text_splitter.split_documents(code_descriptions)\n",
    "    \n",
    "    # Create embeddings using Ollama's local model\n",
    "    embeddings = OllamaEmbeddings(model=local_llm_model)\n",
    "    \n",
    "    # Create a Chroma vector store from the split document chunks\n",
    "    vectorstore = Chroma.from_documents(documents=split_docs, embedding=embeddings)\n",
    "    return vectorstore.as_retriever()\n",
    "\n",
    "# Define a function to provide code recommendations using an LLM\n",
    "def provide_code_recommendations(retriever, query):\n",
    "    \"\"\"\n",
    "    Provide recommendations on code structure and content based on a query.\n",
    "\n",
    "    Args:\n",
    "    - retriever (object): A retriever for retrieving relevant code documents.\n",
    "    - query (str): The query or question related to code recommendations.\n",
    "\n",
    "    Returns:\n",
    "    - str: The LLM-generated recommendation or response.\n",
    "    \"\"\"\n",
    "    # Retrieve relevant documents using the query\n",
    "    retrieved_docs = retriever.query(query)\n",
    "    \n",
    "    # Combine the retrieved content for context\n",
    "    context = \"\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "    \n",
    "    # Generate a recommendation using a local LLM (e.g., LLaMA, Ollama)\n",
    "    llm = OllamaEmbeddings(model=\"llama3.2\")\n",
    "    response = llm.invoke([{\"role\": \"user\", \"content\": f\"Question: {query}\\n\\nContext: {context}\"}])\n",
    "    return response.content\n",
    "\n",
    "# Example usage: Initialize retriever for a repository's code structure and provide recommendations\n",
    "repo_info = extract_git_tree(\"../../\", include_dotfiles=False)  # Extract repo structure\n",
    "print(f\"Branches: {repo_info['branches']}\")\n",
    "print(f\"Files: {repo_info['files'][:5]}\")  # Print first 5 files for brevity\n",
    "\n",
    "# Create a vectorstore retriever for the code structure information\n",
    "try:\n",
    "    retriever = create_vectorstore_from_code(repo_info)\n",
    "    print(\"Code vector store created successfully.\")\n",
    "    \n",
    "    # Provide a sample code recommendation\n",
    "    query = \"How can I improve the function structures in the repository?\"\n",
    "    recommendation = provide_code_recommendations(retriever, query)\n",
    "    print(f\"Recommendation:\\n{recommendation}\")\n",
    "except ValueError as ve:\n",
    "    print(f\"Error creating vector store: {ve}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../src/sports_news_rag/modules/data_crawling.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../src/sports_news_rag/modules/data_crawling.py\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "import concurrent.futures  # For parallel processing\n",
    "\n",
    "def crawl_and_ingest(url, debug=False):\n",
    "    \"\"\"\n",
    "    Crawls a given URL, splits the document, generates propositions, runs quality checks, and returns the processed documents.\n",
    "    \"\"\"\n",
    "    if debug:\n",
    "        print(f\"Crawling data from: {url}\")\n",
    "\n",
    "    # Load documents from the web URL\n",
    "    loader = WebBaseLoader(url)\n",
    "    docs = loader.load()\n",
    "\n",
    "    # Split the documents into smaller chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)  # Adjust these values\n",
    "    document_chunks = text_splitter.split_documents(docs)\n",
    "    if debug:\n",
    "        print(f\"Number of document chunks crawled and ingested: {len(document_chunks)}\")\n",
    "\n",
    "    # Generate propositions from each document chunk and perform quality checks\n",
    "    proposition_documents = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        futures = [executor.submit(process_chunk, chunk, debug) for chunk in document_chunks]\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            proposition_documents.extend(future.result())\n",
    "\n",
    "    if debug:\n",
    "        print(f\"Total number of high-quality propositions generated: {len(proposition_documents)}\")\n",
    "\n",
    "    return proposition_documents\n",
    "\n",
    "def process_chunk(chunk, debug=False):\n",
    "    \"\"\"\n",
    "    Generates and quality checks propositions for a given chunk.\n",
    "    \"\"\"\n",
    "    propositions = generate_propositions(chunk.page_content, debug)\n",
    "    high_quality_propositions = quality_check_propositions(propositions, debug)\n",
    "    return [Document(page_content=prop) for prop in high_quality_propositions]\n",
    "\n",
    "def generate_propositions(text, debug=False):\n",
    "    \"\"\"\n",
    "    Generates propositions from the given text using an LLM.\n",
    "    \"\"\"\n",
    "    llm = ChatOllama(model=\"llama3.2\", temperature=0)\n",
    "    max_length = 2000\n",
    "    text = text[:max_length] if len(text) > max_length else text\n",
    "\n",
    "    proposition_prompt = (\n",
    "        f\"Break down the following text into concise, complete, and meaningful factual statements:\\n\\n{text}\\n\\n\"\n",
    "        \"Provide each proposition as a separate statement.\"\n",
    "    )\n",
    "    response = llm.invoke([{\"role\": \"user\", \"content\": proposition_prompt}]).content\n",
    "\n",
    "    propositions = [prop.strip() for prop in response.split('\\n') if prop.strip()]\n",
    "\n",
    "    if debug:\n",
    "        print(f\"Generated propositions: {propositions}\")\n",
    "\n",
    "    return propositions\n",
    "\n",
    "def quality_check_propositions(propositions, debug=False):\n",
    "    \"\"\"\n",
    "    Checks the quality of the propositions for accuracy, clarity, completeness, and conciseness.\n",
    "    \"\"\"\n",
    "    llm = ChatOllama(model=\"llama3.2\", temperature=0)\n",
    "    high_quality_propositions = []\n",
    "\n",
    "    batch_size = 5\n",
    "    for i in range(0, len(propositions), batch_size):\n",
    "        batch = propositions[i:i + batch_size]\n",
    "        quality_prompt = (\n",
    "            f\"Evaluate the following propositions for accuracy, clarity, completeness, and conciseness. \"\n",
    "            f\"Score each aspect from 1 to 10 and provide an overall assessment. Reply with 'pass' if the proposition is acceptable:\\n\\n\"\n",
    "            f\"{', '.join(batch)}\"\n",
    "        )\n",
    "        response = llm.invoke([{\"role\": \"user\", \"content\": quality_prompt}]).content\n",
    "\n",
    "        results = response.lower().split('\\n')\n",
    "\n",
    "        if debug:\n",
    "            print(f\"Batch being processed: {batch}\")\n",
    "            print(f\"LLM Response: {response}\")\n",
    "            print(f\"Number of results received: {len(results)}, Number of propositions in batch: {len(batch)}\")\n",
    "\n",
    "        min_length = min(len(results), len(batch))\n",
    "        for j in range(min_length):\n",
    "            if 'pass' in results[j]:\n",
    "                high_quality_propositions.append(batch[j])\n",
    "\n",
    "    return high_quality_propositions\n",
    "\n",
    "\n",
    "\n",
    "def main(debug=False):\n",
    "    # Sample sites for testing\n",
    "    sports_sites = [\"https://www.nba.com/\", \"https://www.espn.com/\"]\n",
    "    all_documents = []\n",
    "    for site in sports_sites:\n",
    "        documents = crawl_and_ingest(site, debug)\n",
    "        all_documents.extend(documents)\n",
    "    if debug:\n",
    "        print(f\"Total documents ingested: {len(all_documents)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../src/sports_news_rag/modules/vector_store.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../src/sports_news_rag/modules/vector_store.py\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "from langchain.schema import Document  # Import the Document class\n",
    "import os \n",
    "\n",
    "def create_vectorstore(documents, persist_directory='../../data/chroma_dbs', debug=False):\n",
    "    \"\"\"\n",
    "    Creates a vector store from the provided documents, embedding them for later retrieval.\n",
    "    \"\"\"\n",
    "    # Ensure each document is a Document object\n",
    "    if not all(isinstance(doc, Document) for doc in documents):\n",
    "        documents = [Document(page_content=doc[\"page_content\"]) for doc in documents]\n",
    "\n",
    "    embeddings = OllamaEmbeddings(model=\"llama3.2\")\n",
    "    if debug:\n",
    "        print(f\"Creating vector store with {len(documents)} high-quality propositions...\")\n",
    "\n",
    "    # Create Chroma vector store from documents\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=documents,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=persist_directory\n",
    "    )\n",
    "    if debug:\n",
    "        print(f\"Vector store created at {persist_directory}\")\n",
    "\n",
    "    return vectorstore\n",
    "\n",
    "def create_pre_ingested_vectorstore(site_name, documents):\n",
    "    # Create directory if it doesn't exist\n",
    "    directory = f\"../../data/vectorstores/{site_name.lower()}\"\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    \n",
    "    # Create the vector store\n",
    "    embeddings = OllamaEmbeddings(model=\"llama3.2\")\n",
    "    vectorstore = Chroma.from_documents(documents, embedding=embeddings, persist_directory=directory)\n",
    "    print(f\"Vector store for {site_name} created and saved at {directory}\")\n",
    "\n",
    "def main(debug=False):\n",
    "    # Use a list of high-quality Document objects instead of dictionaries\n",
    "    sample_docs = [Document(page_content=\"This is a high-quality sample document for testing.\")]\n",
    "    vectorstore = create_vectorstore(sample_docs, debug=debug)\n",
    "    if debug:\n",
    "        print(\"Vector store successfully created.\")\n",
    "        \n",
    "    # Example usage:\n",
    "    site_name = \"ESPN\"\n",
    "    documents = [Document(page_content=\"This is a sample document for NFL data.\")]\n",
    "    create_pre_ingested_vectorstore(site_name, documents)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../src/sports_news_rag/modules/contextual_retrieval.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../src/sports_news_rag/modules/contextual_retrieval.py\n",
    "\n",
    "import copy\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "from langchain.schema import Document  # Import the Document class\n",
    "\n",
    "def create_contextual_nodes(documents, debug=False):\n",
    "    \"\"\"\n",
    "    Creates contextual nodes by enriching each document with additional context.\n",
    "    \n",
    "    Parameters:\n",
    "    - documents (List[Document]): List of LangChain Document objects.\n",
    "    - debug (bool): Flag for printing debug information.\n",
    "    \n",
    "    Returns:\n",
    "    - List[Document]: List of contextually enriched Document objects.\n",
    "    \"\"\"\n",
    "    # Initialize the LLM\n",
    "    llm = ChatOllama(model=\"llama3.2\", temperature=0)\n",
    "    \n",
    "    contextual_documents = []\n",
    "    for doc in documents:\n",
    "        # Generate contextual information using LLM\n",
    "        context_prompt = (\n",
    "            f\"Given the following document, generate contextual information that would help better understand its content:\\n\\n{doc.page_content}\\n\\n\"\n",
    "            \"Contextual information:\"\n",
    "        )\n",
    "        context = llm.invoke([{\"role\": \"user\", \"content\": context_prompt}]).content\n",
    "        \n",
    "        # Append the context to the document's metadata\n",
    "        enriched_doc = copy.deepcopy(doc)\n",
    "        enriched_doc.metadata[\"context\"] = context\n",
    "        contextual_documents.append(enriched_doc)\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"Generated context for document: {context}\")\n",
    "\n",
    "    return contextual_documents\n",
    "\n",
    "def create_embedding_retriever(documents, persist_directory='../../data/chroma_dbs', debug=False):\n",
    "    \"\"\"\n",
    "    Creates a Chroma vector store retriever using contextual nodes.\n",
    "    \n",
    "    Parameters:\n",
    "    - documents (List[Document]): List of contextually enriched Document objects.\n",
    "    - persist_directory (str): Directory to persist the Chroma database.\n",
    "    - debug (bool): Flag for printing debug information.\n",
    "    \n",
    "    Returns:\n",
    "    - Chroma: Chroma vector store retriever object.\n",
    "    \"\"\"\n",
    "    # Create embeddings with Ollama\n",
    "    embeddings = OllamaEmbeddings(model=\"llama3.2\")\n",
    "    \n",
    "    # Create the Chroma vector store\n",
    "    if debug:\n",
    "        print(f\"Creating vector store with {len(documents)} contextually enriched documents...\")\n",
    "        \n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=documents,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=persist_directory\n",
    "    )\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"Vector store created at {persist_directory}\")\n",
    "    \n",
    "    return vectorstore\n",
    "\n",
    "def main(debug=True):\n",
    "    \"\"\"\n",
    "    Main function to test the contextual retrieval pipeline.\n",
    "    \"\"\"\n",
    "    # Sample documents for testing\n",
    "    sample_docs = [Document(page_content=\"The Boston Celtics won the NBA Finals in 2023.\")]\n",
    "    \n",
    "    # Create contextual nodes\n",
    "    contextual_docs = create_contextual_nodes(sample_docs, debug=debug)\n",
    "    \n",
    "    # Create and test the vector store\n",
    "    vectorstore = create_embedding_retriever(contextual_docs, debug=debug)\n",
    "    \n",
    "    # Output a message indicating successful creation of contextual retriever\n",
    "    if debug:\n",
    "        print(f\"Successfully created contextual retriever with {len(contextual_docs)} contextually enriched documents.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../src/sports_news_rag/modules/hyde_rag.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../src/sports_news_rag/modules/hyde_rag.py\n",
    "\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "# from modules.contextual_retrieval import create_contextual_nodes, create_embedding_retriever\n",
    "from langchain.schema import Document\n",
    "\n",
    "def contextual_retrieval(question, retriever, debug=False):\n",
    "    \"\"\"\n",
    "    Performs contextual retrieval based on a given question and contextually enriched documents.\n",
    "    \n",
    "    Parameters:\n",
    "    - question (str): The query or question to retrieve documents for.\n",
    "    - retriever: The retriever object created from the contextual vector store.\n",
    "    - debug (bool): Flag for printing debug information.\n",
    "    \n",
    "    Returns:\n",
    "    - List[Document]: List of retrieved documents based on the contextual retriever.\n",
    "    \"\"\"\n",
    "    # Generate a hypothetical answer to enrich the retrieval process\n",
    "    llm = ChatOllama(model=\"llama3.2\", temperature=0)\n",
    "    hypo_prompt = f\"Generate a detailed answer to the following question:\\n\\n{question}\\n\\nAnswer:\"\n",
    "    hypo_answer = llm.invoke([{\"role\": \"user\", \"content\": hypo_prompt}]).content\n",
    "\n",
    "    if debug:\n",
    "        print(f\"Hypothetical answer generated: {hypo_answer}\")\n",
    "\n",
    "    # Retrieve documents using the contextual retriever\n",
    "    retrieved_docs = retriever.invoke(hypo_answer)\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"Number of documents retrieved based on hypothetical answer: {len(retrieved_docs)}\")\n",
    "        \n",
    "    return retrieved_docs\n",
    "\n",
    "def main(debug=False):\n",
    "    \"\"\"\n",
    "    Main function to test the contextual retrieval.\n",
    "    \"\"\"\n",
    "    question = \"What are the recent updates in the NBA?\"\n",
    "    \n",
    "    # Create a sample document\n",
    "    sample_docs = [Document(page_content=\"The Boston Celtics won the NBA Finals in 2023.\")]\n",
    "    \n",
    "    # Create contextual nodes and retriever\n",
    "    contextual_docs = create_contextual_nodes(sample_docs, debug=debug)\n",
    "    vectorstore = create_embedding_retriever(contextual_docs, debug=debug)\n",
    "    retriever = vectorstore.as_retriever()\n",
    "    \n",
    "    # Test the contextual retrieval\n",
    "    contextual_retrieval(question, retriever, debug)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../src/sports_news_rag/modules/corrective_rag.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../src/sports_news_rag/modules/corrective_rag.py\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "from langchain.schema import Document  # Import the Document class\n",
    "\n",
    "def corrective_rag(question, retrieved_docs, debug=False):\n",
    "    # Convert the list of dicts to Document objects if necessary\n",
    "    if not all(isinstance(doc, Document) for doc in retrieved_docs):\n",
    "        retrieved_docs = [Document(page_content=doc[\"page_content\"]) for doc in retrieved_docs]\n",
    "\n",
    "    llm = ChatOllama(model=\"llama3.2\", temperature=0)\n",
    "    context = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "\n",
    "    initial_prompt = f\"Context: {context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
    "    initial_answer = llm.invoke([{\"role\": \"user\", \"content\": initial_prompt}]).content\n",
    "\n",
    "    if debug:\n",
    "        print(f\"Initial answer generated: {initial_answer}\")\n",
    "\n",
    "    max_iterations = 2\n",
    "    for i in range(max_iterations):\n",
    "        verify_prompt = f\"Context: {context}\\n\\nAnswer: {initial_answer}\\n\\nIs the answer fully supported by the context? Identify any inaccuracies.\"\n",
    "        verification = llm.invoke([{\"role\": \"user\", \"content\": verify_prompt}]).content\n",
    "\n",
    "        if \"no inaccuracies\" in verification.lower():\n",
    "            if debug:\n",
    "                print(f\"No inaccuracies found. Answer is verified on iteration {i + 1}.\")\n",
    "            break\n",
    "        else:\n",
    "            refine_prompt = f\"Context: {context}\\n\\nThe initial answer may have inaccuracies: {verification}\\n\\nQuestion: {question}\\n\\nProvide a corrected answer:\"\n",
    "            initial_answer = llm.invoke([{\"role\": \"user\", \"content\": refine_prompt}]).content\n",
    "\n",
    "    return initial_answer\n",
    "\n",
    "def main(debug=False):\n",
    "    # Sample usage for testing\n",
    "    question = \"Who won the NBA Finals in 2023?\"\n",
    "    # Use a list of Document objects instead of dictionaries for the retrieved documents\n",
    "    retrieved_docs = [Document(page_content=\"The Boston Celtics won the NBA Finals in 2024.\")]\n",
    "    answer = corrective_rag(question, retrieved_docs, debug=debug)\n",
    "    if debug:\n",
    "        print(f\"Final corrected answer: {answer}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../src/sports_news_rag/modules/self_rag.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../src/sports_news_rag/modules/self_rag.py\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "\n",
    "def self_rag(question, initial_answer, debug=False):\n",
    "    \"\"\"Refine an initial answer by performing self-reflection and improvements.\"\"\"\n",
    "    llm = ChatOllama(model=\"llama3.2\", temperature=0)\n",
    "    if debug:\n",
    "        print(f\"Initial answer before self-refinement: {initial_answer}\")\n",
    "    \n",
    "    max_reflections = 2  # Number of self-reflection iterations\n",
    "    for i in range(max_reflections):\n",
    "        # Self-reflection step\n",
    "        reflect_prompt = f\"Answer: {initial_answer}\\n\\nReflect on the answer and identify any areas for improvement.\"\n",
    "        reflection = llm.invoke([{\"role\": \"user\", \"content\": reflect_prompt}]).content\n",
    "\n",
    "        if debug:\n",
    "            print(f\"Reflection result for iteration {i+1}: {reflection}\")\n",
    "\n",
    "        # If no improvements are needed, break out of the loop\n",
    "        if \"no improvements\" in reflection.lower():\n",
    "            if debug:\n",
    "                print(f\"No further improvements suggested after {i+1} iterations.\")\n",
    "            break\n",
    "        else:\n",
    "            # Improve the answer based on the reflection\n",
    "            improve_prompt = f\"Based on the reflection: {reflection}\\n\\nProvide an improved answer to the question: {question}\"\n",
    "            initial_answer = llm.invoke([{\"role\": \"user\", \"content\": improve_prompt}]).content\n",
    "\n",
    "            if debug:\n",
    "                print(f\"Improved answer after iteration {i+1}: {initial_answer}\")\n",
    "\n",
    "    return initial_answer\n",
    "\n",
    "def main(debug=False):\n",
    "    # Sample usage for testing\n",
    "    question = \"Who won the NBA Finals in 2023?\"\n",
    "    initial_answer = \"The winner of the 2023 NBA Finals is unknown to me as my knowledge cutoff is December 2023.\"\n",
    "    refined_answer = self_rag(question, initial_answer, debug=debug)\n",
    "    if debug:\n",
    "        print(f\"Final refined answer: {refined_answer}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../src/sports_news_rag/modules/web_search.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../src/sports_news_rag/modules/web_search.py\n",
    "from langchain_community.retrievers import TavilySearchAPIRetriever\n",
    "\n",
    "tavily_retriever = TavilySearchAPIRetriever(k=3)\n",
    "\n",
    "def tavily_search(question, debug=False):\n",
    "    docs = tavily_retriever.invoke(question)\n",
    "    context = \"\\n\\n\".join(f\"Source {i+1} ({doc.metadata.get('source')}):\\n{doc.page_content}\" for i, doc in enumerate(docs))\n",
    "    if debug:\n",
    "        print(f\"Web search context retrieved: {context[:500]}...\")  # Display first 500 chars\n",
    "    return context\n",
    "\n",
    "def main(debug=False):\n",
    "    question = \"Who was the first pick in the 2024 NBA Draft?\"\n",
    "    context = tavily_search(question, debug)\n",
    "    if debug:\n",
    "        print(f\"Retrieved context from Tavily search: {context}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../src/sports_news_rag/modules/decision_mechanism.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../src/sports_news_rag/modules/decision_mechanism.py\n",
    "from modules.hyde_rag import contextual_retrieval\n",
    "from modules.corrective_rag import corrective_rag\n",
    "from modules.web_search import tavily_search\n",
    "from modules.self_rag import self_rag  # Include the self_rag module for refinement\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "\n",
    "\n",
    "def evaluate_confidence(answer, debug=False):\n",
    "    \"\"\"Evaluate the confidence of an answer using a language model.\"\"\"\n",
    "    llm = ChatOllama(model=\"llama3.2\", temperature=0)\n",
    "    eval_prompt = (\n",
    "        f\"Evaluate the confidence level (on a scale of 1-10) of the following answer being correct, \"\n",
    "        f\"fully supported by reliable sources, and free from contradictions or inaccuracies:\\n\\n{answer}\\n\\n\"\n",
    "        \"Confidence Score:\"\n",
    "    )\n",
    "    confidence_score = llm.invoke([{\"role\": \"user\", \"content\": eval_prompt}]).content\n",
    "    try:\n",
    "        score = int(confidence_score.strip())\n",
    "    except ValueError:\n",
    "        score = 5  # Default to medium confidence if the evaluation fails\n",
    "    if debug:\n",
    "        print(f\"Confidence score evaluated: {score}\")\n",
    "    return score\n",
    "\n",
    "def decide_and_answer(question, retriever, progress_bar=None, progress_status=None, debug=False):\n",
    "    \"\"\"Generate answers using RAG and Tavily, and decide the best answer with self-refinement.\"\"\"\n",
    "    progress_step = 0.25\n",
    "\n",
    "    # Step 1: Use contextual retrieval to get documents and generate an initial RAG-based answer\n",
    "    if progress_status:\n",
    "        progress_status.text(\"Step 1/4: Running HyDE retrieval...\")\n",
    "    retrieved_docs = contextual_retrieval(question, retriever, debug)\n",
    "    if progress_bar:\n",
    "        progress_bar.progress(progress_step)\n",
    "\n",
    "    # Step 2: Generate a corrective RAG-based answer\n",
    "    if progress_status:\n",
    "        progress_status.text(\"Step 2/4: Generating a corrective RAG answer...\")\n",
    "    rag_answer = corrective_rag(question, retrieved_docs, debug)\n",
    "    rag_refined_answer = self_rag(question, rag_answer, debug)  # Refine RAG answer with self-rag\n",
    "    rag_confidence = evaluate_confidence(rag_refined_answer, debug)\n",
    "    progress_step += 0.25\n",
    "    if progress_bar:\n",
    "        progress_bar.progress(progress_step)\n",
    "\n",
    "    # Step 3: Use Tavily search to generate an answer\n",
    "    if progress_status:\n",
    "        progress_status.text(\"Step 3/4: Running Tavily search for additional context...\")\n",
    "    tavily_context = tavily_search(question, debug)\n",
    "    tavily_prompt = f\"Context: {tavily_context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
    "    llm = ChatOllama(model=\"llama3.2\", temperature=0)\n",
    "    tavily_initial_answer = llm.invoke([{\"role\": \"user\", \"content\": tavily_prompt}]).content\n",
    "    tavily_refined_answer = self_rag(question, tavily_initial_answer, debug)  # Refine Tavily answer with self-rag\n",
    "    tavily_confidence = evaluate_confidence(tavily_refined_answer, debug)\n",
    "    progress_step += 0.25\n",
    "    if progress_bar:\n",
    "        progress_bar.progress(progress_step)\n",
    "\n",
    "    # Step 4: Decision mechanism to choose the final answer based on confidence scores\n",
    "    if progress_status:\n",
    "        progress_status.text(\"Step 4/4: Making the final decision...\")\n",
    "    if rag_confidence > tavily_confidence:\n",
    "        final_answer = rag_refined_answer\n",
    "        source = \"RAG-based response\"\n",
    "    elif tavily_confidence > rag_confidence:\n",
    "        final_answer = tavily_refined_answer\n",
    "        source = \"Tavily-based response\"\n",
    "    else:\n",
    "        # Combine answers if confidence scores are similar\n",
    "        combined_prompt = (\n",
    "            f\"Here are two potential answers to the question:\\n\\n\"\n",
    "            f\"Answer 1 (RAG-based):\\n{rag_refined_answer}\\n\\n\"\n",
    "            f\"Answer 2 (Tavily-based):\\n{tavily_refined_answer}\\n\\n\"\n",
    "            f\"Based on these, provide the best possible answer to the question: {question}\"\n",
    "        )\n",
    "        final_answer = llm.invoke([{\"role\": \"user\", \"content\": combined_prompt}]).content\n",
    "        source = \"Combined response\"\n",
    "\n",
    "    if debug:\n",
    "        print(f\"Selected final answer from: {source}\")\n",
    "    return final_answer\n",
    "\n",
    "\n",
    "\n",
    "import streamlit as st\n",
    "\n",
    "def main(debug=False):\n",
    "    \"\"\"Main function to test the decision mechanism.\"\"\"\n",
    "    question = \"What pick of the draft what Bronny James?\"\n",
    "    sample_docs = [{\"page_content\": \"This is a sample document for testing.\"}]\n",
    "    vectorstore = create_vectorstore(sample_docs, debug=debug)\n",
    "    retriever = vectorstore.as_retriever()\n",
    "\n",
    "    # Create Streamlit progress bar and status\n",
    "    progress_bar = st.progress(0)  # Creates a Streamlit progress bar\n",
    "    progress_status = st.empty()  # Placeholder for status messages\n",
    "\n",
    "    # Pass these objects when calling decide_and_answer\n",
    "    final_answer = decide_and_answer(question, retriever, progress_bar, progress_status, debug)\n",
    "    st.write(f\"Final answer selected: {final_answer}\")  # Display the final answer\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(debug=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../src/sports_news_rag/modules/fact_checker.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../src/sports_news_rag/modules/fact_checker.py\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "from modules.hyde_rag import contextual_retrieval  # Import contextual_retrieval from the hyde_rag module\n",
    "from modules.web_search import tavily_search  # Import tavily_search from the web_search module\n",
    "from langchain.schema import Document\n",
    "\n",
    "def final_fact_check(question, answer, retriever, debug=False):\n",
    "    \"\"\"\n",
    "    Perform a final fact-check of the answer based on a combined context from retrieved documents and web search results.\n",
    "\n",
    "    Parameters:\n",
    "    question (str): The question asked by the user.\n",
    "    answer (str): The initial answer generated by the RAG or web search.\n",
    "    retriever: The retriever object created from the vector store.\n",
    "    debug (bool): If True, print debug information.\n",
    "\n",
    "    Returns:\n",
    "    str: The fact-checked and potentially corrected answer.\n",
    "    \"\"\"\n",
    "    # Initialize the LLM for fact-checking\n",
    "    llm = ChatOllama(model=\"llama3.2\", temperature=0)\n",
    "\n",
    "    # Retrieve documents using HyDE\n",
    "    retrieved_docs = contextual_retrieval(question, retriever, debug=debug)\n",
    "    context = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs) if retrieved_docs else \"\"\n",
    "\n",
    "    # Retrieve web context using Tavily search\n",
    "    tavily_context = tavily_search(question, debug=debug)\n",
    "\n",
    "    # Combine both contexts\n",
    "    combined_context = context + \"\\n\\n\" + tavily_context\n",
    "\n",
    "    # Debug output for context combination\n",
    "    if debug:\n",
    "        print(f\"Combined context for fact-checking:\\n{combined_context}\")\n",
    "\n",
    "    # Create the fact-checking prompt\n",
    "    fact_check_prompt = (\n",
    "        f\"Context: {combined_context}\\n\\nAnswer: {answer}\\n\\n\"\n",
    "        f\"Verify the accuracy of the answer based on the context. Provide a corrected answer if necessary.\"\n",
    "    )\n",
    "\n",
    "    # Generate the fact-checked answer using the LLM\n",
    "    final_answer = llm.invoke([{\"role\": \"user\", \"content\": fact_check_prompt}]).content\n",
    "\n",
    "    # Debug output for final answer\n",
    "    if debug:\n",
    "        print(f\"Fact-checked answer: {final_answer}\")\n",
    "\n",
    "    return final_answer\n",
    "\n",
    "def main(debug=False):\n",
    "    \"\"\"\n",
    "    Test the final_fact_check function with sample input.\n",
    "    \"\"\"\n",
    "    # Sample question and answer for testing\n",
    "    question = \"Who won the NBA Finals in 2023?\"\n",
    "    initial_answer = \"The Los Angeles Lakers won the NBA Finals in 2023.\"  # Sample incorrect answer\n",
    "\n",
    "    # Create a sample retriever for testing (assuming documents are already in vector store)\n",
    "    # from modules.vector_store import create_vectorstore  # Import create_vectorstore function\n",
    "    sample_docs = [Document(page_content=\"The Golden State Warriors won the NBA Finals in 2023.\")]\n",
    "    vectorstore = create_vectorstore(sample_docs, debug=debug)\n",
    "    retriever = vectorstore.as_retriever()\n",
    "\n",
    "    # Run the final_fact_check function\n",
    "    corrected_answer = final_fact_check(question, initial_answer, retriever, debug=debug)\n",
    "    if debug:\n",
    "        print(f\"Corrected answer after final fact-check: {corrected_answer}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../src/sports_news_rag/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../src/sports_news_rag/main.py\n",
    "\n",
    "from modules.data_crawling import crawl_and_ingest\n",
    "from modules.vector_store import create_vectorstore\n",
    "from modules.decision_mechanism import decide_and_answer\n",
    "from modules.fact_checker import final_fact_check\n",
    "from modules.hyde_rag import contextual_retrieval  # Use the new contextual retrieval function\n",
    "\n",
    "def main(debug=False):\n",
    "    \"\"\"\n",
    "    Main function to run the entire RAG bot pipeline, integrating all modules.\n",
    "    \"\"\"\n",
    "    # Step 1: Crawl and ingest data from sample sports sites\n",
    "    sports_sites = [\"https://www.nba.com/\", \"https://www.espn.com/\"]\n",
    "    all_documents = []\n",
    "    for site in sports_sites:\n",
    "        documents = crawl_and_ingest(site, debug)\n",
    "        all_documents.extend(documents)\n",
    "\n",
    "    # Step 2: Create contextual nodes and vector store\n",
    "    contextual_docs = create_contextual_nodes(all_documents, debug=debug)\n",
    "    vectorstore = create_embedding_retriever(contextual_docs, debug=debug)\n",
    "    retriever = vectorstore.as_retriever()\n",
    "\n",
    "    # Step 3: Use the contextual retrieval to generate an answer to a sample question\n",
    "    question = \"What pick of the Draft was Bronny James jr\"\n",
    "    initial_answer = contextual_retrieval(question, retriever, debug)\n",
    "\n",
    "    # Step 4: Perform a final fact-check on the selected answer\n",
    "    final_answer = final_fact_check(question, initial_answer, retriever, debug)\n",
    "    print(f\"Final answer for the question '{question}': {final_answer}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../src/sports_news_rag/app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../src/sports_news_rag/app.py\n",
    "import streamlit as st\n",
    "from modules.decision_mechanism import decide_and_answer\n",
    "from modules.vector_store import create_vectorstore\n",
    "from modules.data_crawling import crawl_and_ingest\n",
    "from modules.fact_checker import final_fact_check\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.vectorstores import Chroma  # Import Chroma\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# Load environment variables\n",
    "dotenv_path = os.path.join(os.getcwd(), '../../.env')\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "# Set up the Streamlit app title and description\n",
    "st.set_page_config(page_title=\"Advanced Sports News RAG Bot\", layout=\"wide\")\n",
    "st.title(\"Advanced Sports News RAG Bot\")\n",
    "st.write(\"Get the most up-to-date sports news using advanced RAG techniques. This bot can combine information from various sources and fact-check responses to ensure you get the most reliable information.\")\n",
    "\n",
    "# Adding the introduction tab\n",
    "tabs = st.tabs([\"Introduction\", \"Ask a Question\"])\n",
    "\n",
    "# Introduction tab content\n",
    "with tabs[0]:\n",
    "    st.header(\"Approaches Used in Advanced Versatile RAG Bot\")\n",
    "    st.write(\"\"\"\n",
    "    This project leverages a variety of Retrieval-Augmented Generation (RAG) strategies to create an interactive assistant capable of providing reliable, up-to-date information for any type of website, though it has been initially applied to sports news. Below, we detail the approaches utilized, how they contribute to the quality of answers, and the innovative combination of different RAG methodologies.\n",
    "    \"\"\")\n",
    "\n",
    "    st.subheader(\"Simple RAG\")\n",
    "    st.write(\"\"\"\n",
    "    This is the foundation of our bot. It involves retrieving relevant documents based on a user query and generating answers using a large language model (LLM). This approach ensures that the generated responses are grounded in relevant information, minimizing the hallucination issues typical of generative models.\n",
    "    \"\"\")\n",
    "\n",
    "    st.subheader(\"Branched RAG\")\n",
    "    st.write(\"\"\"\n",
    "    To improve the quality of our retrieval, we implemented a Branched RAG strategy that performs multiple retrieval steps. By refining the search based on intermediate results, we are able to narrow down to the most contextually appropriate documents. This iterative narrowing process results in more specific and higher quality answers for complex queries.\n",
    "    \"\"\")\n",
    "\n",
    "    st.subheader(\"Contextual Retrieval (Replacing HyDE)\")\n",
    "    st.write(\"\"\"\n",
    "    Contextual Retrieval enhances the retrieval process by adding contextual knowledge to each document before performing the search. Instead of generating a hypothetical ideal document as in HyDE, the contextual retrieval process first creates enriched contextual nodes for each document by using a large language model (LLM). These contextual nodes contain additional relevant information, context, or explanations that provide deeper insight into the document's content.\n",
    "\n",
    "    The enriched contextual nodes are then stored in a vector database, making them more informative and aligned with user queries. This means that when a user asks a question, the retrieval process can better understand and match the query with documents that have been contextually enhanced, resulting in higher precision and recall.\n",
    "\n",
    "    This approach is particularly beneficial for scenarios where direct matches may not yield good results due to lack of context or nuanced query intent. By performing contextual retrieval, we ensure that the documents retrieved are not only relevant based on keyword matching but also contextually aligned, providing better support for the generated answers.\n",
    "    \"\"\")\n",
    "\n",
    "    st.subheader(\"Corrective RAG (CRAG)\")\n",
    "    st.write(\"\"\"\n",
    "    Corrective RAG plays an important role in ensuring the quality of our generated responses. By iteratively refining the response and checking it against the retrieved documents, CRAG helps ensure the answer is not only relevant but also factually correct. This approach is critical for maintaining trustworthiness, particularly when accuracy is key.\n",
    "    \"\"\")\n",
    "\n",
    "    st.subheader(\"Self-RAG\")\n",
    "    st.write(\"\"\"\n",
    "    Once an answer is generated, our system uses Self-RAG to critique and improve its own response through self-reflection. This means the language model can re-evaluate the initial answer and refine it to be clearer, more concise, or more accurate. This layer of self-assessment adds robustness to the overall response quality.\n",
    "    \"\"\")\n",
    "\n",
    "    st.subheader(\"Agentic RAG\")\n",
    "    st.write(\"\"\"\n",
    "    For more complex multi-step queries, our implementation uses Agentic RAG, allowing the bot to behave more like an autonomous agent capable of executing several steps to reach an answer. This might involve retrieving documents, performing corrective checks, synthesizing information, and finally fact-checking, all in a cohesive process. The agentic approach gives the bot the ability to intelligently navigate between information sources and make informed decisions about how to assemble an answer.\n",
    "    \"\"\")\n",
    "\n",
    "    st.subheader(\"Tavily Search for Web Information\")\n",
    "    st.write(\"\"\"\n",
    "    To complement our RAG methods, we employ Tavily Search, which allows the bot to dynamically search the web to provide additional context or verify information when the retrieved documents do not contain sufficient detail. This external search capability ensures the assistant can always provide the most recent and comprehensive information available.\n",
    "    \"\"\")\n",
    "\n",
    "    st.header(\"How These Approaches Work Together\")\n",
    "    st.write(\"\"\"\n",
    "    In creating this bot, we found that no single RAG strategy could solve all the complexities inherent in answering dynamic questions from different types of websites. Therefore, combining these RAG approaches has allowed us to create a more reliable and versatile assistant. Here is how they come together:\n",
    "\n",
    "    First, the bot initiates a Simple RAG to gather relevant documents from the specified websites. This can be adapted for any type of domain—whether it's sports, finance, education, or other topics.\n",
    "\n",
    "    Second, it applies Branched RAG and Contextual Retrieval to refine the search and create more robust retrieval pathways, particularly if the initial results are sparse or irrelevant.\n",
    "\n",
    "    Third, once a preliminary answer is generated, Corrective RAG is used to verify the facts, cross-referencing them with additional sources to ensure correctness.\n",
    "\n",
    "    Fourth, the Self-RAG component refines the response further, evaluating its clarity and precision before delivering it to the user.\n",
    "\n",
    "    Fifth, Tavily Search is used when necessary to provide additional up-to-date information, especially when initial documents do not fully address the query.\n",
    "\n",
    "    Finally, in scenarios that require multiple interconnected steps, Agentic RAG allows the bot to navigate complex information spaces, ensuring that all retrieved information is used cohesively.\n",
    "    \"\"\")\n",
    "\n",
    "    st.header(\"The Value of Combined RAG Approaches\")\n",
    "    st.write(\"\"\"\n",
    "    By integrating these methods, we achieve a system capable of:\n",
    "\n",
    "    - High accuracy: Through corrective checks and iterative refining.\n",
    "    - Adaptability: Using contextual document enhancement to bridge knowledge gaps and improve retrieval accuracy.\n",
    "    - Depth in retrieval: Thanks to branched and agentic RAG approaches, which enable deeper contextual understanding.\n",
    "    - Versatility across domains: This system is designed to work with various types of websites, making it applicable to numerous domains beyond just sports.\n",
    "    - Real-time information: With Tavily Search integration, the bot can dynamically pull the latest information from the web.\n",
    "    - Context retention: Enabling the assistant to carry information from prior questions when beneficial, creating more interactive and insightful conversations.\n",
    "\n",
    "    These combined efforts make the Versatile RAG Bot not only capable of answering questions reliably but also explaining its sources, refining its own outputs intelligently, and adapting across various domains with ease.\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "# Ask a Question tab content\n",
    "with tabs[1]:\n",
    "    # Sidebar configuration options\n",
    "    st.sidebar.title(\"Configuration\")\n",
    "    enable_debug = st.sidebar.checkbox(\"Enable Debugging\", value=False)\n",
    "    include_fact_check = st.sidebar.checkbox(\"Include Final Fact-Check\", value=True)\n",
    "\n",
    "    # Allow the user to choose between pre-ingested data or dynamic ingestion\n",
    "    use_pre_ingested_data = st.sidebar.checkbox(\"Use Pre-Ingested Data\", value=True)\n",
    "\n",
    "    # Ensure the session state attributes are initialized\n",
    "    if \"vectorstore\" not in st.session_state:\n",
    "        st.session_state.vectorstore = None\n",
    "    if \"retriever\" not in st.session_state:\n",
    "        st.session_state.retriever = None\n",
    "    if \"all_documents\" not in st.session_state:\n",
    "        st.session_state.all_documents = []\n",
    "\n",
    "\n",
    "    # Pre-Ingested Data Loading Section\n",
    "    if use_pre_ingested_data:\n",
    "        st.sidebar.subheader(\"Pre-Ingested Data Loading\")\n",
    "\n",
    "        # Predefined sites with pre-ingested data available\n",
    "        known_sites = [\"NBA\", \"ESPN\", \"NFL\"]\n",
    "        selected_site = st.sidebar.selectbox(\"Select Pre-Ingested Site:\", known_sites)\n",
    "\n",
    "        # Button to load pre-ingested data\n",
    "        if st.sidebar.button(\"Load Pre-Ingested Data\"):\n",
    "            with st.spinner(f\"Loading pre-ingested data for {selected_site}...\"):\n",
    "                # Define the path to pre-ingested vector store based on site selection\n",
    "                pre_ingested_vectorstore_path = f\"/data/vectorstores/{selected_site.lower()}\"\n",
    "\n",
    "                # Debug: Print the path being accessed\n",
    "                st.sidebar.write(f\"Looking for pre-ingested data at: {pre_ingested_vectorstore_path}\")\n",
    "\n",
    "                # Check if the pre-ingested vector store directory exists\n",
    "                if os.path.exists(pre_ingested_vectorstore_path):\n",
    "                    try:\n",
    "                        # Correctly instantiate the Chroma vector store using the persist_directory\n",
    "                        st.session_state.vectorstore = Chroma(\n",
    "                            persist_directory=pre_ingested_vectorstore_path,\n",
    "                            collection_name=None  # Set this to your collection name if applicable\n",
    "                        )\n",
    "\n",
    "                        # Ensure that the vectorstore is not None before accessing its properties\n",
    "                        if st.session_state.vectorstore is None:\n",
    "                            raise ValueError(\"Chroma vector store initialization returned None.\")\n",
    "\n",
    "                        # Debug: Print loaded collection contents to verify documents\n",
    "                        if hasattr(st.session_state.vectorstore, '_collection'):\n",
    "                            total_documents = len(st.session_state.vectorstore._collection._collection)\n",
    "                            st.sidebar.write(f\"Total documents loaded: {total_documents}\")\n",
    "\n",
    "                        # Set up retriever\n",
    "                        st.session_state.retriever = st.session_state.vectorstore.as_retriever()\n",
    "                        st.sidebar.success(f\"Loaded pre-ingested data for {selected_site}.\")\n",
    "                    except Exception as e:\n",
    "                        # Handle loading errors and provide feedback\n",
    "                        st.sidebar.error(f\"Error loading pre-ingested data for {selected_site}: {str(e)}\")\n",
    "                else:\n",
    "                    # Provide feedback if the directory is not found\n",
    "                    st.sidebar.error(f\"Pre-ingested data for {selected_site} not found at path: {pre_ingested_vectorstore_path}\")\n",
    "\n",
    "    # Step 2: Handle Dynamic Data Ingestion\n",
    "    else:\n",
    "        # Allow the user to select new websites to dynamically ingest data\n",
    "        sports_sites = st.sidebar.multiselect(\n",
    "            \"Select sports websites to crawl for data\",\n",
    "            [\"https://www.nba.com/\", \"https://www.espn.com/\", \"https://www.nfl.com/\"],\n",
    "            default=[\"https://www.nba.com/\", \"https://www.espn.com/\"]\n",
    "        )\n",
    "\n",
    "        # Step 2.1: Crawl and ingest data from selected sites\n",
    "        st.sidebar.subheader(\"Data Ingestion for New Websites\")\n",
    "        if st.sidebar.button(\"Ingest Data\"):\n",
    "            with st.spinner(\"Crawling and ingesting data...\"):\n",
    "                # Reset existing data if new ingestion is requested\n",
    "                st.session_state.all_documents = None\n",
    "                st.session_state.vectorstore = None\n",
    "                st.session_state.retriever = None\n",
    "\n",
    "                # Crawl and ingest data from selected sites\n",
    "                all_documents = []\n",
    "                for site in sports_sites:\n",
    "                    documents = crawl_and_ingest(site, debug=enable_debug)\n",
    "                    all_documents.extend(documents)\n",
    "\n",
    "                # Store new documents in session state\n",
    "                st.session_state.all_documents = all_documents\n",
    "                st.sidebar.success(f\"Data ingested from {len(sports_sites)} sites. Total documents: {len(st.session_state.all_documents)}\")\n",
    "\n",
    "        # Step 2.2: Create vector store from dynamically ingested documents\n",
    "        st.sidebar.subheader(\"Create Vector Store from Dynamic Data\")\n",
    "        if st.sidebar.button(\"Create Vector Store\"):\n",
    "            with st.spinner(\"Creating vector store from dynamically ingested data...\"):\n",
    "                # Create a vector store if there are any ingested documents\n",
    "                if st.session_state.all_documents:\n",
    "                    st.session_state.vectorstore = create_vectorstore(st.session_state.all_documents, debug=enable_debug)\n",
    "                    st.session_state.retriever = st.session_state.vectorstore.as_retriever()\n",
    "                    st.sidebar.success(\"Vector store created successfully and retriever is set up.\")\n",
    "                else:\n",
    "                    st.sidebar.error(\"No documents available for vector store creation. Please ingest data first.\")\n",
    "\n",
    "    # Step 4: Input field for user question\n",
    "    st.subheader(\"Ask a Sports-Related Question\")\n",
    "    user_question = st.text_input(\"Enter your question about sports news or events:\")\n",
    "\n",
    "    # Step 5: Generate the answer using the decision mechanism\n",
    "    if st.button(\"Get Answer\"):\n",
    "        # Check if 'retriever' is properly initialized\n",
    "        if \"retriever\" not in st.session_state or st.session_state.retriever is None:\n",
    "            st.error(\"Vector store and retriever not set up. Please load or create the vector store first.\")\n",
    "        elif user_question:\n",
    "            # Initialize progress bar\n",
    "            progress_bar = st.progress(0)\n",
    "            progress_status = st.empty()\n",
    "\n",
    "            with st.spinner(\"Generating answer...\"):\n",
    "                # Step 5.1: Use decision mechanism to get an answer\n",
    "                progress_status.text(\"Step 1/4: Running Contextual retrieval...\")\n",
    "                answer = decide_and_answer(user_question, st.session_state.retriever, progress_bar, progress_status, enable_debug)\n",
    "\n",
    "                # Step 5.2: Optional final fact-check if enabled\n",
    "                if include_fact_check:\n",
    "                    progress_status.text(\"Step 4/4: Performing final fact-check...\")\n",
    "                    answer = final_fact_check(user_question, answer, st.session_state.retriever, debug=enable_debug)\n",
    "                    progress_bar.progress(100)\n",
    "\n",
    "                # Display the final answer\n",
    "                st.subheader(\"Answer\")\n",
    "                st.write(answer)\n",
    "        else:\n",
    "            st.error(\"Please enter a question to get an answer.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science_ollama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
