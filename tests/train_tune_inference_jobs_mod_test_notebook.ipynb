{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspaces/spl_freethrow_biomechanics_analysis_ml_prediction/notebooks/freethrow_predictions\n"
     ]
    }
   ],
   "source": [
    "%cd ../notebooks/freethrow_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ml/mlflow/mlflow_logger.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ml/mlflow/mlflow_logger.py\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.data import from_pandas\n",
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "\n",
    "class MLflowLogger:\n",
    "    def __init__(self, tracking_uri=None, experiment_name=\"Default Experiment\", enable_mlflow=True):\n",
    "        self.enable_mlflow = enable_mlflow\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        if not self.logger.handlers:\n",
    "            handler = logging.StreamHandler()\n",
    "            formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "            handler.setFormatter(formatter)\n",
    "            self.logger.addHandler(handler)\n",
    "            self.logger.setLevel(logging.INFO)\n",
    "\n",
    "        if self.enable_mlflow:\n",
    "            if tracking_uri:\n",
    "                mlflow.set_tracking_uri(tracking_uri)\n",
    "                self.logger.info(f\"MLflow Tracking URI set to: {tracking_uri}\")\n",
    "            else:\n",
    "                self.logger.warning(\"MLflow Tracking URI not provided. Using default.\")\n",
    "            self.experiment_name = experiment_name\n",
    "            mlflow.set_experiment(self.experiment_name)\n",
    "            self.logger.info(f\"MLflow Experiment set to: {self.experiment_name}\")\n",
    "\n",
    "    def run_context(self, run_name, nested=False, tags=None):\n",
    "        \"\"\"\n",
    "        Returns a context manager wrapping mlflow.start_run(...).\n",
    "        \"\"\"\n",
    "        if self.enable_mlflow:\n",
    "            return mlflow.start_run(run_name=run_name, nested=nested, tags=tags)\n",
    "        else:\n",
    "            # Dummy context manager when MLflow is disabled.\n",
    "            from contextlib import nullcontext\n",
    "            return nullcontext()\n",
    "\n",
    "    def log_run(self, run_name, params, metrics, artifacts=None, tags=None, datasets=None, nested=False):\n",
    "        \"\"\"\n",
    "        Log a single run with MLflow or basic logging.\n",
    "\n",
    "        Args:\n",
    "            run_name (str): Name of the run.\n",
    "            params (dict): Parameters to log.\n",
    "            metrics (dict): Metrics to log.\n",
    "            artifacts (str or list): Paths to artifacts (files or directories).\n",
    "            tags (dict): Tags to set for the run.\n",
    "            datasets (list): List of datasets to log.\n",
    "            nested (bool): Whether this run is nested under an active run.\n",
    "        \"\"\"\n",
    "        if self.enable_mlflow:\n",
    "            try:\n",
    "                with mlflow.start_run(run_name=run_name, nested=nested):\n",
    "                    self.logger.info(f\"Started MLflow run: {run_name}\")\n",
    "                    \n",
    "                    # Log parameters\n",
    "                    if params:\n",
    "                        mlflow.log_params(params)\n",
    "                        self.logger.debug(f\"Logged parameters: {params}\")\n",
    "                    \n",
    "                    # Log metrics\n",
    "                    if metrics:\n",
    "                        for key, value in metrics.items():\n",
    "                            if isinstance(value, (list, tuple)):  # Handle multiple values (e.g., per epoch)\n",
    "                                for step, metric_value in enumerate(value):\n",
    "                                    mlflow.log_metric(key, metric_value, step=step)\n",
    "                                    self.logger.debug(f\"Logged metric '{key}' at step {step}: {metric_value}\")\n",
    "                            else:\n",
    "                                mlflow.log_metric(key, value)\n",
    "                                self.logger.debug(f\"Logged metric '{key}': {value}\")\n",
    "                    \n",
    "                    # Log tags\n",
    "                    if tags:\n",
    "                        mlflow.set_tags(tags)\n",
    "                        self.logger.debug(f\"Set tags: {tags}\")\n",
    "                    \n",
    "                    # Log artifacts\n",
    "                    if artifacts:\n",
    "                        if isinstance(artifacts, str):  # Single file or directory\n",
    "                            mlflow.log_artifact(artifacts)\n",
    "                            self.logger.debug(f\"Logged artifact: {artifacts}\")\n",
    "                        elif isinstance(artifacts, list):  # Multiple paths\n",
    "                            for artifact_path in artifacts:\n",
    "                                mlflow.log_artifact(artifact_path)\n",
    "                                self.logger.debug(f\"Logged artifact: {artifact_path}\")\n",
    "                    \n",
    "                    # Log datasets\n",
    "                    if datasets:\n",
    "                        for dataset in datasets:\n",
    "                            self.log_datasets(dataset[\"dataframe\"], dataset[\"source\"], dataset[\"name\"])\n",
    "                    \n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Failed to log MLflow run '{run_name}': {e}\")\n",
    "                raise\n",
    "        else:\n",
    "            # Basic Logging\n",
    "            self.logger.info(f\"Run Name: {run_name}\")\n",
    "            if params:\n",
    "                self.logger.info(f\"Parameters: {params}\")\n",
    "            if metrics:\n",
    "                self.logger.info(f\"Metrics: {metrics}\")\n",
    "            if tags:\n",
    "                self.logger.info(f\"Tags: {tags}\")\n",
    "            if artifacts:\n",
    "                self.logger.info(f\"Artifacts: {artifacts}\")\n",
    "            if datasets:\n",
    "                for dataset in datasets:\n",
    "                    self.logger.info(f\"Dataset Logged: {dataset['name']} from {dataset['source']}\")\n",
    "\n",
    "    def log_model(self, model, model_name, conda_env=None):\n",
    "        \"\"\"\n",
    "        Log a model to MLflow or perform basic logging.\n",
    "\n",
    "        Args:\n",
    "            model: Trained model object.\n",
    "            model_name (str): Name of the model.\n",
    "            conda_env (str): Path to a Conda environment file.\n",
    "        \"\"\"\n",
    "        if self.enable_mlflow:\n",
    "            try:\n",
    "                mlflow.sklearn.log_model(model, artifact_path=model_name, conda_env=conda_env)\n",
    "                self.logger.info(f\"Model '{model_name}' logged to MLflow.\")\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Failed to log model '{model_name}' to MLflow: {e}\")\n",
    "                raise\n",
    "        else:\n",
    "            # Basic Logging\n",
    "            self.logger.info(f\"Model '{model_name}' training complete. (MLflow logging disabled)\")\n",
    "\n",
    "    def log_datasets(self, dataframe, source, name):\n",
    "        \"\"\"\n",
    "        Create a dataset log entry or perform basic logging.\n",
    "\n",
    "        Args:\n",
    "            dataframe (pd.DataFrame): The dataset.\n",
    "            source (str): Data source (e.g., file path, S3 URI).\n",
    "            name (str): Dataset name.\n",
    "\n",
    "        Returns:\n",
    "            Dataset object logged to MLflow or None.\n",
    "        \"\"\"\n",
    "        if self.enable_mlflow:\n",
    "            try:\n",
    "                dataset = from_pandas(dataframe, source=source, name=name)\n",
    "                mlflow.log_input(dataset)\n",
    "                self.logger.info(f\"Dataset '{name}' logged to MLflow from source '{source}'.\")\n",
    "                return dataset\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Failed to log dataset '{name}' to MLflow: {e}\")\n",
    "                raise\n",
    "        else:\n",
    "            # Basic Logging\n",
    "            self.logger.info(f\"Dataset '{name}' from source '{source}' logged. (MLflow logging disabled)\")\n",
    "            return None\n",
    "\n",
    "    def get_active_run(self):\n",
    "        \"\"\"\n",
    "        Retrieve the current active MLflow run or log a message.\n",
    "\n",
    "        Returns:\n",
    "            Active run info object or None.\n",
    "        \"\"\"\n",
    "        if self.enable_mlflow:\n",
    "            return mlflow.active_run()\n",
    "        else:\n",
    "            self.logger.info(\"No active MLflow run. MLflow logging is disabled.\")\n",
    "            return None\n",
    "\n",
    "    def get_last_run(self):\n",
    "        \"\"\"\n",
    "        Retrieve the last active MLflow run or log a message.\n",
    "\n",
    "        Returns:\n",
    "            Last run info object or None.\n",
    "        \"\"\"\n",
    "        if self.enable_mlflow:\n",
    "            return mlflow.last_active_run()\n",
    "        else:\n",
    "            self.logger.info(\"No last MLflow run available. MLflow logging is disabled.\")\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ml/train_utils/model_savers.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ml/train_utils/model_savers.py\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "import mlflow.sklearn\n",
    "\n",
    "class LocalModelSaver:\n",
    "    def __init__(self, save_dir: Path):\n",
    "        self.save_dir = save_dir\n",
    "        self.save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def save(self, model, model_name: str):\n",
    "        model_path = self.save_dir / f\"{model_name}_model.pkl\"\n",
    "        joblib.dump(model, model_path)\n",
    "        return model_path\n",
    "\n",
    "    def load(self, model_name: str):\n",
    "        model_path = self.save_dir / f\"{model_name}_model.pkl\"\n",
    "        return joblib.load(model_path)\n",
    "\n",
    "class MLflowModelSaver:\n",
    "    def __init__(self, mlflow_logger, artifact_path: str = \"model\"):\n",
    "        self.mlflow_logger = mlflow_logger\n",
    "        self.artifact_path = artifact_path\n",
    "\n",
    "    def save(self, model, model_name: str, conda_env: str = None):\n",
    "        mlflow.sklearn.log_model(model, artifact_path=model_name, conda_env=conda_env)\n",
    "        # Optionally return the model URI if needed\n",
    "        model_uri = f\"models:/{model_name}/1\"\n",
    "        return model_uri\n",
    "\n",
    "    def load(self, model_uri: str):\n",
    "        return mlflow.sklearn.load_model(model_uri)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ml/jobs/tuning_job.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ml/jobs/tuning_job.py\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from ml.config.config_loader import load_config\n",
    "from ml.config.config_models import AppConfig\n",
    "from ml.train_utils.train_utils import bayes_best_model_train\n",
    "from ml.mlflow.mlflow_logger import MLflowLogger\n",
    "\n",
    "# Import the feature metadata loader\n",
    "from ml.feature_selection.feature_importance_calculator import manage_features\n",
    "\n",
    "class TuningJob:\n",
    "    def __init__(self, config_path: Path, mlflow_logger: MLflowLogger):\n",
    "        self.config_path = config_path\n",
    "        self.config: AppConfig = load_config(config_path)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.mlflow_logger = mlflow_logger\n",
    "\n",
    "    def run(self):\n",
    "        # Extract configurations (paths, models, etc.)\n",
    "        paths_config = self.config.paths\n",
    "        model_save_dir = Path(paths_config.model_save_base_dir).resolve()\n",
    "        classification_report_path = model_save_dir / \"classification_report.txt\"\n",
    "        tuning_results_save_path = model_save_dir / \"tuning_results.json\"\n",
    "\n",
    "        # Load your training data â€“ adjust as needed\n",
    "        raw_data_file = Path(paths_config.data_dir).resolve() / paths_config.raw_data\n",
    "        try:\n",
    "            df = pd.read_csv(raw_data_file)\n",
    "            self.logger.info(f\"Loaded data from {raw_data_file}.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to load dataset: {e}\")\n",
    "            return\n",
    "\n",
    "        # Load feature metadata using manage_features (same as in train.py)\n",
    "        feature_paths = {\n",
    "            'features': '../../data/preprocessor/features_info/final_ml_df_selected_features_columns.pkl',\n",
    "            'ordinal_categoricals': '../../data/preprocessor/features_info/ordinal_categoricals.pkl',\n",
    "            'nominal_categoricals': '../../data/preprocessor/features_info/nominal_categoricals.pkl',\n",
    "            'numericals': '../../data/preprocessor/features_info/numericals.pkl',\n",
    "            'y_variable': '../../data/preprocessor/features_info/y_variable.pkl'\n",
    "        }\n",
    "        loaded = manage_features(mode='load', paths=feature_paths)\n",
    "        if loaded:\n",
    "            y_var = loaded.get('y_variable')\n",
    "        else:\n",
    "            self.logger.error(\"Failed to load feature metadata.\")\n",
    "            return\n",
    "\n",
    "        # Now initialize the DataPreprocessor with actual feature metadata (instead of blank lists)\n",
    "        from datapreprocessor import DataPreprocessor\n",
    "        preprocessor = DataPreprocessor(\n",
    "            model_type=\"Tree Based Classifier\",\n",
    "            y_variable=y_var,\n",
    "            ordinal_categoricals=loaded.get('ordinal_categoricals'),\n",
    "            nominal_categoricals=loaded.get('nominal_categoricals'),\n",
    "            numericals=loaded.get('numericals'),\n",
    "            mode='train',\n",
    "            debug=self.config.logging.debug,\n",
    "            normalize_debug=False,\n",
    "            normalize_graphs_output=False,\n",
    "            graphs_output_dir=Path(paths_config.plots_output_dir),\n",
    "            transformers_dir=Path(paths_config.transformers_save_base_dir)\n",
    "        )\n",
    "        try:\n",
    "            X_train, X_test, y_train, y_test, *_ = preprocessor.final_preprocessing(df)\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error during preprocessing: {e}\")\n",
    "            return\n",
    "\n",
    "        # Wrap the tuning job in an MLflow run context\n",
    "        with self.mlflow_logger.run_context(\"Tuning Job\"):\n",
    "            try:\n",
    "                bayes_best_model_train(\n",
    "                    X_train=X_train,\n",
    "                    y_train=y_train,\n",
    "                    X_test=X_test,\n",
    "                    y_test=y_test,\n",
    "                    selection_metric=self.config.models.selection_metric,\n",
    "                    model_save_dir=model_save_dir,\n",
    "                    classification_save_path=classification_report_path,\n",
    "                    tuning_results_save=tuning_results_save_path,\n",
    "                    selected_models=self.config.models.selected_models,\n",
    "                    use_pca=True\n",
    "                )\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Tuning failed: {e}\")\n",
    "                raise\n",
    "\n",
    "        self.logger.info(\"Tuning job completed successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    from pathlib import Path\n",
    "    import logging\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    # Change the tracking URI from \"databricks\" to the local folder.\n",
    "    abs_tracking_uri = \"file:///\" + str(Path(\"../../data/model/mlruns\").resolve())\n",
    "    mlflow_logger = MLflowLogger(tracking_uri=abs_tracking_uri, experiment_name=\"SPL Feedback Experiment\")\n",
    "    print(\"Using absolute MLflow Tracking URI:\", abs_tracking_uri)\n",
    "\n",
    "    config_path = Path('../../data/model/preprocessor_config/preprocessor_config.yaml')\n",
    "    job = TuningJob(config_path, mlflow_logger)\n",
    "    job.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ml/jobs/training_job.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ml/jobs/training_job.py\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from ml.config.config_loader import load_config\n",
    "from ml.config.config_models import AppConfig\n",
    "from ml.train_utils.train_utils import save_model, load_model, bayes_best_model_train\n",
    "from ml.mlflow.mlflow_logger import MLflowLogger\n",
    "\n",
    "# Import feature metadata loader\n",
    "from ml.feature_selection.feature_importance_calculator import manage_features\n",
    "from datapreprocessor import DataPreprocessor\n",
    "\n",
    "class TrainingJob:\n",
    "    def __init__(self, config_path: Path, mlflow_logger: MLflowLogger):\n",
    "        self.config_path = config_path\n",
    "        self.config: AppConfig = load_config(config_path)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.mlflow_logger = mlflow_logger\n",
    "\n",
    "    def run(self):\n",
    "        # Extract configurations (paths, models, etc.)\n",
    "        paths_config = self.config.paths\n",
    "        model_save_dir = Path(paths_config.model_save_base_dir).resolve()\n",
    "        classification_report_path = model_save_dir / \"classification_report.txt\"\n",
    "        tuning_results_save_path = model_save_dir / \"tuning_results.json\"\n",
    "\n",
    "        # Load the complete training data\n",
    "        raw_data_file = Path(paths_config.data_dir).resolve() / paths_config.raw_data\n",
    "        try:\n",
    "            df = pd.read_csv(raw_data_file)\n",
    "            self.logger.info(f\"Loaded data from {raw_data_file}. Shape: {df.shape}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to load dataset: {e}\")\n",
    "            return\n",
    "\n",
    "        # Load feature metadata using manage_features (same as in train.py)\n",
    "        feature_paths = {\n",
    "            'features': '../../data/preprocessor/features_info/final_ml_df_selected_features_columns.pkl',\n",
    "            'ordinal_categoricals': '../../data/preprocessor/features_info/ordinal_categoricals.pkl',\n",
    "            'nominal_categoricals': '../../data/preprocessor/features_info/nominal_categoricals.pkl',\n",
    "            'numericals': '../../data/preprocessor/features_info/numericals.pkl',\n",
    "            'y_variable': '../../data/preprocessor/features_info/y_variable.pkl'\n",
    "        }\n",
    "        loaded_features = manage_features(mode='load', paths=feature_paths)\n",
    "        if loaded_features:\n",
    "            y_var = loaded_features.get('y_variable')\n",
    "        else:\n",
    "            self.logger.error(\"Failed to load feature metadata.\")\n",
    "            return\n",
    "\n",
    "        # Initialize the DataPreprocessor with feature metadata (for training mode)\n",
    "        preprocessor = DataPreprocessor(\n",
    "            model_type=\"Tree Based Classifier\",\n",
    "            y_variable=y_var,\n",
    "            ordinal_categoricals=loaded_features.get('ordinal_categoricals'),\n",
    "            nominal_categoricals=loaded_features.get('nominal_categoricals'),\n",
    "            numericals=loaded_features.get('numericals'),\n",
    "            mode='train',\n",
    "            debug=self.config.logging.debug,\n",
    "            normalize_debug=False,\n",
    "            normalize_graphs_output=False,\n",
    "            graphs_output_dir=Path(paths_config.plots_output_dir),\n",
    "            transformers_dir=Path(paths_config.transformers_save_base_dir)\n",
    "        )\n",
    "        try:\n",
    "            # Process the data to obtain training and test splits\n",
    "            X_train, X_test, y_train, y_test, *_ = preprocessor.final_preprocessing(df)\n",
    "            self.logger.info(f\"Preprocessing complete. X_train shape: {X_train.shape}, X_test shape: {X_test.shape}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error during preprocessing: {e}\")\n",
    "            return\n",
    "\n",
    "        # Load tuning results to get best model info\n",
    "        try:\n",
    "            with tuning_results_save_path.open(\"r\") as f:\n",
    "                tuning_results = json.load(f)\n",
    "            best_model_info = tuning_results.get(\"Best Model\")\n",
    "            if not best_model_info:\n",
    "                self.logger.error(\"No best model found in tuning results.\")\n",
    "                return\n",
    "            best_model_name = best_model_info.get(\"model_name\")\n",
    "            self.logger.info(f\"Best model selected from tuning: {best_model_name}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error loading tuning results: {e}\")\n",
    "            return\n",
    "\n",
    "        # Load the best model from disk\n",
    "        try:\n",
    "            model = load_model(best_model_name, model_save_dir)\n",
    "            self.logger.info(f\"Loaded best model '{best_model_name}' from disk.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error loading model '{best_model_name}': {e}\")\n",
    "            return\n",
    "\n",
    "        # Retrain the best model on the full training data\n",
    "        try:\n",
    "            model.fit(X_train, y_train)\n",
    "            self.logger.info(f\"Model '{best_model_name}' retrained on full training data.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error during retraining of model '{best_model_name}': {e}\")\n",
    "            return\n",
    "\n",
    "        # Save the retrained model\n",
    "        try:\n",
    "            save_model(model, best_model_name, save_dir=model_save_dir)\n",
    "            self.logger.info(f\"Retrained model '{best_model_name}' saved successfully.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to save retrained model '{best_model_name}': {e}\")\n",
    "            return\n",
    "\n",
    "        # Optionally, log the retrained model to MLflow\n",
    "        with self.mlflow_logger.run_context(\"Training Job\"):\n",
    "            try:\n",
    "                self.mlflow_logger.log_model(model, best_model_name)\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Failed to log retrained model '{best_model_name}' to MLflow: {e}\")\n",
    "                return\n",
    "\n",
    "        self.logger.info(\"Training job completed successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    from pathlib import Path\n",
    "    import logging\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    # Change the tracking URI from \"databricks\" to the local folder.\n",
    "    abs_tracking_uri = \"file:///\" + str(Path(\"../../data/model/mlruns\").resolve())\n",
    "    mlflow_logger = MLflowLogger(tracking_uri=abs_tracking_uri, experiment_name=\"SPL Feedback Experiment\")\n",
    "    print(\"Using absolute MLflow Tracking URI:\", abs_tracking_uri)\n",
    "\n",
    "    config_path = Path('../../data/model/preprocessor_config/preprocessor_config.yaml')\n",
    "    job = TrainingJob(config_path, mlflow_logger)\n",
    "    job.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ml/jobs/inference_job.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ml/jobs/inference_job.py\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from ml.config.config_loader import load_config\n",
    "from ml.config.config_models import AppConfig\n",
    "from ml.mlflow.mlflow_logger import MLflowLogger\n",
    "from ml.train_utils.train_utils import load_model\n",
    "\n",
    "# Import feature metadata loader\n",
    "from ml.feature_selection.feature_importance_calculator import manage_features\n",
    "from datapreprocessor import DataPreprocessor\n",
    "from ml.predict.predict import predict_and_attach_predict_probs\n",
    "\n",
    "class InferenceJob:\n",
    "    def __init__(self, config_path: Path, mlflow_logger: MLflowLogger):\n",
    "        self.config_path = config_path\n",
    "        self.config: AppConfig = load_config(config_path)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.mlflow_logger = mlflow_logger\n",
    "\n",
    "    def run(self):\n",
    "        paths_config = self.config.paths\n",
    "        model_save_dir = Path(paths_config.model_save_base_dir).resolve()\n",
    "        tuning_results_path = model_save_dir / \"tuning_results.json\"\n",
    "        try:\n",
    "            with tuning_results_path.open(\"r\") as f:\n",
    "                tuning_results = json.load(f)\n",
    "            best_model_info = tuning_results.get(\"Best Model\")\n",
    "            if not best_model_info:\n",
    "                self.logger.error(\"No best model info found in tuning results.\")\n",
    "                return\n",
    "            best_model_name = best_model_info.get(\"model_name\")\n",
    "            self.logger.info(f\"Best model for inference: {best_model_name}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error loading tuning results: {e}\")\n",
    "            return\n",
    "\n",
    "        # Load the prediction dataset\n",
    "        raw_data_file = Path(paths_config.data_dir).resolve() / paths_config.raw_data\n",
    "        try:\n",
    "            df = pd.read_csv(raw_data_file)\n",
    "            self.logger.info(f\"Loaded prediction data from {raw_data_file}.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error loading prediction data: {e}\")\n",
    "            return\n",
    "\n",
    "        # Load feature metadata using manage_features for preprocessing in prediction mode\n",
    "        feature_paths = {\n",
    "            'features': '../../data/preprocessor/features_info/final_ml_df_selected_features_columns.pkl',\n",
    "            'ordinal_categoricals': '../../data/preprocessor/features_info/ordinal_categoricals.pkl',\n",
    "            'nominal_categoricals': '../../data/preprocessor/features_info/nominal_categoricals.pkl',\n",
    "            'numericals': '../../data/preprocessor/features_info/numericals.pkl',\n",
    "            'y_variable': '../../data/preprocessor/features_info/y_variable.pkl'\n",
    "        }\n",
    "        loaded_features = manage_features(mode='load', paths=feature_paths)\n",
    "        if not loaded_features:\n",
    "            self.logger.error(\"Failed to load feature metadata.\")\n",
    "            return\n",
    "\n",
    "        # Initialize DataPreprocessor for prediction (using the loaded feature metadata)\n",
    "        preprocessor = DataPreprocessor(\n",
    "            model_type=\"Tree Based Classifier\",\n",
    "            y_variable=loaded_features.get('y_variable'),  # may be optional in prediction mode\n",
    "            ordinal_categoricals=loaded_features.get('ordinal_categoricals'),\n",
    "            nominal_categoricals=loaded_features.get('nominal_categoricals'),\n",
    "            numericals=loaded_features.get('numericals'),\n",
    "            mode='predict',\n",
    "            options={},  # any extra options for prediction\n",
    "            debug=self.config.logging.debug,\n",
    "            normalize_debug=False,\n",
    "            normalize_graphs_output=False,\n",
    "            graphs_output_dir=Path(paths_config.plots_output_dir),\n",
    "            transformers_dir=Path(paths_config.transformers_save_base_dir)\n",
    "        )\n",
    "        try:\n",
    "            X_preprocessed, recommendations, X_inversed = preprocessor.final_preprocessing(df)\n",
    "            self.logger.info(f\"Preprocessing complete for inference. Processed data shape: {X_preprocessed.shape}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error during preprocessing: {e}\")\n",
    "            return\n",
    "\n",
    "        # Load the best model from disk\n",
    "        try:\n",
    "            trained_model = load_model(best_model_name, model_save_dir)\n",
    "            self.logger.info(f\"Loaded model '{best_model_name}' successfully.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error loading model '{best_model_name}': {e}\")\n",
    "            return\n",
    "\n",
    "        # Make predictions and optionally log them via MLflow\n",
    "        with self.mlflow_logger.run_context(\"Inference Job\"):\n",
    "            try:\n",
    "                predictions, prediction_probs, X_inversed = predict_and_attach_predict_probs(\n",
    "                    trained_model, X_preprocessed, X_inversed\n",
    "                )\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error during prediction: {e}\")\n",
    "                return\n",
    "\n",
    "            # Save predictions to a CSV file\n",
    "            predictions_output_dir = Path(paths_config.predictions_output_dir).resolve()\n",
    "            predictions_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "            predictions_filename = predictions_output_dir / f'predictions_{best_model_name.replace(\" \", \"_\")}.csv'\n",
    "            try:\n",
    "                X_inversed.to_csv(predictions_filename, index=False)\n",
    "                self.logger.info(f\"Predictions saved to {predictions_filename}\")\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error saving predictions: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    from pathlib import Path\n",
    "    import logging\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    # Change the tracking URI from \"databricks\" to the local folder.\n",
    "    abs_tracking_uri = \"file:///\" + str(Path(\"../../data/model/mlruns\").resolve())\n",
    "    mlflow_logger = MLflowLogger(tracking_uri=abs_tracking_uri, experiment_name=\"SPL Feedback Experiment\")\n",
    "    print(\"Using absolute MLflow Tracking URI:\", abs_tracking_uri)\n",
    "\n",
    "    config_path = Path('../../data/model/preprocessor_config/preprocessor_config.yaml')\n",
    "    job = InferenceJob(config_path, mlflow_logger)\n",
    "    job.run()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science_ft_bio_predictions",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
