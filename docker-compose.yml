services:
  mlops:
    build:
      context: .
      dockerfile: .devcontainer/Dockerfile
    runtime: nvidia
    volumes:
      - .:/workspace
    ports:
      - "8888:8888"    # Jupyter Notebook, if needed
      - "6006:6006"    # TensorBoard, if used
      - "8501:8501"    # Streamlit app port (for serving ML predictions)
      - "5000:5000"    # MLflow Tracking UI (if you run mlflow server)
    environment:
      - ENV_NAME=data_science_ft_bio_predictions
      - PYTHON_VER=3.10
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    # Command options:
    # Option 1 (run Streamlit app): Use this command if you have a Streamlit interface (for example, app.py in your project)
    # command: ["conda", "run", "-n", "data_science_ft_bio_predictions"]
    command: ["/workspace/entrypoint.sh"]

    # Option 2 (run MLflow UI server): Uncomment to start MLflow UI
    # command: ["conda", "run", "-n", "data_science_ft_bio_predictions", "mlflow", "ui", "--host", "0.0.0.0", "--port", "5000"]
